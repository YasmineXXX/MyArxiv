{"2024-01-02T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2401.01335v1","updated":"2024-01-02T18:53:13Z","published":"2024-01-02T18:53:13Z","title":"Self-Play Fine-Tuning Converts Weak Language Models to Strong Language\n  Models","summary":"  Harnessing the power of human-annotated data through Supervised Fine-Tuning\n(SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we\ndelve into the prospect of growing a strong LLM out of a weak one without the\nneed for acquiring additional human-annotated data. We propose a new\nfine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a\nsupervised fine-tuned model. At the heart of SPIN lies a self-play mechanism,\nwhere the LLM refines its capability by playing against instances of itself.\nMore specifically, the LLM generates its own training data from its previous\niterations, refining its policy by discerning these self-generated responses\nfrom those obtained from human-annotated data. Our method progressively\nelevates the LLM from a nascent model to a formidable one, unlocking the full\npotential of human-annotated demonstration data for SFT. Theoretically, we\nprove that the global optimum to the training objective function of our method\nis achieved only when the LLM policy aligns with the target data distribution.\nEmpirically, we evaluate our method on several benchmark datasets including the\nHuggingFace Open LLM Leaderboard, MT-Bench, and datasets from Big-Bench. Our\nresults show that SPIN can significantly improve the LLM's performance across a\nvariety of benchmarks and even outperform models trained through direct\npreference optimization (DPO) supplemented with extra GPT-4 preference data.\nThis sheds light on the promise of self-play, enabling the achievement of\nhuman-level performance in LLMs without the need for expert opponents.\n","authors":["Zixiang Chen","Yihe Deng","Huizhuo Yuan","Kaixuan Ji","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2401.01335v1.pdf","comment":"28 pages, 6 figures, 6 tables"},{"id":"http://arxiv.org/abs/2401.01330v1","updated":"2024-01-02T18:40:03Z","published":"2024-01-02T18:40:03Z","title":"TREC iKAT 2023: The Interactive Knowledge Assistance Track Overview","summary":"  Conversational Information Seeking stands as a pivotal research area with\nsignificant contributions from previous works. The TREC Interactive Knowledge\nAssistance Track (iKAT) builds on the foundational work of the TREC\nConversational Assistance Track (CAsT). However, iKAT distinctively emphasizes\nthe creation and research of conversational search agents that adapt responses\nbased on user's prior interactions and present context. The challenge lies in\nenabling Conversational Search Agents (CSA) to incorporate this personalized\ncontext to efficiency and effectively guide users through the relevant\ninformation to them. iKAT also emphasizes decisional search tasks, where users\nsift through data and information to weigh up options in order to reach a\nconclusion or perform an action. These tasks, prevalent in everyday\ninformation-seeking decisions -- be it related to travel, health, or shopping\n-- often revolve around a subset of high-level information operators where\nqueries or questions about the information space include: finding options,\ncomparing options, identifying the pros and cons of options, etc. Given the\ndifferent personas and their information need (expressed through the sequence\nof questions), diverse conversation trajectories will arise -- because the\nanswers to these similar queries will be very different. In this paper, we\nreport on the first year of TREC iKAT, describing the task, topics, data\ncollection, and evaluation framework. We further review the submissions and\nsummarize the findings.\n","authors":["Mohammad Aliannejadi","Zahra Abbasiantaeb","Shubham Chatterjee","Jeffery Dalton","Leif Azzopardi"],"pdf_url":"https://arxiv.org/pdf/2401.01330v1.pdf","comment":"TREC 2023 Overview Paper"},{"id":"http://arxiv.org/abs/2401.01326v1","updated":"2024-01-02T18:32:14Z","published":"2024-01-02T18:32:14Z","title":"An Autoregressive Text-to-Graph Framework for Joint Entity and Relation\n  Extraction","summary":"  In this paper, we propose a novel method for joint entity and relation\nextraction from unstructured text by framing it as a conditional sequence\ngeneration problem. In contrast to conventional generative information\nextraction models that are left-to-right token-level generators, our approach\nis \\textit{span-based}. It generates a linearized graph where nodes represent\ntext spans and edges represent relation triplets. Our method employs a\ntransformer encoder-decoder architecture with pointing mechanism on a dynamic\nvocabulary of spans and relation types. Our model can capture the structural\ncharacteristics and boundaries of entities and relations through span\nrepresentations while simultaneously grounding the generated output in the\noriginal text thanks to the pointing mechanism. Evaluation on benchmark\ndatasets validates the effectiveness of our approach, demonstrating competitive\nresults. Code is available at https://github.com/urchade/ATG.\n","authors":["Zaratiana Urchade","Nadi Tomeh","Pierre Holat","Thierry Charnois"],"pdf_url":"https://arxiv.org/pdf/2401.01326v1.pdf","comment":"AAAI 2024"},{"id":"http://arxiv.org/abs/2401.01325v1","updated":"2024-01-02T18:30:51Z","published":"2024-01-02T18:30:51Z","title":"LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning","summary":"  This work elicits LLMs' inherent ability to handle long contexts without\nfine-tuning. The limited length of the training sequence during training may\nlimit the application of Large Language Models (LLMs) on long input sequences\nfor inference. In this work, we argue that existing LLMs themselves have\ninherent capabilities for handling long contexts. Based on this argument, we\nsuggest extending LLMs' context window by themselves to fully utilize the\ninherent ability.We propose Self-Extend to stimulate LLMs' long context\nhandling potential. The basic idea is to construct bi-level attention\ninformation: the group level and the neighbor level. The two levels are\ncomputed by the original model's self-attention, which means the proposed does\nnot require any training. With only four lines of code modification, the\nproposed method can effortlessly extend existing LLMs' context window without\nany fine-tuning. We conduct comprehensive experiments and the results show that\nthe proposed method can effectively extend existing LLMs' context window's\nlength.\n","authors":["Hongye Jin","Xiaotian Han","Jingfeng Yang","Zhimeng Jiang","Zirui Liu","Chia-Yuan Chang","Huiyuan Chen","Xia Hu"],"pdf_url":"https://arxiv.org/pdf/2401.01325v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01313v1","updated":"2024-01-02T17:56:30Z","published":"2024-01-02T17:56:30Z","title":"A Comprehensive Survey of Hallucination Mitigation Techniques in Large\n  Language Models","summary":"  As Large Language Models (LLMs) continue to advance in their ability to write\nhuman-like text, a key challenge remains around their tendency to hallucinate\ngenerating content that appears factual but is ungrounded. This issue of\nhallucination is arguably the biggest hindrance to safely deploying these\npowerful LLMs into real-world production systems that impact people's lives.\nThe journey toward widespread adoption of LLMs in practical settings heavily\nrelies on addressing and mitigating hallucinations. Unlike traditional AI\nsystems focused on limited tasks, LLMs have been exposed to vast amounts of\nonline text data during training. While this allows them to display impressive\nlanguage fluency, it also means they are capable of extrapolating information\nfrom the biases in training data, misinterpreting ambiguous prompts, or\nmodifying the information to align superficially with the input. This becomes\nhugely alarming when we rely on language generation capabilities for sensitive\napplications, such as summarizing medical records, financial analysis reports,\netc. This paper presents a comprehensive survey of over 32 techniques developed\nto mitigate hallucination in LLMs. Notable among these are Retrieval Augmented\nGeneration (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023),\nCoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023). Furthermore, we\nintroduce a detailed taxonomy categorizing these methods based on various\nparameters, such as dataset utilization, common tasks, feedback mechanisms, and\nretriever types. This classification helps distinguish the diverse approaches\nspecifically designed to tackle hallucination issues in LLMs. Additionally, we\nanalyze the challenges and limitations inherent in these techniques, providing\na solid foundation for future research in addressing hallucinations and related\nphenomena within the realm of LLMs.\n","authors":["S. M Towhidul Islam Tonmoy","S M Mehedi Zaman","Vinija Jain","Anku Rani","Vipula Rawte","Aman Chadha","Amitava Das"],"pdf_url":"https://arxiv.org/pdf/2401.01313v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2311.09677,\n  arXiv:2308.11764 by other authors"},{"id":"http://arxiv.org/abs/2401.01301v1","updated":"2024-01-02T17:28:06Z","published":"2024-01-02T17:28:06Z","title":"Large Legal Fictions: Profiling Legal Hallucinations in Large Language\n  Models","summary":"  Large language models (LLMs) have the potential to transform the practice of\nlaw, but this potential is threatened by the presence of legal hallucinations\n-- responses from these models that are not consistent with legal facts. We\ninvestigate the extent of these hallucinations using an original suite of legal\nqueries, comparing LLMs' responses to structured legal metadata and examining\ntheir consistency. Our work makes four key contributions: (1) We develop a\ntypology of legal hallucinations, providing a conceptual framework for future\nresearch in this area. (2) We find that legal hallucinations are alarmingly\nprevalent, occurring between 69% of the time with ChatGPT 3.5 and 88% with\nLlama 2, when these models are asked specific, verifiable questions about\nrandom federal court cases. (3) We illustrate that LLMs often fail to correct a\nuser's incorrect legal assumptions in a contra-factual question setup. (4) We\nprovide evidence that LLMs cannot always predict, or do not always know, when\nthey are producing legal hallucinations. Taken together, these findings caution\nagainst the rapid and unsupervised integration of popular LLMs into legal\ntasks. Even experienced lawyers must remain wary of legal hallucinations, and\nthe risks are highest for those who stand to benefit from LLMs the most -- pro\nse litigants or those without access to traditional legal resources.\n","authors":["Matthew Dahl","Varun Magesh","Mirac Suzgun","Daniel E. Ho"],"pdf_url":"https://arxiv.org/pdf/2401.01301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01286v1","updated":"2024-01-02T16:54:58Z","published":"2024-01-02T16:54:58Z","title":"A Comprehensive Study of Knowledge Editing for Large Language Models","summary":"  Large Language Models (LLMs) have shown extraordinary capabilities in\nunderstanding and generating text that closely mirrors human communication.\nHowever, a primary limitation lies in the significant computational demands\nduring training, arising from their extensive parameterization. This challenge\nis further intensified by the dynamic nature of the world, necessitating\nfrequent updates to LLMs to correct outdated information or integrate new\nknowledge, thereby ensuring their continued relevance. Note that many\napplications demand continual model adjustments post-training to address\ndeficiencies or undesirable behaviors. There is an increasing interest in\nefficient, lightweight methods for on-the-fly model modifications. To this end,\nrecent years have seen a burgeoning in the techniques of knowledge editing for\nLLMs, which aim to efficiently modify LLMs' behaviors within specific domains\nwhile preserving overall performance across various inputs. In this paper, we\nfirst define the knowledge editing problem and then provide a comprehensive\nreview of cutting-edge approaches. Drawing inspiration from educational and\ncognitive research theories, we propose a unified categorization criterion that\nclassifies knowledge editing methods into three groups: resorting to external\nknowledge, merging knowledge into the model, and editing intrinsic knowledge.\nFurthermore, we introduce a new benchmark, KnowEdit, for a comprehensive\nempirical evaluation of representative knowledge editing approaches.\nAdditionally, we provide an in-depth analysis of knowledge location, which can\nprovide a deeper understanding of the knowledge structures inherent within\nLLMs. Finally, we discuss several potential applications of knowledge editing,\noutlining its broad and impactful implications.\n","authors":["Ningyu Zhang","Yunzhi Yao","Bozhong Tian","Peng Wang","Shumin Deng","Mengru Wang","Zekun Xi","Shengyu Mao","Jintian Zhang","Yuansheng Ni","Siyuan Cheng","Ziwen Xu","Xin Xu","Jia-Chen Gu","Yong Jiang","Pengjun Xie","Fei Huang","Lei Liang","Zhiqiang Zhang","Xiaowei Zhu","Jun Zhou","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2401.01286v1.pdf","comment":"Ongoing work; 50 pages, 265 citations; benchmark is available at\n  https://huggingface.co/datasets/zjunlp/KnowEdit; code is available at\n  https://github.com/zjunlp/EasyEdit; paper list is available at\n  https://github.com/zjunlp/KnowledgeEditingPapers"},{"id":"http://arxiv.org/abs/2401.01283v1","updated":"2024-01-02T16:51:17Z","published":"2024-01-02T16:51:17Z","title":"Quality and Quantity of Machine Translation References for Automated\n  Metrics","summary":"  Automatic machine translation metrics often use human translations to\ndetermine the quality system translations. Common wisdom in the field dictates\nthat the human references should be of very high quality. However, there are no\ncost-benefit analyses that could be used to guide practitioners who plan to\ncollect references for machine translation evaluation. We find that\nhigher-quality references lead to better metric correlations with humans at the\nsegment-level. Having up to 7 references per segment and taking their average\nhelps all metrics. Interestingly, the references from vendors of different\nqualities can be mixed together and improve metric success. Higher quality\nreferences, however, cost more to create and we frame this as an optimization\nproblem: given a specific budget, what references should be collected to\nmaximize metric success. These findings can be used by evaluators of shared\ntasks when references need to be created under a certain budget.\n","authors":["Vilém Zouhar","Ondřej Bojar"],"pdf_url":"https://arxiv.org/pdf/2401.01283v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01275v1","updated":"2024-01-02T16:20:40Z","published":"2024-01-02T16:20:40Z","title":"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent\n  Evaluation","summary":"  Recently, the advent of large language models (LLMs) has revolutionized\ngenerative agents. Among them, Role-Playing Conversational Agents (RPCAs)\nattract considerable attention due to their ability to emotionally engage\nusers. However, the absence of a comprehensive benchmark impedes progress in\nthis field. To bridge this gap, we introduce CharacterEval, a Chinese benchmark\nfor comprehensive RPCA assessment, complemented by a tailored high-quality\ndataset. The dataset comprises 1,785 multi-turn role-playing dialogues,\nencompassing 23,020 examples and featuring 77 characters derived from Chinese\nnovels and scripts. It was carefully constructed, beginning with initial\ndialogue extraction via GPT-4, followed by rigorous human-led quality control,\nand enhanced with in-depth character profiles sourced from Baidu Baike.\nCharacterEval employs a multifaceted evaluation approach, encompassing thirteen\ntargeted metrics on four dimensions. Comprehensive experiments on CharacterEval\ndemonstrate that Chinese LLMs exhibit more promising capabilities than GPT-4 in\nChinese role-playing conversation. Source code, data source and reward model\nwill be publicly accessible at https://github.com/morecry/CharacterEval.\n","authors":["Quan Tu","Shilong Fan","Zihang Tian","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2401.01275v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01262v1","updated":"2024-01-02T16:09:36Z","published":"2024-01-02T16:09:36Z","title":"Fairness Certification for Natural Language Processing and Large\n  Language Models","summary":"  Natural Language Processing (NLP) plays an important role in our daily lives,\nparticularly due to the enormous progress of Large Language Models (LLM).\nHowever, NLP has many fairness-critical use cases, e.g., as an expert system in\nrecruitment or as an LLM-based tutor in education. Since NLP is based on human\nlanguage, potentially harmful biases can diffuse into NLP systems and produce\nunfair results, discriminate against minorities or generate legal issues.\nHence, it is important to develop a fairness certification for NLP approaches.\nWe follow a qualitative research approach towards a fairness certification for\nNLP. In particular, we have reviewed a large body of literature on algorithmic\nfairness, and we have conducted semi-structured expert interviews with a wide\nrange of experts from that area. We have systematically devised six fairness\ncriteria for NLP, which can be further refined into 18 sub-categories. Our\ncriteria offer a foundation for operationalizing and testing processes to\ncertify fairness, both from the perspective of the auditor and the audited\norganization.\n","authors":["Vincent Freiberger","Erik Buchmann"],"pdf_url":"https://arxiv.org/pdf/2401.01262v1.pdf","comment":"In depth discussion of our results can be found in the Appendix"},{"id":"http://arxiv.org/abs/2401.01256v1","updated":"2024-01-02T15:56:48Z","published":"2024-01-02T15:56:48Z","title":"VideoDrafter: Content-Consistent Multi-Scene Video Generation with LLM","summary":"  The recent innovations and breakthroughs in diffusion models have\nsignificantly expanded the possibilities of generating high-quality videos for\nthe given prompts. Most existing works tackle the single-scene scenario with\nonly one video event occurring in a single background. Extending to generate\nmulti-scene videos nevertheless is not trivial and necessitates to nicely\nmanage the logic in between while preserving the consistent visual appearance\nof key content across video scenes. In this paper, we propose a novel\nframework, namely VideoDrafter, for content-consistent multi-scene video\ngeneration. Technically, VideoDrafter leverages Large Language Models (LLM) to\nconvert the input prompt into comprehensive multi-scene script that benefits\nfrom the logical knowledge learnt by LLM. The script for each scene includes a\nprompt describing the event, the foreground/background entities, as well as\ncamera movement. VideoDrafter identifies the common entities throughout the\nscript and asks LLM to detail each entity. The resultant entity description is\nthen fed into a text-to-image model to generate a reference image for each\nentity. Finally, VideoDrafter outputs a multi-scene video by generating each\nscene video via a diffusion process that takes the reference images, the\ndescriptive prompt of the event and camera movement into account. The diffusion\nmodel incorporates the reference images as the condition and alignment to\nstrengthen the content consistency of multi-scene videos. Extensive experiments\ndemonstrate that VideoDrafter outperforms the SOTA video generation models in\nterms of visual quality, content consistency, and user preference.\n","authors":["Fuchen Long","Zhaofan Qiu","Ting Yao","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2401.01256v1.pdf","comment":"Project website: https://videodrafter.github.io"},{"id":"http://arxiv.org/abs/2308.06035v3","updated":"2024-01-02T15:33:20Z","published":"2023-08-11T09:30:07Z","title":"Multimodality and Attention Increase Alignment in Natural Language\n  Prediction Between Humans and Computational Models","summary":"  The potential of multimodal generative artificial intelligence (mAI) to\nreplicate human grounded language understanding, including the pragmatic,\ncontext-rich aspects of communication, remains to be clarified. Humans are\nknown to use salient multimodal features, such as visual cues, to facilitate\nthe processing of upcoming words. Correspondingly, multimodal computational\nmodels can integrate visual and linguistic data using a visual attention\nmechanism to assign next-word probabilities. To test whether these processes\nalign, we tasked both human participants (N = 200) as well as several\nstate-of-the-art computational models with evaluating the predictability of\nforthcoming words after viewing short audio-only or audio-visual clips with\nspeech. During the task, the model's attention weights were recorded and human\nattention was indexed via eye tracking. Results show that predictability\nestimates from humans aligned more closely with scores generated from\nmultimodal models vs. their unimodal counterparts. Furthermore, including an\nattention mechanism doubled alignment with human judgments when visual and\nlinguistic context facilitated predictions. In these cases, the model's\nattention patches and human eye tracking significantly overlapped. Our results\nindicate that improved modeling of naturalistic language processing in mAI does\nnot merely depend on training diet but can be driven by multimodality in\ncombination with attention-based architectures. Humans and computational models\nalike can leverage the predictive constraints of multimodal information by\nattending to relevant features in the input.\n","authors":["Viktor Kewenig","Andrew Lampinen","Samuel A. Nastase","Christopher Edwards","Quitterie Lacome DEstalenx","Akilles Rechardt","Jeremy I Skipper","Gabriella Vigliocco"],"pdf_url":"https://arxiv.org/pdf/2308.06035v3.pdf","comment":"20 pages, 4 figures, submitted to Nature Human Behaviour"},{"id":"http://arxiv.org/abs/2305.07490v5","updated":"2024-01-02T15:29:53Z","published":"2023-05-12T14:04:30Z","title":"ArtGPT-4: Towards Artistic-understanding Large Vision-Language Models\n  with Enhanced Adapter","summary":"  In recent years, advancements in large language models have been remarkable,\nwith models such as ChatGPT demonstrating exceptional proficiency in diverse\nlinguistic tasks. The pre-training of large models with billions of parameters,\nposes a formidable challenge, primarily due to the scarcity of datasets of a\ncommensurate scale for effective training. Nevertheless, innovative strategies\nhave emerged, including methods to fine-tune these pre-trained models using\nfewer parameters set, as evidenced by models like MiniGPT-4 and LLaVA. Despite\ntheir potential in various domains, these models remain limited in their\nunderstanding of artistic imagery. They have yet to fully grasp the intricate\nnuances of art images or to provide an objective articulation of the emotions\nthey evoke, in a manner akin to human perception. This work introduces\nArtGPT-4, a pioneering large vision-language model tailored to address the\ndeficiencies of contemporary models in artistic comprehension. ArtGPT-4\nunderwent training on image-text pairs utilizing a Tesla A100 device in a mere\n2 hours, with a dataset comprising approximately 0.52M entries. Impressively,\nthe model can render images with an artistic-understanding and convey the\nemotions they inspire, mirroring human interpretation. Additionally, this work\npresents a unique dataset designed to evaluate the efficacy of vision-language\nmodels. In subsequent evaluations, ArtGPT-4 not only achieved state-of-the-art\nperformance on the ArtEmis and ArtEmis-v2.0 datasets but also exceeded the\nestablished benchmarks introduced in This study, lagging behind professional\nartists' descriptions by a negligible 0.15 points on a 6-point scale. The code\nand the pre-trained model are accessible in\nhttps://huggingface.co/Tyrannosaurus/ArtGPT-4.\n","authors":["Zhengqing Yuan","Xinyi Wang","Kun Wang","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2305.07490v5.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2311.15218v4","updated":"2024-01-02T15:13:44Z","published":"2023-11-26T07:19:10Z","title":"Real-Time Online Stock Forecasting Utilizing Integrated Quantitative and\n  Qualitative Analysis","summary":"  The application of Machine learning to finance has become a familiar\napproach, even more so in stock market forecasting. The stock market is highly\nvolatile, and huge amounts of data are generated every minute globally. The\nextraction of effective intelligence from this data is of critical importance.\nHowever, a collaboration of numerical stock data with qualitative text data can\nbe a challenging task. In this work, we accomplish this by providing an\nunprecedented, publicly available dataset with technical and fundamental data\nand sentiment that we gathered from news archives, TV news captions, radio\ntranscripts, tweets, daily financial newspapers, etc. The text data entries\nused for sentiment extraction total more than 1.4 Million. The dataset consists\nof daily entries from January 2018 to December 2022 for eight companies\nrepresenting diverse industrial sectors and the Dow Jones Industrial Average\n(DJIA) as a whole. Holistic Fundamental and Technical data is provided training\nready for Model learning and deployment. Most importantly, the data generated\ncould be used for incremental online learning with real-time data points\nretrieved daily since no stagnant data was utilized. All the data was retired\nfrom APIs or self-designed robust information retrieval technologies with\nextremely low latency and zero monetary cost. These adaptable technologies\nfacilitate data extraction for any stock. Moreover, the utilization of\nSpearman's rank correlation over real-time data, linking stock returns with\nsentiment analysis has produced noteworthy results for the DJIA and the eight\nother stocks, achieving accuracy levels surpassing 60%. The dataset is made\navailable at https://github.com/batking24/Huge-Stock-Dataset.\n","authors":["Sai Akash Bathini","Dagli Cihan"],"pdf_url":"https://arxiv.org/pdf/2311.15218v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.17296v2","updated":"2024-01-02T14:48:56Z","published":"2023-12-28T16:25:52Z","title":"Structured Packing in LLM Training Improves Long Context Utilization","summary":"  Recent advances in long-context Large Language Models (LCLMs) have generated\nsignificant interest, especially in applications such as querying scientific\nresearch papers. However, their potential is often limited by inadequate\ncontext utilization. We identify the absence of long-range semantic\ndependencies in typical training data as a primary hindrance. To address this,\nwe delve into the benefits of frequently incorporating related documents into\ntraining inputs. Using the inherent directory structure of code data as a\nsource of training examples, we demonstrate improvements in perplexity, even\nfor tasks unrelated to coding. Building on these findings, but with a broader\nfocus, we introduce Structured Packing for Long Context (SPLiCe). SPLiCe is an\ninnovative method for creating training examples by using a retrieval method to\ncollate the most mutually relevant documents into a single training context.\nOur results indicate that \\method{} enhances model performance and can be used\nto train large models to utilize long contexts better. We validate our results\nby training a large $3$B model, showing both perplexity improvements and better\nlong-context performance on downstream tasks.\n","authors":["Konrad Staniszewski","Szymon Tworkowski","Sebastian Jaszczur","Henryk Michalewski","Łukasz Kuciński","Piotr Miłoś"],"pdf_url":"https://arxiv.org/pdf/2312.17296v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.11300v5","updated":"2024-01-02T14:18:02Z","published":"2023-06-20T05:30:59Z","title":"RS5M and GeoRSCLIP: A Large Scale Vision-Language Dataset and A Large\n  Vision-Language Model for Remote Sensing","summary":"  Pre-trained Vision-Language Models (VLMs) utilizing extensive image-text\npaired data have demonstrated unprecedented image-text association\ncapabilities, achieving remarkable results across various downstream tasks. A\ncritical challenge is how to make use of existing large-scale pre-trained VLMs,\nwhich are trained on common objects, to perform the domain-specific transfer\nfor accomplishing domain-related downstream tasks. A critical challenge is how\nto make use of existing large-scale pre-trained VLMs, which are trained on\ncommon objects, to perform the domain-specific transfer for accomplishing\ndomain-related downstream tasks. In this paper, we propose a new framework that\nincludes the Domain pre-trained Vision-Language Model (DVLM), bridging the gap\nbetween the General Vision-Language Model (GVLM) and domain-specific downstream\ntasks. Moreover, we present an image-text paired dataset in the field of remote\nsensing (RS), RS5M, which has 5 million RS images with English descriptions.\nThe dataset is obtained from filtering publicly available image-text paired\ndatasets and captioning label-only RS datasets with pre-trained VLM. These\nconstitute the first large-scale RS image-text paired dataset. Additionally, we\nfine-tuned the CLIP model and tried several Parameter-Efficient Fine-Tuning\nmethods on RS5M to implement the DVLM. Experimental results show that our\nproposed dataset is highly effective for various tasks, and our model GeoRSCLIP\nimproves upon the baseline or previous state-of-the-art model by $3\\%\\sim20\\%$\nin Zero-shot Classification (ZSC), $3\\%\\sim6\\%$ in Remote Sensing Cross-Modal\nText-Image Retrieval (RSCTIR) and $4\\%\\sim5\\%$ in Semantic Localization (SeLo)\ntasks. Dataset and models have been released in:\n\\url{https://github.com/om-ai-lab/RS5M}.\n","authors":["Zilun Zhang","Tiancheng Zhao","Yulong Guo","Jianwei Yin"],"pdf_url":"https://arxiv.org/pdf/2306.11300v5.pdf","comment":"RS5M dataset v5"},{"id":"http://arxiv.org/abs/2401.01218v1","updated":"2024-01-02T14:12:41Z","published":"2024-01-02T14:12:41Z","title":"Zero-Shot Position Debiasing for Large Language Models","summary":"  Fine-tuning has been demonstrated to be an effective method to improve the\ndomain performance of large language models (LLMs). However, LLMs might fit the\ndataset bias and shortcuts for prediction, leading to poor generation\nperformance. Experimental result shows that LLMs are prone to exhibit position\nbias, i.e., leveraging information positioned at the beginning or end, or\nspecific positional cues within the input. Existing works on mitigating\nposition bias require external bias knowledge or annotated non-biased samples,\nwhich is unpractical in reality. In this work, we propose a zero-shot position\ndebiasing (ZOE) framework to mitigate position bias for LLMs. ZOE leverages\nunsupervised responses from pre-trained LLMs for debiasing, thus without any\nexternal knowledge or datasets. To improve the quality of unsupervised\nresponses, we propose a master-slave alignment (MSA) module to prune these\nresponses. Experiments on eight datasets and five tasks show that ZOE\nconsistently outperforms existing methods in mitigating four types of position\nbiases. Besides, ZOE achieves this by sacrificing only a small performance on\nbiased samples, which is simple and effective.\n","authors":["Zhongkun Liu","Zheng Chen","Mengqi Zhang","Zhaochun Ren","Zhumin Chen","Pengjie Ren"],"pdf_url":"https://arxiv.org/pdf/2401.01218v1.pdf","comment":"16 pages, 22 figures"},{"id":"http://arxiv.org/abs/2401.01197v1","updated":"2024-01-02T13:01:50Z","published":"2024-01-02T13:01:50Z","title":"Uncertainty Resolution in Misinformation Detection","summary":"  Misinformation poses a variety of risks, such as undermining public trust and\ndistorting factual discourse. Large Language Models (LLMs) like GPT-4 have been\nshown effective in mitigating misinformation, particularly in handling\nstatements where enough context is provided. However, they struggle to assess\nambiguous or context-deficient statements accurately. This work introduces a\nnew method to resolve uncertainty in such statements. We propose a framework to\ncategorize missing information and publish category labels for the LIAR-New\ndataset, which is adaptable to cross-domain content with missing information.\nWe then leverage this framework to generate effective user queries for missing\ncontext. Compared to baselines, our method improves the rate at which generated\nquestions are answerable by the user by 38 percentage points and classification\nperformance by over 10 percentage points macro F1. Thus, this approach may\nprovide a valuable component for future misinformation mitigation pipelines.\n","authors":["Yury Orlovskiy","Camille Thibault","Anne Imouza","Jean-François Godbout","Reihaneh Rabbany","Kellin Pelrine"],"pdf_url":"https://arxiv.org/pdf/2401.01197v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10741v2","updated":"2024-01-02T12:59:20Z","published":"2023-12-17T15:26:16Z","title":"StyleSinger: Style Transfer for Out-of-Domain Singing Voice Synthesis","summary":"  Style transfer for out-of-domain (OOD) singing voice synthesis (SVS) focuses\non generating high-quality singing voices with unseen styles (such as timbre,\nemotion, pronunciation, and articulation skills) derived from reference singing\nvoice samples. However, the endeavor to model the intricate nuances of singing\nvoice styles is an arduous task, as singing voices possess a remarkable degree\nof expressiveness. Moreover, existing SVS methods encounter a decline in the\nquality of synthesized singing voices in OOD scenarios, as they rest upon the\nassumption that the target vocal attributes are discernible during the training\nphase. To overcome these challenges, we propose StyleSinger, the first singing\nvoice synthesis model for zero-shot style transfer of out-of-domain reference\nsinging voice samples. StyleSinger incorporates two critical approaches for\nenhanced effectiveness: 1) the Residual Style Adaptor (RSA) which employs a\nresidual quantization module to capture diverse style characteristics in\nsinging voices, and 2) the Uncertainty Modeling Layer Normalization (UMLN) to\nperturb the style attributes within the content representation during the\ntraining phase and thus improve the model generalization. Our extensive\nevaluations in zero-shot style transfer undeniably establish that StyleSinger\noutperforms baseline models in both audio quality and similarity to the\nreference singing voice samples. Access to singing voice samples can be found\nat https://stylesinger.github.io/.\n","authors":["Yu Zhang","Rongjie Huang","Ruiqi Li","JinZheng He","Yan Xia","Feiyang Chen","Xinyu Duan","Baoxing Huai","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2312.10741v2.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2401.01183v1","updated":"2024-01-02T12:23:49Z","published":"2024-01-02T12:23:49Z","title":"Unifying Structured Data as Graph for Data-to-Text Pre-Training","summary":"  Data-to-text (D2T) generation aims to transform structured data into natural\nlanguage text. Data-to-text pre-training has proved to be powerful in enhancing\nD2T generation and yields impressive performances. However, previous\npre-training methods either oversimplified structured data into a sequence\nwithout considering input structures or designed training objectives tailored\nfor a specific data structure (e.g., table or knowledge graph). In this paper,\nwe unify different types of structured data (i.e., table, key-value data,\nknowledge graph) into the graph format and cast different data-to-text\ngeneration tasks as graph-to-text generation. To effectively exploit the\nstructural information of the input graph, we propose a structure-enhanced\npre-training method for D2T generation by designing a structure-enhanced\nTransformer. Concretely, we devise a position matrix for the Transformer,\nencoding relative positional information of connected nodes in the input graph.\nIn addition, we propose a new attention matrix to incorporate graph structures\ninto the original Transformer by taking the available explicit connectivity\nstructure into account. Extensive experiments on six benchmark datasets show\nthe effectiveness of our model. Our source codes are available at\nhttps://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/unid2t.\n","authors":["Shujie Li","Liang Li","Ruiying Geng","Min Yang","Binhua Li","Guanghu Yuan","Wanwei He","Shao Yuan","Can Ma","Fei Huang","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2401.01183v1.pdf","comment":"Accepted for TACL. Pre-MIT Press publication version"},{"id":"http://arxiv.org/abs/2310.19923v2","updated":"2024-01-02T10:01:51Z","published":"2023-10-30T18:35:30Z","title":"Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long\n  Documents","summary":"  Text embedding models have emerged as powerful tools for transforming\nsentences into fixed-sized feature vectors that encapsulate semantic\ninformation. While these models are essential for tasks like information\nretrieval, semantic clustering, and text re-ranking, most existing open-source\nmodels, especially those built on architectures like BERT, struggle to\nrepresent lengthy documents and often resort to truncation. One common approach\nto mitigate this challenge involves splitting documents into smaller paragraphs\nfor embedding. However, this strategy results in a much larger set of vectors,\nconsequently leading to increased memory consumption and computationally\nintensive vector searches with elevated latency.\n  To address these challenges, we introduce Jina Embeddings 2, an open-source\ntext embedding model capable of accommodating up to 8192 tokens. This model is\ndesigned to transcend the conventional 512-token limit and adeptly process long\ndocuments. Jina Embeddings 2 not only achieves state-of-the-art performance on\na range of embedding-related tasks in the MTEB benchmark but also matches the\nperformance of OpenAI's proprietary ada-002 model. Additionally, our\nexperiments indicate that an extended context can enhance performance in tasks\nsuch as NarrativeQA.\n","authors":["Michael Günther","Jackmin Ong","Isabelle Mohr","Alaeddine Abdessalem","Tanguy Abel","Mohammad Kalim Akram","Susana Guzman","Georgios Mastrapas","Saba Sturua","Bo Wang","Maximilian Werk","Nan Wang","Han Xiao"],"pdf_url":"https://arxiv.org/pdf/2310.19923v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2312.12850v3","updated":"2024-01-02T10:01:28Z","published":"2023-12-20T09:01:01Z","title":"A Stochastic Analysis of the Linguistic Provenance of English Place\n  Names","summary":"  In English place name analysis, meanings are often derived from the\nresemblance of roots in place names to topographical features, proper names\nand/or habitation terms in one of the languages that have had an influence on\nEnglish place names. The problem here is that it is sometimes difficult to\ndetermine the base language to use to interpret the roots. The purpose of this\npaper is to stochastically determine the resemblance between 18799 English\nplace names and 84687 place names from Ireland, Scotland, Wales, Denmark,\nNorway, Sweden, France, Germany, the Netherlands and Ancient Rome. Each English\nplace name is ranked according to the extent to which it resembles place names\nfrom the other countries, and this provides a basis for determining the likely\nlanguage to use to interpret the place name. A number of observations can be\nmade using the ranking provided. In particular, it is found that `Harlington'\nis the most archetypically English place name in the English sample, and `Anna'\nis the least. Furthermore, it is found that the place names in the non-English\ndatasets are most similar to Norwegian place names and least similar to Welsh\nplace names.\n","authors":["Michael Dalvean"],"pdf_url":"https://arxiv.org/pdf/2312.12850v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13631v2","updated":"2024-01-02T09:35:33Z","published":"2023-03-21T08:39:56Z","title":"In-depth analysis of music structure as a text network","summary":"  Music, enchanting and poetic, permeates every corner of human civilization.\nAlthough music is not unfamiliar to people, our understanding of its essence\nremains limited, and there is still no universally accepted scientific\ndescription. This is primarily due to music being regarded as a product of both\nreason and emotion, making it difficult to define. In this article, we focus on\nthe fundamental elements of music and construct an evolutionary network from\nthe perspective of music as a natural language, aligning with the statistical\ncharacteristics of texts. Through this approach, we aim to comprehend the\nstructural differences in music across different periods, enabling a more\nscientific exploration of music. Relying on the advantages of structuralism, we\ncan concentrate on the relationships and order between the physical elements of\nmusic, rather than getting entangled in the blurred boundaries of science and\nphilosophy. The scientific framework we present not only conforms to past\nconclusions in music, but also serves as a bridge that connects music to\nnatural language processing and knowledge graphs.\n","authors":["Ping-Rui Tsai","Yen-Ting Chou","Nathan-Christopher Wang","Hui-Ling Chen","Hong-Yue Huang","Zih-Jia Luo","Tzay-Ming Hong"],"pdf_url":"https://arxiv.org/pdf/2303.13631v2.pdf","comment":"7 pages, 8 figures"},{"id":"http://arxiv.org/abs/2401.01108v1","updated":"2024-01-02T08:58:01Z","published":"2024-01-02T08:58:01Z","title":"Unveiling Comparative Sentiments in Vietnamese Product Reviews: A\n  Sequential Classification Framework","summary":"  Comparative opinion mining is a specialized field of sentiment analysis that\naims to identify and extract sentiments expressed comparatively. To address\nthis task, we propose an approach that consists of solving three sequential\nsub-tasks: (i) identifying comparative sentence, i.e., if a sentence has a\ncomparative meaning, (ii) extracting comparative elements, i.e., what are\ncomparison subjects, objects, aspects, predicates, and (iii) classifying\ncomparison types which contribute to a deeper comprehension of user sentiments\nin Vietnamese product reviews. Our method is ranked fifth at the Vietnamese\nLanguage and Speech Processing (VLSP) 2023 challenge on Comparative Opinion\nMining (ComOM) from Vietnamese Product Reviews.\n","authors":["Ha Le","Bao Tran","Phuong Le","Tan Nguyen","Dac Nguyen","Ngoan Pham","Dang Huynh"],"pdf_url":"https://arxiv.org/pdf/2401.01108v1.pdf","comment":"Accepted manuscript at VLSP 2023"},{"id":"http://arxiv.org/abs/2401.01089v1","updated":"2024-01-02T08:14:48Z","published":"2024-01-02T08:14:48Z","title":"Quokka: An Open-source Large Language Model ChatBot for Material Science","summary":"  This paper presents the development of a specialized chatbot for materials\nscience, leveraging the Llama-2 language model, and continuing pre-training on\nthe expansive research articles in the materials science domain from the S2ORC\ndataset. The methodology involves an initial pretraining phase on over one\nmillion domain-specific papers, followed by an instruction-tuning process to\nrefine the chatbot's capabilities. The chatbot is designed to assist\nresearchers, educators, and students by providing instant, context-aware\nresponses to queries in the field of materials science. We make the four\ntrained checkpoints (7B, 13B, with or without chat ability) freely available to\nthe research community at https://github.com/Xianjun-Yang/Quokka.\n","authors":["Xianjun Yang","Stephen D. Wilson","Linda Petzold"],"pdf_url":"https://arxiv.org/pdf/2401.01089v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2310.10477v4","updated":"2024-01-02T07:51:33Z","published":"2023-10-16T14:59:10Z","title":"Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake\n  Analysis","summary":"  The rapid development of large language models (LLMs) has not only provided\nnumerous opportunities but also presented significant challenges. This becomes\nparticularly evident when LLMs inadvertently generate harmful or toxic content,\neither unintentionally or because of intentional inducement. Existing alignment\nmethods usually direct LLMs toward the favorable outcomes by utilizing\nhuman-annotated, flawless instruction-response pairs. Conversely, this study\nproposes a novel alignment technique based on mistake analysis, which\ndeliberately exposes LLMs to erroneous content to learn the reasons for\nmistakes and how to avoid them. In this case, mistakes are repurposed into\nvaluable data for alignment, effectively helping to avoid the production of\nerroneous responses. Without external models or human annotations, our method\nleverages a model's intrinsic ability to discern undesirable mistakes and\nimproves the safety of its generated responses. Experimental results reveal\nthat our method outperforms existing alignment approaches in enhancing model\nsafety while maintaining the overall utility.\n","authors":["Kai Chen","Chunwei Wang","Kuo Yang","Jianhua Han","Lanqing Hong","Fei Mi","Hang Xu","Zhengying Liu","Wenyong Huang","Zhenguo Li","Dit-Yan Yeung","Lifeng Shang","Xin Jiang","Qun Liu"],"pdf_url":"https://arxiv.org/pdf/2310.10477v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01078v1","updated":"2024-01-02T07:46:34Z","published":"2024-01-02T07:46:34Z","title":"Vietnamese Poem Generation & The Prospect Of Cross-Language Poem-To-Poem\n  Translation","summary":"  Poetry generation has been a challenging task in the field of Natural\nLanguage Processing, as it requires the model to understand the nuances of\nlanguage, sentiment, and style. In this paper, we propose using Large Language\nModels to generate Vietnamese poems from natural language prompts, thereby\nfacilitating an intuitive process with enhanced content control. Our most\nefficacious model, the GPT-3 Babbage variant, achieves a custom evaluation\nscore of 0.8, specifically tailored to the \"luc bat\" genre of Vietnamese\npoetry. Furthermore, we also explore the idea of paraphrasing poems into normal\ntext prompts and yield a relatively high score of 0.718 in the \"luc bat\" genre.\nThis experiment presents the potential for cross-Language poem-to-poem\ntranslation with translated poems as the inputs while concurrently maintaining\ncomplete control over the generated content.\n","authors":["Triet Huynh Minh","Quan Le Bao"],"pdf_url":"https://arxiv.org/pdf/2401.01078v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01076v1","updated":"2024-01-02T07:40:12Z","published":"2024-01-02T07:40:12Z","title":"DialCLIP: Empowering CLIP as Multi-Modal Dialog Retriever","summary":"  Recently, substantial advancements in pre-trained vision-language models have\ngreatly enhanced the capabilities of multi-modal dialog systems. These models\nhave demonstrated significant improvements by fine-tuning on downstream tasks.\nHowever, the existing pre-trained models primarily focus on effectively\ncapturing the alignment between vision and language modalities, often ignoring\nthe intricate nature of dialog context. In this paper, we propose a\nparameter-efficient prompt-tuning method named DialCLIP for multi-modal dialog\nretrieval. Specifically, our approach introduces a multi-modal context prompt\ngenerator to learn context features which are subsequently distilled into\nprompts within the pre-trained vision-language model CLIP. Besides, we\nintroduce domain prompt to mitigate the disc repancy from the downstream dialog\ndata. To facilitate various types of retrieval, we also design multiple experts\nto learn mappings from CLIP outputs to multi-modal representation space, with\neach expert being responsible to one specific retrieval type. Extensive\nexperiments show that DialCLIP achieves state-of-the-art performance on two\nwidely recognized benchmark datasets (i.e., PhotoChat and MMDialog) by tuning a\nmere 0.04% of the total parameters. These results highlight the efficacy and\nefficiency of our proposed approach, underscoring its potential to advance the\nfield of multi-modal dialog retrieval.\n","authors":["Zhichao Yin","Binyuan Hui","Min Yang","Fei Huang","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2401.01076v1.pdf","comment":"arxiv: first version"},{"id":"http://arxiv.org/abs/2310.14587v2","updated":"2024-01-02T07:22:04Z","published":"2023-10-23T05:52:09Z","title":"Large Search Model: Redefining Search Stack in the Era of LLMs","summary":"  Modern search engines are built on a stack of different components, including\nquery understanding, retrieval, multi-stage ranking, and question answering,\namong others. These components are often optimized and deployed independently.\nIn this paper, we introduce a novel conceptual framework called large search\nmodel, which redefines the conventional search stack by unifying search tasks\nwith one large language model (LLM). All tasks are formulated as autoregressive\ntext generation problems, allowing for the customization of tasks through the\nuse of natural language prompts. This proposed framework capitalizes on the\nstrong language understanding and reasoning capabilities of LLMs, offering the\npotential to enhance search result quality while simultaneously simplifying the\nexisting cumbersome search stack. To substantiate the feasibility of this\nframework, we present a series of proof-of-concept experiments and discuss the\npotential challenges associated with implementing this approach within\nreal-world search systems.\n","authors":["Liang Wang","Nan Yang","Xiaolong Huang","Linjun Yang","Rangan Majumder","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2310.14587v2.pdf","comment":"SIGIR Forum, Vol. 57 No. 2 - December 2023"},{"id":"http://arxiv.org/abs/2401.01068v1","updated":"2024-01-02T07:00:24Z","published":"2024-01-02T07:00:24Z","title":"Discovering Significant Topics from Legal Decisions with Selective\n  Inference","summary":"  We propose and evaluate an automated pipeline for discovering significant\ntopics from legal decision texts by passing features synthesized with topic\nmodels through penalised regressions and post-selection significance tests. The\nmethod identifies case topics significantly correlated with outcomes,\ntopic-word distributions which can be manually-interpreted to gain insights\nabout significant topics, and case-topic weights which can be used to identify\nrepresentative cases for each topic. We demonstrate the method on a new dataset\nof domain name disputes and a canonical dataset of European Court of Human\nRights violation cases. Topic models based on latent semantic analysis as well\nas language model embeddings are evaluated. We show that topics derived by the\npipeline are consistent with legal doctrines in both areas and can be useful in\nother related legal analysis tasks.\n","authors":["Jerrold Soh"],"pdf_url":"https://arxiv.org/pdf/2401.01068v1.pdf","comment":"This is an accepted manuscript of work forthcoming in PhilTrans A.\n  Please cite the publisher's version only"},{"id":"http://arxiv.org/abs/2401.01055v1","updated":"2024-01-02T06:29:02Z","published":"2024-01-02T06:29:02Z","title":"LLaMA Beyond English: An Empirical Study on Language Capability Transfer","summary":"  In recent times, substantial advancements have been witnessed in large\nlanguage models (LLMs), exemplified by ChatGPT, showcasing remarkable\nproficiency across a range of complex tasks. However, many mainstream LLMs\n(e.g. LLaMA) are pretrained on English-dominant corpus, which limits their\nperformance in other non-English languages. In this paper, we focus on how to\neffectively transfer the capabilities of language generation and following\ninstructions to a non-English language. To answer this question, we conduct an\nextensive empirical investigation based on LLaMA, accumulating over 1440 GPU\nhours. We analyze the impact of key factors such as vocabulary extension,\nfurther pretraining, and instruction tuning on transfer. To accurately assess\nthe model's level of knowledge, we employ four widely used standardized testing\nbenchmarks: C-Eval, MMLU, AGI-Eval, and GAOKAO-Bench. Furthermore, a\ncomprehensive evaluation of the model's response quality is conducted,\nconsidering aspects such as accuracy, fluency, informativeness, logical\ncoherence, and harmlessness, based on LLM-Eval, a benchmarks consisting\ninstruction tasks from 17 diverse categories. Our evaluation results\ndemonstrate that comparable performance to state-of-the-art transfer models can\nbe achieved with less than 1% of the pretraining data, both in terms of\nknowledge alignment and response quality. Furthermore, the experimental\noutcomes across the thirteen low-resource languages also exhibit similar\ntrends. We anticipate that the conclusions revealed by the experiments will aid\nthe community in developing non-English LLMs.\n","authors":["Jun Zhao","Zhihao Zhang","Qi Zhang","Tao Gui","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2401.01055v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01053v1","updated":"2024-01-02T06:24:13Z","published":"2024-01-02T06:24:13Z","title":"Cheetah: Natural Language Generation for 517 African Languages","summary":"  Low-resource African languages pose unique challenges for natural language\nprocessing (NLP) tasks, including natural language generation (NLG). In this\npaper, we develop Cheetah, a massively multilingual NLG language model for\nAfrican languages. Cheetah supports 517 African languages and language\nvarieties, allowing us to address the scarcity of NLG resources and provide a\nsolution to foster linguistic diversity. We demonstrate the effectiveness of\nCheetah through comprehensive evaluations across seven generation downstream\ntasks. In five of the seven tasks, Cheetah significantly outperforms other\nmodels, showcasing its remarkable performance for generating coherent and\ncontextually appropriate text in a wide range of African languages. We\nadditionally conduct a detailed human evaluation to delve deeper into the\nlinguistic capabilities of Cheetah. The introduction of Cheetah has\nfar-reaching benefits for linguistic diversity. By leveraging pretrained models\nand adapting them to specific languages, our approach facilitates the\ndevelopment of practical NLG applications for African communities. The findings\nof this study contribute to advancing NLP research in low-resource settings,\nenabling greater accessibility and inclusion for African languages in a rapidly\nexpanding digital landscape. We will publicly release our models for research.\n","authors":["Ife Adebara","AbdelRahim Elmadany","Muhammad Abdul-Mageed"],"pdf_url":"https://arxiv.org/pdf/2401.01053v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01044v1","updated":"2024-01-02T05:42:14Z","published":"2024-01-02T05:42:14Z","title":"Auffusion: Leveraging the Power of Diffusion and Large Language Models\n  for Text-to-Audio Generation","summary":"  Recent advancements in diffusion models and large language models (LLMs) have\nsignificantly propelled the field of AIGC. Text-to-Audio (TTA), a burgeoning\nAIGC application designed to generate audio from natural language prompts, is\nattracting increasing attention. However, existing TTA studies often struggle\nwith generation quality and text-audio alignment, especially for complex\ntextual inputs. Drawing inspiration from state-of-the-art Text-to-Image (T2I)\ndiffusion models, we introduce Auffusion, a TTA system adapting T2I model\nframeworks to TTA task, by effectively leveraging their inherent generative\nstrengths and precise cross-modal alignment. Our objective and subjective\nevaluations demonstrate that Auffusion surpasses previous TTA approaches using\nlimited data and computational resource. Furthermore, previous studies in T2I\nrecognizes the significant impact of encoder choice on cross-modal alignment,\nlike fine-grained details and object bindings, while similar evaluation is\nlacking in prior TTA works. Through comprehensive ablation studies and\ninnovative cross-attention map visualizations, we provide insightful\nassessments of text-audio alignment in TTA. Our findings reveal Auffusion's\nsuperior capability in generating audios that accurately match textual\ndescriptions, which further demonstrated in several related tasks, such as\naudio style transfer, inpainting and other manipulations. Our implementation\nand demos are available at https://auffusion.github.io.\n","authors":["Jinlong Xue","Yayue Deng","Yingming Gao","Ya Li"],"pdf_url":"https://arxiv.org/pdf/2401.01044v1.pdf","comment":"Demo and implementation at https://auffusion.github.io"},{"id":"http://arxiv.org/abs/2312.04021v3","updated":"2024-01-02T05:10:27Z","published":"2023-12-07T03:37:39Z","title":"A Study on the Calibration of In-context Learning","summary":"  Accurate uncertainty quantification is crucial for the safe deployment of\nlanguage models (LMs), and prior research has demonstrated improvements in the\ncalibration of modern LMs. Our study focuses on in-context learning (ICL), a\nprevalent method for adapting static LMs through tailored prompts, and examines\nthe balance between performance and calibration across a broad spectrum of\nnatural language understanding and reasoning tasks. Through comprehensive\nexperiments, we observe that, with an increasing number of ICL examples, models\ninitially exhibit increased miscalibration before achieving better calibration\nand miscalibration tends to arise in low-shot settings. Moreover, we find that\nmethods aimed at improving usability, such as fine-tuning and chain-of-thought\n(CoT) prompting, can lead to miscalibration and unreliable natural language\nexplanations, suggesting that new methods may be required for scenarios where\nmodels are expected to be reliable.\n","authors":["Hanlin Zhang","Yi-Fan Zhang","Yaodong Yu","Dhruv Madeka","Dean Foster","Eric Xing","Himabindu Lakkaraju","Sham Kakade"],"pdf_url":"https://arxiv.org/pdf/2312.04021v3.pdf","comment":"Spotlight Talk at NeurIPS 2023 Workshop on Failure Modes in the Age\n  of Foundation Models"},{"id":"http://arxiv.org/abs/2310.09520v4","updated":"2024-01-02T00:04:13Z","published":"2023-10-14T07:19:47Z","title":"Reward-Augmented Decoding: Efficient Controlled Text Generation With a\n  Unidirectional Reward Model","summary":"  While large language models have proven effective in a huge range of\ndownstream applications, they often generate text that is problematic or lacks\na desired attribute. In this paper, we introduce Reward-Augmented Decoding\n(RAD), a text generation procedure that uses a small unidirectional reward\nmodel to encourage a language model to generate text that has certain\nproperties. Specifically, RAD uses the reward model to score generations as\nthey are produced and rescales sampling probabilities to favor high-reward\ntokens. By using a unidirectional reward model, RAD can cache activations from\nprior generation steps to decrease computational overhead. Through experiments\non generating non-toxic and sentiment-controlled text, we demonstrate that RAD\nperforms best among methods that change only the generation procedure and\nmatches the performance of state-of-the-art methods that involve re-training\nthe language model. We further validate that RAD is effective on very large\nlanguage models while incurring a minimal computational overhead.\n","authors":["Haikang Deng","Colin Raffel"],"pdf_url":"https://arxiv.org/pdf/2310.09520v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.08163v2","updated":"2024-01-02T23:00:41Z","published":"2023-09-15T05:19:39Z","title":"Self-Assessment Tests are Unreliable Measures of LLM Personality","summary":"  As large language models (LLM) evolve in their capabilities, various recent\nstudies have tried to quantify their behavior using psychological tools created\nto study human behavior. One such example is the measurement of \"personality\"\nof LLMs using self-assessment personality tests developed to measure human\npersonality. Yet almost none of these works verify the applicability of these\ntests on LLMs. In this paper, we analyze the reliability of LLM personality\nscores obtained from self-assessment personality tests using two simple\nexperiments. We first introduce the property of prompt sensitivity, where three\nsemantically equivalent prompts representing three intuitive ways of\nadministering self-assessment tests on LLMs are used to measure the personality\nof the same LLM. We find that all three prompts lead to very different\npersonality scores, a difference that is statistically significant for all\ntraits in a large majority of scenarios. We then introduce the property of\noption-order symmetry for personality measurement of LLMs. Since most of the\nself-assessment tests exist in the form of multiple choice question (MCQ)\nquestions, we argue that the scores should also be robust to not just the\nprompt template but also the order in which the options are presented. This\ntest unsurprisingly reveals that the self-assessment test scores are not robust\nto the order of the options. These simple tests, done on ChatGPT and three\nLlama2 models of different sizes, show that self-assessment personality tests\ncreated for humans are unreliable measures of personality in LLMs.\n","authors":["Akshat Gupta","Xiaoyang Song","Gopala Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2309.08163v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.19555v3","updated":"2024-01-02T22:30:00Z","published":"2023-05-31T04:50:29Z","title":"Large Language Models Are Not Strong Abstract Reasoners","summary":"  Large Language Models have shown tremendous performance on a large variety of\nnatural language processing tasks, ranging from text comprehension to common\nsense reasoning. However, the mechanisms responsible for this success remain\nopaque, and it is unclear whether LLMs can achieve human-like cognitive\ncapabilities or whether these models are still fundamentally circumscribed.\nAbstract reasoning is a fundamental task for cognition, consisting of finding\nand applying a general pattern from few data. Evaluating deep neural\narchitectures on this task could give insight into their potential limitations\nregarding reasoning and their broad generalisation abilities, yet this is\ncurrently an under-explored area. In this paper, we introduce a new benchmark\nfor evaluating language models beyond memorization on abstract reasoning tasks.\nWe perform extensive evaluations of state-of-the-art LLMs, showing that they\ncurrently achieve very limited performance in contrast with other natural\nlanguage tasks, even when applying techniques that have been shown to improve\nperformance on other NLP tasks. We argue that guiding LLM generation to follow\ncausal paths could help improve the generalisation and reasoning abilities of\nLLMs.\n","authors":["Gaël Gendron","Qiming Bao","Michael Witbrock","Gillian Dobbie"],"pdf_url":"https://arxiv.org/pdf/2305.19555v3.pdf","comment":"50 pages, 14 pages for the main paper and 36 pages for the\n  supplement, 35 figures, 17 tables. V3: performed additional experiments"},{"id":"http://arxiv.org/abs/2307.05134v2","updated":"2024-01-02T21:18:48Z","published":"2023-07-11T09:23:05Z","title":"TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation","summary":"  The progress in the generation of synthetic images has made it crucial to\nassess their quality. While several metrics have been proposed to assess the\nrendering of images, it is crucial for Text-to-Image (T2I) models, which\ngenerate images based on a prompt, to consider additional aspects such as to\nwhich extent the generated image matches the important content of the prompt.\nMoreover, although the generated images usually result from a random starting\npoint, the influence of this one is generally not considered. In this article,\nwe propose a new metric based on prompt templates to study the alignment\nbetween the content specified in the prompt and the corresponding generated\nimages. It allows us to better characterize the alignment in terms of the type\nof the specified objects, their number, and their color. We conducted a study\non several recent T2I models about various aspects. An additional interesting\nresult we obtained with our approach is that image quality can vary drastically\ndepending on the noise used as a seed for the images. We also quantify the\ninfluence of the number of concepts in the prompt, their order as well as their\n(color) attributes. Finally, our method allows us to identify some seeds that\nproduce better images than others, opening novel directions of research on this\nunderstudied topic.\n","authors":["Paul Grimal","Hervé Le Borgne","Olivier Ferret","Julien Tourille"],"pdf_url":"https://arxiv.org/pdf/2307.05134v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01419v1","updated":"2024-01-02T20:05:56Z","published":"2024-01-02T20:05:56Z","title":"To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine\n  Translation vs Human Translation","summary":"  We conduct a large-scale fine-grained comparative analysis of machine\ntranslations (MT) against human translations (HT) through the lens of\nmorphosyntactic divergence. Across three language pairs and two types of\ndivergence defined as the structural difference between the source and the\ntarget, MT is consistently more conservative than HT, with less morphosyntactic\ndiversity, more convergent patterns, and more one-to-one alignments. Through\nanalysis on different decoding algorithms, we attribute this discrepancy to the\nuse of beam search that biases MT towards more convergent patterns. This bias\nis most amplified when the convergent pattern appears around 50% of the time in\ntraining data. Lastly, we show that for a majority of morphosyntactic\ndivergences, their presence in HT is correlated with decreased MT performance,\npresenting a greater challenge for MT systems.\n","authors":["Jiaming Luo","Colin Cherry","George Foster"],"pdf_url":"https://arxiv.org/pdf/2401.01419v1.pdf","comment":"TACL, pre-MIT Press publication version"},{"id":"http://arxiv.org/abs/2401.01405v1","updated":"2024-01-02T19:00:17Z","published":"2024-01-02T19:00:17Z","title":"Quantifying the Uniqueness of Donald Trump in Presidential Discourse","summary":"  Does Donald Trump speak differently from other presidents? If so, in what\nways? Are these differences confined to any single medium of communication? To\ninvestigate these questions, this paper introduces a novel metric of uniqueness\nbased on large language models, develops a new lexicon for divisive speech, and\npresents a framework for comparing the lexical features of political opponents.\nApplying these tools to a variety of corpora of presidential speeches, we find\nconsiderable evidence that Trump's speech patterns diverge from those of all\nmajor party nominees for the presidency in recent history. Some notable\nfindings include Trump's employment of particularly divisive and antagonistic\nlanguage targeting of his political opponents and his patterns of repetition\nfor emphasis. Furthermore, Trump is significantly more distinctive than his\nfellow Republicans, whose uniqueness values are comparably closer to those of\nthe Democrats. These differences hold across a variety of measurement\nstrategies, arise on both the campaign trail and in official presidential\naddresses, and do not appear to be an artifact of secular time trends.\n","authors":["Karen Zhou","Alexander A. Meitus","Milo Chase","Grace Wang","Anne Mykland","William Howell","Chenhao Tan"],"pdf_url":"https://arxiv.org/pdf/2401.01405v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2401.01339v1","updated":"2024-01-02T18:59:55Z","published":"2024-01-02T18:59:55Z","title":"Street Gaussians for Modeling Dynamic Urban Scenes","summary":"  This paper aims to tackle the problem of modeling dynamic urban street scenes\nfrom monocular videos. Recent methods extend NeRF by incorporating tracked\nvehicle poses to animate vehicles, enabling photo-realistic view synthesis of\ndynamic urban street scenes. However, significant limitations are their slow\ntraining and rendering speed, coupled with the critical need for high precision\nin tracked vehicle poses. We introduce Street Gaussians, a new explicit scene\nrepresentation that tackles all these limitations. Specifically, the dynamic\nurban street is represented as a set of point clouds equipped with semantic\nlogits and 3D Gaussians, each associated with either a foreground vehicle or\nthe background. To model the dynamics of foreground object vehicles, each\nobject point cloud is optimized with optimizable tracked poses, along with a\ndynamic spherical harmonics model for the dynamic appearance. The explicit\nrepresentation allows easy composition of object vehicles and background, which\nin turn allows for scene editing operations and rendering at 133 FPS\n(1066$\\times$1600 resolution) within half an hour of training. The proposed\nmethod is evaluated on multiple challenging benchmarks, including KITTI and\nWaymo Open datasets. Experiments show that the proposed method consistently\noutperforms state-of-the-art methods across all datasets. Furthermore, the\nproposed representation delivers performance on par with that achieved using\nprecise ground-truth poses, despite relying only on poses from an off-the-shelf\ntracker. The code is available at https://zju3dv.github.io/street_gaussians/.\n","authors":["Yunzhi Yan","Haotong Lin","Chenxu Zhou","Weijie Wang","Haiyang Sun","Kun Zhan","Xianpeng Lang","Xiaowei Zhou","Sida Peng"],"pdf_url":"https://arxiv.org/pdf/2401.01339v1.pdf","comment":"Project page: https://zju3dv.github.io/street_gaussians/"},{"id":"http://arxiv.org/abs/2401.01303v1","updated":"2024-01-02T17:30:45Z","published":"2024-01-02T17:30:45Z","title":"Integrating Edges into U-Net Models with Explainable Activation Maps for\n  Brain Tumor Segmentation using MR Images","summary":"  Manual delineation of tumor regions from magnetic resonance (MR) images is\ntime-consuming, requires an expert, and is prone to human error. In recent\nyears, deep learning models have been the go-to approach for the segmentation\nof brain tumors. U-Net and its' variants for semantic segmentation of medical\nimages have achieved good results in the literature. However, U-Net and its'\nvariants tend to over-segment tumor regions and may not accurately segment the\ntumor edges. The edges of the tumor are as important as the tumor regions for\naccurate diagnosis, surgical precision, and treatment planning. In the proposed\nwork, the authors aim to extract edges from the ground truth using a\nderivative-like filter followed by edge reconstruction to obtain an edge ground\ntruth in addition to the brain tumor ground truth. Utilizing both ground\ntruths, the author studies several U-Net and its' variant architectures with\nand without tumor edges ground truth as a target along with the tumor ground\ntruth for brain tumor segmentation. The author used the BraTS2020 benchmark\ndataset to perform the study and the results are tabulated for the dice and\nHausdorff95 metrics. The mean and median metrics are calculated for the whole\ntumor (WT), tumor core (TC), and enhancing tumor (ET) regions. Compared to the\nbaseline U-Net and its variants, the models that learned edges along with the\ntumor regions performed well in core tumor regions in both training and\nvalidation datasets. The improved performance of edge-trained models trained on\nbaseline models like U-Net and V-Net achieved performance similar to baseline\nstate-of-the-art models like Swin U-Net and hybrid MR-U-Net. The edge-target\ntrained models are capable of generating edge maps that can be useful for\ntreatment planning. Additionally, for further explainability of the results,\nthe activation map generated by the hybrid MR-U-Net has been studied.\n","authors":["Subin Sahayam","Umarani Jayaraman"],"pdf_url":"https://arxiv.org/pdf/2401.01303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.17050v2","updated":"2024-01-02T17:24:53Z","published":"2023-12-28T14:51:45Z","title":"KeDuSR: Real-World Dual-Lens Super-Resolution via Kernel-Free Matching","summary":"  Dual-lens super-resolution (SR) is a practical scenario for reference (Ref)\nbased SR by utilizing the telephoto image (Ref) to assist the super-resolution\nof the low-resolution wide-angle image (LR input). Different from general\nRefSR, the Ref in dual-lens SR only covers the overlapped field of view (FoV)\narea. However, current dual-lens SR methods rarely utilize these specific\ncharacteristics and directly perform dense matching between the LR input and\nRef. Due to the resolution gap between LR and Ref, the matching may miss the\nbest-matched candidate and destroy the consistent structures in the overlapped\nFoV area. Different from them, we propose to first align the Ref with the\ncenter region (namely the overlapped FoV area) of the LR input by combining\nglobal warping and local warping to make the aligned Ref be sharp and\nconsistent. Then, we formulate the aligned Ref and LR center as value-key\npairs, and the corner region of the LR is formulated as queries. In this way,\nwe propose a kernel-free matching strategy by matching between the LR-corner\n(query) and LR-center (key) regions, and the corresponding aligned Ref (value)\ncan be warped to the corner region of the target. Our kernel-free matching\nstrategy avoids the resolution gap between LR and Ref, which makes our network\nhave better generalization ability. In addition, we construct a DuSR-Real\ndataset with (LR, Ref, HR) triples, where the LR and HR are well aligned.\nExperiments on three datasets demonstrate that our method outperforms the\nsecond-best method by a large margin. Our code and dataset are available at\nhttps://github.com/ZifanCui/KeDuSR.\n","authors":["Huanjing Yue","Zifan Cui","Kun Li","Jingyu Yang"],"pdf_url":"https://arxiv.org/pdf/2312.17050v2.pdf","comment":"14 pages, 10 figures. Accepted by AAAI-2024"},{"id":"http://arxiv.org/abs/2401.01288v1","updated":"2024-01-02T16:56:13Z","published":"2024-01-02T16:56:13Z","title":"Physics-informed Generalizable Wireless Channel Modeling with\n  Segmentation and Deep Learning: Fundamentals, Methodologies, and Challenges","summary":"  Channel modeling is fundamental in advancing wireless systems and has thus\nattracted considerable research focus. Recent trends have seen a growing\nreliance on data-driven techniques to facilitate the modeling process and yield\naccurate channel predictions. In this work, we first provide a concise overview\nof data-driven channel modeling methods, highlighting their limitations.\nSubsequently, we introduce the concept and advantages of physics-informed\nneural network (PINN)-based modeling and a summary of recent contributions in\nthis area. Our findings demonstrate that PINN-based approaches in channel\nmodeling exhibit promising attributes such as generalizability,\ninterpretability, and robustness. We offer a comprehensive architecture for\nPINN methodology, designed to inform and inspire future model development. A\ncase-study of our recent work on precise indoor channel prediction with\nsemantic segmentation and deep learning is presented. The study concludes by\naddressing the challenges faced and suggesting potential research directions in\nthis field.\n","authors":["Ethan Zhu","Haijian Sun","Mingyue Ji"],"pdf_url":"https://arxiv.org/pdf/2401.01288v1.pdf","comment":"Submitted to IEEE Magazine for potential future publications"},{"id":"http://arxiv.org/abs/2401.01286v1","updated":"2024-01-02T16:54:58Z","published":"2024-01-02T16:54:58Z","title":"A Comprehensive Study of Knowledge Editing for Large Language Models","summary":"  Large Language Models (LLMs) have shown extraordinary capabilities in\nunderstanding and generating text that closely mirrors human communication.\nHowever, a primary limitation lies in the significant computational demands\nduring training, arising from their extensive parameterization. This challenge\nis further intensified by the dynamic nature of the world, necessitating\nfrequent updates to LLMs to correct outdated information or integrate new\nknowledge, thereby ensuring their continued relevance. Note that many\napplications demand continual model adjustments post-training to address\ndeficiencies or undesirable behaviors. There is an increasing interest in\nefficient, lightweight methods for on-the-fly model modifications. To this end,\nrecent years have seen a burgeoning in the techniques of knowledge editing for\nLLMs, which aim to efficiently modify LLMs' behaviors within specific domains\nwhile preserving overall performance across various inputs. In this paper, we\nfirst define the knowledge editing problem and then provide a comprehensive\nreview of cutting-edge approaches. Drawing inspiration from educational and\ncognitive research theories, we propose a unified categorization criterion that\nclassifies knowledge editing methods into three groups: resorting to external\nknowledge, merging knowledge into the model, and editing intrinsic knowledge.\nFurthermore, we introduce a new benchmark, KnowEdit, for a comprehensive\nempirical evaluation of representative knowledge editing approaches.\nAdditionally, we provide an in-depth analysis of knowledge location, which can\nprovide a deeper understanding of the knowledge structures inherent within\nLLMs. Finally, we discuss several potential applications of knowledge editing,\noutlining its broad and impactful implications.\n","authors":["Ningyu Zhang","Yunzhi Yao","Bozhong Tian","Peng Wang","Shumin Deng","Mengru Wang","Zekun Xi","Shengyu Mao","Jintian Zhang","Yuansheng Ni","Siyuan Cheng","Ziwen Xu","Xin Xu","Jia-Chen Gu","Yong Jiang","Pengjun Xie","Fei Huang","Lei Liang","Zhiqiang Zhang","Xiaowei Zhu","Jun Zhou","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2401.01286v1.pdf","comment":"Ongoing work; 50 pages, 265 citations; benchmark is available at\n  https://huggingface.co/datasets/zjunlp/KnowEdit; code is available at\n  https://github.com/zjunlp/EasyEdit; paper list is available at\n  https://github.com/zjunlp/KnowledgeEditingPapers"},{"id":"http://arxiv.org/abs/2401.01272v1","updated":"2024-01-02T16:17:43Z","published":"2024-01-02T16:17:43Z","title":"MOC-RVQ: Multilevel Codebook-assisted Digital Generative Semantic\n  Communication","summary":"  Vector quantization-based image semantic communication systems have\nsuccessfully boosted transmission efficiency, but face a challenge with\nconflicting requirements between codebook design and digital constellation\nmodulation. Traditional codebooks need a wide index range, while modulation\nfavors few discrete states. To address this, we propose a multilevel generative\nsemantic communication system with a two-stage training framework. In the first\nstage, we train a high-quality codebook, using a multi-head octonary codebook\n(MOC) to compress the index range. We also integrate a residual vector\nquantization (RVQ) mechanism for effective multilevel communication. In the\nsecond stage, a noise reduction block (NRB) based on Swin Transformer is\nintroduced, coupled with the multilevel codebook from the first stage, serving\nas a high-quality semantic knowledge base (SKB) for generative feature\nrestoration. Experimental results highlight MOC-RVQ's superior performance over\nmethods like BPG or JPEG, even without channel error correction coding.\n","authors":["Yingbin Zhou","Yaping Sun","Guanying Chen","Xiaodong Xu","Hao Chen","Binhong Huang","Shuguang Cui","Ping Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.01272v1.pdf","comment":"6 pages, 5 figures"},{"id":"http://arxiv.org/abs/2104.02206v8","updated":"2024-01-02T16:12:32Z","published":"2021-04-06T00:53:01Z","title":"Tuned Compositional Feature Replays for Efficient Stream Learning","summary":"  Our brains extract durable, generalizable knowledge from transient\nexperiences of the world. Artificial neural networks come nowhere close to this\nability. When tasked with learning to classify objects by training on\nnon-repeating video frames in temporal order (online stream learning), models\nthat learn well from shuffled datasets catastrophically forget old knowledge\nupon learning new stimuli. We propose a new continual learning algorithm,\nCompositional Replay Using Memory Blocks (CRUMB), which mitigates forgetting by\nreplaying feature maps reconstructed by combining generic parts. CRUMB\nconcatenates trainable and re-usable \"memory block\" vectors to compositionally\nreconstruct feature map tensors in convolutional neural networks. Storing the\nindices of memory blocks used to reconstruct new stimuli enables memories of\nthe stimuli to be replayed during later tasks. This reconstruction mechanism\nalso primes the neural network to minimize catastrophic forgetting by biasing\nit towards attending to information about object shapes more than information\nabout image textures, and stabilizes the network during stream learning by\nproviding a shared feature-level basis for all training examples. These\nproperties allow CRUMB to outperform an otherwise identical algorithm that\nstores and replays raw images, while occupying only 3.6% as much memory. We\nstress-tested CRUMB alongside 13 competing methods on 7 challenging datasets.\nTo address the limited number of existing online stream learning datasets, we\nintroduce 2 new benchmarks by adapting existing datasets for stream learning.\nWith only 3.7-4.1% as much memory and 15-43% as much runtime, CRUMB mitigates\ncatastrophic forgetting more effectively than the state-of-the-art. Our code is\navailable at https://github.com/MorganBDT/crumb.git.\n","authors":["Morgan B. Talbot","Rushikesh Zawar","Rohil Badkundri","Mengmi Zhang","Gabriel Kreiman"],"pdf_url":"https://arxiv.org/pdf/2104.02206v8.pdf","comment":"Copyright 2023 IEEE. The journal version of this article is hosted at\n  https://ieeexplore.ieee.org/document/10373937 and\n  https://klab.tch.harvard.edu/publications/PDFs/gk8019.pdf"},{"id":"http://arxiv.org/abs/2401.01256v1","updated":"2024-01-02T15:56:48Z","published":"2024-01-02T15:56:48Z","title":"VideoDrafter: Content-Consistent Multi-Scene Video Generation with LLM","summary":"  The recent innovations and breakthroughs in diffusion models have\nsignificantly expanded the possibilities of generating high-quality videos for\nthe given prompts. Most existing works tackle the single-scene scenario with\nonly one video event occurring in a single background. Extending to generate\nmulti-scene videos nevertheless is not trivial and necessitates to nicely\nmanage the logic in between while preserving the consistent visual appearance\nof key content across video scenes. In this paper, we propose a novel\nframework, namely VideoDrafter, for content-consistent multi-scene video\ngeneration. Technically, VideoDrafter leverages Large Language Models (LLM) to\nconvert the input prompt into comprehensive multi-scene script that benefits\nfrom the logical knowledge learnt by LLM. The script for each scene includes a\nprompt describing the event, the foreground/background entities, as well as\ncamera movement. VideoDrafter identifies the common entities throughout the\nscript and asks LLM to detail each entity. The resultant entity description is\nthen fed into a text-to-image model to generate a reference image for each\nentity. Finally, VideoDrafter outputs a multi-scene video by generating each\nscene video via a diffusion process that takes the reference images, the\ndescriptive prompt of the event and camera movement into account. The diffusion\nmodel incorporates the reference images as the condition and alignment to\nstrengthen the content consistency of multi-scene videos. Extensive experiments\ndemonstrate that VideoDrafter outperforms the SOTA video generation models in\nterms of visual quality, content consistency, and user preference.\n","authors":["Fuchen Long","Zhaofan Qiu","Ting Yao","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2401.01256v1.pdf","comment":"Project website: https://videodrafter.github.io"},{"id":"http://arxiv.org/abs/2203.01923v6","updated":"2024-01-02T15:38:20Z","published":"2022-03-03T18:56:08Z","title":"Recovering 3D Human Mesh from Monocular Images: A Survey","summary":"  Estimating human pose and shape from monocular images is a long-standing\nproblem in computer vision. Since the release of statistical body models, 3D\nhuman mesh recovery has been drawing broader attention. With the same goal of\nobtaining well-aligned and physically plausible mesh results, two paradigms\nhave been developed to overcome challenges in the 2D-to-3D lifting process: i)\nan optimization-based paradigm, where different data terms and regularization\nterms are exploited as optimization objectives; and ii) a regression-based\nparadigm, where deep learning techniques are embraced to solve the problem in\nan end-to-end fashion. Meanwhile, continuous efforts are devoted to improving\nthe quality of 3D mesh labels for a wide range of datasets. Though remarkable\nprogress has been achieved in the past decade, the task is still challenging\ndue to flexible body motions, diverse appearances, complex environments, and\ninsufficient in-the-wild annotations. To the best of our knowledge, this is the\nfirst survey that focuses on the task of monocular 3D human mesh recovery. We\nstart with the introduction of body models and then elaborate recovery\nframeworks and training objectives by providing in-depth analyses of their\nstrengths and weaknesses. We also summarize datasets, evaluation metrics, and\nbenchmark results. Open issues and future directions are discussed in the end,\nhoping to motivate researchers and facilitate their research in this area. A\nregularly updated project page can be found at\nhttps://github.com/tinatiansjz/hmr-survey.\n","authors":["Yating Tian","Hongwen Zhang","Yebin Liu","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2203.01923v6.pdf","comment":"Published in IEEE TPAMI, Survey on monocular 3D human mesh recovery,\n  Project page: https://github.com/tinatiansjz/hmr-survey"},{"id":"http://arxiv.org/abs/2305.07490v5","updated":"2024-01-02T15:29:53Z","published":"2023-05-12T14:04:30Z","title":"ArtGPT-4: Towards Artistic-understanding Large Vision-Language Models\n  with Enhanced Adapter","summary":"  In recent years, advancements in large language models have been remarkable,\nwith models such as ChatGPT demonstrating exceptional proficiency in diverse\nlinguistic tasks. The pre-training of large models with billions of parameters,\nposes a formidable challenge, primarily due to the scarcity of datasets of a\ncommensurate scale for effective training. Nevertheless, innovative strategies\nhave emerged, including methods to fine-tune these pre-trained models using\nfewer parameters set, as evidenced by models like MiniGPT-4 and LLaVA. Despite\ntheir potential in various domains, these models remain limited in their\nunderstanding of artistic imagery. They have yet to fully grasp the intricate\nnuances of art images or to provide an objective articulation of the emotions\nthey evoke, in a manner akin to human perception. This work introduces\nArtGPT-4, a pioneering large vision-language model tailored to address the\ndeficiencies of contemporary models in artistic comprehension. ArtGPT-4\nunderwent training on image-text pairs utilizing a Tesla A100 device in a mere\n2 hours, with a dataset comprising approximately 0.52M entries. Impressively,\nthe model can render images with an artistic-understanding and convey the\nemotions they inspire, mirroring human interpretation. Additionally, this work\npresents a unique dataset designed to evaluate the efficacy of vision-language\nmodels. In subsequent evaluations, ArtGPT-4 not only achieved state-of-the-art\nperformance on the ArtEmis and ArtEmis-v2.0 datasets but also exceeded the\nestablished benchmarks introduced in This study, lagging behind professional\nartists' descriptions by a negligible 0.15 points on a 6-point scale. The code\nand the pre-trained model are accessible in\nhttps://huggingface.co/Tyrannosaurus/ArtGPT-4.\n","authors":["Zhengqing Yuan","Xinyi Wang","Kun Wang","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2305.07490v5.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2401.01247v1","updated":"2024-01-02T15:23:09Z","published":"2024-01-02T15:23:09Z","title":"Deep Learning-Based Computational Model for Disease Identification in\n  Cocoa Pods (Theobroma cacao L.)","summary":"  The early identification of diseases in cocoa pods is an important task to\nguarantee the production of high-quality cocoa. The use of artificial\nintelligence techniques such as machine learning, computer vision and deep\nlearning are promising solutions to help identify and classify diseases in\ncocoa pods. In this paper we introduce the development and evaluation of a deep\nlearning computational model applied to the identification of diseases in cocoa\npods, focusing on \"monilia\" and \"black pod\" diseases. An exhaustive review of\nstate-of-the-art of computational models was carried out, based on scientific\narticles related to the identification of plant diseases using computer vision\nand deep learning techniques. As a result of the search, EfficientDet-Lite4, an\nefficient and lightweight model for object detection, was selected. A dataset,\nincluding images of both healthy and diseased cocoa pods, has been utilized to\ntrain the model to detect and pinpoint disease manifestations with considerable\naccuracy. Significant enhancements in the model training and evaluation\ndemonstrate the capability of recognizing and classifying diseases through\nimage analysis. Furthermore, the functionalities of the model were integrated\ninto an Android native mobile with an user-friendly interface, allowing to\nyounger or inexperienced farmers a fast and accuracy identification of health\nstatus of cocoa pods\n","authors":["Darlyn Buenaño Vera","Byron Oviedo","Washington Chiriboga Casanova","Cristian Zambrano-Vega"],"pdf_url":"https://arxiv.org/pdf/2401.01247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01244v1","updated":"2024-01-02T15:20:50Z","published":"2024-01-02T15:20:50Z","title":"Temporal Adaptive RGBT Tracking with Modality Prompt","summary":"  RGBT tracking has been widely used in various fields such as robotics,\nsurveillance processing, and autonomous driving. Existing RGBT trackers fully\nexplore the spatial information between the template and the search region and\nlocate the target based on the appearance matching results. However, these RGBT\ntrackers have very limited exploitation of temporal information, either\nignoring temporal information or exploiting it through online sampling and\ntraining. The former struggles to cope with the object state changes, while the\nlatter neglects the correlation between spatial and temporal information. To\nalleviate these limitations, we propose a novel Temporal Adaptive RGBT Tracking\nframework, named as TATrack. TATrack has a spatio-temporal two-stream structure\nand captures temporal information by an online updated template, where the\ntwo-stream structure refers to the multi-modal feature extraction and\ncross-modal interaction for the initial template and the online update template\nrespectively. TATrack contributes to comprehensively exploit spatio-temporal\ninformation and multi-modal information for target localization. In addition,\nwe design a spatio-temporal interaction (STI) mechanism that bridges two\nbranches and enables cross-modal interaction to span longer time scales.\nExtensive experiments on three popular RGBT tracking benchmarks show that our\nmethod achieves state-of-the-art performance, while running at real-time speed.\n","authors":["Hongyu Wang","Xiaotao Liu","Yifan Li","Meng Sun","Dian Yuan","Jing Liu"],"pdf_url":"https://arxiv.org/pdf/2401.01244v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10144v2","updated":"2024-01-02T15:16:36Z","published":"2023-12-15T19:00:07Z","title":"Data-Efficient Multimodal Fusion on a Single GPU","summary":"  The goal of multimodal alignment is to learn a single latent space that is\nshared between multimodal inputs. The most powerful models in this space have\nbeen trained using massive datasets of paired inputs and large-scale\ncomputational resources, making them prohibitively expensive to train in many\npractical scenarios. We surmise that existing unimodal encoders pre-trained on\nlarge amounts of unimodal data should provide an effective bootstrap to create\nmultimodal models from unimodal ones at much lower costs. We therefore propose\nFuseMix, a multimodal augmentation scheme that operates on the latent spaces of\narbitrary pre-trained unimodal encoders. Using FuseMix for multimodal\nalignment, we achieve competitive performance -- and in certain cases\noutperform state-of-the art methods -- in both image-text and audio-text\nretrieval, with orders of magnitude less compute and data: for example, we\noutperform CLIP on the Flickr30K text-to-image retrieval task with $\\sim \\!\n600\\times$ fewer GPU days and $\\sim \\! 80\\times$ fewer image-text pairs.\nAdditionally, we show how our method can be applied to convert pre-trained\ntext-to-image generative models into audio-to-image ones. Code is available at:\nhttps://github.com/layer6ai-labs/fusemix.\n","authors":["Noël Vouitsis","Zhaoyan Liu","Satya Krishna Gorti","Valentin Villecroze","Jesse C. Cresswell","Guangwei Yu","Gabriel Loaiza-Ganem","Maksims Volkovs"],"pdf_url":"https://arxiv.org/pdf/2312.10144v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.06950v8","updated":"2024-01-02T14:49:22Z","published":"2022-09-14T21:53:27Z","title":"Lossy Image Compression with Conditional Diffusion Models","summary":"  This paper outlines an end-to-end optimized lossy image compression framework\nusing diffusion generative models. The approach relies on the transform coding\nparadigm, where an image is mapped into a latent space for entropy coding and,\nfrom there, mapped back to the data space for reconstruction. In contrast to\nVAE-based neural compression, where the (mean) decoder is a deterministic\nneural network, our decoder is a conditional diffusion model. Our approach thus\nintroduces an additional ``content'' latent variable on which the reverse\ndiffusion process is conditioned and uses this variable to store information\nabout the image. The remaining ``texture'' variables characterizing the\ndiffusion process are synthesized at decoding time. We show that the model's\nperformance can be tuned toward perceptual metrics of interest. Our extensive\nexperiments involving multiple datasets and image quality assessment metrics\nshow that our approach yields stronger reported FID scores than the GAN-based\nmodel, while also yielding competitive performance with VAE-based models in\nseveral distortion metrics. Furthermore, training the diffusion with\n$\\mathcal{X}$-parameterization enables high-quality reconstructions in only a\nhandful of decoding steps, greatly affecting the model's practicality. Our code\nis available at: \\url{https://github.com/buggyyang/CDC_compression}\n","authors":["Ruihan Yang","Stephan Mandt"],"pdf_url":"https://arxiv.org/pdf/2209.06950v8.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01227v1","updated":"2024-01-02T14:36:28Z","published":"2024-01-02T14:36:28Z","title":"IdentiFace : A VGG Based Multimodal Facial Biometric System","summary":"  The development of facial biometric systems has contributed greatly to the\ndevelopment of the computer vision field. Nowadays, there's always a need to\ndevelop a multimodal system that combines multiple biometric traits in an\nefficient, meaningful way. In this paper, we introduce \"IdentiFace\" which is a\nmultimodal facial biometric system that combines the core of facial recognition\nwith some of the most important soft biometric traits such as gender, face\nshape, and emotion. We also focused on developing the system using only VGG-16\ninspired architecture with minor changes across different subsystems. This\nunification allows for simpler integration across modalities. It makes it\neasier to interpret the learned features between the tasks which gives a good\nindication about the decision-making process across the facial modalities and\npotential connection. For the recognition problem, we acquired a 99.2% test\naccuracy for five classes with high intra-class variations using data collected\nfrom the FERET database[1]. We achieved 99.4% on our dataset and 95.15% on the\npublic dataset[2] in the gender recognition problem. We were also able to\nachieve a testing accuracy of 88.03% in the face-shape problem using the\ncelebrity face-shape dataset[3]. Finally, we achieved a decent testing accuracy\nof 66.13% in the emotion task which is considered a very acceptable accuracy\ncompared to related work on the FER2013 dataset[4].\n","authors":["Mahmoud Rabea","Hanya Ahmed","Sohaila Mahmoud","Nourhan Sayed"],"pdf_url":"https://arxiv.org/pdf/2401.01227v1.pdf","comment":"12 pages, 22 figures and 9 images"},{"id":"http://arxiv.org/abs/2304.08965v5","updated":"2024-01-02T14:32:16Z","published":"2023-04-18T12:58:21Z","title":"PointDC:Unsupervised Semantic Segmentation of 3D Point Clouds via\n  Cross-modal Distillation and Super-Voxel Clustering","summary":"  Semantic segmentation of point clouds usually requires exhausting efforts of\nhuman annotations, hence it attracts wide attention to the challenging topic of\nlearning from unlabeled or weaker forms of annotations. In this paper, we take\nthe first attempt for fully unsupervised semantic segmentation of point clouds,\nwhich aims to delineate semantically meaningful objects without any form of\nannotations. Previous works of unsupervised pipeline on 2D images fails in this\ntask of point clouds, due to: 1) Clustering Ambiguity caused by limited\nmagnitude of data and imbalanced class distribution; 2) Irregularity Ambiguity\ncaused by the irregular sparsity of point cloud. Therefore, we propose a novel\nframework, PointDC, which is comprised of two steps that handle the\naforementioned problems respectively: Cross-Modal Distillation (CMD) and\nSuper-Voxel Clustering (SVC). In the first stage of CMD, multi-view visual\nfeatures are back-projected to the 3D space and aggregated to a unified point\nfeature to distill the training of the point representation. In the second\nstage of SVC, the point features are aggregated to super-voxels and then fed to\nthe iterative clustering process for excavating semantic classes. PointDC\nyields a significant improvement over the prior state-of-the-art unsupervised\nmethods, on both the ScanNet-v2 (+18.4 mIoU) and S3DIS (+11.5 mIoU) semantic\nsegmentation benchmarks.\n","authors":["Zisheng Chen","Hongbin Xu","Weitao Chen","Zhipeng Zhou","Haihong Xiao","Baigui Sun","Xuansong Xie","Wenxiong Kang"],"pdf_url":"https://arxiv.org/pdf/2304.08965v5.pdf","comment":"Accepted by International Conference on Computer Vision (ICCV) 2023"},{"id":"http://arxiv.org/abs/2309.10399v3","updated":"2024-01-02T14:24:42Z","published":"2023-09-19T08:00:26Z","title":"Exploiting Causality Signals in Medical Images: A Pilot Study with\n  Empirical Results","summary":"  We present a novel technique to discover and exploit weak causal signals\ndirectly from images via neural networks for classification purposes. This way,\nwe model how the presence of a feature in one part of the image affects the\nappearance of another feature in a different part of the image. Our method\nconsists of a convolutional neural network backbone and a causality-factors\nextractor module, which computes weights to enhance each feature map according\nto its causal influence in the scene. We develop different architecture\nvariants and empirically evaluate all the models on two public datasets of\nprostate MRI images and breast histopathology slides for cancer diagnosis. We\nstudy the effectiveness of our module both in fully-supervised and few-shot\nlearning, we assess its addition to existing attention-based solutions, we\nconduct ablation studies, and investigate the explainability of our models via\nclass activation maps. Our findings show that our lightweight block extracts\nmeaningful information and improves the overall classification, together with\nproducing more robust predictions that focus on relevant parts of the image.\nThat is crucial in medical imaging, where accurate and reliable classifications\nare essential for effective diagnosis and treatment planning.\n","authors":["Gianluca Carloni","Sara Colantonio"],"pdf_url":"https://arxiv.org/pdf/2309.10399v3.pdf","comment":"Added experiments in which we integrate our Mulcat module to existing\n  models using Bottleneck Attention Modules, and added experiments in Few-Shot\n  Learning; 19 pages"},{"id":"http://arxiv.org/abs/2401.01219v1","updated":"2024-01-02T14:18:11Z","published":"2024-01-02T14:18:11Z","title":"Distribution Matching for Multi-Task Learning of Classification Tasks: a\n  Large-Scale Study on Faces & Beyond","summary":"  Multi-Task Learning (MTL) is a framework, where multiple related tasks are\nlearned jointly and benefit from a shared representation space, or parameter\ntransfer. To provide sufficient learning support, modern MTL uses annotated\ndata with full, or sufficiently large overlap across tasks, i.e., each input\nsample is annotated for all, or most of the tasks. However, collecting such\nannotations is prohibitive in many real applications, and cannot benefit from\ndatasets available for individual tasks. In this work, we challenge this setup\nand show that MTL can be successful with classification tasks with little, or\nnon-overlapping annotations, or when there is big discrepancy in the size of\nlabeled data per task. We explore task-relatedness for co-annotation and\nco-training, and propose a novel approach, where knowledge exchange is enabled\nbetween the tasks via distribution matching. To demonstrate the general\napplicability of our method, we conducted diverse case studies in the domains\nof affective computing, face recognition, species recognition, and shopping\nitem classification using nine datasets. Our large-scale study of affective\ntasks for basic expression recognition and facial action unit detection\nillustrates that our approach is network agnostic and brings large performance\nimprovements compared to the state-of-the-art in both tasks and across all\nstudied databases. In all case studies, we show that co-training via\ntask-relatedness is advantageous and prevents negative transfer (which occurs\nwhen MT model's performance is worse than that of at least one single-task\nmodel).\n","authors":["Dimitrios Kollias","Viktoriia Sharmanska","Stefanos Zafeiriou"],"pdf_url":"https://arxiv.org/pdf/2401.01219v1.pdf","comment":"accepted at AAAI 2024. arXiv admin note: text overlap with\n  arXiv:2105.03790"},{"id":"http://arxiv.org/abs/2306.11300v5","updated":"2024-01-02T14:18:02Z","published":"2023-06-20T05:30:59Z","title":"RS5M and GeoRSCLIP: A Large Scale Vision-Language Dataset and A Large\n  Vision-Language Model for Remote Sensing","summary":"  Pre-trained Vision-Language Models (VLMs) utilizing extensive image-text\npaired data have demonstrated unprecedented image-text association\ncapabilities, achieving remarkable results across various downstream tasks. A\ncritical challenge is how to make use of existing large-scale pre-trained VLMs,\nwhich are trained on common objects, to perform the domain-specific transfer\nfor accomplishing domain-related downstream tasks. A critical challenge is how\nto make use of existing large-scale pre-trained VLMs, which are trained on\ncommon objects, to perform the domain-specific transfer for accomplishing\ndomain-related downstream tasks. In this paper, we propose a new framework that\nincludes the Domain pre-trained Vision-Language Model (DVLM), bridging the gap\nbetween the General Vision-Language Model (GVLM) and domain-specific downstream\ntasks. Moreover, we present an image-text paired dataset in the field of remote\nsensing (RS), RS5M, which has 5 million RS images with English descriptions.\nThe dataset is obtained from filtering publicly available image-text paired\ndatasets and captioning label-only RS datasets with pre-trained VLM. These\nconstitute the first large-scale RS image-text paired dataset. Additionally, we\nfine-tuned the CLIP model and tried several Parameter-Efficient Fine-Tuning\nmethods on RS5M to implement the DVLM. Experimental results show that our\nproposed dataset is highly effective for various tasks, and our model GeoRSCLIP\nimproves upon the baseline or previous state-of-the-art model by $3\\%\\sim20\\%$\nin Zero-shot Classification (ZSC), $3\\%\\sim6\\%$ in Remote Sensing Cross-Modal\nText-Image Retrieval (RSCTIR) and $4\\%\\sim5\\%$ in Semantic Localization (SeLo)\ntasks. Dataset and models have been released in:\n\\url{https://github.com/om-ai-lab/RS5M}.\n","authors":["Zilun Zhang","Tiancheng Zhao","Yulong Guo","Jianwei Yin"],"pdf_url":"https://arxiv.org/pdf/2306.11300v5.pdf","comment":"RS5M dataset v5"},{"id":"http://arxiv.org/abs/2401.01216v1","updated":"2024-01-02T14:10:21Z","published":"2024-01-02T14:10:21Z","title":"Noise-NeRF: Hide Information in Neural Radiance Fields using Trainable\n  Noise","summary":"  Neural radiance fields (NeRF) have been proposed as an innovative 3D\nrepresentation method. While attracting lots of attention, NeRF faces critical\nissues such as information confidentiality and security. Steganography is a\ntechnique used to embed information in another object as a means of protecting\ninformation security. Currently, there are few related studies on NeRF\nsteganography, facing challenges in low steganography quality, model weight\ndamage, and a limited amount of steganographic information. This paper proposes\na novel NeRF steganography method based on trainable noise: Noise-NeRF.\nFurthermore, we propose the Adaptive Pixel Selection strategy and Pixel\nPerturbation strategy to improve the steganography quality and efficiency. The\nextensive experiments on open-source datasets show that Noise-NeRF provides\nstate-of-the-art performances in both steganography quality and rendering\nquality, as well as effectiveness in super-resolution image steganography.\n","authors":["Qinglong Huang","Yong Liao","Yanbin Hao","Pengyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2401.01216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01214v1","updated":"2024-01-02T14:04:42Z","published":"2024-01-02T14:04:42Z","title":"YOLO algorithm with hybrid attention feature pyramid network for solder\n  joint defect detection","summary":"  Traditional manual detection for solder joint defect is no longer applied\nduring industrial production due to low efficiency, inconsistent evaluation,\nhigh cost and lack of real-time data. A new approach has been proposed to\naddress the issues of low accuracy, high false detection rates and\ncomputational cost of solder joint defect detection in surface mount technology\nof industrial scenarios. The proposed solution is a hybrid attention mechanism\ndesigned specifically for the solder joint defect detection algorithm to\nimprove quality control in the manufacturing process by increasing the accuracy\nwhile reducing the computational cost. The hybrid attention mechanism comprises\na proposed enhanced multi-head self-attention and coordinate attention\nmechanisms increase the ability of attention networks to perceive contextual\ninformation and enhances the utilization range of network features. The\ncoordinate attention mechanism enhances the connection between different\nchannels and reduces location information loss. The hybrid attention mechanism\nenhances the capability of the network to perceive long-distance position\ninformation and learn local features. The improved algorithm model has good\ndetection ability for solder joint defect detection, with mAP reaching 91.5%,\n4.3% higher than the You Only Look Once version 5 algorithm and better than\nother comparative algorithms. Compared to other versions, mean Average\nPrecision, Precision, Recall, and Frame per Seconds indicators have also\nimproved. The improvement of detection accuracy can be achieved while meeting\nreal-time detection requirements.\n","authors":["Li Ang","Siti Khatijah Nor Abdul Rahim","Raseeda Hamzah","Raihah Aminuddin","Gao Yousheng"],"pdf_url":"https://arxiv.org/pdf/2401.01214v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.04718v2","updated":"2024-01-02T13:56:50Z","published":"2022-11-09T07:23:28Z","title":"On the Application of Efficient Neural Mapping to Real-Time Indoor\n  Localisation for Unmanned Ground Vehicles","summary":"  Global localisation from visual data is a challenging problem applicable to\nmany robotics domains. Prior works have shown that neural networks can be\ntrained to map images of an environment to absolute camera pose within that\nenvironment, learning an implicit neural mapping in the process. In this work\nwe evaluate the applicability of such an approach to real-world robotics\nscenarios, demonstrating that by constraining the problem to 2-dimensions and\nsignificantly increasing the quantity of training data, a compact model capable\nof real-time inference on embedded platforms can be used to achieve\nlocalisation accuracy of several centimetres. We deploy our trained model\nonboard a UGV platform, demonstrating its effectiveness in a waypoint\nnavigation task, wherein it is able to localise with a mean accuracy of 9cm at\na rate of 6fps running on the UGV onboard CPU, 35fps on an embedded GPU, or\n220fps on a desktop GPU. Along with this work we will release a novel\nlocalisation dataset comprising simulated and real environments, each with\ntraining samples numbering in the tens of thousands.\n","authors":["Christopher J. Holder","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2211.04718v2.pdf","comment":"\\c{opyright} 2024 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2401.00616v2","updated":"2024-01-02T13:47:19Z","published":"2024-01-01T00:08:39Z","title":"GD^2-NeRF: Generative Detail Compensation via GAN and Diffusion for\n  One-shot Generalizable Neural Radiance Fields","summary":"  In this paper, we focus on the One-shot Novel View Synthesis (O-NVS) task\nwhich targets synthesizing photo-realistic novel views given only one reference\nimage per scene. Previous One-shot Generalizable Neural Radiance Fields\n(OG-NeRF) methods solve this task in an inference-time finetuning-free manner,\nyet suffer the blurry issue due to the encoder-only architecture that highly\nrelies on the limited reference image. On the other hand, recent\ndiffusion-based image-to-3d methods show vivid plausible results via distilling\npre-trained 2D diffusion models into a 3D representation, yet require tedious\nper-scene optimization. Targeting these issues, we propose the GD$^2$-NeRF, a\nGenerative Detail compensation framework via GAN and Diffusion that is both\ninference-time finetuning-free and with vivid plausible details. In detail,\nfollowing a coarse-to-fine strategy, GD$^2$-NeRF is mainly composed of a\nOne-stage Parallel Pipeline (OPP) and a 3D-consistent Detail Enhancer\n(Diff3DE). At the coarse stage, OPP first efficiently inserts the GAN model\ninto the existing OG-NeRF pipeline for primarily relieving the blurry issue\nwith in-distribution priors captured from the training dataset, achieving a\ngood balance between sharpness (LPIPS, FID) and fidelity (PSNR, SSIM). Then, at\nthe fine stage, Diff3DE further leverages the pre-trained image diffusion\nmodels to complement rich out-distribution details while maintaining decent 3D\nconsistency. Extensive experiments on both the synthetic and real-world\ndatasets show that GD$^2$-NeRF noticeably improves the details while without\nper-scene finetuning.\n","authors":["Xiao Pan","Zongxin Yang","Shuai Bai","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2401.00616v2.pdf","comment":"Reading with Macbook Preview is recommended for best quality;\n  Submitted to Journal"},{"id":"http://arxiv.org/abs/2401.01208v1","updated":"2024-01-02T13:31:51Z","published":"2024-01-02T13:31:51Z","title":"FGENet: Fine-Grained Extraction Network for Congested Crowd Counting","summary":"  Crowd counting has gained significant popularity due to its practical\napplications. However, mainstream counting methods ignore precise individual\nlocalization and suffer from annotation noise because of counting from\nestimating density maps. Additionally, they also struggle with high-density\nimages.To address these issues, we propose an end-to-end model called\nFine-Grained Extraction Network (FGENet). Different from methods estimating\ndensity maps, FGENet directly learns the original coordinate points that\nrepresent the precise localization of individuals.This study designs a fusion\nmodule, named Fine-Grained Feature Pyramid(FGFP), that is used to fuse feature\nmaps extracted by the backbone of FGENet. The fused features are then passed to\nboth regression and classification heads, where the former provides predicted\npoint coordinates for a given image, and the latter determines the confidence\nlevel for each predicted point being an individual. At the end, FGENet\nestablishes correspondences between prediction points and ground truth points\nby employing the Hungarian algorithm. For training FGENet, we design a robust\nloss function, named Three-Task Combination (TTC), to mitigate the impact of\nannotation noise. Extensive experiments are conducted on four widely used crowd\ncounting datasets. Experimental results demonstrate the effectiveness of\nFGENet. Notably, our method achieves a remarkable improvement of 3.14 points in\nMean Absolute Error (MAE) on the ShanghaiTech Part A dataset, showcasing its\nsuperiority over the existing state-of-the-art methods. Even more impressively,\nFGENet surpasses previous benchmarks on the UCF\\_CC\\_50 dataset with an\nastounding enhancement of 30.16 points in MAE.\n","authors":["Hao-Yuan Ma","Li Zhang","Xiang-Yi Wei"],"pdf_url":"https://arxiv.org/pdf/2401.01208v1.pdf","comment":"Accepted by 30th International Conference on MultiMedia Modeling"},{"id":"http://arxiv.org/abs/2401.01207v1","updated":"2024-01-02T13:28:39Z","published":"2024-01-02T13:28:39Z","title":"Towards a Simultaneous and Granular Identity-Expression Control in\n  Personalized Face Generation","summary":"  In human-centric content generation, the pre-trained text-to-image models\nstruggle to produce user-wanted portrait images, which retain the identity of\nindividuals while exhibiting diverse expressions. This paper introduces our\nefforts towards personalized face generation. To this end, we propose a novel\nmulti-modal face generation framework, capable of simultaneous\nidentity-expression control and more fine-grained expression synthesis. Our\nexpression control is so sophisticated that it can be specialized by the\nfine-grained emotional vocabulary. We devise a novel diffusion model that can\nundertake the task of simultaneously face swapping and reenactment. Due to the\nentanglement of identity and expression, it's nontrivial to separately and\nprecisely control them in one framework, thus has not been explored yet. To\novercome this, we propose several innovative designs in the conditional\ndiffusion model, including balancing identity and expression encoder, improved\nmidpoint sampling, and explicitly background conditioning. Extensive\nexperiments have demonstrated the controllability and scalability of the\nproposed framework, in comparison with state-of-the-art text-to-image, face\nswapping, and face reenactment methods.\n","authors":["Renshuai Liu","Bowen Ma","Wei Zhang","Zhipeng Hu","Changjie Fan","Tangjie Lv","Yu Ding","Xuan Cheng"],"pdf_url":"https://arxiv.org/pdf/2401.01207v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04028v2","updated":"2024-01-02T13:18:34Z","published":"2023-12-07T03:53:53Z","title":"ImFace++: A Sophisticated Nonlinear 3D Morphable Face Model with\n  Implicit Neural Representations","summary":"  Accurate representations of 3D faces are of paramount importance in various\ncomputer vision and graphics applications. However, the challenges persist due\nto the limitations imposed by data discretization and model linearity, which\nhinder the precise capture of identity and expression clues in current studies.\nThis paper presents a novel 3D morphable face model, named ImFace++, to learn a\nsophisticated and continuous space with implicit neural representations.\nImFace++ first constructs two explicitly disentangled deformation fields to\nmodel complex shapes associated with identities and expressions, respectively,\nwhich simultaneously facilitate the automatic learning of correspondences\nacross diverse facial shapes. To capture more sophisticated facial details, a\nrefinement displacement field within the template space is further\nincorporated, enabling a fine-grained learning of individual-specific facial\ndetails. Furthermore, a Neural Blend-Field is designed to reinforce the\nrepresentation capabilities through adaptive blending of an array of local\nfields. In addition to ImFace++, we have devised an improved learning strategy\nto extend expression embeddings, allowing for a broader range of expression\nvariations. Comprehensive qualitative and quantitative evaluations demonstrate\nthat ImFace++ significantly advances the state-of-the-art in terms of both face\nreconstruction fidelity and correspondence accuracy.\n","authors":["Mingwu Zheng","Haiyu Zhang","Hongyu Yang","Liming Chen","Di Huang"],"pdf_url":"https://arxiv.org/pdf/2312.04028v2.pdf","comment":"Project page:\n  https://github.com/MingwuZheng/ImFace/tree/imface%2B%2B. arXiv admin note:\n  text overlap with arXiv:2203.14510"},{"id":"http://arxiv.org/abs/2307.01530v2","updated":"2024-01-02T13:13:49Z","published":"2023-07-04T07:33:53Z","title":"Tomato Maturity Recognition with Convolutional Transformers","summary":"  Tomatoes are a major crop worldwide, and accurately classifying their\nmaturity is important for many agricultural applications, such as harvesting,\ngrading, and quality control. In this paper, the authors propose a novel method\nfor tomato maturity classification using a convolutional transformer. The\nconvolutional transformer is a hybrid architecture that combines the strengths\nof convolutional neural networks (CNNs) and transformers. Additionally, this\nstudy introduces a new tomato dataset named KUTomaData, explicitly designed to\ntrain deep-learning models for tomato segmentation and classification.\nKUTomaData is a compilation of images sourced from a greenhouse in the UAE,\nwith approximately 700 images available for training and testing. The dataset\nis prepared under various lighting conditions and viewing perspectives and\nemploys different mobile camera sensors, distinguishing it from existing\ndatasets. The contributions of this paper are threefold:Firstly, the authors\npropose a novel method for tomato maturity classification using a modular\nconvolutional transformer. Secondly, the authors introduce a new tomato image\ndataset that contains images of tomatoes at different maturity levels. Lastly,\nthe authors show that the convolutional transformer outperforms\nstate-of-the-art methods for tomato maturity classification. The effectiveness\nof the proposed framework in handling cluttered and occluded tomato instances\nwas evaluated using two additional public datasets, Laboro Tomato and Rob2Pheno\nAnnotated Tomato, as benchmarks. The evaluation results across these three\ndatasets demonstrate the exceptional performance of our proposed framework,\nsurpassing the state-of-the-art by 58.14%, 65.42%, and 66.39% in terms of mean\naverage precision scores for KUTomaData, Laboro Tomato, and Rob2Pheno Annotated\nTomato, respectively.\n","authors":["Asim Khan","Taimur Hassan","Muhammad Shafay","Israa Fahmy","Naoufel Werghi","Lakmal Seneviratne","Irfan Hussain"],"pdf_url":"https://arxiv.org/pdf/2307.01530v2.pdf","comment":"23 pages, 6 figures and 8 Tables"},{"id":"http://arxiv.org/abs/2401.01201v1","updated":"2024-01-02T13:04:41Z","published":"2024-01-02T13:04:41Z","title":"Whole-examination AI estimation of fetal biometrics from 20-week\n  ultrasound scans","summary":"  The current approach to fetal anomaly screening is based on biometric\nmeasurements derived from individually selected ultrasound images. In this\npaper, we introduce a paradigm shift that attains human-level performance in\nbiometric measurement by aggregating automatically extracted biometrics from\nevery frame across an entire scan, with no need for operator intervention. We\nuse a convolutional neural network to classify each frame of an ultrasound\nvideo recording. We then measure fetal biometrics in every frame where\nappropriate anatomy is visible. We use a Bayesian method to estimate the true\nvalue of each biometric from a large number of measurements and\nprobabilistically reject outliers. We performed a retrospective experiment on\n1457 recordings (comprising 48 million frames) of 20-week ultrasound scans,\nestimated fetal biometrics in those scans and compared our estimates to the\nmeasurements sonographers took during the scan. Our method achieves human-level\nperformance in estimating fetal biometrics and estimates well-calibrated\ncredible intervals in which the true biometric value is expected to lie.\n","authors":["Lorenzo Venturini","Samuel Budd","Alfonso Farruggia","Robert Wright","Jacqueline Matthew","Thomas G. Day","Bernhard Kainz","Reza Razavi","Jo V. Hajnal"],"pdf_url":"https://arxiv.org/pdf/2401.01201v1.pdf","comment":"14 pages, 16 figures. Submitted to NPJ digital medicine. For\n  associated video file, see\n  http://wp.doc.ic.ac.uk/ifind/wp-content/uploads/sites/79/2023/12/realtime.gif"},{"id":"http://arxiv.org/abs/2401.01200v1","updated":"2024-01-02T13:03:39Z","published":"2024-01-02T13:03:39Z","title":"Skin cancer diagnosis using NIR spectroscopy data of skin lesions in\n  vivo using machine learning algorithms","summary":"  Skin lesions are classified in benign or malignant. Among the malignant,\nmelanoma is a very aggressive cancer and the major cause of deaths. So, early\ndiagnosis of skin cancer is very desired. In the last few years, there is a\ngrowing interest in computer aided diagnostic (CAD) using most image and\nclinical data of the lesion. These sources of information present limitations\ndue to their inability to provide information of the molecular structure of the\nlesion. NIR spectroscopy may provide an alternative source of information to\nautomated CAD of skin lesions. The most commonly used techniques and\nclassification algorithms used in spectroscopy are Principal Component Analysis\n(PCA), Partial Least Squares - Discriminant Analysis (PLS-DA), and Support\nVector Machines (SVM). Nonetheless, there is a growing interest in applying the\nmodern techniques of machine and deep learning (MDL) to spectroscopy. One of\nthe main limitations to apply MDL to spectroscopy is the lack of public\ndatasets. Since there is no public dataset of NIR spectral data to skin\nlesions, as far as we know, an effort has been made and a new dataset named\nNIR-SC-UFES, has been collected, annotated and analyzed generating the\ngold-standard for classification of NIR spectral data to skin cancer. Next, the\nmachine learning algorithms XGBoost, CatBoost, LightGBM, 1D-convolutional\nneural network (1D-CNN) were investigated to classify cancer and non-cancer\nskin lesions. Experimental results indicate the best performance obtained by\nLightGBM with pre-processing using standard normal variate (SNV), feature\nextraction providing values of 0.839 for balanced accuracy, 0.851 for recall,\n0.852 for precision, and 0.850 for F-score. The obtained results indicate the\nfirst steps in CAD of skin lesions aiming the automated triage of patients with\nskin lesions in vivo using NIR spectral data.\n","authors":["Flavio P. Loss","Pedro H. da Cunha","Matheus B. Rocha","Madson Poltronieri Zanoni","Leandro M. de Lima","Isadora Tavares Nascimento","Isabella Rezende","Tania R. P. Canuto","Luciana de Paula Vieira","Renan Rossoni","Maria C. S. Santos","Patricia Lyra Frasson","Wanderson Romão","Paulo R. Filgueiras","Renato A. Krohling"],"pdf_url":"https://arxiv.org/pdf/2401.01200v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01199v1","updated":"2024-01-02T13:03:29Z","published":"2024-01-02T13:03:29Z","title":"JMA: a General Algorithm to Craft Nearly Optimal Targeted Adversarial\n  Example","summary":"  Most of the approaches proposed so far to craft targeted adversarial examples\nagainst Deep Learning classifiers are highly suboptimal and typically rely on\nincreasing the likelihood of the target class, thus implicitly focusing on\none-hot encoding settings. In this paper, we propose a more general,\ntheoretically sound, targeted attack that resorts to the minimization of a\nJacobian-induced MAhalanobis distance (JMA) term, taking into account the\neffort (in the input space) required to move the latent space representation of\nthe input sample in a given direction. The minimization is solved by exploiting\nthe Wolfe duality theorem, reducing the problem to the solution of a\nNon-Negative Least Square (NNLS) problem. The proposed algorithm provides an\noptimal solution to a linearized version of the adversarial example problem\noriginally introduced by Szegedy et al. \\cite{szegedy2013intriguing}. The\nexperiments we carried out confirm the generality of the proposed attack which\nis proven to be effective under a wide variety of output encoding schemes.\nNoticeably, the JMA attack is also effective in a multi-label classification\nscenario, being capable to induce a targeted modification of up to half the\nlabels in a complex multilabel classification scenario with 20 labels, a\ncapability that is out of reach of all the attacks proposed so far. As a\nfurther advantage, the JMA attack usually requires very few iterations, thus\nresulting more efficient than existing methods.\n","authors":["Benedetta Tondi","Wei Guo","Mauro Barni"],"pdf_url":"https://arxiv.org/pdf/2401.01199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.02803v2","updated":"2024-01-02T12:33:45Z","published":"2023-04-11T14:53:57Z","title":"Tensor PCA from basis in tensor space","summary":"  The aim of this paper is to present a mathematical framework for tensor PCA.\nThe proposed approach is able to overcome the limitations of previous methods\nthat extract a low dimensional subspace by iteratively solving an optimization\nproblem. The core of the proposed approach is the derivation of a basis in\ntensor space from a real self-adjoint tensor operator, thus reducing the\nproblem of deriving a basis to an eigenvalue problem. Three different cases\nhave been studied to derive: i) a basis from a self-adjoint tensor operator;\nii) a rank-1 basis; iii) a basis in a subspace. In particular, the equivalence\nbetween eigenvalue equation for a real self-adjoint tensor operator and\nstandard matrix eigenvalue equation has been proven. For all the three cases\nconsidered, a subspace approach has been adopted to derive a tensor PCA.\nExperiments on image datasets validate the proposed mathematical framework.\n","authors":["Claudio Turchetti"],"pdf_url":"https://arxiv.org/pdf/2305.02803v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2208.03754v3","updated":"2024-01-02T12:27:46Z","published":"2022-08-07T15:57:18Z","title":"Exploring Long- and Short-Range Temporal Information for Learned Video\n  Compression","summary":"  Learned video compression methods have gained a variety of interest in the\nvideo coding community since they have matched or even exceeded the\nrate-distortion (RD) performance of traditional video codecs. However, many\ncurrent learning-based methods are dedicated to utilizing short-range temporal\ninformation, thus limiting their performance. In this paper, we focus on\nexploiting the unique characteristics of video content and further exploring\ntemporal information to enhance compression performance. Specifically, for\nlong-range temporal information exploitation, we propose temporal prior that\ncan update continuously within the group of pictures (GOP) during inference. In\nthat case temporal prior contains valuable temporal information of all decoded\nimages within the current GOP. As for short-range temporal information, we\npropose a progressive guided motion compensation to achieve robust and\neffective compensation. In detail, we design a hierarchical structure to\nachieve multi-scale compensation. More importantly, we use optical flow\nguidance to generate pixel offsets between feature maps at each scale, and the\ncompensation results at each scale will be used to guide the following scale's\ncompensation. Sufficient experimental results demonstrate that our method can\nobtain better RD performance than state-of-the-art video compression\napproaches. The code is publicly available on:\nhttps://github.com/Huairui/LSTVC.\n","authors":["Huairui Wang","Zhenzhong Chen"],"pdf_url":"https://arxiv.org/pdf/2208.03754v3.pdf","comment":"arXiv admin note: text overlap with arXiv:2207.04589"},{"id":"http://arxiv.org/abs/2312.07418v2","updated":"2024-01-02T12:24:25Z","published":"2023-12-12T16:39:12Z","title":"Attention Based Encoder Decoder Model for Video Captioning in Nepali\n  (2023)","summary":"  Video captioning in Nepali, a language written in the Devanagari script,\npresents a unique challenge due to the lack of existing academic work in this\ndomain. This work develops a novel encoder-decoder paradigm for Nepali video\ncaptioning to tackle this difficulty. LSTM and GRU sequence-to-sequence models\nare used in the model to produce related textual descriptions based on features\nretrieved from video frames using CNNs. Using Google Translate and manual\npost-editing, a Nepali video captioning dataset is generated from the Microsoft\nResearch Video Description Corpus (MSVD) dataset created using Google\nTranslate, and manual post-editing work. The efficacy of the model for\nDevanagari-scripted video captioning is demonstrated by BLEU, METOR, and ROUGE\nmeasures, which are used to assess its performance.\n","authors":["Kabita Parajuli","Shashidhar Ram Joshi"],"pdf_url":"https://arxiv.org/pdf/2312.07418v2.pdf","comment":"Result are wrong and took some time"},{"id":"http://arxiv.org/abs/2401.01181v1","updated":"2024-01-02T12:18:40Z","published":"2024-01-02T12:18:40Z","title":"Query-Based Knowledge Sharing for Open-Vocabulary Multi-Label\n  Classification","summary":"  Identifying labels that did not appear during training, known as multi-label\nzero-shot learning, is a non-trivial task in computer vision. To this end,\nrecent studies have attempted to explore the multi-modal knowledge of\nvision-language pre-training (VLP) models by knowledge distillation, allowing\nto recognize unseen labels in an open-vocabulary manner. However, experimental\nevidence shows that knowledge distillation is suboptimal and provides limited\nperformance gain in unseen label prediction. In this paper, a novel query-based\nknowledge sharing paradigm is proposed to explore the multi-modal knowledge\nfrom the pretrained VLP model for open-vocabulary multi-label classification.\nSpecifically, a set of learnable label-agnostic query tokens is trained to\nextract critical vision knowledge from the input image, and further shared\nacross all labels, allowing them to select tokens of interest as visual clues\nfor recognition. Besides, we propose an effective prompt pool for robust label\nembedding, and reformulate the standard ranking learning into a form of\nclassification to allow the magnitude of feature vectors for matching, which\nboth significantly benefit label recognition. Experimental results show that\nour framework significantly outperforms state-of-the-art methods on zero-shot\ntask by 5.9% and 4.5% in mAP on the NUS-WIDE and Open Images, respectively.\n","authors":["Xuelin Zhu","Jian Liu","Dongqi Tang","Jiawei Ge","Weijia Liu","Bo Liu","Jiuxin Cao"],"pdf_url":"https://arxiv.org/pdf/2401.01181v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01180v1","updated":"2024-01-02T12:16:01Z","published":"2024-01-02T12:16:01Z","title":"Accurate and Efficient Urban Street Tree Inventory with Deep Learning on\n  Mobile Phone Imagery","summary":"  Deforestation, a major contributor to climate change, poses detrimental\nconsequences such as agricultural sector disruption, global warming, flash\nfloods, and landslides. Conventional approaches to urban street tree inventory\nsuffer from inaccuracies and necessitate specialised equipment. To overcome\nthese challenges, this paper proposes an innovative method that leverages deep\nlearning techniques and mobile phone imaging for urban street tree inventory.\nOur approach utilises a pair of images captured by smartphone cameras to\naccurately segment tree trunks and compute the diameter at breast height (DBH).\nCompared to traditional methods, our approach exhibits several advantages,\nincluding superior accuracy, reduced dependency on specialised equipment, and\napplicability in hard-to-reach areas. We evaluated our method on a\ncomprehensive dataset of 400 trees and achieved a DBH estimation accuracy with\nan error rate of less than 2.5%. Our method holds significant potential for\nsubstantially improving forest management practices. By enhancing the accuracy\nand efficiency of tree inventory, our model empowers urban management to\nmitigate the adverse effects of deforestation and climate change.\n","authors":["Asim Khan","Umair Nawaz","Anwaar Ulhaq","Iqbal Gondal","Sajid Javed"],"pdf_url":"https://arxiv.org/pdf/2401.01180v1.pdf","comment":"8 Pages, 7 figures and 5 Tables"},{"id":"http://arxiv.org/abs/2401.01179v1","updated":"2024-01-02T12:14:41Z","published":"2024-01-02T12:14:41Z","title":"Freeze the backbones: A Parameter-Efficient Contrastive Approach to\n  Robust Medical Vision-Language Pre-training","summary":"  Modern healthcare often utilises radiographic images alongside textual\nreports for diagnostics, encouraging the use of Vision-Language Self-Supervised\nLearning (VL-SSL) with large pre-trained models to learn versatile medical\nvision representations. However, most existing VL-SSL frameworks are trained\nend-to-end, which is computation-heavy and can lose vital prior information\nembedded in pre-trained encoders. To address both issues, we introduce the\nbackbone-agnostic Adaptor framework, which preserves medical knowledge in\npre-trained image and text encoders by keeping them frozen, and employs a\nlightweight Adaptor module for cross-modal learning. Experiments on medical\nimage classification and segmentation tasks across three datasets reveal that\nour framework delivers competitive performance while cutting trainable\nparameters by over 90% compared to current pre-training approaches. Notably,\nwhen fine-tuned with just 1% of data, Adaptor outperforms several\nTransformer-based methods trained on full datasets in medical image\nsegmentation.\n","authors":["Jiuming Qin","Che Liu","Sibo Cheng","Yike Guo","Rossella Arcucci"],"pdf_url":"https://arxiv.org/pdf/2401.01179v1.pdf","comment":"Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2401.01178v1","updated":"2024-01-02T12:13:35Z","published":"2024-01-02T12:13:35Z","title":"GBSS:a global building semantic segmentation dataset for large-scale\n  remote sensing building extraction","summary":"  Semantic segmentation techniques for extracting building footprints from\nhigh-resolution remote sensing images have been widely used in many fields such\nas urban planning. However, large-scale building extraction demands higher\ndiversity in training samples. In this paper, we construct a Global Building\nSemantic Segmentation (GBSS) dataset (The dataset will be released), which\ncomprises 116.9k pairs of samples (about 742k buildings) from six continents.\nThere are significant variations of building samples in terms of size and\nstyle, so the dataset can be a more challenging benchmark for evaluating the\ngeneralization and robustness of building semantic segmentation models. We\nvalidated through quantitative and qualitative comparisons between different\ndatasets, and further confirmed the potential application in the field of\ntransfer learning by conducting experiments on subsets.\n","authors":["Yuping Hu","Xin Huang","Jiayi Li","Zhen Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.01178v1.pdf","comment":"5 pages,6 figures"},{"id":"http://arxiv.org/abs/2401.01175v1","updated":"2024-01-02T12:09:06Z","published":"2024-01-02T12:09:06Z","title":"Learning Surface Scattering Parameters From SAR Images Using\n  Differentiable Ray Tracing","summary":"  Simulating high-resolution Synthetic Aperture Radar (SAR) images in complex\nscenes has consistently presented a significant research challenge. The\ndevelopment of a microwave-domain surface scattering model and its\nreversibility are poised to play a pivotal role in enhancing the authenticity\nof SAR image simulations and facilitating the reconstruction of target\nparameters. Drawing inspiration from the field of computer graphics, this paper\nproposes a surface microwave rendering model that comprehensively considers\nboth Specular and Diffuse contributions. The model is analytically represented\nby the coherent spatially varying bidirectional scattering distribution\nfunction (CSVBSDF) based on the Kirchhoff approximation (KA) and the\nperturbation method (SPM). And SAR imaging is achieved through the synergistic\ncombination of ray tracing and fast mapping projection techniques. Furthermore,\na differentiable ray tracing (DRT) engine based on SAR images was constructed\nfor CSVBSDF surface scattering parameter learning. Within this SAR image\nsimulation engine, the use of differentiable reverse ray tracing enables the\nrapid estimation of parameter gradients from SAR images. The effectiveness of\nthis approach has been validated through simulations and comparisons with real\nSAR images. By learning the surface scattering parameters, substantial\nenhancements in SAR image simulation performance under various observation\nconditions have been demonstrated.\n","authors":["Jiangtao Wei","Yixiang Luomei","Xu Zhang","Feng Xu"],"pdf_url":"https://arxiv.org/pdf/2401.01175v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01173v1","updated":"2024-01-02T12:06:31Z","published":"2024-01-02T12:06:31Z","title":"En3D: An Enhanced Generative Model for Sculpting 3D Humans from 2D\n  Synthetic Data","summary":"  We present En3D, an enhanced generative scheme for sculpting high-quality 3D\nhuman avatars. Unlike previous works that rely on scarce 3D datasets or limited\n2D collections with imbalanced viewing angles and imprecise pose priors, our\napproach aims to develop a zero-shot 3D generative scheme capable of producing\nvisually realistic, geometrically accurate and content-wise diverse 3D humans\nwithout relying on pre-existing 3D or 2D assets. To address this challenge, we\nintroduce a meticulously crafted workflow that implements accurate physical\nmodeling to learn the enhanced 3D generative model from synthetic 2D data.\nDuring inference, we integrate optimization modules to bridge the gap between\nrealistic appearances and coarse 3D shapes. Specifically, En3D comprises three\nmodules: a 3D generator that accurately models generalizable 3D humans with\nrealistic appearance from synthesized balanced, diverse, and structured human\nimages; a geometry sculptor that enhances shape quality using multi-view normal\nconstraints for intricate human anatomy; and a texturing module that\ndisentangles explicit texture maps with fidelity and editability, leveraging\nsemantical UV partitioning and a differentiable rasterizer. Experimental\nresults show that our approach significantly outperforms prior works in terms\nof image quality, geometry accuracy and content diversity. We also showcase the\napplicability of our generated avatars for animation and editing, as well as\nthe scalability of our approach for content-style free adaptation.\n","authors":["Yifang Men","Biwen Lei","Yuan Yao","Miaomiao Cui","Zhouhui Lian","Xuansong Xie"],"pdf_url":"https://arxiv.org/pdf/2401.01173v1.pdf","comment":"Project Page: https://menyifang.github.io/projects/En3D/index.html"},{"id":"http://arxiv.org/abs/2207.13165v3","updated":"2024-01-02T11:54:36Z","published":"2022-07-26T19:41:59Z","title":"YOLO and Mask R-CNN for Vehicle Number Plate Identification","summary":"  License plate scanners have grown in popularity in parking lots during the\npast few years. In order to quickly identify license plates, traditional plate\nrecognition devices used in parking lots employ a fixed source of light and\nshooting angles. For skewed angles, such as license plate images taken with\nultra-wide angle or fisheye lenses, deformation of the license plate\nrecognition plate can also be quite severe, impairing the ability of standard\nlicense plate recognition systems to identify the plate. Mask RCNN gadget that\nmay be utilised for oblique pictures and various shooting angles. The results\nof the experiments show that the suggested design will be capable of\nclassifying license plates with bevel angles larger than 0/60. Character\nrecognition using the suggested Mask R-CNN approach has advanced significantly\nas well. The proposed Mask R-CNN method has also achieved significant progress\nin character recognition, which is tilted more than 45 degrees as compared to\nthe strategy of employing the YOLOv2 model. Experiment results also suggest\nthat the methodology presented in the open data plate collecting is better than\nother techniques (known as the AOLP dataset).\n","authors":["Siddharth Ganjoo"],"pdf_url":"https://arxiv.org/pdf/2207.13165v3.pdf","comment":"changes to be done"},{"id":"http://arxiv.org/abs/2401.01164v1","updated":"2024-01-02T11:46:44Z","published":"2024-01-02T11:46:44Z","title":"Distilling Local Texture Features for Colorectal Tissue Classification\n  in Low Data Regimes","summary":"  Multi-class colorectal tissue classification is a challenging problem that is\ntypically addressed in a setting, where it is assumed that ample amounts of\ntraining data is available. However, manual annotation of fine-grained\ncolorectal tissue samples of multiple classes, especially the rare ones like\nstromal tumor and anal cancer is laborious and expensive. To address this, we\npropose a knowledge distillation-based approach, named KD-CTCNet, that\neffectively captures local texture information from few tissue samples, through\na distillation loss, to improve the standard CNN features. The resulting\nenriched feature representation achieves improved classification performance\nspecifically in low data regimes. Extensive experiments on two public datasets\nof colorectal tissues reveal the merits of the proposed contributions, with a\nconsistent gain achieved over different approaches across low data settings.\nThe code and models are publicly available on GitHub.\n","authors":["Dmitry Demidov","Roba Al Majzoub","Amandeep Kumar","Fahad Khan"],"pdf_url":"https://arxiv.org/pdf/2401.01164v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01163v1","updated":"2024-01-02T11:46:42Z","published":"2024-01-02T11:46:42Z","title":"NU-Class Net: A Novel Deep Learning-based Approach for Video Quality\n  Enhancement","summary":"  Video content has experienced a surge in popularity, asserting its dominance\nover internet traffic and Internet of Things (IoT) networks. Video compression\nhas long been regarded as the primary means of efficiently managing the\nsubstantial multimedia traffic generated by video-capturing devices.\nNevertheless, video compression algorithms entail significant computational\ndemands in order to achieve substantial compression ratios. This complexity\npresents a formidable challenge when implementing efficient video coding\nstandards in resource-constrained embedded systems, such as IoT edge node\ncameras. To tackle this challenge, this paper introduces NU-Class Net, an\ninnovative deep-learning model designed to mitigate compression artifacts\nstemming from lossy compression codecs. This enhancement significantly elevates\nthe perceptible quality of low-bit-rate videos. By employing the NU-Class Net,\nthe video encoder within the video-capturing node can reduce output quality,\nthereby generating low-bit-rate videos and effectively curtailing both\ncomputation and bandwidth requirements at the edge. On the decoder side, which\nis typically less encumbered by resource limitations, NU-Class Net is applied\nafter the video decoder to compensate for artifacts and approximate the quality\nof the original video. Experimental results affirm the efficacy of the proposed\nmodel in enhancing the perceptible quality of videos, especially those streamed\nat low bit rates.\n","authors":["Parham Zilouchian Moghaddam","Mehdi Modarressi","MohammadAmin Sadeghi"],"pdf_url":"https://arxiv.org/pdf/2401.01163v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01160v1","updated":"2024-01-02T11:43:49Z","published":"2024-01-02T11:43:49Z","title":"Train-Free Segmentation in MRI with Cubical Persistent Homology","summary":"  We describe a new general method for segmentation in MRI scans using\nTopological Data Analysis (TDA), offering several advantages over traditional\nmachine learning approaches. It works in three steps, first identifying the\nwhole object to segment via automatic thresholding, then detecting a\ndistinctive subset whose topology is known in advance, and finally deducing the\nvarious components of the segmentation. Although convoking classical ideas of\nTDA, such an algorithm has never been proposed separately from deep learning\nmethods. To achieve this, our approach takes into account, in addition to the\nhomology of the image, the localization of representative cycles, a piece of\ninformation that seems never to have been exploited in this context. In\nparticular, it offers the ability to perform segmentation without the need for\nlarge annotated data sets. TDA also provides a more interpretable and stable\nframework for segmentation by explicitly mapping topological features to\nsegmentation components. By adapting the geometric object to be detected, the\nalgorithm can be adjusted to a wide range of data segmentation challenges. We\ncarefully study the examples of glioblastoma segmentation in brain MRI, where a\nsphere is to be detected, as well as myocardium in cardiac MRI, involving a\ncylinder, and cortical plate detection in fetal brain MRI, whose 2D slices are\ncircles. We compare our method to state-of-the-art algorithms.\n","authors":["Anton François","Raphaël Tinarrage"],"pdf_url":"https://arxiv.org/pdf/2401.01160v1.pdf","comment":"preprint, 17 pages, 19 figures"},{"id":"http://arxiv.org/abs/2401.00029v2","updated":"2024-01-02T11:29:16Z","published":"2023-12-29T05:28:35Z","title":"6D-Diff: A Keypoint Diffusion Framework for 6D Object Pose Estimation","summary":"  Estimating the 6D object pose from a single RGB image often involves noise\nand indeterminacy due to challenges such as occlusions and cluttered\nbackgrounds. Meanwhile, diffusion models have shown appealing performance in\ngenerating high-quality images from random noise with high indeterminacy\nthrough step-by-step denoising. Inspired by their denoising capability, we\npropose a novel diffusion-based framework (6D-Diff) to handle the noise and\nindeterminacy in object pose estimation for better performance. In our\nframework, to establish accurate 2D-3D correspondence, we formulate 2D\nkeypoints detection as a reverse diffusion (denoising) process. To facilitate\nsuch a denoising process, we design a Mixture-of-Cauchy-based forward diffusion\nprocess and condition the reverse process on the object features. Extensive\nexperiments on the LM-O and YCB-V datasets demonstrate the effectiveness of our\nframework.\n","authors":["Li Xu","Haoxuan Qu","Yujun Cai","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2401.00029v2.pdf","comment":"Fix typo"},{"id":"http://arxiv.org/abs/2401.01134v1","updated":"2024-01-02T10:22:06Z","published":"2024-01-02T10:22:06Z","title":"Hybrid Pooling and Convolutional Network for Improving Accuracy and\n  Training Convergence Speed in Object Detection","summary":"  This paper introduces HPC-Net, a high-precision and rapidly convergent object\ndetection network.\n","authors":["Shiwen Zhao","Wei Wang","Junhui Hou","Hai Wu"],"pdf_url":"https://arxiv.org/pdf/2401.01134v1.pdf","comment":"10 pages,5 figures, conference"},{"id":"http://arxiv.org/abs/2401.01130v1","updated":"2024-01-02T10:10:29Z","published":"2024-01-02T10:10:29Z","title":"Joint Generative Modeling of Scene Graphs and Images via Diffusion\n  Models","summary":"  In this paper, we present a novel generative task: joint scene graph - image\ngeneration. While previous works have explored image generation conditioned on\nscene graphs or layouts, our task is distinctive and important as it involves\ngenerating scene graphs themselves unconditionally from noise, enabling\nefficient and interpretable control for image generation. Our task is\nchallenging, requiring the generation of plausible scene graphs with\nheterogeneous attributes for nodes (objects) and edges (relations among\nobjects), including continuous object bounding boxes and discrete object and\nrelation categories. We introduce a novel diffusion model, DiffuseSG, that\njointly models the adjacency matrix along with heterogeneous node and edge\nattributes. We explore various types of encodings for the categorical data,\nrelaxing it into a continuous space. With a graph transformer being the\ndenoiser, DiffuseSG successively denoises the scene graph representation in a\ncontinuous space and discretizes the final representation to generate the clean\nscene graph. Additionally, we introduce an IoU regularization to enhance the\nempirical performance. Our model significantly outperforms existing methods in\nscene graph generation on the Visual Genome and COCO-Stuff datasets, both on\nstandard and newly introduced metrics that better capture the problem\ncomplexity. Moreover, we demonstrate the additional benefits of our model in\ntwo downstream applications: 1) excelling in a series of scene graph completion\ntasks, and 2) improving scene graph detection models by using extra training\nsamples generated from DiffuseSG.\n","authors":["Bicheng Xu","Qi Yan","Renjie Liao","Lele Wang","Leonid Sigal"],"pdf_url":"https://arxiv.org/pdf/2401.01130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02067v2","updated":"2024-01-02T10:04:27Z","published":"2023-10-03T14:09:27Z","title":"Content Bias in Deep Learning Image Age Approximation: A new Approach\n  Towards better Explainability","summary":"  In the context of temporal image forensics, it is not evident that a neural\nnetwork, trained on images from different time-slots (classes), exploits solely\nimage age related features. Usually, images taken in close temporal proximity\n(e.g., belonging to the same age class) share some common content properties.\nSuch content bias can be exploited by a neural network. In this work, a novel\napproach is proposed that evaluates the influence of image content. This\napproach is verified using synthetic images (where content bias can be ruled\nout) with an age signal embedded. Based on the proposed approach, it is shown\nthat a deep learning approach proposed in the context of age classification is\nmost likely highly dependent on the image content. As a possible\ncountermeasure, two different models from the field of image steganalysis,\nalong with three different preprocessing techniques to increase the\nsignal-to-noise ratio (age signal to image content), are evaluated using the\nproposed method.\n","authors":["Robert Jöchl","Andreas Uhl"],"pdf_url":"https://arxiv.org/pdf/2310.02067v2.pdf","comment":"This is a preprint, the paper is currently under consideration at\n  Pattern Recognition Letters"},{"id":"http://arxiv.org/abs/2401.01128v1","updated":"2024-01-02T09:51:39Z","published":"2024-01-02T09:51:39Z","title":"SSP: A Simple and Safe automatic Prompt engineering method towards\n  realistic image synthesis on LVM","summary":"  Recently, text-to-image (T2I) synthesis has undergone significant\nadvancements, particularly with the emergence of Large Language Models (LLM)\nand their enhancement in Large Vision Models (LVM), greatly enhancing the\ninstruction-following capabilities of traditional T2I models. Nevertheless,\nprevious methods focus on improving generation quality but introduce unsafe\nfactors into prompts. We explore that appending specific camera descriptions to\nprompts can enhance safety performance. Consequently, we propose a simple and\nsafe prompt engineering method (SSP) to improve image generation quality by\nproviding optimal camera descriptions. Specifically, we create a dataset from\nmulti-datasets as original prompts. To select the optimal camera, we design an\noptimal camera matching approach and implement a classifier for original\nprompts capable of automatically matching. Appending camera descriptions to\noriginal prompts generates optimized prompts for further LVM image generation.\nExperiments demonstrate that SSP improves semantic consistency by an average of\n16% compared to others and safety metrics by 48.9%.\n","authors":["Weijin Cheng","Jianzhi Liu","Jiawen Deng","Fuji Ren"],"pdf_url":"https://arxiv.org/pdf/2401.01128v1.pdf","comment":"10 pages, 8 figures"},{"id":"http://arxiv.org/abs/2401.01117v1","updated":"2024-01-02T09:11:23Z","published":"2024-01-02T09:11:23Z","title":"Q-Refine: A Perceptual Quality Refiner for AI-Generated Image","summary":"  With the rapid evolution of the Text-to-Image (T2I) model in recent years,\ntheir unsatisfactory generation result has become a challenge. However,\nuniformly refining AI-Generated Images (AIGIs) of different qualities not only\nlimited optimization capabilities for low-quality AIGIs but also brought\nnegative optimization to high-quality AIGIs. To address this issue, a\nquality-award refiner named Q-Refine is proposed. Based on the preference of\nthe Human Visual System (HVS), Q-Refine uses the Image Quality Assessment (IQA)\nmetric to guide the refining process for the first time, and modify images of\ndifferent qualities through three adaptive pipelines. Experimental shows that\nfor mainstream T2I models, Q-Refine can perform effective optimization to AIGIs\nof different qualities. It can be a general refiner to optimize AIGIs from both\nfidelity and aesthetic quality levels, thus expanding the application of the\nT2I generation models.\n","authors":["Chunyi Li","Haoning Wu","Zicheng Zhang","Hongkun Hao","Kaiwei Zhang","Lei Bai","Xiaohong Liu","Xiongkuo Min","Weisi Lin","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2401.01117v1.pdf","comment":"6 pages, 5 figures"},{"id":"http://arxiv.org/abs/2401.01107v1","updated":"2024-01-02T08:57:09Z","published":"2024-01-02T08:57:09Z","title":"CityPulse: Fine-Grained Assessment of Urban Change with Street View Time\n  Series","summary":"  Urban transformations have profound societal impact on both individuals and\ncommunities at large. Accurately assessing these shifts is essential for\nunderstanding their underlying causes and ensuring sustainable urban planning.\nTraditional measurements often encounter constraints in spatial and temporal\ngranularity, failing to capture real-time physical changes. While street view\nimagery, capturing the heartbeat of urban spaces from a pedestrian point of\nview, can add as a high-definition, up-to-date, and on-the-ground visual proxy\nof urban change. We curate the largest street view time series dataset to date,\nand propose an end-to-end change detection model to effectively capture\nphysical alterations in the built environment at scale. We demonstrate the\neffectiveness of our proposed method by benchmark comparisons with previous\nliterature and implementing it at the city-wide level. Our approach has the\npotential to supplement existing dataset and serve as a fine-grained and\naccurate assessment of urban change.\n","authors":["Tianyuan Huang","Zejia Wu","Jiajun Wu","Jackelyn Hwang","Ram Rajagopal"],"pdf_url":"https://arxiv.org/pdf/2401.01107v1.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2309.01141v3","updated":"2024-01-02T08:48:47Z","published":"2023-09-03T11:32:28Z","title":"VGDiffZero: Text-to-image Diffusion Models Can Be Zero-shot Visual\n  Grounders","summary":"  Large-scale text-to-image diffusion models have shown impressive capabilities\nfor generative tasks by leveraging strong vision-language alignment from\npre-training. However, most vision-language discriminative tasks require\nextensive fine-tuning on carefully-labeled datasets to acquire such alignment,\nwith great cost in time and computing resources. In this work, we explore\ndirectly applying a pre-trained generative diffusion model to the challenging\ndiscriminative task of visual grounding without any fine-tuning and additional\ntraining dataset. Specifically, we propose VGDiffZero, a simple yet effective\nzero-shot visual grounding framework based on text-to-image diffusion models.\nWe also design a comprehensive region-scoring method considering both global\nand local contexts of each isolated proposal. Extensive experiments on RefCOCO,\nRefCOCO+, and RefCOCOg show that VGDiffZero achieves strong performance on\nzero-shot visual grounding. Our code is available at\nhttps://github.com/xuyang-liu16/VGDiffZero.\n","authors":["Xuyang Liu","Siteng Huang","Yachen Kang","Honggang Chen","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2309.01141v3.pdf","comment":"Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2401.01102v1","updated":"2024-01-02T08:45:41Z","published":"2024-01-02T08:45:41Z","title":"Dual Teacher Knowledge Distillation with Domain Alignment for Face\n  Anti-spoofing","summary":"  Face recognition systems have raised concerns due to their vulnerability to\ndifferent presentation attacks, and system security has become an increasingly\ncritical concern. Although many face anti-spoofing (FAS) methods perform well\nin intra-dataset scenarios, their generalization remains a challenge. To\naddress this issue, some methods adopt domain adversarial training (DAT) to\nextract domain-invariant features. However, the competition between the encoder\nand the domain discriminator can cause the network to be difficult to train and\nconverge. In this paper, we propose a domain adversarial attack (DAA) method to\nmitigate the training instability problem by adding perturbations to the input\nimages, which makes them indistinguishable across domains and enables domain\nalignment. Moreover, since models trained on limited data and types of attacks\ncannot generalize well to unknown attacks, we propose a dual perceptual and\ngenerative knowledge distillation framework for face anti-spoofing that\nutilizes pre-trained face-related models containing rich face priors.\nSpecifically, we adopt two different face-related models as teachers to\ntransfer knowledge to the target student model. The pre-trained teacher models\nare not from the task of face anti-spoofing but from perceptual and generative\ntasks, respectively, which implicitly augment the data. By combining both DAA\nand dual-teacher knowledge distillation, we develop a dual teacher knowledge\ndistillation with domain alignment framework (DTDA) for face anti-spoofing. The\nadvantage of our proposed method has been verified through extensive ablation\nstudies and comparison with state-of-the-art methods on public datasets across\nmultiple protocols.\n","authors":["Zhe Kong","Wentian Zhang","Tao Wang","Kaihao Zhang","Yuexiang Li","Xiaoying Tang","Wenhan Luo"],"pdf_url":"https://arxiv.org/pdf/2401.01102v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01097v1","updated":"2024-01-02T08:33:36Z","published":"2024-01-02T08:33:36Z","title":"Robust single-particle cryo-EM image denoising and restoration","summary":"  Cryo-electron microscopy (cryo-EM) has achieved near-atomic level resolution\nof biomolecules by reconstructing 2D micrographs. However, the resolution and\naccuracy of the reconstructed particles are significantly reduced due to the\nextremely low signal-to-noise ratio (SNR) and complex noise structure of\ncryo-EM images. In this paper, we introduce a diffusion model with\npost-processing framework to effectively denoise and restore single particle\ncryo-EM images. Our method outperforms the state-of-the-art (SOTA) denoising\nmethods by effectively removing structural noise that has not been addressed\nbefore. Additionally, more accurate and high-resolution three-dimensional\nreconstruction structures can be obtained from denoised cryo-EM images.\n","authors":["Jing Zhang","Tengfei Zhao","ShiYu Hu","Xin Zhao"],"pdf_url":"https://arxiv.org/pdf/2401.01097v1.pdf","comment":"This paper is accepted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2307.09330v3","updated":"2024-01-02T08:29:04Z","published":"2023-07-18T15:13:15Z","title":"Visual Validation versus Visual Estimation: A Study on the Average Value\n  in Scatterplots","summary":"  We investigate the ability of individuals to visually validate statistical\nmodels in terms of their fit to the data. While visual model estimation has\nbeen studied extensively, visual model validation remains under-investigated.\nIt is unknown how well people are able to visually validate models, and how\ntheir performance compares to visual and computational estimation. As a\nstarting point, we conducted a study across two populations (crowdsourced and\nvolunteers). Participants had to both visually estimate (i.e, draw) and\nvisually validate (i.e., accept or reject) the frequently studied model of\naverages. Across both populations, the level of accuracy of the models that\nwere considered valid was lower than the accuracy of the estimated models. We\nfind that participants' validation and estimation were unbiased. Moreover,\ntheir natural critical point between accepting and rejecting a given mean value\nis close to the boundary of its 95% confidence interval, indicating that the\nvisually perceived confidence interval corresponds to a common statistical\nstandard. Our work contributes to the understanding of visual model validation\nand opens new research opportunities.\n","authors":["Daniel Braun","Ashley Suh","Remco Chang","Michael Gleicher","Tatiana von Landesberger"],"pdf_url":"https://arxiv.org/pdf/2307.09330v3.pdf","comment":"Preprint and Author Version of a Short Paper, accepted to the 2023\n  IEEE Visualization Conference (VIS)"},{"id":"http://arxiv.org/abs/2401.01093v1","updated":"2024-01-02T08:28:38Z","published":"2024-01-02T08:28:38Z","title":"Exploring Hyperspectral Anomaly Detection with Human Vision: A Small\n  Target Aware Detector","summary":"  Hyperspectral anomaly detection (HAD) aims to localize pixel points whose\nspectral features differ from the background. HAD is essential in scenarios of\nunknown or camouflaged target features, such as water quality monitoring, crop\ngrowth monitoring and camouflaged target detection, where prior information of\ntargets is difficult to obtain. Existing HAD methods aim to objectively detect\nand distinguish background and anomalous spectra, which can be achieved almost\neffortlessly by human perception. However, the underlying processes of human\nvisual perception are thought to be quite complex. In this paper, we analyze\nhyperspectral image (HSI) features under human visual perception, and transfer\nthe solution process of HAD to the more robust feature space for the first\ntime. Specifically, we propose a small target aware detector (STAD), which\nintroduces saliency maps to capture HSI features closer to human visual\nperception. STAD not only extracts more anomalous representations, but also\nreduces the impact of low-confidence regions through a proposed small target\nfilter (STF). Furthermore, considering the possibility of HAD algorithms being\napplied to edge devices, we propose a full connected network to convolutional\nnetwork knowledge distillation strategy. It can learn the spectral and spatial\nfeatures of the HSI while lightening the network. We train the network on the\nHAD100 training set and validate the proposed method on the HAD100 test set.\nOur method provides a new solution space for HAD that is closer to human visual\nperception with high confidence. Sufficient experiments on real HSI with\nmultiple method comparisons demonstrate the excellent performance and unique\npotential of the proposed method. The code is available at\nhttps://github.com/majitao-xd/STAD-HAD.\n","authors":["Jitao Ma","Weiying Xie","Yunsong Li"],"pdf_url":"https://arxiv.org/pdf/2401.01093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00374v2","updated":"2024-01-02T08:05:02Z","published":"2023-12-31T02:25:41Z","title":"EMAGE: Towards Unified Holistic Co-Speech Gesture Generation via Masked\n  Audio Gesture Modeling","summary":"  We propose EMAGE, a framework to generate full-body human gestures from audio\nand masked gestures, encompassing facial, local body, hands, and global\nmovements. To achieve this, we first introduce BEATX (BEAT-SMPLX-FLAME), a new\nmesh-level holistic co-speech dataset. BEATX combines MoShed SMPLX body with\nFLAME head parameters and further refines the modeling of head, neck, and\nfinger movements, offering a community-standardized, high-quality 3D motion\ncaptured dataset. EMAGE leverages masked body gesture priors during training to\nboost inference performance. It involves a Masked Audio Gesture Transformer,\nfacilitating joint training on audio-to-gesture generation and masked gesture\nreconstruction to effectively encode audio and body gesture hints. Encoded body\nhints from masked gestures are then separately employed to generate facial and\nbody movements. Moreover, EMAGE adaptively merges speech features from the\naudio's rhythm and content and utilizes four compositional VQ-VAEs to enhance\nthe results' fidelity and diversity. Experiments demonstrate that EMAGE\ngenerates holistic gestures with state-of-the-art performance and is flexible\nin accepting predefined spatial-temporal gesture inputs, generating complete,\naudio-synchronized results. Our code and dataset are available at\nhttps://pantomatrix.github.io/EMAGE/\n","authors":["Haiyang Liu","Zihao Zhu","Giorgio Becherini","Yichen Peng","Mingyang Su","You Zhou","Naoya Iwamoto","Bo Zheng","Michael J. Black"],"pdf_url":"https://arxiv.org/pdf/2401.00374v2.pdf","comment":"Project Page: https://pantomatrix.github.io/EMAGE/"},{"id":"http://arxiv.org/abs/2401.01075v1","updated":"2024-01-02T07:34:09Z","published":"2024-01-02T07:34:09Z","title":"Depth-discriminative Metric Learning for Monocular 3D Object Detection","summary":"  Monocular 3D object detection poses a significant challenge due to the lack\nof depth information in RGB images. Many existing methods strive to enhance the\nobject depth estimation performance by allocating additional parameters for\nobject depth estimation, utilizing extra modules or data. In contrast, we\nintroduce a novel metric learning scheme that encourages the model to extract\ndepth-discriminative features regardless of the visual attributes without\nincreasing inference time and model size. Our method employs the\ndistance-preserving function to organize the feature space manifold in relation\nto ground-truth object depth. The proposed (K, B, eps)-quasi-isometric loss\nleverages predetermined pairwise distance restriction as guidance for adjusting\nthe distance among object descriptors without disrupting the non-linearity of\nthe natural feature manifold. Moreover, we introduce an auxiliary head for\nobject-wise depth estimation, which enhances depth quality while maintaining\nthe inference time. The broad applicability of our method is demonstrated\nthrough experiments that show improvements in overall performance when\nintegrated into various baselines. The results show that our method\nconsistently improves the performance of various baselines by 23.51% and 5.78%\non average across KITTI and Waymo, respectively.\n","authors":["Wonhyeok Choi","Mingyu Shin","Sunghoon Im"],"pdf_url":"https://arxiv.org/pdf/2401.01075v1.pdf","comment":"Accepted at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2401.01074v1","updated":"2024-01-02T07:28:21Z","published":"2024-01-02T07:28:21Z","title":"AliFuse: Aligning and Fusing Multi-modal Medical Data for Computer-Aided\n  Diagnosis","summary":"  Medical data collected for making a diagnostic decision are typically\nmulti-modal and provide complementary perspectives of a subject. A\ncomputer-aided diagnosis system welcomes multi-modal inputs; however, how to\neffectively fuse such multi-modal data is a challenging task and attracts a lot\nof attention in the medical research field. In this paper, we propose a\ntransformer-based framework, called Alifuse, for aligning and fusing\nmulti-modal medical data. Specifically, we convert images and unstructured and\nstructured texts into vision and language tokens, and use intramodal and\nintermodal attention mechanisms to learn holistic representations of all\nimaging and non-imaging data for classification. We apply Alifuse to classify\nAlzheimer's disease and obtain state-of-the-art performance on five public\ndatasets, by outperforming eight baselines. The source code will be available\nonline later.\n","authors":["Qiuhui Chen","Xinyue Hu","Zirui Wang","Yi Hong"],"pdf_url":"https://arxiv.org/pdf/2401.01074v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11035v2","updated":"2024-01-02T07:00:32Z","published":"2023-12-18T09:11:28Z","title":"Multi-Moving Camera Pedestrian Tracking with a New Dataset and Global\n  Link Model","summary":"  Ensuring driving safety for autonomous vehicles has become increasingly\ncrucial, highlighting the need for systematic tracking of pedestrians on the\nroad. Most vehicles are equipped with visual sensors, however, the large-scale\nvisual dataset from different agents has not been well studied yet. Basically,\nmost of the multi-target multi-camera (MTMC) tracking systems are composed of\ntwo modules: single camera tracking (SCT) and inter-camera tracking (ICT). To\nreliably coordinate between them, MTMC tracking has been a very complicated\ntask, while tracking across multi-moving cameras makes it even more\nchallenging. In this paper, we focus on multi-target multi-moving camera\n(MTMMC) tracking, which is attracting increasing attention from the research\ncommunity. Observing there are few datasets for MTMMC tracking, we collect a\nnew dataset, called Multi-Moving Camera Track (MMCT), which contains sequences\nunder various driving scenarios. To address the common problems of identity\nswitch easily faced by most existing SCT trackers, especially for moving\ncameras due to ego-motion between the camera and targets, a lightweight\nappearance-free global link model, called Linker, is proposed to mitigate the\nidentity switch by associating two disjoint tracklets of the same target into a\ncomplete trajectory within the same camera. Incorporated with Linker, existing\nSCT trackers generally obtain a significant improvement. Moreover, a strong\nbaseline approach of re-identification (Re-ID) is effectively incorporated to\nextract robust appearance features under varying surroundings for pedestrian\nassociation across moving cameras for ICT, resulting in a much improved MTMMC\ntracking system, which can constitute a step further towards coordinated mining\nof multiple moving cameras. The dataset is available at\nhttps://github.com/dhu-mmct/DHU-MMCT}{https://github.com/dhu-mmct/DHU-MMCT .\n","authors":["Yanting Zhang","Shuanghong Wang","Qingxiang Wang","Cairong Yan","Rui Fan"],"pdf_url":"https://arxiv.org/pdf/2312.11035v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01066v1","updated":"2024-01-02T06:56:57Z","published":"2024-01-02T06:56:57Z","title":"DTBS: Dual-Teacher Bi-directional Self-training for Domain Adaptation in\n  Nighttime Semantic Segmentation","summary":"  Due to the poor illumination and the difficulty in annotating, nighttime\nconditions pose a significant challenge for autonomous vehicle perception\nsystems. Unsupervised domain adaptation (UDA) has been widely applied to\nsemantic segmentation on such images to adapt models from normal conditions to\ntarget nighttime-condition domains. Self-training (ST) is a paradigm in UDA,\nwhere a momentum teacher is utilized for pseudo-label prediction, but a\nconfirmation bias issue exists. Because the one-directional knowledge transfer\nfrom a single teacher is insufficient to adapt to a large domain shift. To\nmitigate this issue, we propose to alleviate domain gap by incrementally\nconsidering style influence and illumination change. Therefore, we introduce a\none-stage Dual-Teacher Bi-directional Self-training (DTBS) framework for smooth\nknowledge transfer and feedback. Based on two teacher models, we present a\nnovel pipeline to respectively decouple style and illumination shift. In\naddition, we propose a new Re-weight exponential moving average (EMA) to merge\nthe knowledge of style and illumination factors, and provide feedback to the\nstudent model. In this way, our method can be embedded in other UDA methods to\nenhance their performance. For example, the Cityscapes to ACDC night task\nyielded 53.8 mIoU (\\%), which corresponds to an improvement of +5\\% over the\nprevious state-of-the-art. The code is available at\n\\url{https://github.com/hf618/DTBS}.\n","authors":["Fanding Huang","Zihao Yao","Wenhui Zhou"],"pdf_url":"https://arxiv.org/pdf/2401.01066v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01065v1","updated":"2024-01-02T06:56:23Z","published":"2024-01-02T06:56:23Z","title":"BEV-CLIP: Multi-modal BEV Retrieval Methodology for Complex Scene in\n  Autonomous Driving","summary":"  The demand for the retrieval of complex scene data in autonomous driving is\nincreasing, especially as passenger vehicles have been equipped with the\nability to navigate urban settings, with the imperative to address long-tail\nscenarios. Meanwhile, under the pre-existing two dimensional image retrieval\nmethod, some problems may arise with scene retrieval, such as lack of global\nfeature representation and subpar text retrieval ability. To address these\nissues, we have proposed \\textbf{BEV-CLIP}, the first multimodal Bird's-Eye\nView(BEV) retrieval methodology that utilizes descriptive text as an input to\nretrieve corresponding scenes. This methodology applies the semantic feature\nextraction abilities of a large language model (LLM) to facilitate zero-shot\nretrieval of extensive text descriptions, and incorporates semi-structured\ninformation from a knowledge graph to improve the semantic richness and variety\nof the language embedding. Our experiments result in 87.66% accuracy on\nNuScenes dataset in text-to-BEV feature retrieval. The demonstrated cases in\nour paper support that our retrieval method is also indicated to be effective\nin identifying certain long-tail corner scenes.\n","authors":["Dafeng Wei","Tian Gao","Zhengyu Jia","Changwei Cai","Chengkai Hou","Peng Jia","Fu Liu","Kun Zhan","Jingchen Fan","Yixing Zhao","Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2401.01065v1.pdf","comment":"Under review of CVPR 2024"},{"id":"http://arxiv.org/abs/2401.01042v1","updated":"2024-01-02T05:10:08Z","published":"2024-01-02T05:10:08Z","title":"Relating Events and Frames Based on Self-Supervised Learning and\n  Uncorrelated Conditioning for Unsupervised Domain Adaptation","summary":"  Event-based cameras provide accurate and high temporal resolution\nmeasurements for performing computer vision tasks in challenging scenarios,\nsuch as high-dynamic range environments and fast-motion maneuvers. Despite\ntheir advantages, utilizing deep learning for event-based vision encounters a\nsignificant obstacle due to the scarcity of annotated data caused by the\nrelatively recent emergence of event-based cameras. To overcome this\nlimitation, leveraging the knowledge available from annotated data obtained\nwith conventional frame-based cameras presents an effective solution based on\nunsupervised domain adaptation. We propose a new algorithm tailored for\nadapting a deep neural network trained on annotated frame-based data to\ngeneralize well on event-based unannotated data. Our approach incorporates\nuncorrelated conditioning and self-supervised learning in an adversarial\nlearning scheme to close the gap between the two source and target domains. By\napplying self-supervised learning, the algorithm learns to align the\nrepresentations of event-based data with those from frame-based camera data,\nthereby facilitating knowledge transfer.Furthermore, the inclusion of\nuncorrelated conditioning ensures that the adapted model effectively\ndistinguishes between event-based and conventional data, enhancing its ability\nto classify event-based images accurately.Through empirical experimentation and\nevaluation, we demonstrate that our algorithm surpasses existing approaches\ndesigned for the same purpose using two benchmarks. The superior performance of\nour solution is attributed to its ability to effectively utilize annotated data\nfrom frame-based cameras and transfer the acquired knowledge to the event-based\nvision domain.\n","authors":["Mohammad Rostami","Dayuan Jian"],"pdf_url":"https://arxiv.org/pdf/2401.01042v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.09496v2","updated":"2024-01-02T05:01:26Z","published":"2023-09-18T05:38:49Z","title":"CLIP-based Synergistic Knowledge Transfer for Text-based Person\n  Retrieval","summary":"  Text-based Person Retrieval (TPR) aims to retrieve the target person images\ngiven a textual query. The primary challenge lies in bridging the substantial\ngap between vision and language modalities, especially when dealing with\nlimited large-scale datasets. In this paper, we introduce a CLIP-based\nSynergistic Knowledge Transfer (CSKT) approach for TPR. Specifically, to\nexplore the CLIP's knowledge on input side, we first propose a Bidirectional\nPrompts Transferring (BPT) module constructed by text-to-image and\nimage-to-text bidirectional prompts and coupling projections. Secondly, Dual\nAdapters Transferring (DAT) is designed to transfer knowledge on output side of\nMulti-Head Attention (MHA) in vision and language. This synergistic two-way\ncollaborative mechanism promotes the early-stage feature fusion and efficiently\nexploits the existing knowledge of CLIP. CSKT outperforms the state-of-the-art\napproaches across three benchmark datasets when the training parameters merely\naccount for 7.4% of the entire model, demonstrating its remarkable efficiency,\neffectiveness and generalization.\n","authors":["Yating Liu","Yaowei Li","Zimo Liu","Wenming Yang","Yaowei Wang","Qingmin Liao"],"pdf_url":"https://arxiv.org/pdf/2309.09496v2.pdf","comment":"ICASSP2024(accepted). minor typos revision compared to version 1 in\n  arxiv"},{"id":"http://arxiv.org/abs/2401.01035v1","updated":"2024-01-02T04:48:49Z","published":"2024-01-02T04:48:49Z","title":"Online Continual Domain Adaptation for Semantic Image Segmentation Using\n  Internal Representations","summary":"  Semantic segmentation models trained on annotated data fail to generalize\nwell when the input data distribution changes over extended time period,\nleading to requiring re-training to maintain performance. Classic Unsupervised\ndomain adaptation (UDA) attempts to address a similar problem when there is\ntarget domain with no annotated data points through transferring knowledge from\na source domain with annotated data. We develop an online UDA algorithm for\nsemantic segmentation of images that improves model generalization on\nunannotated domains in scenarios where source data access is restricted during\nadaptation. We perform model adaptation is by minimizing the distributional\ndistance between the source latent features and the target features in a shared\nembedding space. Our solution promotes a shared domain-agnostic latent feature\nspace between the two domains, which allows for classifier generalization on\nthe target dataset. To alleviate the need of access to source samples during\nadaptation, we approximate the source latent feature distribution via an\nappropriate surrogate distribution, in this case a Gassian mixture model (GMM).\nWe evaluate our approach on well established semantic segmentation datasets and\ndemonstrate it compares favorably against state-of-the-art (SOTA) UDA semantic\nsegmentation methods.\n","authors":["Serban Stan","Mohammad Rostami"],"pdf_url":"https://arxiv.org/pdf/2401.01035v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.04789v2","updated":"2024-01-02T04:39:09Z","published":"2023-08-09T08:28:25Z","title":"Multi-Scale Memory Comparison for Zero-/Few-Shot Anomaly Detection","summary":"  Anomaly detection has gained considerable attention due to its broad range of\napplications, particularly in industrial defect detection. To address the\nchallenges of data collection, researchers have introduced zero-/few-shot\nanomaly detection techniques that require minimal normal images for each\ncategory. However, complex industrial scenarios often involve multiple objects,\npresenting a significant challenge. In light of this, we propose a\nstraightforward yet powerful multi-scale memory comparison framework for\nzero-/few-shot anomaly detection. Our approach employs a global memory bank to\ncapture features across the entire image, while an individual memory bank\nfocuses on simplified scenes containing a single object. The efficacy of our\nmethod is validated by its remarkable achievement of 4th place in the zero-shot\ntrack and 2nd place in the few-shot track of the Visual Anomaly and Novelty\nDetection (VAND) competition.\n","authors":["Chaoqin Huang","Aofan Jiang","Ya Zhang","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2308.04789v2.pdf","comment":"VAND Runner-up Winner in CVPR 2023"},{"id":"http://arxiv.org/abs/2307.10875v3","updated":"2024-01-02T04:15:02Z","published":"2023-07-20T13:47:30Z","title":"Risk-optimized Outlier Removal for Robust 3D Point Cloud Classification","summary":"  With the growth of 3D sensing technology, deep learning system for 3D point\nclouds has become increasingly important, especially in applications like\nautonomous vehicles where safety is a primary concern. However, there are also\ngrowing concerns about the reliability of these systems when they encounter\nnoisy point clouds, whether occurring naturally or introduced with malicious\nintent. This paper highlights the challenges of point cloud classification\nposed by various forms of noise, from simple background noise to malicious\nbackdoor attacks that can intentionally skew model predictions. While there's\nan urgent need for optimized point cloud denoising, current point outlier\nremoval approaches, an essential step for denoising, rely heavily on\nhandcrafted strategies and are not adapted for higher-level tasks, such as\nclassification. To address this issue, we introduce an innovative point outlier\ncleansing method that harnesses the power of downstream classification models.\nBy employing gradient-based attribution analysis, we define a novel concept:\npoint risk. Drawing inspiration from tail risk minimization in finance, we\nrecast the outlier removal process as an optimization problem, named PointCVaR.\nExtensive experiments show that our proposed technique not only robustly\nfilters diverse point cloud outliers but also consistently and significantly\nenhances existing robust methods for point cloud classification.\n","authors":["Xinke Li","Junchi Lu","Henghui Ding","Changsheng Sun","Joey Tianyi Zhou","Chee Yeow Meng"],"pdf_url":"https://arxiv.org/pdf/2307.10875v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11460v3","updated":"2024-01-02T04:11:12Z","published":"2023-12-18T18:59:06Z","title":"Hybrid Internal Model: Learning Agile Legged Locomotion with Simulated\n  Robot Response","summary":"  Robust locomotion control depends on accurate state estimations. However, the\nsensors of most legged robots can only provide partial and noisy observations,\nmaking the estimation particularly challenging, especially for external states\nlike terrain frictions and elevation maps. Inspired by the classical Internal\nModel Control principle, we consider these external states as disturbances and\nintroduce Hybrid Internal Model (HIM) to estimate them according to the\nresponse of the robot. The response, which we refer to as the hybrid internal\nembedding, contains the robot's explicit velocity and implicit stability\nrepresentation, corresponding to two primary goals for locomotion tasks:\nexplicitly tracking velocity and implicitly maintaining stability. We use\ncontrastive learning to optimize the embedding to be close to the robot's\nsuccessor state, in which the response is naturally embedded. HIM has several\nappealing benefits: It only needs the robot's proprioceptions, i.e., those from\njoint encoders and IMU as observations. It innovatively maintains consistent\nobservations between simulation reference and reality that avoids information\nloss in mimicking learning. It exploits batch-level information that is more\nrobust to noises and keeps better sample efficiency. It only requires 1 hour of\ntraining on an RTX 4090 to enable a quadruped robot to traverse any terrain\nunder any disturbances. A wealth of real-world experiments demonstrates its\nagility, even in high-difficulty tasks and cases never occurred during the\ntraining process, revealing remarkable open-world generalizability.\n","authors":["Junfeng Long","Zirui Wang","Quanyi Li","Jiawei Gao","Liu Cao","Jiangmiao Pang"],"pdf_url":"https://arxiv.org/pdf/2312.11460v3.pdf","comment":"Use 1 hour to train a quadruped robot capable of traversing any\n  terrain under any disturbances in the open world, Project Page:\n  https://github.com/OpenRobotLab/HIMLoco"},{"id":"http://arxiv.org/abs/2401.01010v1","updated":"2024-01-02T03:37:11Z","published":"2024-01-02T03:37:11Z","title":"Unsupervised Continual Anomaly Detection with Contrastively-learned\n  Prompt","summary":"  Unsupervised Anomaly Detection (UAD) with incremental training is crucial in\nindustrial manufacturing, as unpredictable defects make obtaining sufficient\nlabeled data infeasible. However, continual learning methods primarily rely on\nsupervised annotations, while the application in UAD is limited due to the\nabsence of supervision. Current UAD methods train separate models for different\nclasses sequentially, leading to catastrophic forgetting and a heavy\ncomputational burden. To address this issue, we introduce a novel Unsupervised\nContinual Anomaly Detection framework called UCAD, which equips the UAD with\ncontinual learning capability through contrastively-learned prompts. In the\nproposed UCAD, we design a Continual Prompting Module (CPM) by utilizing a\nconcise key-prompt-knowledge memory bank to guide task-invariant `anomaly'\nmodel predictions using task-specific `normal' knowledge. Moreover,\nStructure-based Contrastive Learning (SCL) is designed with the Segment\nAnything Model (SAM) to improve prompt learning and anomaly segmentation\nresults. Specifically, by treating SAM's masks as structure, we draw features\nwithin the same mask closer and push others apart for general feature\nrepresentations. We conduct comprehensive experiments and set the benchmark on\nunsupervised continual anomaly detection and segmentation, demonstrating that\nour method is significantly better than anomaly detection methods, even with\nrehearsal training. The code will be available at\nhttps://github.com/shirowalker/UCAD.\n","authors":["Jiaqi Liu","Kai Wu","Qiang Nie","Ying Chen","Bin-Bin Gao","Yong Liu","Jinbao Wang","Chengjie Wang","Feng Zheng"],"pdf_url":"https://arxiv.org/pdf/2401.01010v1.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2312.11231v2","updated":"2024-01-02T03:34:46Z","published":"2023-12-18T14:30:41Z","title":"Global Feature Pyramid Network","summary":"  The visual feature pyramid has proven its effectiveness and efficiency in\ntarget detection tasks. Yet, current methodologies tend to overly emphasize\ninter-layer feature interaction, neglecting the crucial aspect of intra-layer\nfeature adjustment. Experience underscores the significant advantages of\nintra-layer feature interaction in enhancing target detection tasks. While some\napproaches endeavor to learn condensed intra-layer feature representations\nusing attention mechanisms or visual transformers, they overlook the\nincorporation of global information interaction. This oversight results in\nincreased false detections and missed targets.To address this critical issue,\nthis paper introduces the Global Feature Pyramid Network (GFPNet), an augmented\nversion of PAFPN that integrates global information for enhanced target\ndetection. Specifically, we leverage a lightweight MLP to capture global\nfeature information, utilize the VNC encoder to process these features, and\nemploy a parallel learnable mechanism to extract intra-layer features from the\ninput image. Building on this foundation, we retain the PAFPN method to\nfacilitate inter-layer feature interaction, extracting rich feature details\nacross various levels.Compared to conventional feature pyramids, GFPN not only\neffectively focuses on inter-layer feature information but also captures global\nfeature details, fostering intra-layer feature interaction and generating a\nmore comprehensive and impactful feature representation. GFPN consistently\ndemonstrates performance improvements over object detection baselines.\n","authors":["Weilin Xiao","Ming Xu","Yonggui Lin"],"pdf_url":"https://arxiv.org/pdf/2312.11231v2.pdf","comment":"dataset not open"},{"id":"http://arxiv.org/abs/2212.00330v5","updated":"2024-01-02T02:54:35Z","published":"2022-12-01T07:32:56Z","title":"Reliable Joint Segmentation of Retinal Edema Lesions in OCT Images","summary":"  Focusing on the complicated pathological features, such as blurred\nboundaries, severe scale differences between symptoms, background noise\ninterference, etc., in the task of retinal edema lesions joint segmentation\nfrom OCT images and enabling the segmentation results more reliable. In this\npaper, we propose a novel reliable multi-scale wavelet-enhanced transformer\nnetwork, which can provide accurate segmentation results with reliability\nassessment. Specifically, aiming at improving the model's ability to learn the\ncomplex pathological features of retinal edema lesions in OCT images, we\ndevelop a novel segmentation backbone that integrates a wavelet-enhanced\nfeature extractor network and a multi-scale transformer module of our newly\ndesigned. Meanwhile, to make the segmentation results more reliable, a novel\nuncertainty segmentation head based on the subjective logical evidential theory\nis introduced to generate the final segmentation results with a corresponding\noverall uncertainty evaluation score map. We conduct comprehensive experiments\non the public database of AI-Challenge 2018 for retinal edema lesions\nsegmentation, and the results show that our proposed method achieves better\nsegmentation accuracy with a high degree of reliability as compared to other\nstate-of-the-art segmentation approaches. The code will be released on:\nhttps://github.com/LooKing9218/ReliableRESeg.\n","authors":["Meng Wang","Kai Yu","Chun-Mei Feng","Ke Zou","Yanyu Xu","Qingquan Meng","Rick Siow Mong Goh","Yong Liu","Huazhu Fu"],"pdf_url":"https://arxiv.org/pdf/2212.00330v5.pdf","comment":"Improving algorithm"},{"id":"http://arxiv.org/abs/2309.11715v2","updated":"2024-01-02T02:20:13Z","published":"2023-09-21T01:35:13Z","title":"Deshadow-Anything: When Segment Anything Model Meets Zero-shot shadow\n  removal","summary":"  Segment Anything (SAM), an advanced universal image segmentation model\ntrained on an expansive visual dataset, has set a new benchmark in image\nsegmentation and computer vision. However, it faced challenges when it came to\ndistinguishing between shadows and their backgrounds. To address this, we\ndeveloped Deshadow-Anything, considering the generalization of large-scale\ndatasets, and we performed Fine-tuning on large-scale datasets to achieve image\nshadow removal. The diffusion model can diffuse along the edges and textures of\nan image, helping to remove shadows while preserving the details of the image.\nFurthermore, we design Multi-Self-Attention Guidance (MSAG) and adaptive input\nperturbation (DDPM-AIP) to accelerate the iterative training speed of\ndiffusion. Experiments on shadow removal tasks demonstrate that these methods\ncan effectively improve image restoration performance.\n","authors":["Xiao Feng Zhang","Tian Yi Song","Jia Wei Yao"],"pdf_url":"https://arxiv.org/pdf/2309.11715v2.pdf","comment":"it need revised"},{"id":"http://arxiv.org/abs/2312.01187v2","updated":"2024-01-02T02:05:01Z","published":"2023-12-02T17:25:30Z","title":"SASSL: Enhancing Self-Supervised Learning via Neural Style Transfer","summary":"  Self-supervised learning relies heavily on data augmentation to extract\nmeaningful representations from unlabeled images. While existing\nstate-of-the-art augmentation pipelines incorporate a wide range of primitive\ntransformations, these often disregard natural image structure. Thus, augmented\nsamples can exhibit degraded semantic information and low stylistic diversity,\naffecting downstream performance of self-supervised representations. To\novercome this, we propose SASSL: Style Augmentations for Self Supervised\nLearning, a novel augmentation technique based on Neural Style Transfer. The\nmethod decouples semantic and stylistic attributes in images and applies\ntransformations exclusively to the style while preserving content, generating\ndiverse augmented samples that better retain their semantic properties.\nExperimental results show our technique achieves a top-1 classification\nperformance improvement of more than 2% on ImageNet compared to the\nwell-established MoCo v2. We also measure transfer learning performance across\nfive diverse datasets, observing significant improvements of up to 3.75%. Our\nexperiments indicate that decoupling style from content information and\ntransferring style across datasets to diversify augmentations can significantly\nimprove downstream performance of self-supervised representations.\n","authors":["Renan A. Rojas-Gomez","Karan Singhal","Ali Etemad","Alex Bijamov","Warren R. Morningstar","Philip Andrew Mansfield"],"pdf_url":"https://arxiv.org/pdf/2312.01187v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00989v1","updated":"2024-01-02T01:56:25Z","published":"2024-01-02T01:56:25Z","title":"Diversity-aware Buffer for Coping with Temporally Correlated Data\n  Streams in Online Test-time Adaptation","summary":"  Since distribution shifts are likely to occur after a model's deployment and\ncan drastically decrease the model's performance, online test-time adaptation\n(TTA) continues to update the model during test-time, leveraging the current\ntest data. In real-world scenarios, test data streams are not always\nindependent and identically distributed (i.i.d.). Instead, they are frequently\ntemporally correlated, making them non-i.i.d. Many existing methods struggle to\ncope with this scenario. In response, we propose a diversity-aware and\ncategory-balanced buffer that can simulate an i.i.d. data stream, even in\nnon-i.i.d. scenarios. Combined with a diversity and entropy-weighted entropy\nloss, we show that a stable adaptation is possible on a wide range of\ncorruptions and natural domain shifts, based on ImageNet. We achieve\nstate-of-the-art results on most considered benchmarks.\n","authors":["Mario Döbler","Florian Marencke","Robert A. Marsden","Bin Yang"],"pdf_url":"https://arxiv.org/pdf/2401.00989v1.pdf","comment":"Accepted at ICASSP 2024. arXiv admin note: text overlap with\n  arXiv:2306.00650"},{"id":"http://arxiv.org/abs/2401.00988v1","updated":"2024-01-02T01:54:22Z","published":"2024-01-02T01:54:22Z","title":"Holistic Autonomous Driving Understanding by Bird's-Eye-View Injected\n  Multi-Modal Large Models","summary":"  The rise of multimodal large language models (MLLMs) has spurred interest in\nlanguage-based driving tasks. However, existing research typically focuses on\nlimited tasks and often omits key multi-view and temporal information which is\ncrucial for robust autonomous driving. To bridge these gaps, we introduce\nNuInstruct, a novel dataset with 91K multi-view video-QA pairs across 17\nsubtasks, where each task demands holistic information (e.g., temporal,\nmulti-view, and spatial), significantly elevating the challenge level. To\nobtain NuInstruct, we propose a novel SQL-based method to generate\ninstruction-response pairs automatically, which is inspired by the driving\nlogical progression of humans. We further present BEV-InMLLM, an end-to-end\nmethod for efficiently deriving instruction-aware Bird's-Eye-View (BEV)\nfeatures, language-aligned for large language models. BEV-InMLLM integrates\nmulti-view, spatial awareness, and temporal semantics to enhance MLLMs'\ncapabilities on NuInstruct tasks. Moreover, our proposed BEV injection module\nis a plug-and-play method for existing MLLMs. Our experiments on NuInstruct\ndemonstrate that BEV-InMLLM significantly outperforms existing MLLMs, e.g.\naround 9% improvement on various tasks. We plan to release our NuInstruct for\nfuture research development.\n","authors":["Xinpeng Ding","Jinahua Han","Hang Xu","Xiaodan Liang","Wei Zhang","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2401.00988v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00986v1","updated":"2024-01-02T01:30:03Z","published":"2024-01-02T01:30:03Z","title":"Real-Time Object Detection in Occluded Environment with Background\n  Cluttering Effects Using Deep Learning","summary":"  Detection of small, undetermined moving objects or objects in an occluded\nenvironment with a cluttered background is the main problem of computer vision.\nThis greatly affects the detection accuracy of deep learning models. To\novercome these problems, we concentrate on deep learning models for real-time\ndetection of cars and tanks in an occluded environment with a cluttered\nbackground employing SSD and YOLO algorithms and improved precision of\ndetection and reduce problems faced by these models. The developed method makes\nthe custom dataset and employs a preprocessing technique to clean the noisy\ndataset. For training the developed model we apply the data augmentation\ntechnique to balance and diversify the data. We fine-tuned, trained, and\nevaluated these models on the established dataset by applying these techniques\nand highlighting the results we got more accurately than without applying these\ntechniques. The accuracy and frame per second of the SSD-Mobilenet v2 model are\nhigher than YOLO V3 and YOLO V4. Furthermore, by employing various techniques\nlike data enhancement, noise reduction, parameter optimization, and model\nfusion we improve the effectiveness of detection and recognition. We further\nadded a counting algorithm, and target attributes experimental comparison, and\nmade a graphical user interface system for the developed model with features of\nobject counting, alerts, status, resolution, and frame per second.\nSubsequently, to justify the importance of the developed method analysis of\nYOLO V3, V4, and SSD were incorporated. Which resulted in the overall\ncompletion of the proposed method.\n","authors":["Syed Muhammad Aamir","Hongbin Ma","Malak Abid Ali Khan","Muhammad Aaqib"],"pdf_url":"https://arxiv.org/pdf/2401.00986v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00979v1","updated":"2024-01-02T00:42:06Z","published":"2024-01-02T00:42:06Z","title":"3D Visibility-aware Generalizable Neural Radiance Fields for Interacting\n  Hands","summary":"  Neural radiance fields (NeRFs) are promising 3D representations for scenes,\nobjects, and humans. However, most existing methods require multi-view inputs\nand per-scene training, which limits their real-life applications. Moreover,\ncurrent methods focus on single-subject cases, leaving scenes of interacting\nhands that involve severe inter-hand occlusions and challenging view variations\nremain unsolved. To tackle these issues, this paper proposes a generalizable\nvisibility-aware NeRF (VA-NeRF) framework for interacting hands. Specifically,\ngiven an image of interacting hands as input, our VA-NeRF first obtains a\nmesh-based representation of hands and extracts their corresponding geometric\nand textural features. Subsequently, a feature fusion module that exploits the\nvisibility of query points and mesh vertices is introduced to adaptively merge\nfeatures of both hands, enabling the recovery of features in unseen areas.\nAdditionally, our VA-NeRF is optimized together with a novel discriminator\nwithin an adversarial learning paradigm. In contrast to conventional\ndiscriminators that predict a single real/fake label for the synthesized image,\nthe proposed discriminator generates a pixel-wise visibility map, providing\nfine-grained supervision for unseen areas and encouraging the VA-NeRF to\nimprove the visual quality of synthesized images. Experiments on the\nInterhand2.6M dataset demonstrate that our proposed VA-NeRF outperforms\nconventional NeRFs significantly. Project Page:\n\\url{https://github.com/XuanHuang0/VANeRF}.\n","authors":["Xuan Huang","Hanhui Li","Zejun Yang","Zhisheng Wang","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2401.00979v1.pdf","comment":"Accepted by AAAI-24"},{"id":"http://arxiv.org/abs/2308.15752v2","updated":"2024-01-02T23:39:10Z","published":"2023-08-30T04:29:48Z","title":"Large-scale data extraction from the UNOS organ donor documents","summary":"  The scope of our study is all UNOS data of the USA organ donors since 2008.\nThe data is not analyzable in a large scale in the past because it was captured\nin PDF documents known as \"Attachments\", whereby every donor is represented by\ndozens of PDF documents in heterogenous formats. To make the data analyzable,\none needs to convert the content inside these PDFs to an analyzable data\nformat, such as a standard SQL database. In this paper we will focus on 2022\nUNOS data comprised of $\\approx 400,000$ PDF documents spanning millions of\npages. The totality of UNOS data covers 15 years (2008--20022) and our results\nwill be quickly extended to the entire data. Our method captures a portion of\nthe data in DCD flowsheets, kidney perfusion data, and data captured during\npatient hospital stay (e.g. vital signs, ventilator settings, etc.). The\ncurrent paper assumes that the reader is familiar with the content of the UNOS\ndata. The overview of the types of data and challenges they present is a\nsubject of another paper. Here we focus on demonstrating that the goal of\nbuilding a comprehensive, analyzable database from UNOS documents is an\nattainable task, and we provide an overview of our methodology. The project\nresulted in datasets by far larger than previously available even in this\npreliminary phase.\n","authors":["Marek Rychlik","Bekir Tanriover","Yan Han"],"pdf_url":"https://arxiv.org/pdf/2308.15752v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01461v1","updated":"2024-01-02T23:28:20Z","published":"2024-01-02T23:28:20Z","title":"Efficient Hybrid Zoom using Camera Fusion on Mobile Phones","summary":"  DSLR cameras can achieve multiple zoom levels via shifting lens distances or\nswapping lens types. However, these techniques are not possible on smartphone\ndevices due to space constraints. Most smartphone manufacturers adopt a hybrid\nzoom system: commonly a Wide (W) camera at a low zoom level and a Telephoto (T)\ncamera at a high zoom level. To simulate zoom levels between W and T, these\nsystems crop and digitally upsample images from W, leading to significant\ndetail loss. In this paper, we propose an efficient system for hybrid zoom\nsuper-resolution on mobile devices, which captures a synchronous pair of W and\nT shots and leverages machine learning models to align and transfer details\nfrom T to W. We further develop an adaptive blending method that accounts for\ndepth-of-field mismatches, scene occlusion, flow uncertainty, and alignment\nerrors. To minimize the domain gap, we design a dual-phone camera rig to\ncapture real-world inputs and ground-truths for supervised training. Our method\ngenerates a 12-megapixel image in 500ms on a mobile platform and compares\nfavorably against state-of-the-art methods under extensive evaluation on\nreal-world scenarios.\n","authors":["Xiaotong Wu","Wei-Sheng Lai","YiChang Shih","Charles Herrmann","Michael Krainin","Deqing Sun","Chia-Kai Liang"],"pdf_url":"https://arxiv.org/pdf/2401.01461v1.pdf","comment":"Accepted to SIGGRAPH Asia 2023 (ACM TOG). Project website:\n  https://www.wslai.net/publications/fusion_zoom"},{"id":"http://arxiv.org/abs/2401.01456v1","updated":"2024-01-02T22:46:12Z","published":"2024-01-02T22:46:12Z","title":"ColorizeDiffusion: Adjustable Sketch Colorization with Reference Image\n  and Text","summary":"  Recently, diffusion models have demonstrated their effectiveness in\ngenerating extremely high-quality images and have found wide-ranging\napplications, including automatic sketch colorization. However, most existing\nmodels use text to guide the conditional generation, with fewer attempts\nexploring the potential advantages of using image tokens as conditional inputs\nfor networks. As such, this paper exhaustively investigates image-guided\nmodels, specifically targeting reference-based sketch colorization, which aims\nto colorize sketch images using reference color images. We investigate three\ncritical aspects of reference-based diffusion models: the shortcomings compared\nto text-based counterparts, the training strategies, and the capability in\nzero-shot, sequential text-based manipulation. We introduce two variations of\nan image-guided latent diffusion model using different image tokens from the\npre-trained CLIP image encoder, and we propose corresponding manipulation\nmethods to adjust their results sequentially using weighted text inputs. We\nconduct comprehensive evaluations of our models through qualitative and\nquantitative experiments, as well as a user study.\n","authors":["Dingkun Yan","Liang Yuan","Yuma Nishioka","Issei Fujishiro","Suguru Saito"],"pdf_url":"https://arxiv.org/pdf/2401.01456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01454v1","updated":"2024-01-02T22:35:33Z","published":"2024-01-02T22:35:33Z","title":"A Survey on Autonomous Driving Datasets: Data Statistic, Annotation, and\n  Outlook","summary":"  Autonomous driving has rapidly developed and shown promising performance with\nrecent advances in hardware and deep learning methods. High-quality datasets\nare fundamental for developing reliable autonomous driving algorithms. Previous\ndataset surveys tried to review the datasets but either focused on a limited\nnumber or lacked detailed investigation of the characters of datasets. To this\nend, we present an exhaustive study of over 200 autonomous driving datasets\nfrom multiple perspectives, including sensor modalities, data size, tasks, and\ncontextual conditions. We introduce a novel metric to evaluate the impact of\neach dataset, which can also be a guide for establishing new datasets. We\nfurther analyze the annotation process and quality of datasets. Additionally,\nwe conduct an in-depth analysis of the data distribution of several vital\ndatasets. Finally, we discuss the development trend of the future autonomous\ndriving datasets.\n","authors":["Mingyu Liu","Ekim Yurtsever","Xingcheng Zhou","Jonathan Fossaert","Yuning Cui","Bare Luka Zagar","Alois C. Knoll"],"pdf_url":"https://arxiv.org/pdf/2401.01454v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01448v1","updated":"2024-01-02T22:15:20Z","published":"2024-01-02T22:15:20Z","title":"ProbMCL: Simple Probabilistic Contrastive Learning for Multi-label\n  Visual Classification","summary":"  Multi-label image classification presents a challenging task in many domains,\nincluding computer vision and medical imaging. Recent advancements have\nintroduced graph-based and transformer-based methods to improve performance and\ncapture label dependencies. However, these methods often include complex\nmodules that entail heavy computation and lack interpretability. In this paper,\nwe propose Probabilistic Multi-label Contrastive Learning (ProbMCL), a novel\nframework to address these challenges in multi-label image classification\ntasks. Our simple yet effective approach employs supervised contrastive\nlearning, in which samples that share enough labels with an anchor image based\non a decision threshold are introduced as a positive set. This structure\ncaptures label dependencies by pulling positive pair embeddings together and\npushing away negative samples that fall below the threshold. We enhance\nrepresentation learning by incorporating a mixture density network into\ncontrastive learning and generating Gaussian mixture distributions to explore\nthe epistemic uncertainty of the feature encoder. We validate the effectiveness\nof our framework through experimentation with datasets from the computer vision\nand medical imaging domains. Our method outperforms the existing\nstate-of-the-art methods while achieving a low computational footprint on both\ndatasets. Visualization analyses also demonstrate that ProbMCL-learned\nclassifiers maintain a meaningful semantic topology.\n","authors":["Ahmad Sajedi","Samir Khaki","Yuri A. Lawryshyn","Konstantinos N. Plataniotis"],"pdf_url":"https://arxiv.org/pdf/2401.01448v1.pdf","comment":"This paper has been accepted for the IEEE International Conference on\n  Acoustics, Speech and Signal Processing (ICASSP) 2024"},{"id":"http://arxiv.org/abs/2401.01445v1","updated":"2024-01-02T22:07:44Z","published":"2024-01-02T22:07:44Z","title":"Indoor Obstacle Discovery on Reflective Ground via Monocular Camera","summary":"  Visual obstacle discovery is a key step towards autonomous navigation of\nindoor mobile robots. Successful solutions have many applications in multiple\nscenes. One of the exceptions is the reflective ground. In this case, the\nreflections on the floor resemble the true world, which confuses the obstacle\ndiscovery and leaves navigation unsuccessful. We argue that the key to this\nproblem lies in obtaining discriminative features for reflections and\nobstacles. Note that obstacle and reflection can be separated by the ground\nplane in 3D space. With this observation, we firstly introduce a\npre-calibration based ground detection scheme that uses robot motion to predict\nthe ground plane. Due to the immunity of robot motion to reflection, this\nscheme avoids failed ground detection caused by reflection. Given the detected\nground, we design a ground-pixel parallax to describe the location of a pixel\nrelative to the ground. Based on this, a unified appearance-geometry feature\nrepresentation is proposed to describe objects inside rectangular boxes.\nEventually, based on segmenting by detection framework, an appearance-geometry\nfusion regressor is designed to utilize the proposed feature to discover the\nobstacles. It also prevents our model from concentrating too much on parts of\nobstacles instead of whole obstacles. For evaluation, we introduce a new\ndataset for Obstacle on Reflective Ground (ORG), which comprises 15 scenes with\nvarious ground reflections, a total of more than 200 image sequences and 3400\nRGB images. The pixel-wise annotations of ground and obstacle provide a\ncomparison to our method and other methods. By reducing the misdetection of the\nreflection, the proposed approach outperforms others. The source code and the\ndataset will be available at\nhttps://github.com/XuefengBUPT/IndoorObstacleDiscovery-RG.\n","authors":["Feng Xue","Yicong Chang","Tianxi Wang","Yu Zhou","Anlong Ming"],"pdf_url":"https://arxiv.org/pdf/2401.01445v1.pdf","comment":"International Journal of Computer Vision (IJCV) 2023. Project Page:\n  https://xuefeng-cvr.github.io/IODRG"},{"id":"http://arxiv.org/abs/2303.06876v3","updated":"2024-01-02T21:47:08Z","published":"2023-03-13T05:51:35Z","title":"A Test Statistic Estimation-based Approach for Establishing\n  Self-interpretable CNN-based Binary Classifiers","summary":"  Interpretability is highly desired for deep neural network-based classifiers,\nespecially when addressing high-stake decisions in medical imaging. Commonly\nused post-hoc interpretability methods have the limitation that they can\nproduce plausible but different interpretations of a given model, leading to\nambiguity about which one to choose. To address this problem, a novel\ndecision-theory-inspired approach is investigated to establish a\nself-interpretable model, given a pre-trained deep binary black-box medical\nimage classifier. This approach involves utilizing a self-interpretable\nencoder-decoder model in conjunction with a single-layer fully connected\nnetwork with unity weights. The model is trained to estimate the test statistic\nof the given trained black-box deep binary classifier to maintain a similar\naccuracy. The decoder output image, referred to as an equivalency map, is an\nimage that represents a transformed version of the to-be-classified image that,\nwhen processed by the fixed fully connected layer, produces the same test\nstatistic value as the original classifier. The equivalency map provides a\nvisualization of the transformed image features that directly contribute to the\ntest statistic value and, moreover, permits quantification of their relative\ncontributions. Unlike the traditional post-hoc interpretability methods, the\nproposed method is self-interpretable, quantitative. Detailed quantitative and\nqualitative analyses have been performed with three different medical image\nbinary classification tasks.\n","authors":["Sourya Sengupta","Mark A. Anastasio"],"pdf_url":"https://arxiv.org/pdf/2303.06876v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01439v1","updated":"2024-01-02T21:27:43Z","published":"2024-01-02T21:27:43Z","title":"Off-Road LiDAR Intensity Based Semantic Segmentation","summary":"  LiDAR is used in autonomous driving to provide 3D spatial information and\nenable accurate perception in off-road environments, aiding in obstacle\ndetection, mapping, and path planning. Learning-based LiDAR semantic\nsegmentation utilizes machine learning techniques to automatically classify\nobjects and regions in LiDAR point clouds. Learning-based models struggle in\noff-road environments due to the presence of diverse objects with varying\ncolors, textures, and undefined boundaries, which can lead to difficulties in\naccurately classifying and segmenting objects using traditional geometric-based\nfeatures. In this paper, we address this problem by harnessing the LiDAR\nintensity parameter to enhance object segmentation in off-road environments.\nOur approach was evaluated in the RELLIS-3D data set and yielded promising\nresults as a preliminary analysis with improved mIoU for classes \"puddle\" and\n\"grass\" compared to more complex deep learning-based benchmarks. The\nmethodology was evaluated for compatibility across both Velodyne and Ouster\nLiDAR systems, assuring its cross-platform applicability. This analysis\nadvocates for the incorporation of calibrated intensity as a supplementary\ninput, aiming to enhance the prediction accuracy of learning based semantic\nsegmentation frameworks.\nhttps://github.com/MOONLABIISERB/lidar-intensity-predictor/tree/main\n","authors":["Kasi Viswanath","Peng Jiang","Sujit PB","Srikanth Saripalli"],"pdf_url":"https://arxiv.org/pdf/2401.01439v1.pdf","comment":"Accepted to ISER 2023"},{"id":"http://arxiv.org/abs/2307.05134v2","updated":"2024-01-02T21:18:48Z","published":"2023-07-11T09:23:05Z","title":"TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation","summary":"  The progress in the generation of synthetic images has made it crucial to\nassess their quality. While several metrics have been proposed to assess the\nrendering of images, it is crucial for Text-to-Image (T2I) models, which\ngenerate images based on a prompt, to consider additional aspects such as to\nwhich extent the generated image matches the important content of the prompt.\nMoreover, although the generated images usually result from a random starting\npoint, the influence of this one is generally not considered. In this article,\nwe propose a new metric based on prompt templates to study the alignment\nbetween the content specified in the prompt and the corresponding generated\nimages. It allows us to better characterize the alignment in terms of the type\nof the specified objects, their number, and their color. We conducted a study\non several recent T2I models about various aspects. An additional interesting\nresult we obtained with our approach is that image quality can vary drastically\ndepending on the noise used as a seed for the images. We also quantify the\ninfluence of the number of concepts in the prompt, their order as well as their\n(color) attributes. Finally, our method allows us to identify some seeds that\nproduce better images than others, opening novel directions of research on this\nunderstudied topic.\n","authors":["Paul Grimal","Hervé Le Borgne","Olivier Ferret","Julien Tourille"],"pdf_url":"https://arxiv.org/pdf/2307.05134v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.09073v3","updated":"2024-01-02T20:53:03Z","published":"2023-05-16T00:03:09Z","title":"Consensus and Subjectivity of Skin Tone Annotation for ML Fairness","summary":"  Understanding different human attributes and how they affect model behavior\nmay become a standard need for all model creation and usage, from traditional\ncomputer vision tasks to the newest multimodal generative AI systems. In\ncomputer vision specifically, we have relied on datasets augmented with\nperceived attribute signals (e.g., gender presentation, skin tone, and age) and\nbenchmarks enabled by these datasets. Typically labels for these tasks come\nfrom human annotators. However, annotating attribute signals, especially skin\ntone, is a difficult and subjective task. Perceived skin tone is affected by\ntechnical factors, like lighting conditions, and social factors that shape an\nannotator's lived experience. This paper examines the subjectivity of skin tone\nannotation through a series of annotation experiments using the Monk Skin Tone\n(MST) scale, a small pool of professional photographers, and a much larger pool\nof trained crowdsourced annotators. Along with this study we release the Monk\nSkin Tone Examples (MST-E) dataset, containing 1515 images and 31 videos spread\nacross the full MST scale. MST-E is designed to help train human annotators to\nannotate MST effectively. Our study shows that annotators can reliably annotate\nskin tone in a way that aligns with an expert in the MST scale, even under\nchallenging environmental conditions. We also find evidence that annotators\nfrom different geographic regions rely on different mental models of MST\ncategories resulting in annotations that systematically vary across regions.\nGiven this, we advise practitioners to use a diverse set of annotators and a\nhigher replication count for each image when annotating skin tone for fairness\nresearch.\n","authors":["Candice Schumann","Gbolahan O. Olanubi","Auriel Wright","Ellis Monk Jr.","Courtney Heldreth","Susanna Ricco"],"pdf_url":"https://arxiv.org/pdf/2305.09073v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01425v1","updated":"2024-01-02T20:28:06Z","published":"2024-01-02T20:28:06Z","title":"SwapTransformer: highway overtaking tactical planner model via imitation\n  learning on OSHA dataset","summary":"  This paper investigates the high-level decision-making problem in highway\nscenarios regarding lane changing and over-taking other slower vehicles. In\nparticular, this paper aims to improve the Travel Assist feature for automatic\novertaking and lane changes on highways. About 9 million samples including lane\nimages and other dynamic objects are collected in simulation. This data;\nOvertaking on Simulated HighwAys (OSHA) dataset is released to tackle this\nchallenge. To solve this problem, an architecture called SwapTransformer is\ndesigned and implemented as an imitation learning approach on the OSHA dataset.\nMoreover, auxiliary tasks such as future points and car distance network\npredictions are proposed to aid the model in better understanding the\nsurrounding environment. The performance of the proposed solution is compared\nwith a multi-layer perceptron (MLP) and multi-head self-attention networks as\nbaselines in a simulation environment. We also demonstrate the performance of\nthe model with and without auxiliary tasks. All models are evaluated based on\ndifferent metrics such as time to finish each lap, number of overtakes, and\nspeed difference with speed limit. The evaluation shows that the\nSwapTransformer model outperforms other models in different traffic densities\nin the inference phase.\n","authors":["Alireza Shamsoshoara","Safin B Salih","Pedram Aghazadeh"],"pdf_url":"https://arxiv.org/pdf/2401.01425v1.pdf","comment":"19 pages, 12 Figures, 1 Algorithm, 2 Tables"},{"id":"http://arxiv.org/abs/2312.11195v2","updated":"2024-01-02T19:58:09Z","published":"2023-12-18T13:41:21Z","title":"Cross-Age Contrastive Learning for Age-Invariant Face Recognition","summary":"  Cross-age facial images are typically challenging and expensive to collect,\nmaking noise-free age-oriented datasets relatively small compared to\nwidely-used large-scale facial datasets. Additionally, in real scenarios,\nimages of the same subject at different ages are usually hard or even\nimpossible to obtain. Both of these factors lead to a lack of supervised data,\nwhich limits the versatility of supervised methods for age-invariant face\nrecognition, a critical task in applications such as security and biometrics.\nTo address this issue, we propose a novel semi-supervised learning approach\nnamed Cross-Age Contrastive Learning (CACon). Thanks to the identity-preserving\npower of recent face synthesis models, CACon introduces a new contrastive\nlearning method that leverages an additional synthesized sample from the input\nimage. We also propose a new loss function in association with CACon to perform\ncontrastive learning on a triplet of samples. We demonstrate that our method\nnot only achieves state-of-the-art performance in homogeneous-dataset\nexperiments on several age-invariant face recognition benchmarks but also\noutperforms other methods by a large margin in cross-dataset experiments.\n","authors":["Haoyi Wang","Victor Sanchez","Chang-Tsun Li"],"pdf_url":"https://arxiv.org/pdf/2312.11195v2.pdf","comment":"ICASSP 2024"},{"id":"http://arxiv.org/abs/2401.01702v1","updated":"2024-01-02T18:59:35Z","published":"2024-01-02T18:59:35Z","title":"Image Sculpting: Precise Object Editing with 3D Geometry Control","summary":"  We present Image Sculpting, a new framework for editing 2D images by\nincorporating tools from 3D geometry and graphics. This approach differs\nmarkedly from existing methods, which are confined to 2D spaces and typically\nrely on textual instructions, leading to ambiguity and limited control. Image\nSculpting converts 2D objects into 3D, enabling direct interaction with their\n3D geometry. Post-editing, these objects are re-rendered into 2D, merging into\nthe original image to produce high-fidelity results through a coarse-to-fine\nenhancement process. The framework supports precise, quantifiable, and\nphysically-plausible editing options such as pose editing, rotation,\ntranslation, 3D composition, carving, and serial addition. It marks an initial\nstep towards combining the creative freedom of generative models with the\nprecision of graphics pipelines.\n","authors":["Jiraphon Yenphraphai","Xichen Pan","Sainan Liu","Daniele Panozzo","Saining Xie"],"pdf_url":"https://arxiv.org/pdf/2401.01702v1.pdf","comment":"Code and project page: https://image-sculpting.github.io"},{"id":"http://arxiv.org/abs/2401.01395v1","updated":"2024-01-02T18:03:57Z","published":"2024-01-02T18:03:57Z","title":"Deep autoregressive modeling for land use land cover","summary":"  Land use / land cover (LULC) modeling is a challenging task due to long-range\ndependencies between geographic features and distinct spatial patterns related\nto topography, ecology, and human development. We identify a close connection\nbetween modeling of spatial patterns of land use and the task of image\ninpainting from computer vision and conduct a study of a modified PixelCNN\narchitecture with approximately 19 million parameters for modeling LULC. In\ncomparison with a benchmark spatial statistical model, we find that the former\nis capable of capturing much richer spatial correlation patterns such as roads\nand water bodies but does not produce a calibrated predictive distribution,\nsuggesting the need for additional tuning. We find evidence of predictive\nunderdispersion with regard to important ecologically-relevant land use\nstatistics such as patch count and adjacency which can be ameliorated to some\nextent by manipulating sampling variability.\n","authors":["Christopher Krapu","Mark Borsuk","Ryan Calder"],"pdf_url":"https://arxiv.org/pdf/2401.01395v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2401.01330v1","updated":"2024-01-02T18:40:03Z","published":"2024-01-02T18:40:03Z","title":"TREC iKAT 2023: The Interactive Knowledge Assistance Track Overview","summary":"  Conversational Information Seeking stands as a pivotal research area with\nsignificant contributions from previous works. The TREC Interactive Knowledge\nAssistance Track (iKAT) builds on the foundational work of the TREC\nConversational Assistance Track (CAsT). However, iKAT distinctively emphasizes\nthe creation and research of conversational search agents that adapt responses\nbased on user's prior interactions and present context. The challenge lies in\nenabling Conversational Search Agents (CSA) to incorporate this personalized\ncontext to efficiency and effectively guide users through the relevant\ninformation to them. iKAT also emphasizes decisional search tasks, where users\nsift through data and information to weigh up options in order to reach a\nconclusion or perform an action. These tasks, prevalent in everyday\ninformation-seeking decisions -- be it related to travel, health, or shopping\n-- often revolve around a subset of high-level information operators where\nqueries or questions about the information space include: finding options,\ncomparing options, identifying the pros and cons of options, etc. Given the\ndifferent personas and their information need (expressed through the sequence\nof questions), diverse conversation trajectories will arise -- because the\nanswers to these similar queries will be very different. In this paper, we\nreport on the first year of TREC iKAT, describing the task, topics, data\ncollection, and evaluation framework. We further review the submissions and\nsummarize the findings.\n","authors":["Mohammad Aliannejadi","Zahra Abbasiantaeb","Shubham Chatterjee","Jeffery Dalton","Leif Azzopardi"],"pdf_url":"https://arxiv.org/pdf/2401.01330v1.pdf","comment":"TREC 2023 Overview Paper"},{"id":"http://arxiv.org/abs/2311.00333v2","updated":"2024-01-02T15:18:54Z","published":"2023-11-01T06:52:41Z","title":"Caseformer: Pre-training for Legal Case Retrieval Based on Inter-Case\n  Distinctions","summary":"  Legal case retrieval aims to help legal workers find relevant cases related\nto their cases at hand, which is important for the guarantee of fairness and\njustice in legal judgments. While recent advances in neural retrieval methods\nhave significantly improved the performance of open-domain retrieval tasks\n(e.g., Web search), their advantages have not been observed in legal case\nretrieval due to their thirst for annotated data. As annotating large-scale\ntraining data in legal domains is prohibitive due to the need for domain\nexpertise, traditional search techniques based on lexical matching such as\nTF-IDF, BM25, and Query Likelihood are still prevalent in legal case retrieval\nsystems. While previous studies have designed several pre-training methods for\nIR models in open-domain tasks, these methods are usually suboptimal in legal\ncase retrieval because they cannot understand and capture the key knowledge and\ndata structures in the legal corpus. To this end, we propose a novel\npre-training framework named Caseformer that enables the pre-trained models to\nlearn legal knowledge and domain-specific relevance information in legal case\nretrieval without any human-labeled data. Through three unsupervised learning\ntasks, Caseformer is able to capture the special language, document structure,\nand relevance patterns of legal case documents, making it a strong backbone for\ndownstream legal case retrieval tasks. Experimental results show that our model\nhas achieved state-of-the-art performance in both zero-shot and full-data\nfine-tuning settings. Also, experiments on both Chinese and English legal\ndatasets demonstrate that the effectiveness of Caseformer is\nlanguage-independent in legal case retrieval.\n","authors":["Weihang Su","Qingyao Ai","Yueyue Wu","Yixiao Ma","Haitao Li","Yiqun Liu","Zhijing Wu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.00333v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14587v2","updated":"2024-01-02T07:22:04Z","published":"2023-10-23T05:52:09Z","title":"Large Search Model: Redefining Search Stack in the Era of LLMs","summary":"  Modern search engines are built on a stack of different components, including\nquery understanding, retrieval, multi-stage ranking, and question answering,\namong others. These components are often optimized and deployed independently.\nIn this paper, we introduce a novel conceptual framework called large search\nmodel, which redefines the conventional search stack by unifying search tasks\nwith one large language model (LLM). All tasks are formulated as autoregressive\ntext generation problems, allowing for the customization of tasks through the\nuse of natural language prompts. This proposed framework capitalizes on the\nstrong language understanding and reasoning capabilities of LLMs, offering the\npotential to enhance search result quality while simultaneously simplifying the\nexisting cumbersome search stack. To substantiate the feasibility of this\nframework, we present a series of proof-of-concept experiments and discuss the\npotential challenges associated with implementing this approach within\nreal-world search systems.\n","authors":["Liang Wang","Nan Yang","Xiaolong Huang","Linjun Yang","Rangan Majumder","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2310.14587v2.pdf","comment":"SIGIR Forum, Vol. 57 No. 2 - December 2023"},{"id":"http://arxiv.org/abs/2312.15826v3","updated":"2024-01-02T02:08:36Z","published":"2023-12-25T23:14:03Z","title":"Adversarial Item Promotion on Visually-Aware Recommender Systems by\n  Guided Diffusion","summary":"  Visually-aware recommender systems have found widespread application in\ndomains where visual elements significantly contribute to the inference of\nusers' potential preferences. While the incorporation of visual information\nholds the promise of enhancing recommendation accuracy and alleviating the\ncold-start problem, it is essential to point out that the inclusion of item\nimages may introduce substantial security challenges. Some existing works have\nshown that the item provider can manipulate item exposure rates to its\nadvantage by constructing adversarial images. However, these works cannot\nreveal the real vulnerability of visually-aware recommender systems because (1)\nThe generated adversarial images are markedly distorted, rendering them easily\ndetectable by human observers; (2) The effectiveness of the attacks is\ninconsistent and even ineffective in some scenarios. To shed light on the real\nvulnerabilities of visually-aware recommender systems when confronted with\nadversarial images, this paper introduces a novel attack method, IPDGI (Item\nPromotion by Diffusion Generated Image). Specifically, IPDGI employs a guided\ndiffusion model to generate adversarial samples designed to deceive\nvisually-aware recommender systems. Taking advantage of accurately modeling\nbenign images' distribution by diffusion models, the generated adversarial\nimages have high fidelity with original images, ensuring the stealth of our\nIPDGI. To demonstrate the effectiveness of our proposed methods, we conduct\nextensive experiments on two commonly used e-commerce recommendation datasets\n(Amazon Beauty and Amazon Baby) with several typical visually-aware recommender\nsystems. The experimental results show that our attack method has a significant\nimprovement in both the performance of promoting the long-tailed (i.e.,\nunpopular) items and the quality of generated adversarial images.\n","authors":["Lijian Chen","Wei Yuan","Tong Chen","Guanhua Ye","Quoc Viet Hung Nguyen","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2312.15826v3.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2401.01335v1","updated":"2024-01-02T18:53:13Z","published":"2024-01-02T18:53:13Z","title":"Self-Play Fine-Tuning Converts Weak Language Models to Strong Language\n  Models","summary":"  Harnessing the power of human-annotated data through Supervised Fine-Tuning\n(SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we\ndelve into the prospect of growing a strong LLM out of a weak one without the\nneed for acquiring additional human-annotated data. We propose a new\nfine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a\nsupervised fine-tuned model. At the heart of SPIN lies a self-play mechanism,\nwhere the LLM refines its capability by playing against instances of itself.\nMore specifically, the LLM generates its own training data from its previous\niterations, refining its policy by discerning these self-generated responses\nfrom those obtained from human-annotated data. Our method progressively\nelevates the LLM from a nascent model to a formidable one, unlocking the full\npotential of human-annotated demonstration data for SFT. Theoretically, we\nprove that the global optimum to the training objective function of our method\nis achieved only when the LLM policy aligns with the target data distribution.\nEmpirically, we evaluate our method on several benchmark datasets including the\nHuggingFace Open LLM Leaderboard, MT-Bench, and datasets from Big-Bench. Our\nresults show that SPIN can significantly improve the LLM's performance across a\nvariety of benchmarks and even outperform models trained through direct\npreference optimization (DPO) supplemented with extra GPT-4 preference data.\nThis sheds light on the promise of self-play, enabling the achievement of\nhuman-level performance in LLMs without the need for expert opponents.\n","authors":["Zixiang Chen","Yihe Deng","Huizhuo Yuan","Kaixuan Ji","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2401.01335v1.pdf","comment":"28 pages, 6 figures, 6 tables"},{"id":"http://arxiv.org/abs/2312.17300v2","updated":"2024-01-02T18:43:50Z","published":"2023-12-28T17:24:13Z","title":"Improving Intrusion Detection with Domain-Invariant Representation\n  Learning in Latent Space","summary":"  Domain generalization focuses on leveraging knowledge from multiple related\ndomains with ample training data and labels to enhance inference on unseen\nin-distribution (IN) and out-of-distribution (OOD) domains. In our study, we\nintroduce a two-phase representation learning technique using multi-task\nlearning. This approach aims to cultivate a latent space from features spanning\nmultiple domains, encompassing both native and cross-domains, to amplify\ngeneralization to IN and OOD territories. Additionally, we attempt to\ndisentangle the latent space by minimizing the mutual information between the\nprior and latent space, effectively de-correlating spurious feature\ncorrelations. Collectively, the joint optimization will facilitate\ndomain-invariant feature learning. We assess the model's efficacy across\nmultiple cybersecurity datasets, using standard classification metrics on both\nunseen IN and OOD sets, and juxtapose the results with contemporary domain\ngeneralization methods.\n","authors":["Padmaksha Roy","Tyler Cody","Himanshu Singhal","Kevin Choi","Ming Jin"],"pdf_url":"https://arxiv.org/pdf/2312.17300v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01326v1","updated":"2024-01-02T18:32:14Z","published":"2024-01-02T18:32:14Z","title":"An Autoregressive Text-to-Graph Framework for Joint Entity and Relation\n  Extraction","summary":"  In this paper, we propose a novel method for joint entity and relation\nextraction from unstructured text by framing it as a conditional sequence\ngeneration problem. In contrast to conventional generative information\nextraction models that are left-to-right token-level generators, our approach\nis \\textit{span-based}. It generates a linearized graph where nodes represent\ntext spans and edges represent relation triplets. Our method employs a\ntransformer encoder-decoder architecture with pointing mechanism on a dynamic\nvocabulary of spans and relation types. Our model can capture the structural\ncharacteristics and boundaries of entities and relations through span\nrepresentations while simultaneously grounding the generated output in the\noriginal text thanks to the pointing mechanism. Evaluation on benchmark\ndatasets validates the effectiveness of our approach, demonstrating competitive\nresults. Code is available at https://github.com/urchade/ATG.\n","authors":["Zaratiana Urchade","Nadi Tomeh","Pierre Holat","Thierry Charnois"],"pdf_url":"https://arxiv.org/pdf/2401.01326v1.pdf","comment":"AAAI 2024"},{"id":"http://arxiv.org/abs/2401.01325v1","updated":"2024-01-02T18:30:51Z","published":"2024-01-02T18:30:51Z","title":"LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning","summary":"  This work elicits LLMs' inherent ability to handle long contexts without\nfine-tuning. The limited length of the training sequence during training may\nlimit the application of Large Language Models (LLMs) on long input sequences\nfor inference. In this work, we argue that existing LLMs themselves have\ninherent capabilities for handling long contexts. Based on this argument, we\nsuggest extending LLMs' context window by themselves to fully utilize the\ninherent ability.We propose Self-Extend to stimulate LLMs' long context\nhandling potential. The basic idea is to construct bi-level attention\ninformation: the group level and the neighbor level. The two levels are\ncomputed by the original model's self-attention, which means the proposed does\nnot require any training. With only four lines of code modification, the\nproposed method can effortlessly extend existing LLMs' context window without\nany fine-tuning. We conduct comprehensive experiments and the results show that\nthe proposed method can effectively extend existing LLMs' context window's\nlength.\n","authors":["Hongye Jin","Xiaotian Han","Jingfeng Yang","Zhimeng Jiang","Zirui Liu","Chia-Yuan Chang","Huiyuan Chen","Xia Hu"],"pdf_url":"https://arxiv.org/pdf/2401.01325v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.14082v5","updated":"2024-01-02T18:23:59Z","published":"2021-09-28T23:00:30Z","title":"Sample-Efficient Safety Assurances using Conformal Prediction","summary":"  When deploying machine learning models in high-stakes robotics applications,\nthe ability to detect unsafe situations is crucial. Early warning systems can\nprovide alerts when an unsafe situation is imminent (in the absence of\ncorrective action). To reliably improve safety, these warning systems should\nhave a provable false negative rate; i.e. of the situations that are unsafe,\nfewer than $\\epsilon$ will occur without an alert. In this work, we present a\nframework that combines a statistical inference technique known as conformal\nprediction with a simulator of robot/environment dynamics, in order to tune\nwarning systems to provably achieve an $\\epsilon$ false negative rate using as\nfew as $1/\\epsilon$ data points. We apply our framework to a driver warning\nsystem and a robotic grasping application, and empirically demonstrate\nguaranteed false negative rate while also observing low false detection\n(positive) rate.\n","authors":["Rachel Luo","Shengjia Zhao","Jonathan Kuck","Boris Ivanovic","Silvio Savarese","Edward Schmerling","Marco Pavone"],"pdf_url":"https://arxiv.org/pdf/2109.14082v5.pdf","comment":"International Journal of Robotics Research, 2023"},{"id":"http://arxiv.org/abs/2312.01479v5","updated":"2024-01-02T17:45:43Z","published":"2023-12-03T18:41:54Z","title":"OpenVoice: Versatile Instant Voice Cloning","summary":"  We introduce OpenVoice, a versatile voice cloning approach that requires only\na short audio clip from the reference speaker to replicate their voice and\ngenerate speech in multiple languages. OpenVoice represents a significant\nadvancement in addressing the following open challenges in the field: 1)\nFlexible Voice Style Control. OpenVoice enables granular control over voice\nstyles, including emotion, accent, rhythm, pauses, and intonation, in addition\nto replicating the tone color of the reference speaker. The voice styles are\nnot directly copied from and constrained by the style of the reference speaker.\nPrevious approaches lacked the ability to flexibly manipulate voice styles\nafter cloning. 2) Zero-Shot Cross-Lingual Voice Cloning. OpenVoice achieves\nzero-shot cross-lingual voice cloning for languages not included in the\nmassive-speaker training set. Unlike previous approaches, which typically\nrequire extensive massive-speaker multi-lingual (MSML) dataset for all\nlanguages, OpenVoice can clone voices into a new language without any\nmassive-speaker training data for that language. OpenVoice is also\ncomputationally efficient, costing tens of times less than commercially\navailable APIs that offer even inferior performance. To foster further research\nin the field, we have made the source code and trained model publicly\naccessible. We also provide qualitative results in our demo website. Prior to\nits public release, our internal version of OpenVoice was used tens of millions\nof times by users worldwide between May and October 2023, serving as the\nbackend of MyShell.\n","authors":["Zengyi Qin","Wenliang Zhao","Xumin Yu","Xin Sun"],"pdf_url":"https://arxiv.org/pdf/2312.01479v5.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2401.01306v1","updated":"2024-01-02T17:32:53Z","published":"2024-01-02T17:32:53Z","title":"Learning solutions to some toy constrained optimization problems in\n  infinite dimensional Hilbert spaces","summary":"  In this work we present deep learning implementations of two popular\ntheoretical constrained optimization algorithms in infinite dimensional Hilbert\nspaces, namely, the penalty and the augmented Lagrangian methods. We test these\nalgorithms on some toy problems originating in either calculus of variations or\nphysics. We demonstrate that both methods are able to produce decent\napproximations for the test problems and are comparable in terms of different\nerrors. Leveraging the common occurrence of the Lagrange multiplier update rule\nbeing computationally less expensive than solving subproblems in the penalty\nmethod, we achieve significant speedups in cases when the output of the\nconstraint function is itself a function.\n","authors":["Pinak Mandal"],"pdf_url":"https://arxiv.org/pdf/2401.01306v1.pdf","comment":"16 pages, 10 figures"},{"id":"http://arxiv.org/abs/2401.01303v1","updated":"2024-01-02T17:30:45Z","published":"2024-01-02T17:30:45Z","title":"Integrating Edges into U-Net Models with Explainable Activation Maps for\n  Brain Tumor Segmentation using MR Images","summary":"  Manual delineation of tumor regions from magnetic resonance (MR) images is\ntime-consuming, requires an expert, and is prone to human error. In recent\nyears, deep learning models have been the go-to approach for the segmentation\nof brain tumors. U-Net and its' variants for semantic segmentation of medical\nimages have achieved good results in the literature. However, U-Net and its'\nvariants tend to over-segment tumor regions and may not accurately segment the\ntumor edges. The edges of the tumor are as important as the tumor regions for\naccurate diagnosis, surgical precision, and treatment planning. In the proposed\nwork, the authors aim to extract edges from the ground truth using a\nderivative-like filter followed by edge reconstruction to obtain an edge ground\ntruth in addition to the brain tumor ground truth. Utilizing both ground\ntruths, the author studies several U-Net and its' variant architectures with\nand without tumor edges ground truth as a target along with the tumor ground\ntruth for brain tumor segmentation. The author used the BraTS2020 benchmark\ndataset to perform the study and the results are tabulated for the dice and\nHausdorff95 metrics. The mean and median metrics are calculated for the whole\ntumor (WT), tumor core (TC), and enhancing tumor (ET) regions. Compared to the\nbaseline U-Net and its variants, the models that learned edges along with the\ntumor regions performed well in core tumor regions in both training and\nvalidation datasets. The improved performance of edge-trained models trained on\nbaseline models like U-Net and V-Net achieved performance similar to baseline\nstate-of-the-art models like Swin U-Net and hybrid MR-U-Net. The edge-target\ntrained models are capable of generating edge maps that can be useful for\ntreatment planning. Additionally, for further explainability of the results,\nthe activation map generated by the hybrid MR-U-Net has been studied.\n","authors":["Subin Sahayam","Umarani Jayaraman"],"pdf_url":"https://arxiv.org/pdf/2401.01303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05332v3","updated":"2024-01-02T17:19:39Z","published":"2023-12-08T19:33:22Z","title":"Bridging the Gaps: Learning Verifiable Model-Free Quadratic Programming\n  Controllers Inspired by Model Predictive Control","summary":"  In this paper, we introduce a new class of parameterized controllers, drawing\ninspiration from Model Predictive Control (MPC). The controller resembles a\nQuadratic Programming (QP) solver of a linear MPC problem, with the parameters\nof the controller being trained via Deep Reinforcement Learning (DRL) rather\nthan derived from system models. This approach addresses the limitations of\ncommon controllers with Multi-Layer Perceptron (MLP) or other general neural\nnetwork architecture used in DRL, in terms of verifiability and performance\nguarantees, and the learned controllers possess verifiable properties like\npersistent feasibility and asymptotic stability akin to MPC. On the other hand,\nnumerical examples illustrate that the proposed controller empirically matches\nMPC and MLP controllers in terms of control performance and has superior\nrobustness against modeling uncertainty and noises. Furthermore, the proposed\ncontroller is significantly more computationally efficient compared to MPC and\nrequires fewer parameters to learn than MLP controllers. Real-world experiments\non vehicle drift maneuvering task demonstrate the potential of these\ncontrollers for robotics and other demanding control tasks.\n","authors":["Yiwen Lu","Zishuo Li","Yihan Zhou","Na Li","Yilin Mo"],"pdf_url":"https://arxiv.org/pdf/2312.05332v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01294v1","updated":"2024-01-02T17:13:34Z","published":"2024-01-02T17:13:34Z","title":"Efficient Sparse Least Absolute Deviation Regression with Differential\n  Privacy","summary":"  In recent years, privacy-preserving machine learning algorithms have\nattracted increasing attention because of their important applications in many\nscientific fields. However, in the literature, most privacy-preserving\nalgorithms demand learning objectives to be strongly convex and Lipschitz\nsmooth, which thus cannot cover a wide class of robust loss functions (e.g.,\nquantile/least absolute loss). In this work, we aim to develop a fast\nprivacy-preserving learning solution for a sparse robust regression problem.\nOur learning loss consists of a robust least absolute loss and an $\\ell_1$\nsparse penalty term. To fast solve the non-smooth loss under a given privacy\nbudget, we develop a Fast Robust And Privacy-Preserving Estimation (FRAPPE)\nalgorithm for least absolute deviation regression. Our algorithm achieves a\nfast estimation by reformulating the sparse LAD problem as a penalized least\nsquare estimation problem and adopts a three-stage noise injection to guarantee\nthe $(\\epsilon,\\delta)$-differential privacy. We show that our algorithm can\nachieve better privacy and statistical accuracy trade-off compared with the\nstate-of-the-art privacy-preserving regression algorithms. In the end, we\nconduct experiments to verify the efficiency of our proposed FRAPPE algorithm.\n","authors":["Weidong Liu","Xiaojun Mao","Xiaofei Zhang","Xin Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.01294v1.pdf","comment":"IEEE Transactions on Information Forensics and Security, 2024"},{"id":"http://arxiv.org/abs/2401.01286v1","updated":"2024-01-02T16:54:58Z","published":"2024-01-02T16:54:58Z","title":"A Comprehensive Study of Knowledge Editing for Large Language Models","summary":"  Large Language Models (LLMs) have shown extraordinary capabilities in\nunderstanding and generating text that closely mirrors human communication.\nHowever, a primary limitation lies in the significant computational demands\nduring training, arising from their extensive parameterization. This challenge\nis further intensified by the dynamic nature of the world, necessitating\nfrequent updates to LLMs to correct outdated information or integrate new\nknowledge, thereby ensuring their continued relevance. Note that many\napplications demand continual model adjustments post-training to address\ndeficiencies or undesirable behaviors. There is an increasing interest in\nefficient, lightweight methods for on-the-fly model modifications. To this end,\nrecent years have seen a burgeoning in the techniques of knowledge editing for\nLLMs, which aim to efficiently modify LLMs' behaviors within specific domains\nwhile preserving overall performance across various inputs. In this paper, we\nfirst define the knowledge editing problem and then provide a comprehensive\nreview of cutting-edge approaches. Drawing inspiration from educational and\ncognitive research theories, we propose a unified categorization criterion that\nclassifies knowledge editing methods into three groups: resorting to external\nknowledge, merging knowledge into the model, and editing intrinsic knowledge.\nFurthermore, we introduce a new benchmark, KnowEdit, for a comprehensive\nempirical evaluation of representative knowledge editing approaches.\nAdditionally, we provide an in-depth analysis of knowledge location, which can\nprovide a deeper understanding of the knowledge structures inherent within\nLLMs. Finally, we discuss several potential applications of knowledge editing,\noutlining its broad and impactful implications.\n","authors":["Ningyu Zhang","Yunzhi Yao","Bozhong Tian","Peng Wang","Shumin Deng","Mengru Wang","Zekun Xi","Shengyu Mao","Jintian Zhang","Yuansheng Ni","Siyuan Cheng","Ziwen Xu","Xin Xu","Jia-Chen Gu","Yong Jiang","Pengjun Xie","Fei Huang","Lei Liang","Zhiqiang Zhang","Xiaowei Zhu","Jun Zhou","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2401.01286v1.pdf","comment":"Ongoing work; 50 pages, 265 citations; benchmark is available at\n  https://huggingface.co/datasets/zjunlp/KnowEdit; code is available at\n  https://github.com/zjunlp/EasyEdit; paper list is available at\n  https://github.com/zjunlp/KnowledgeEditingPapers"},{"id":"http://arxiv.org/abs/2401.01280v1","updated":"2024-01-02T16:37:42Z","published":"2024-01-02T16:37:42Z","title":"GEqO: ML-Accelerated Semantic Equivalence Detection","summary":"  Large scale analytics engines have become a core dependency for modern\ndata-driven enterprises to derive business insights and drive actions. These\nengines support a large number of analytic jobs processing huge volumes of data\non a daily basis, and workloads are often inundated with overlapping\ncomputations across multiple jobs. Reusing common computation is crucial for\nefficient cluster resource utilization and reducing job execution time.\nDetecting common computation is the first and key step for reducing this\ncomputational redundancy. However, detecting equivalence on large-scale\nanalytics engines requires efficient and scalable solutions that are fully\nautomated. In addition, to maximize computation reuse, equivalence needs to be\ndetected at the semantic level instead of just the syntactic level (i.e., the\nability to detect semantic equivalence of seemingly different-looking queries).\nUnfortunately, existing solutions fall short of satisfying these requirements.\n  In this paper, we take a major step towards filling this gap by proposing\nGEqO, a portable and lightweight machine-learning-based framework for\nefficiently identifying semantically equivalent computations at scale. GEqO\nintroduces two machine-learning-based filters that quickly prune out\nnonequivalent subexpressions and employs a semi-supervised learning feedback\nloop to iteratively improve its model with an intelligent sampling mechanism.\nFurther, with its novel database-agnostic featurization method, GEqO can\ntransfer the learning from one workload and database to another. Our extensive\nempirical evaluation shows that, on TPC-DS-like queries, GEqO yields\nsignificant performance gains-up to 200x faster than automated verifiers-and\nfinds up to 2x more equivalences than optimizer and signature-based equivalence\ndetection approaches.\n","authors":["Brandon Haynes","Rana Alotaibi","Anna Pavlenko","Jyoti Leeka","Alekh Jindal","Yuanyuan Tian"],"pdf_url":"https://arxiv.org/pdf/2401.01280v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01273v1","updated":"2024-01-02T16:18:53Z","published":"2024-01-02T16:18:53Z","title":"Learning-based agricultural management in partially observable\n  environments subject to climate variability","summary":"  Agricultural management, with a particular focus on fertilization strategies,\nholds a central role in shaping crop yield, economic profitability, and\nenvironmental sustainability. While conventional guidelines offer valuable\ninsights, their efficacy diminishes when confronted with extreme weather\nconditions, such as heatwaves and droughts. In this study, we introduce an\ninnovative framework that integrates Deep Reinforcement Learning (DRL) with\nRecurrent Neural Networks (RNNs). Leveraging the Gym-DSSAT simulator, we train\nan intelligent agent to master optimal nitrogen fertilization management.\nThrough a series of simulation experiments conducted on corn crops in Iowa, we\ncompare Partially Observable Markov Decision Process (POMDP) models with Markov\nDecision Process (MDP) models. Our research underscores the advantages of\nutilizing sequential observations in developing more efficient nitrogen input\npolicies. Additionally, we explore the impact of climate variability,\nparticularly during extreme weather events, on agricultural outcomes and\nmanagement. Our findings demonstrate the adaptability of fertilization policies\nto varying climate conditions. Notably, a fixed policy exhibits resilience in\nthe face of minor climate fluctuations, leading to commendable corn yields,\ncost-effectiveness, and environmental conservation. However, our study\nilluminates the need for agent retraining to acquire new optimal policies under\nextreme weather events. This research charts a promising course toward\nadaptable fertilization strategies that can seamlessly align with dynamic\nclimate scenarios, ultimately contributing to the optimization of crop\nmanagement practices.\n","authors":["Zhaoan Wang","Shaoping Xiao","Junchao Li","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2401.01273v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01270v1","updated":"2024-01-02T16:14:35Z","published":"2024-01-02T16:14:35Z","title":"Optimal Rates of Kernel Ridge Regression under Source Condition in Large\n  Dimensions","summary":"  Motivated by the studies of neural networks (e.g.,the neural tangent kernel\ntheory), we perform a study on the large-dimensional behavior of kernel ridge\nregression (KRR) where the sample size $n \\asymp d^{\\gamma}$ for some $\\gamma >\n0$. Given an RKHS $\\mathcal{H}$ associated with an inner product kernel defined\non the sphere $\\mathbb{S}^{d}$, we suppose that the true function $f_{\\rho}^{*}\n\\in [\\mathcal{H}]^{s}$, the interpolation space of $\\mathcal{H}$ with source\ncondition $s>0$. We first determined the exact order (both upper and lower\nbound) of the generalization error of kernel ridge regression for the optimally\nchosen regularization parameter $\\lambda$. We then further showed that when\n$0<s\\le1$, KRR is minimax optimal; and when $s>1$, KRR is not minimax optimal\n(a.k.a. he saturation effect). Our results illustrate that the curves of rate\nvarying along $\\gamma$ exhibit the periodic plateau behavior and the multiple\ndescent behavior and show how the curves evolve with $s>0$. Interestingly, our\nwork provides a unified viewpoint of several recent works on kernel regression\nin the large-dimensional setting, which correspond to $s=0$ and $s=1$\nrespectively.\n","authors":["Haobo Zhang","Yicheng Li","Weihao Lu","Qian Lin"],"pdf_url":"https://arxiv.org/pdf/2401.01270v1.pdf","comment":"61 pages, 11 figures"},{"id":"http://arxiv.org/abs/2401.01268v1","updated":"2024-01-02T16:14:02Z","published":"2024-01-02T16:14:02Z","title":"$f$-Divergence Based Classification: Beyond the Use of Cross-Entropy","summary":"  In deep learning, classification tasks are formalized as optimization\nproblems solved via the minimization of the cross-entropy. However, recent\nadvancements in the design of objective functions allow the $f$-divergence\nmeasure to generalize the formulation of the optimization problem for\nclassification. With this goal in mind, we adopt a Bayesian perspective and\nformulate the classification task as a maximum a posteriori probability\nproblem. We propose a class of objective functions based on the variational\nrepresentation of the $f$-divergence, from which we extract a list of five\nposterior probability estimators leveraging well-known $f$-divergences. In\naddition, driven by the challenge of improving the state-of-the-art approach,\nwe propose a bottom-up method that leads us to the formulation of a new\nobjective function (and posterior probability estimator) corresponding to a\nnovel $f$-divergence referred to as shifted log (SL). First, we theoretically\nprove the convergence property of the posterior probability estimators. Then,\nwe numerically test the set of proposed objective functions in three\napplication scenarios: toy examples, image data sets, and signal\ndetection/decoding problems. The analyzed tasks demonstrate the effectiveness\nof the proposed estimators and that the SL divergence achieves the highest\nclassification accuracy in almost all the scenarios.\n","authors":["Nicola Novello","Andrea M. Tonello"],"pdf_url":"https://arxiv.org/pdf/2401.01268v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01262v1","updated":"2024-01-02T16:09:36Z","published":"2024-01-02T16:09:36Z","title":"Fairness Certification for Natural Language Processing and Large\n  Language Models","summary":"  Natural Language Processing (NLP) plays an important role in our daily lives,\nparticularly due to the enormous progress of Large Language Models (LLM).\nHowever, NLP has many fairness-critical use cases, e.g., as an expert system in\nrecruitment or as an LLM-based tutor in education. Since NLP is based on human\nlanguage, potentially harmful biases can diffuse into NLP systems and produce\nunfair results, discriminate against minorities or generate legal issues.\nHence, it is important to develop a fairness certification for NLP approaches.\nWe follow a qualitative research approach towards a fairness certification for\nNLP. In particular, we have reviewed a large body of literature on algorithmic\nfairness, and we have conducted semi-structured expert interviews with a wide\nrange of experts from that area. We have systematically devised six fairness\ncriteria for NLP, which can be further refined into 18 sub-categories. Our\ncriteria offer a foundation for operationalizing and testing processes to\ncertify fairness, both from the perspective of the auditor and the audited\norganization.\n","authors":["Vincent Freiberger","Erik Buchmann"],"pdf_url":"https://arxiv.org/pdf/2401.01262v1.pdf","comment":"In depth discussion of our results can be found in the Appendix"},{"id":"http://arxiv.org/abs/2401.01259v1","updated":"2024-01-02T16:05:23Z","published":"2024-01-02T16:05:23Z","title":"Do Concept Bottleneck Models Obey Locality?","summary":"  Concept-based learning improves a deep learning model's interpretability by\nexplaining its predictions via human-understandable concepts. Deep learning\nmodels trained under this paradigm heavily rely on the assumption that neural\nnetworks can learn to predict the presence or absence of a given concept\nindependently of other concepts. Recent work, however, strongly suggests that\nthis assumption may fail to hold in Concept Bottleneck Models (CBMs), a\nquintessential family of concept-based interpretable architectures. In this\npaper, we investigate whether CBMs correctly capture the degree of conditional\nindependence across concepts when such concepts are localised both spatially,\nby having their values entirely defined by a fixed subset of features, and\nsemantically, by having their values correlated with only a fixed subset of\npredefined concepts. To understand locality, we analyse how changes to features\noutside of a concept's spatial or semantic locality impact concept predictions.\nOur results suggest that even in well-defined scenarios where the presence of a\nconcept is localised to a fixed feature subspace, or whose semantics are\ncorrelated to a small subset of other concepts, CBMs fail to learn this\nlocality. These results cast doubt upon the quality of concept representations\nlearnt by CBMs and strongly suggest that concept-based explanations may be\nfragile to changes outside their localities.\n","authors":["Naveen Raman","Mateo Espinosa Zarlenga","Juyeon Heo","Mateja Jamnik"],"pdf_url":"https://arxiv.org/pdf/2401.01259v1.pdf","comment":"12 pages, accepted at NeurIPS'23 XAI Workshop"},{"id":"http://arxiv.org/abs/2401.01258v1","updated":"2024-01-02T15:59:00Z","published":"2024-01-02T15:59:00Z","title":"Towards Model-Free LQR Control over Rate-Limited Channels","summary":"  Given the success of model-free methods for control design in many problem\nsettings, it is natural to ask how things will change if realistic\ncommunication channels are utilized for the transmission of gradients or\npolicies. While the resulting problem has analogies with the formulations\nstudied under the rubric of networked control systems, the rich literature in\nthat area has typically assumed that the model of the system is known. As a\nstep towards bridging the fields of model-free control design and networked\ncontrol systems, we ask: \\textit{Is it possible to solve basic control problems\n- such as the linear quadratic regulator (LQR) problem - in a model-free manner\nover a rate-limited channel?} Toward answering this question, we study a\nsetting where a worker agent transmits quantized policy gradients (of the LQR\ncost) to a server over a noiseless channel with a finite bit-rate. We propose a\nnew algorithm titled Adaptively Quantized Gradient Descent (\\texttt{AQGD}), and\nprove that above a certain finite threshold bit-rate, \\texttt{AQGD} guarantees\nexponentially fast convergence to the globally optimal policy, with \\textit{no\ndeterioration of the exponent relative to the unquantized setting}. More\ngenerally, our approach reveals the benefits of adaptive quantization in\npreserving fast linear convergence rates, and, as such, may be of independent\ninterest to the literature on compressed optimization.\n","authors":["Aritra Mitra","Lintao Ye","Vijay Gupta"],"pdf_url":"https://arxiv.org/pdf/2401.01258v1.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2210.14826v3","updated":"2024-01-02T15:54:24Z","published":"2022-10-26T16:15:45Z","title":"tf.data service: A Case for Disaggregating ML Input Data Processing","summary":"  Machine learning (ML) computations commonly execute on expensive specialized\nhardware, such as GPUs and TPUs, which provide high FLOPs and\nperformance-per-watt. For cost efficiency, it is essential to keep these\naccelerators highly utilized. This requires preprocessing input data at the\nrate at which the accelerators can ingest and perform ML computations on the\ndata. To avoid data stalls, the host CPU and RAM required for input data\nprocessing per accelerator core used for ML computations varies across jobs.\nHence, the traditional approach of processing input data on ML accelerator\nhosts with a fixed hardware ratio leads to either under-utilizing the\naccelerators or the host CPU and RAM. In this paper, we address these concerns\nby building a disaggregated ML data processing system.\n  We present tf.data service, an open-source disaggregated input data\nprocessing service built on top of tf.data in TensorFlow. We show that\ndisaggregating data preprocessing has three key advantages for large-scale ML\ntraining jobs. First, the service can horizontally scale-out to right-size\nCPU/RAM host resources for data processing in each job, saving 32x training\ntime and 26x cost, on average. Second, the service can share ephemeral\npreprocessed data results across jobs, to optimize CPU usage and reduce\nredundant computations. Finally, the service supports coordinated reads, a\ntechnique that avoids stragglers due to different input sizes in distributed\ntraining, reducing training time by 2.2x, on average. Our design is inspired by\nlessons learned from deploying tf.data service in production, including\nrelaxing data visitation guarantees without impacting model accuracy.\n","authors":["Andrew Audibert","Yang Chen","Dan Graur","Ana Klimovic","Jiri Simsa","Chandramohan A. Thekkath"],"pdf_url":"https://arxiv.org/pdf/2210.14826v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01243v1","updated":"2024-01-02T15:19:01Z","published":"2024-01-02T15:19:01Z","title":"Contrastive Sequential Interaction Network Learning on Co-Evolving\n  Riemannian Spaces","summary":"  The sequential interaction network usually find itself in a variety of\napplications, e.g., recommender system. Herein, inferring future interaction is\nof fundamental importance, and previous efforts are mainly focused on the\ndynamics in the classic zero-curvature Euclidean space. Despite the promising\nresults achieved by previous methods, a range of significant issues still\nlargely remains open: On the bipartite nature, is it appropriate to place user\nand item nodes in one identical space regardless of their inherent difference?\nOn the network dynamics, instead of a fixed curvature space, will the\nrepresentation spaces evolve when new interactions arrive continuously? On the\nlearning paradigm, can we get rid of the label information costly to acquire?\nTo address the aforementioned issues, we propose a novel Contrastive model for\nSequential Interaction Network learning on Co-Evolving RiEmannian spaces,\nCSINCERE. To the best of our knowledge, we are the first to introduce a couple\nof co-evolving representation spaces, rather than a single or static space, and\npropose a co-contrastive learning for the sequential interaction network. In\nCSINCERE, we formulate a Cross-Space Aggregation for message-passing across\nrepresentation spaces of different Riemannian geometries, and design a Neural\nCurvature Estimator based on Ricci curvatures for modeling the space evolvement\nover time. Thereafter, we present a Reweighed Co-Contrast between the temporal\nviews of the sequential network, so that the couple of Riemannian spaces\ninteract with each other for the interaction prediction without labels.\nEmpirical results on 5 public datasets show the superiority of CSINCERE over\nthe state-of-the-art methods.\n","authors":["Li Sun","Junda Ye","Jiawei Zhang","Yong Yang","Mingsheng Liu","Feiyang Wang","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2401.01243v1.pdf","comment":"The extension of ACM WebConf23 (WWW)"},{"id":"http://arxiv.org/abs/2401.01242v1","updated":"2024-01-02T15:18:23Z","published":"2024-01-02T15:18:23Z","title":"Encoding Binary Events from Continuous Time Series in Rooted Trees using\n  Contrastive Learning","summary":"  Broadband infrastructure owners do not always know how their customers are\nconnected in the local networks, which are structured as rooted trees. A recent\nstudy is able to infer the topology of a local network using discrete time\nseries data from the leaves of the tree (customers). In this study we propose a\ncontrastive approach for learning a binary event encoder from continuous time\nseries data. As a preliminary result, we show that our approach has some\npotential in learning a valuable encoder.\n","authors":["Tobias Engelhardt Rasmussen","Siv Sørensen"],"pdf_url":"https://arxiv.org/pdf/2401.01242v1.pdf","comment":"Extended abstract presented as a poster at the Northern Lights Deep\n  Learning Conference 2024 in Troms{\\o}, Norway"},{"id":"http://arxiv.org/abs/2312.10144v2","updated":"2024-01-02T15:16:36Z","published":"2023-12-15T19:00:07Z","title":"Data-Efficient Multimodal Fusion on a Single GPU","summary":"  The goal of multimodal alignment is to learn a single latent space that is\nshared between multimodal inputs. The most powerful models in this space have\nbeen trained using massive datasets of paired inputs and large-scale\ncomputational resources, making them prohibitively expensive to train in many\npractical scenarios. We surmise that existing unimodal encoders pre-trained on\nlarge amounts of unimodal data should provide an effective bootstrap to create\nmultimodal models from unimodal ones at much lower costs. We therefore propose\nFuseMix, a multimodal augmentation scheme that operates on the latent spaces of\narbitrary pre-trained unimodal encoders. Using FuseMix for multimodal\nalignment, we achieve competitive performance -- and in certain cases\noutperform state-of-the art methods -- in both image-text and audio-text\nretrieval, with orders of magnitude less compute and data: for example, we\noutperform CLIP on the Flickr30K text-to-image retrieval task with $\\sim \\!\n600\\times$ fewer GPU days and $\\sim \\! 80\\times$ fewer image-text pairs.\nAdditionally, we show how our method can be applied to convert pre-trained\ntext-to-image generative models into audio-to-image ones. Code is available at:\nhttps://github.com/layer6ai-labs/fusemix.\n","authors":["Noël Vouitsis","Zhaoyan Liu","Satya Krishna Gorti","Valentin Villecroze","Jesse C. Cresswell","Guangwei Yu","Gabriel Loaiza-Ganem","Maksims Volkovs"],"pdf_url":"https://arxiv.org/pdf/2312.10144v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15218v4","updated":"2024-01-02T15:13:44Z","published":"2023-11-26T07:19:10Z","title":"Real-Time Online Stock Forecasting Utilizing Integrated Quantitative and\n  Qualitative Analysis","summary":"  The application of Machine learning to finance has become a familiar\napproach, even more so in stock market forecasting. The stock market is highly\nvolatile, and huge amounts of data are generated every minute globally. The\nextraction of effective intelligence from this data is of critical importance.\nHowever, a collaboration of numerical stock data with qualitative text data can\nbe a challenging task. In this work, we accomplish this by providing an\nunprecedented, publicly available dataset with technical and fundamental data\nand sentiment that we gathered from news archives, TV news captions, radio\ntranscripts, tweets, daily financial newspapers, etc. The text data entries\nused for sentiment extraction total more than 1.4 Million. The dataset consists\nof daily entries from January 2018 to December 2022 for eight companies\nrepresenting diverse industrial sectors and the Dow Jones Industrial Average\n(DJIA) as a whole. Holistic Fundamental and Technical data is provided training\nready for Model learning and deployment. Most importantly, the data generated\ncould be used for incremental online learning with real-time data points\nretrieved daily since no stagnant data was utilized. All the data was retired\nfrom APIs or self-designed robust information retrieval technologies with\nextremely low latency and zero monetary cost. These adaptable technologies\nfacilitate data extraction for any stock. Moreover, the utilization of\nSpearman's rank correlation over real-time data, linking stock returns with\nsentiment analysis has produced noteworthy results for the DJIA and the eight\nother stocks, achieving accuracy levels surpassing 60%. The dataset is made\navailable at https://github.com/batking24/Huge-Stock-Dataset.\n","authors":["Sai Akash Bathini","Dagli Cihan"],"pdf_url":"https://arxiv.org/pdf/2311.15218v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01233v1","updated":"2024-01-02T14:58:59Z","published":"2024-01-02T14:58:59Z","title":"Graph Elimination Networks","summary":"  Graph Neural Networks (GNNs) are widely applied across various domains, yet\nthey perform poorly in deep layers. Existing research typically attributes this\nproblem to node over-smoothing, where node representations become\nindistinguishable after multiple rounds of propagation. In this paper, we delve\ninto the neighborhood propagation mechanism of GNNs and discover that the real\nroot cause of GNNs' performance degradation in deep layers lies in ineffective\nneighborhood feature propagation. This propagation leads to an exponential\ngrowth of a node's current representation at every propagation step, making it\nextremely challenging to capture valuable dependencies between long-distance\nnodes. To address this issue, we introduce Graph Elimination Networks (GENs),\nwhich employ a specific algorithm to eliminate redundancies during neighborhood\npropagation. We demonstrate that GENs can enhance nodes' perception of distant\nneighborhoods and extend the depth of network propagation. Extensive\nexperiments show that GENs outperform the state-of-the-art methods on various\ngraph-level and node-level datasets.\n","authors":["Shuo Wang","Ge Cheng","Yun Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.01233v1.pdf","comment":"Includes 8 pages of main text and 4 pages of appendices"},{"id":"http://arxiv.org/abs/2401.01232v1","updated":"2024-01-02T14:58:26Z","published":"2024-01-02T14:58:26Z","title":"Motif-aware Riemannian Graph Neural Network with Generative-Contrastive\n  Learning","summary":"  Graphs are typical non-Euclidean data of complex structures. In recent years,\nRiemannian graph representation learning has emerged as an exciting alternative\nto Euclidean ones. However, Riemannian methods are still in an early stage:\nmost of them present a single curvature (radius) regardless of structural\ncomplexity, suffer from numerical instability due to the\nexponential/logarithmic map, and lack the ability to capture motif regularity.\nIn light of the issues above, we propose the problem of \\emph{Motif-aware\nRiemannian Graph Representation Learning}, seeking a numerically stable encoder\nto capture motif regularity in a diverse-curvature manifold without labels. To\nthis end, we present a novel Motif-aware Riemannian model with\nGenerative-Contrastive learning (MotifRGC), which conducts a minmax game in\nRiemannian manifold in a self-supervised manner. First, we propose a new type\nof Riemannian GCN (D-GCN), in which we construct a diverse-curvature manifold\nby a product layer with the diversified factor, and replace the\nexponential/logarithmic map by a stable kernel layer. Second, we introduce a\nmotif-aware Riemannian generative-contrastive learning to capture motif\nregularity in the constructed manifold and learn motif-aware node\nrepresentation without external labels. Empirical results show the superiority\nof MofitRGC.\n","authors":["Li Sun","Zhenhao Huang","Zixi Wang","Feiyang Wang","Hao Peng","Philip Yu"],"pdf_url":"https://arxiv.org/pdf/2401.01232v1.pdf","comment":"Accepted by AAAI24"},{"id":"http://arxiv.org/abs/2209.06950v8","updated":"2024-01-02T14:49:22Z","published":"2022-09-14T21:53:27Z","title":"Lossy Image Compression with Conditional Diffusion Models","summary":"  This paper outlines an end-to-end optimized lossy image compression framework\nusing diffusion generative models. The approach relies on the transform coding\nparadigm, where an image is mapped into a latent space for entropy coding and,\nfrom there, mapped back to the data space for reconstruction. In contrast to\nVAE-based neural compression, where the (mean) decoder is a deterministic\nneural network, our decoder is a conditional diffusion model. Our approach thus\nintroduces an additional ``content'' latent variable on which the reverse\ndiffusion process is conditioned and uses this variable to store information\nabout the image. The remaining ``texture'' variables characterizing the\ndiffusion process are synthesized at decoding time. We show that the model's\nperformance can be tuned toward perceptual metrics of interest. Our extensive\nexperiments involving multiple datasets and image quality assessment metrics\nshow that our approach yields stronger reported FID scores than the GAN-based\nmodel, while also yielding competitive performance with VAE-based models in\nseveral distortion metrics. Furthermore, training the diffusion with\n$\\mathcal{X}$-parameterization enables high-quality reconstructions in only a\nhandful of decoding steps, greatly affecting the model's practicality. Our code\nis available at: \\url{https://github.com/buggyyang/CDC_compression}\n","authors":["Ruihan Yang","Stephan Mandt"],"pdf_url":"https://arxiv.org/pdf/2209.06950v8.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.04365v5","updated":"2024-01-02T14:47:26Z","published":"2023-08-08T16:04:42Z","title":"SLEM: Machine Learning for Path Modeling and Causal Inference with Super\n  Learner Equation Modeling","summary":"  Causal inference is a crucial goal of science, enabling researchers to arrive\nat meaningful conclusions regarding the predictions of hypothetical\ninterventions using observational data. Path models, Structural Equation Models\n(SEMs), and, more generally, Directed Acyclic Graphs (DAGs), provide a means to\nunambiguously specify assumptions regarding the causal structure underlying a\nphenomenon. Unlike DAGs, which make very few assumptions about the functional\nand parametric form, SEM assumes linearity. This can result in functional\nmisspecification which prevents researchers from undertaking reliable effect\nsize estimation. In contrast, we propose Super Learner Equation Modeling, a\npath modeling technique integrating machine learning Super Learner ensembles.\nWe empirically demonstrate its ability to provide consistent and unbiased\nestimates of causal effects, its competitive performance for linear models when\ncompared with SEM, and highlight its superiority over SEM when dealing with\nnon-linear relationships. We provide open-source code, and a tutorial notebook\nwith example usage, accentuating the easy-to-use nature of the method.\n","authors":["Matthew J. Vowels"],"pdf_url":"https://arxiv.org/pdf/2308.04365v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14496v3","updated":"2024-01-02T14:35:16Z","published":"2023-09-25T19:45:45Z","title":"Era Splitting -- Invariant Learning for Decision Trees","summary":"  Real-life machine learning problems exhibit distributional shifts in the data\nfrom one time to another or from on place to another. This behavior is beyond\nthe scope of the traditional empirical risk minimization paradigm, which\nassumes i.i.d. distribution of data over time and across locations. The\nemerging field of out-of-distribution (OOD) generalization addresses this\nreality with new theory and algorithms which incorporate environmental, or\nera-wise information into the algorithms. So far, most research has been\nfocused on linear models and/or neural networks. In this research we develop\ntwo new splitting criteria for decision trees, which allow us to apply ideas\nfrom OOD generalization research to decision tree models, including random\nforest and gradient-boosting decision trees. The new splitting criteria use\nera-wise information associated with each data point to allow tree-based models\nto find split points that are optimal across all disjoint eras in the data,\ninstead of optimal over the entire data set pooled together, which is the\ndefault setting. In this paper we describe the problem setup in the context of\nfinancial markets. We describe the new splitting criteria in detail and develop\nunique experiments to showcase the benefits of these new criteria, which\nimprove metrics in our experiments out-of-sample. The new criteria are\nincorporated into the a state-of-the-art gradient boosted decision tree model\nin the Scikit-Learn code base, which is made freely available.\n","authors":["Timothy DeLise"],"pdf_url":"https://arxiv.org/pdf/2309.14496v3.pdf","comment":"20 pages, 8 figures"},{"id":"http://arxiv.org/abs/2401.01218v1","updated":"2024-01-02T14:12:41Z","published":"2024-01-02T14:12:41Z","title":"Zero-Shot Position Debiasing for Large Language Models","summary":"  Fine-tuning has been demonstrated to be an effective method to improve the\ndomain performance of large language models (LLMs). However, LLMs might fit the\ndataset bias and shortcuts for prediction, leading to poor generation\nperformance. Experimental result shows that LLMs are prone to exhibit position\nbias, i.e., leveraging information positioned at the beginning or end, or\nspecific positional cues within the input. Existing works on mitigating\nposition bias require external bias knowledge or annotated non-biased samples,\nwhich is unpractical in reality. In this work, we propose a zero-shot position\ndebiasing (ZOE) framework to mitigate position bias for LLMs. ZOE leverages\nunsupervised responses from pre-trained LLMs for debiasing, thus without any\nexternal knowledge or datasets. To improve the quality of unsupervised\nresponses, we propose a master-slave alignment (MSA) module to prune these\nresponses. Experiments on eight datasets and five tasks show that ZOE\nconsistently outperforms existing methods in mitigating four types of position\nbiases. Besides, ZOE achieves this by sacrificing only a small performance on\nbiased samples, which is simple and effective.\n","authors":["Zhongkun Liu","Zheng Chen","Mengqi Zhang","Zhaochun Ren","Zhumin Chen","Pengjie Ren"],"pdf_url":"https://arxiv.org/pdf/2401.01218v1.pdf","comment":"16 pages, 22 figures"},{"id":"http://arxiv.org/abs/2210.09041v2","updated":"2024-01-02T14:12:23Z","published":"2022-10-14T04:09:01Z","title":"Approximation analysis of CNNs from a feature extraction view","summary":"  Deep learning based on deep neural networks has been very successful in many\npractical applications, but it lacks enough theoretical understanding due to\nthe network architectures and structures. In this paper we establish some\nanalysis for linear feature extraction by a deep multi-channel convolutional\nneural networks (CNNs), which demonstrates the power of deep learning over\ntraditional linear transformations, like Fourier, wavelets, redundant\ndictionary coding methods. Moreover, we give an exact construction presenting\nhow linear features extraction can be conducted efficiently with multi-channel\nCNNs. It can be applied to lower the essential dimension for approximating a\nhigh dimensional function. Rates of function approximation by such deep\nnetworks implemented with channels and followed by fully-connected layers are\ninvestigated as well. Harmonic analysis for factorizing linear features into\nmulti-resolution convolutions plays an essential role in our work.\nNevertheless, a dedicate vectorization of matrices is constructed, which\nbridges 1D CNN and 2D CNN and allows us to have corresponding 2D analysis.\n","authors":["Jianfei Li","Han Feng","Ding-Xuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2210.09041v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.10955v6","updated":"2024-01-02T13:40:50Z","published":"2021-12-21T03:09:43Z","title":"Joint Learning of Linear Time-Invariant Dynamical Systems","summary":"  Linear time-invariant systems are very popular models in system theory and\napplications. A fundamental problem in system identification that remains\nrather unaddressed in extant literature is to leverage commonalities amongst\nrelated linear systems to estimate their transition matrices more accurately.\nTo address this problem, the current paper investigates methods for jointly\nestimating the transition matrices of multiple systems. It is assumed that the\ntransition matrices are unknown linear functions of some unknown shared basis\nmatrices. We establish finite-time estimation error rates that fully reflect\nthe roles of trajectory lengths, dimension, and number of systems under\nconsideration. The presented results are fairly general and show the\nsignificant gains that can be achieved by pooling data across systems in\ncomparison to learning each system individually. Further, they are shown to be\nrobust against model misspecifications. To obtain the results, we develop novel\ntechniques that are of interest for addressing similar joint-learning problems.\nThey include tightly bounding estimation errors in terms of the\neigen-structures of transition matrices, establishing sharp high probability\nbounds for singular values of dependent random matrices, and capturing effects\nof misspecified transition matrices as the systems evolve over time.\n","authors":["Aditya Modi","Mohamad Kazem Shirani Faradonbeh","Ambuj Tewari","George Michailidis"],"pdf_url":"https://arxiv.org/pdf/2112.10955v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.09222v2","updated":"2024-01-02T13:09:37Z","published":"2023-09-17T09:28:47Z","title":"Data-driven Modeling and Inference for Bayesian Gaussian Process ODEs\n  via Double Normalizing Flows","summary":"  Recently, Gaussian processes have been used to model the vector field of\ncontinuous dynamical systems, referred to as GPODEs, which are characterized by\na probabilistic ODE equation. Bayesian inference for these models has been\nextensively studied and applied in tasks such as time series prediction.\nHowever, the use of standard GPs with basic kernels like squared exponential\nkernels has been common in GPODE research, limiting the model's ability to\nrepresent complex scenarios. To address this limitation, we introduce\nnormalizing flows to reparameterize the ODE vector field, resulting in a\ndata-driven prior distribution, thereby increasing flexibility and expressive\npower. We develop a data-driven variational learning algorithm that utilizes\nanalytically tractable probability density functions of normalizing flows,\nenabling simultaneous learning and inference of unknown continuous dynamics.\nAdditionally, we also apply normalizing flows to the posterior inference of GP\nODEs to resolve the issue of strong mean-field assumptions in posterior\ninference. By applying normalizing flows in both these ways, our model improves\naccuracy and uncertainty estimates for Bayesian Gaussian Process ODEs. We\nvalidate the effectiveness of our approach on simulated dynamical systems and\nreal-world human motion data, including time series prediction and missing data\nrecovery tasks. Experimental results show that our proposed method effectively\ncaptures model uncertainty while improving accuracy.\n","authors":["Jian Xu","Shian Du","Junmei Yang","Xinghao Ding","John Paisley","Delu Zeng"],"pdf_url":"https://arxiv.org/pdf/2309.09222v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01201v1","updated":"2024-01-02T13:04:41Z","published":"2024-01-02T13:04:41Z","title":"Whole-examination AI estimation of fetal biometrics from 20-week\n  ultrasound scans","summary":"  The current approach to fetal anomaly screening is based on biometric\nmeasurements derived from individually selected ultrasound images. In this\npaper, we introduce a paradigm shift that attains human-level performance in\nbiometric measurement by aggregating automatically extracted biometrics from\nevery frame across an entire scan, with no need for operator intervention. We\nuse a convolutional neural network to classify each frame of an ultrasound\nvideo recording. We then measure fetal biometrics in every frame where\nappropriate anatomy is visible. We use a Bayesian method to estimate the true\nvalue of each biometric from a large number of measurements and\nprobabilistically reject outliers. We performed a retrospective experiment on\n1457 recordings (comprising 48 million frames) of 20-week ultrasound scans,\nestimated fetal biometrics in those scans and compared our estimates to the\nmeasurements sonographers took during the scan. Our method achieves human-level\nperformance in estimating fetal biometrics and estimates well-calibrated\ncredible intervals in which the true biometric value is expected to lie.\n","authors":["Lorenzo Venturini","Samuel Budd","Alfonso Farruggia","Robert Wright","Jacqueline Matthew","Thomas G. Day","Bernhard Kainz","Reza Razavi","Jo V. Hajnal"],"pdf_url":"https://arxiv.org/pdf/2401.01201v1.pdf","comment":"14 pages, 16 figures. Submitted to NPJ digital medicine. For\n  associated video file, see\n  http://wp.doc.ic.ac.uk/ifind/wp-content/uploads/sites/79/2023/12/realtime.gif"},{"id":"http://arxiv.org/abs/2401.01199v1","updated":"2024-01-02T13:03:29Z","published":"2024-01-02T13:03:29Z","title":"JMA: a General Algorithm to Craft Nearly Optimal Targeted Adversarial\n  Example","summary":"  Most of the approaches proposed so far to craft targeted adversarial examples\nagainst Deep Learning classifiers are highly suboptimal and typically rely on\nincreasing the likelihood of the target class, thus implicitly focusing on\none-hot encoding settings. In this paper, we propose a more general,\ntheoretically sound, targeted attack that resorts to the minimization of a\nJacobian-induced MAhalanobis distance (JMA) term, taking into account the\neffort (in the input space) required to move the latent space representation of\nthe input sample in a given direction. The minimization is solved by exploiting\nthe Wolfe duality theorem, reducing the problem to the solution of a\nNon-Negative Least Square (NNLS) problem. The proposed algorithm provides an\noptimal solution to a linearized version of the adversarial example problem\noriginally introduced by Szegedy et al. \\cite{szegedy2013intriguing}. The\nexperiments we carried out confirm the generality of the proposed attack which\nis proven to be effective under a wide variety of output encoding schemes.\nNoticeably, the JMA attack is also effective in a multi-label classification\nscenario, being capable to induce a targeted modification of up to half the\nlabels in a complex multilabel classification scenario with 20 labels, a\ncapability that is out of reach of all the attacks proposed so far. As a\nfurther advantage, the JMA attack usually requires very few iterations, thus\nresulting more efficient than existing methods.\n","authors":["Benedetta Tondi","Wei Guo","Mauro Barni"],"pdf_url":"https://arxiv.org/pdf/2401.01199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.07175v2","updated":"2024-01-02T12:49:36Z","published":"2022-12-15T21:42:17Z","title":"Scaffold-Based Multi-Objective Drug Candidate Optimization","summary":"  In therapeutic design, balancing various physiochemical properties is crucial\nfor molecule development, similar to how Multiparameter Optimization (MPO)\nevaluates multiple variables to meet a primary goal. While many molecular\nfeatures can now be predicted using \\textit{in silico} methods, aiding early\ndrug development, the vast data generated from high throughput virtual\nscreening challenges the practicality of traditional MPO approaches. Addressing\nthis, we introduce a scaffold focused graph-based Markov chain Monte Carlo\nframework (ScaMARS) built to generate molecules with optimal properties. This\ninnovative framework is capable of self-training and handling a wider array of\nproperties, sampling different chemical spaces according to the starting\nscaffold. The benchmark analysis on several properties shows that ScaMARS has a\ndiversity score of 84.6\\% and has a much higher success rate of 99.5\\% compared\nto conditional models. The integration of new features into MPO significantly\nenhances its adaptability and effectiveness in therapeutic design, facilitating\nthe discovery of candidates that efficiently optimize multiple properties.\n","authors":["Agustin Kruel","Andrew D. McNaughton","Neeraj Kumar"],"pdf_url":"https://arxiv.org/pdf/2301.07175v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00042v2","updated":"2024-01-02T12:41:26Z","published":"2023-11-24T10:42:11Z","title":"DeepTreeGANv2: Iterative Pooling of Point Clouds","summary":"  In High Energy Physics, detailed and time-consuming simulations are used for\nparticle interactions with detectors. To bypass these simulations with a\ngenerative model, the generation of large point clouds in a short time is\nrequired, while the complex dependencies between the particles must be\ncorrectly modelled. Particle showers are inherently tree-based processes, as\neach particle is produced by the decay or detector interaction of a particle of\nthe previous generation. In this work, we present a significant extension to\nDeepTreeGAN, featuring a critic, that is able to aggregate such point clouds\niteratively in a tree-based manner. We show that this model can reproduce\ncomplex distributions, and we evaluate its performance on the public JetNet 150\ndataset.\n","authors":["Moritz Alfons Wilhelm Scham","Dirk Krücker","Kerstin Borras"],"pdf_url":"https://arxiv.org/pdf/2312.00042v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2311.12616"},{"id":"http://arxiv.org/abs/2401.01192v1","updated":"2024-01-02T12:41:17Z","published":"2024-01-02T12:41:17Z","title":"Deep-ELA: Deep Exploratory Landscape Analysis with Self-Supervised\n  Pretrained Transformers for Single- and Multi-Objective Continuous\n  Optimization Problems","summary":"  In many recent works, the potential of Exploratory Landscape Analysis (ELA)\nfeatures to numerically characterize, in particular, single-objective\ncontinuous optimization problems has been demonstrated. These numerical\nfeatures provide the input for all kinds of machine learning tasks on\ncontinuous optimization problems, ranging, i.a., from High-level Property\nPrediction to Automated Algorithm Selection and Automated Algorithm\nConfiguration. Without ELA features, analyzing and understanding the\ncharacteristics of single-objective continuous optimization problems would be\nimpossible.\n  Yet, despite their undisputed usefulness, ELA features suffer from several\ndrawbacks. These include, in particular, (1.) a strong correlation between\nmultiple features, as well as (2.) its very limited applicability to\nmulti-objective continuous optimization problems. As a remedy, recent works\nproposed deep learning-based approaches as alternatives to ELA. In these works,\ne.g., point-cloud transformers were used to characterize an optimization\nproblem's fitness landscape. However, these approaches require a large amount\nof labeled training data.\n  Within this work, we propose a hybrid approach, Deep-ELA, which combines (the\nbenefits of) deep learning and ELA features. Specifically, we pre-trained four\ntransformers on millions of randomly generated optimization problems to learn\ndeep representations of the landscapes of continuous single- and\nmulti-objective optimization problems. Our proposed framework can either be\nused out-of-the-box for analyzing single- and multi-objective continuous\noptimization problems, or subsequently fine-tuned to various tasks focussing on\nalgorithm behavior and problem understanding.\n","authors":["Moritz Vinzent Seiler","Pascal Kerschke","Heike Trautmann"],"pdf_url":"https://arxiv.org/pdf/2401.01192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.02803v2","updated":"2024-01-02T12:33:45Z","published":"2023-04-11T14:53:57Z","title":"Tensor PCA from basis in tensor space","summary":"  The aim of this paper is to present a mathematical framework for tensor PCA.\nThe proposed approach is able to overcome the limitations of previous methods\nthat extract a low dimensional subspace by iteratively solving an optimization\nproblem. The core of the proposed approach is the derivation of a basis in\ntensor space from a real self-adjoint tensor operator, thus reducing the\nproblem of deriving a basis to an eigenvalue problem. Three different cases\nhave been studied to derive: i) a basis from a self-adjoint tensor operator;\nii) a rank-1 basis; iii) a basis in a subspace. In particular, the equivalence\nbetween eigenvalue equation for a real self-adjoint tensor operator and\nstandard matrix eigenvalue equation has been proven. For all the three cases\nconsidered, a subspace approach has been adopted to derive a tensor PCA.\nExperiments on image datasets validate the proposed mathematical framework.\n","authors":["Claudio Turchetti"],"pdf_url":"https://arxiv.org/pdf/2305.02803v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2401.01179v1","updated":"2024-01-02T12:14:41Z","published":"2024-01-02T12:14:41Z","title":"Freeze the backbones: A Parameter-Efficient Contrastive Approach to\n  Robust Medical Vision-Language Pre-training","summary":"  Modern healthcare often utilises radiographic images alongside textual\nreports for diagnostics, encouraging the use of Vision-Language Self-Supervised\nLearning (VL-SSL) with large pre-trained models to learn versatile medical\nvision representations. However, most existing VL-SSL frameworks are trained\nend-to-end, which is computation-heavy and can lose vital prior information\nembedded in pre-trained encoders. To address both issues, we introduce the\nbackbone-agnostic Adaptor framework, which preserves medical knowledge in\npre-trained image and text encoders by keeping them frozen, and employs a\nlightweight Adaptor module for cross-modal learning. Experiments on medical\nimage classification and segmentation tasks across three datasets reveal that\nour framework delivers competitive performance while cutting trainable\nparameters by over 90% compared to current pre-training approaches. Notably,\nwhen fine-tuned with just 1% of data, Adaptor outperforms several\nTransformer-based methods trained on full datasets in medical image\nsegmentation.\n","authors":["Jiuming Qin","Che Liu","Sibo Cheng","Yike Guo","Rossella Arcucci"],"pdf_url":"https://arxiv.org/pdf/2401.01179v1.pdf","comment":"Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2401.01176v1","updated":"2024-01-02T12:10:16Z","published":"2024-01-02T12:10:16Z","title":"Fundamental Limitation of Semantic Communications: Neural Estimation for\n  Rate-Distortion","summary":"  This paper studies the fundamental limit of semantic communications over the\ndiscrete memoryless channel. We consider the scenario to send a semantic source\nconsisting of an observation state and its corresponding semantic state, both\nof which are recovered at the receiver. To derive the performance limitation,\nwe adopt the semantic rate-distortion function (SRDF) to study the relationship\namong the minimum compression rate, observation distortion, semantic\ndistortion, and channel capacity. For the case with unknown semantic source\ndistribution, while only a set of the source samples is available, we propose a\nneural-network-based method by leveraging the generative networks to learn the\nsemantic source distribution. Furthermore, for a special case where the\nsemantic state is a deterministic function of the observation, we design a\ncascade neural network to estimate the SRDF. For the case with perfectly known\nsemantic source distribution, we propose a general Blahut-Arimoto algorithm to\neffectively compute the SRDF. Finally, experimental results validate our\nproposed algorithms for the scenarios with ideal Gaussian semantic source and\nsome practical datasets.\n","authors":["Dongxu Li","Jianhao Huang","Chuan Huang","Xiaoqi Qin","Han Zhang","Ping Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.01176v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01172v1","updated":"2024-01-02T12:02:50Z","published":"2024-01-02T12:02:50Z","title":"Quadratic Time-Frequency Analysis of Vibration Signals for Diagnosing\n  Bearing Faults","summary":"  Diagnosis of bearing faults is paramount to reducing maintenance costs and\noperational breakdowns. Bearing faults are primary contributors to machine\nvibrations, and analyzing their signal morphology offers insights into their\nhealth status. Unfortunately, existing approaches are optimized for controlled\nenvironments, neglecting realistic conditions such as time-varying rotational\nspeeds and the vibration's non-stationary nature. This paper presents a fusion\nof time-frequency analysis and deep learning techniques to diagnose bearing\nfaults under time-varying speeds and varying noise levels. First, we formulate\nthe bearing fault-induced vibrations and discuss the link between their\nnon-stationarity and the bearing's inherent and operational parameters. We also\nelucidate quadratic time-frequency distributions and validate their\neffectiveness in resolving distinctive dynamic patterns associated with\ndifferent bearing faults. Based on this, we design a time-frequency\nconvolutional neural network (TF-CNN) to diagnose various faults in\nrolling-element bearings. Our experimental findings undeniably demonstrate the\nsuperior performance of TF-CNN in comparison to recently developed techniques.\nThey also assert its versatility in capturing fault-relevant non-stationary\nfeatures that couple with speed changes and show its exceptional resilience to\nnoise, consistently surpassing competing methods across various signal-to-noise\nratios and performance metrics. Altogether, the TF-CNN achieves substantial\naccuracy improvements up to 15%, in severe noise conditions.\n","authors":["Mohammad Al-Sa'd","Tuomas Jalonen","Serkan Kiranyaz","Moncef Gabbouj"],"pdf_url":"https://arxiv.org/pdf/2401.01172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01168v1","updated":"2024-01-02T11:53:06Z","published":"2024-01-02T11:53:06Z","title":"FedQV: Leveraging Quadratic Voting in Federated Learning","summary":"  Federated Learning (FL) permits different parties to collaboratively train a\nglobal model without disclosing their respective local labels. A crucial step\nof FL, that of aggregating local models to produce the global one, shares many\nsimilarities with public decision-making, and elections in particular. In that\ncontext, a major weakness of FL, namely its vulnerability to poisoning attacks,\ncan be interpreted as a consequence of the one person one vote (henceforth\n1p1v) principle underpinning most contemporary aggregation rules. In this\npaper, we propose FedQV, a novel aggregation algorithm built upon the quadratic\nvoting scheme, recently proposed as a better alternative to 1p1v-based\nelections. Our theoretical analysis establishes that FedQV is a truthful\nmechanism in which bidding according to one's true valuation is a dominant\nstrategy that achieves a convergence rate that matches those of\nstate-of-the-art methods. Furthermore, our empirical analysis using multiple\nreal-world datasets validates the superior performance of FedQV against\npoisoning attacks. It also shows that combining FedQV with unequal voting\n``budgets'' according to a reputation score increases its performance benefits\neven further. Finally, we show that FedQV can be easily combined with\nByzantine-robust privacy-preserving mechanisms to enhance its robustness\nagainst both poisoning and privacy attacks.\n","authors":["Tianyue Chu","Nikolaos Laoutaris"],"pdf_url":"https://arxiv.org/pdf/2401.01168v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.09167v2","updated":"2024-01-02T11:48:54Z","published":"2023-10-13T15:01:55Z","title":"A Deep Neural Network -- Mechanistic Hybrid Model to Predict\n  Pharmacokinetics in Rat","summary":"  An important aspect in the development of small molecules as drugs or\nagro-chemicals is their systemic availability after intravenous and oral\nadministration. The prediction of the systemic availability from the chemical\nstructure of a potential candidate is highly desirable, as it allows to focus\nthe drug or agrochemical development on compounds with a favorable kinetic\nprofile. However, such pre-dictions are challenging as the availability is the\nresult of the complex interplay between molecular properties, biology and\nphysiology and training data is rare. In this work we improve the hybrid model\ndeveloped earlier [1]. We reduce the median fold change error for the total\noral exposure from 2.85 to 2.35 and for intravenous administration from 1.95 to\n1.62. This is achieved by training on a larger data set, improving the neural\nnetwork architecture as well as the parametrization of mechanistic model.\nFurther, we extend our approach to predict additional endpoints and to handle\ndifferent covariates, like sex and dosage form. In contrast to a pure machine\nlearning model, our model is able to predict new end points on which it has not\nbeen trained. We demonstrate this feature by predicting the exposure over the\nfirst 24h, while the model has only been trained on the total exposure.\n","authors":["Florian Führer","Andrea Gruber","Holger Diedam","Andreas H. Göller","Stephan Menz","Sebastian Schneckener"],"pdf_url":"https://arxiv.org/pdf/2310.09167v2.pdf","comment":"Version accepted by Journal of Computer-Aided Molecular Design"},{"id":"http://arxiv.org/abs/2401.01165v1","updated":"2024-01-02T11:47:58Z","published":"2024-01-02T11:47:58Z","title":"Reinforcement Learning for SAR View Angle Inversion with Differentiable\n  SAR Renderer","summary":"  The electromagnetic inverse problem has long been a research hotspot. This\nstudy aims to reverse radar view angles in synthetic aperture radar (SAR)\nimages given a target model. Nonetheless, the scarcity of SAR data, combined\nwith the intricate background interference and imaging mechanisms, limit the\napplications of existing learning-based approaches. To address these\nchallenges, we propose an interactive deep reinforcement learning (DRL)\nframework, where an electromagnetic simulator named differentiable SAR render\n(DSR) is embedded to facilitate the interaction between the agent and the\nenvironment, simulating a human-like process of angle prediction. Specifically,\nDSR generates SAR images at arbitrary view angles in real-time. And the\ndifferences in sequential and semantic aspects between the view\nangle-corresponding images are leveraged to construct the state space in DRL,\nwhich effectively suppress the complex background interference, enhance the\nsensitivity to temporal variations, and improve the capability to capture\nfine-grained information. Additionally, in order to maintain the stability and\nconvergence of our method, a series of reward mechanisms, such as memory\ndifference, smoothing and boundary penalty, are utilized to form the final\nreward function. Extensive experiments performed on both simulated and real\ndatasets demonstrate the effectiveness and robustness of our proposed method.\nWhen utilized in the cross-domain area, the proposed method greatly mitigates\ninconsistency between simulated and real domains, outperforming reference\nmethods significantly.\n","authors":["Yanni Wang","Hecheng Jia","Shilei Fu","Huiping Lin","Feng Xu"],"pdf_url":"https://arxiv.org/pdf/2401.01165v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01160v1","updated":"2024-01-02T11:43:49Z","published":"2024-01-02T11:43:49Z","title":"Train-Free Segmentation in MRI with Cubical Persistent Homology","summary":"  We describe a new general method for segmentation in MRI scans using\nTopological Data Analysis (TDA), offering several advantages over traditional\nmachine learning approaches. It works in three steps, first identifying the\nwhole object to segment via automatic thresholding, then detecting a\ndistinctive subset whose topology is known in advance, and finally deducing the\nvarious components of the segmentation. Although convoking classical ideas of\nTDA, such an algorithm has never been proposed separately from deep learning\nmethods. To achieve this, our approach takes into account, in addition to the\nhomology of the image, the localization of representative cycles, a piece of\ninformation that seems never to have been exploited in this context. In\nparticular, it offers the ability to perform segmentation without the need for\nlarge annotated data sets. TDA also provides a more interpretable and stable\nframework for segmentation by explicitly mapping topological features to\nsegmentation components. By adapting the geometric object to be detected, the\nalgorithm can be adjusted to a wide range of data segmentation challenges. We\ncarefully study the examples of glioblastoma segmentation in brain MRI, where a\nsphere is to be detected, as well as myocardium in cardiac MRI, involving a\ncylinder, and cortical plate detection in fetal brain MRI, whose 2D slices are\ncircles. We compare our method to state-of-the-art algorithms.\n","authors":["Anton François","Raphaël Tinarrage"],"pdf_url":"https://arxiv.org/pdf/2401.01160v1.pdf","comment":"preprint, 17 pages, 19 figures"},{"id":"http://arxiv.org/abs/2401.01155v1","updated":"2024-01-02T11:13:01Z","published":"2024-01-02T11:13:01Z","title":"Deep Learning-Based Detection for Marker Codes over Insertion and\n  Deletion Channels","summary":"  Marker code is an effective coding scheme to protect data from insertions and\ndeletions. It has potential applications in future storage systems, such as DNA\nstorage and racetrack memory. When decoding marker codes, perfect channel state\ninformation (CSI), i.e., insertion and deletion probabilities, are required to\ndetect insertion and deletion errors. Sometimes, the perfect CSI is not easy to\nobtain or the accurate channel model is unknown. Therefore, it is deserved to\ndevelop detecting algorithms for marker code without the knowledge of perfect\nCSI. In this paper, we propose two CSI-agnostic detecting algorithms for marker\ncode based on deep learning. The first one is a model-driven deep learning\nmethod, which deep unfolds the original iterative detecting algorithm of marker\ncode. In this method, CSI become weights in neural networks and these weights\ncan be learned from training data. The second one is a data-driven method which\nis an end-to-end system based on the deep bidirectional gated recurrent unit\nnetwork. Simulation results show that error performances of the proposed\nmethods are significantly better than that of the original detection algorithm\nwith CSI uncertainty. Furthermore, the proposed data-driven method exhibits\nbetter error performances than other methods for unknown channel models.\n","authors":["Guochen Ma","Xiaopeng Jiao","Jianjun Mu","Hui Han","Yaming Yang"],"pdf_url":"https://arxiv.org/pdf/2401.01155v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01148v1","updated":"2024-01-02T10:58:54Z","published":"2024-01-02T10:58:54Z","title":"PAC-Bayes-Chernoff bounds for unbounded losses","summary":"  We present a new high-probability PAC-Bayes oracle bound for unbounded\nlosses. This result can be understood as a PAC-Bayes version of the Chernoff\nbound. The proof technique relies on uniformly bounding the tail of certain\nrandom variable based on the Cram\\'er transform of the loss. We highlight two\napplications of our main result. First, we show that our bound solves the open\nproblem of optimizing the free parameter on many PAC-Bayes bounds. Finally, we\nshow that our approach allows working with flexible assumptions on the loss\nfunction, resulting in novel bounds that generalize previous ones and can be\nminimized to obtain Gibbs-like posteriors.\n","authors":["Ioar Casado","Luis A. Ortega","Andrés R. Masegosa","Aritz Pérez"],"pdf_url":"https://arxiv.org/pdf/2401.01148v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2401.01145v1","updated":"2024-01-02T10:55:01Z","published":"2024-01-02T10:55:01Z","title":"HAAQI-Net: A non-intrusive neural music quality assessment model for\n  hearing aids","summary":"  This paper introduces HAAQI-Net, a non-intrusive deep learning model for\nmusic quality assessment tailored to hearing aid users. In contrast to\ntraditional methods like the Hearing Aid Audio Quality Index (HAAQI), HAAQI-Net\nutilizes a Bidirectional Long Short-Term Memory (BLSTM) with attention. It\ntakes an assessed music sample and a hearing loss pattern as input, generating\na predicted HAAQI score. The model employs the pre-trained Bidirectional\nEncoder representation from Audio Transformers (BEATs) for acoustic feature\nextraction. Comparing predicted scores with ground truth, HAAQI-Net achieves a\nLongitudinal Concordance Correlation (LCC) of 0.9257, Spearman's Rank\nCorrelation Coefficient (SRCC) of 0.9394, and Mean Squared Error (MSE) of\n0.0080. Notably, this high performance comes with a substantial reduction in\ninference time: from 62.52 seconds (by HAAQI) to 2.71 seconds (by HAAQI-Net),\nserving as an efficient music quality assessment model for hearing aid users.\n","authors":["Dyah A. M. G. Wisnu","Epri Pratiwi","Stefano Rini","Ryandhimas E. Zezario","Hsin-Min Wang","Yu Tsao"],"pdf_url":"https://arxiv.org/pdf/2401.01145v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16741v2","updated":"2024-01-02T10:18:24Z","published":"2023-09-28T08:08:08Z","title":"Multi-Modal Financial Time-Series Retrieval Through Latent Space\n  Projections","summary":"  Financial firms commonly process and store billions of time-series data,\ngenerated continuously and at a high frequency. To support efficient data\nstorage and retrieval, specialized time-series databases and systems have\nemerged. These databases support indexing and querying of time-series by a\nconstrained Structured Query Language(SQL)-like format to enable queries like\n\"Stocks with monthly price returns greater than 5%\", and expressed in rigid\nformats. However, such queries do not capture the intrinsic complexity of high\ndimensional time-series data, which can often be better described by images or\nlanguage (e.g., \"A stock in low volatility regime\"). Moreover, the required\nstorage, computational time, and retrieval complexity to search in the\ntime-series space are often non-trivial. In this paper, we propose and\ndemonstrate a framework to store multi-modal data for financial time-series in\na lower-dimensional latent space using deep encoders, such that the latent\nspace projections capture not only the time series trends but also other\ndesirable information or properties of the financial time-series data (such as\nprice volatility). Moreover, our approach allows user-friendly query\ninterfaces, enabling natural language text or sketches of time-series, for\nwhich we have developed intuitive interfaces. We demonstrate the advantages of\nour method in terms of computational efficiency and accuracy on real historical\ndata as well as synthetic data, and highlight the utility of latent-space\nprojections in the storage and retrieval of financial time-series data with\nintuitive query modalities.\n","authors":["Tom Bamford","Andrea Coletta","Elizabeth Fons","Sriram Gopalakrishnan","Svitlana Vyetrenko","Tucker Balch","Manuela Veloso"],"pdf_url":"https://arxiv.org/pdf/2309.16741v2.pdf","comment":"Accepted to ICAIF 2023"},{"id":"http://arxiv.org/abs/2309.17207v2","updated":"2024-01-02T10:13:18Z","published":"2023-09-29T12:59:28Z","title":"Memory Gym: Towards Endless Tasks to Benchmark Memory Capabilities of\n  Agents","summary":"  Memory Gym presents a suite of 2D partially observable environments, namely\nMortar Mayhem, Mystery Path, and Searing Spotlights, designed to benchmark\nmemory capabilities in decision-making agents. These environments, originally\nwith finite tasks, are expanded into innovative, endless formats, mirroring the\nescalating challenges of cumulative memory games such as ``I packed my bag''.\nThis progression in task design shifts the focus from merely assessing sample\nefficiency to also probing the levels of memory effectiveness in dynamic,\nprolonged scenarios. To address the gap in available memory-based Deep\nReinforcement Learning baselines, we introduce an implementation that\nintegrates Transformer-XL (TrXL) with Proximal Policy Optimization. This\napproach utilizes TrXL as a form of episodic memory, employing a sliding window\ntechnique. Our comparative study between the Gated Recurrent Unit (GRU) and\nTrXL reveals varied performances across different settings. TrXL, on the finite\nenvironments, demonstrates superior sample efficiency in Mystery Path and\noutperforms in Mortar Mayhem. However, GRU is more efficient on Searing\nSpotlights. Most notably, in all endless tasks, GRU makes a remarkable\nresurgence, consistently outperforming TrXL by significant margins. Website and\nSource Code: \\url{https://github.com/MarcoMeter/endless-memory-gym/}\n","authors":["Marco Pleines","Matthias Pallasch","Frank Zimmer","Mike Preuss"],"pdf_url":"https://arxiv.org/pdf/2309.17207v2.pdf","comment":"40 pages, 17 figures, 5 tables, under review"},{"id":"http://arxiv.org/abs/2310.19923v2","updated":"2024-01-02T10:01:51Z","published":"2023-10-30T18:35:30Z","title":"Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long\n  Documents","summary":"  Text embedding models have emerged as powerful tools for transforming\nsentences into fixed-sized feature vectors that encapsulate semantic\ninformation. While these models are essential for tasks like information\nretrieval, semantic clustering, and text re-ranking, most existing open-source\nmodels, especially those built on architectures like BERT, struggle to\nrepresent lengthy documents and often resort to truncation. One common approach\nto mitigate this challenge involves splitting documents into smaller paragraphs\nfor embedding. However, this strategy results in a much larger set of vectors,\nconsequently leading to increased memory consumption and computationally\nintensive vector searches with elevated latency.\n  To address these challenges, we introduce Jina Embeddings 2, an open-source\ntext embedding model capable of accommodating up to 8192 tokens. This model is\ndesigned to transcend the conventional 512-token limit and adeptly process long\ndocuments. Jina Embeddings 2 not only achieves state-of-the-art performance on\na range of embedding-related tasks in the MTEB benchmark but also matches the\nperformance of OpenAI's proprietary ada-002 model. Additionally, our\nexperiments indicate that an extended context can enhance performance in tasks\nsuch as NarrativeQA.\n","authors":["Michael Günther","Jackmin Ong","Isabelle Mohr","Alaeddine Abdessalem","Tanguy Abel","Mohammad Kalim Akram","Susana Guzman","Georgios Mastrapas","Saba Sturua","Bo Wang","Maximilian Werk","Nan Wang","Han Xiao"],"pdf_url":"https://arxiv.org/pdf/2310.19923v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2302.00316v2","updated":"2024-01-02T09:50:04Z","published":"2023-02-01T08:50:48Z","title":"Accelerated First-Order Optimization under Nonlinear Constraints","summary":"  We exploit analogies between first-order algorithms for constrained\noptimization and non-smooth dynamical systems to design a new class of\naccelerated first-order algorithms for constrained optimization. Unlike\nFrank-Wolfe or projected gradients, these algorithms avoid optimization over\nthe entire feasible set at each iteration. We prove convergence to stationary\npoints even in a nonconvex setting and we derive accelerated rates for the\nconvex setting both in continuous time, as well as in discrete time. An\nimportant property of these algorithms is that constraints are expressed in\nterms of velocities instead of positions, which naturally leads to sparse,\nlocal and convex approximations of the feasible set (even if the feasible set\nis nonconvex). Thus, the complexity tends to grow mildly in the number of\ndecision variables and in the number of constraints, which makes the algorithms\nsuitable for machine learning applications. We apply our algorithms to a\ncompressed sensing and a sparse regression problem, showing that we can treat\nnonconvex $\\ell^p$ constraints ($p<1$) efficiently, while recovering\nstate-of-the-art performance for $p=1$.\n","authors":["Michael Muehlebach","Michael I. Jordan"],"pdf_url":"https://arxiv.org/pdf/2302.00316v2.pdf","comment":"44 pages, 6 figures"},{"id":"http://arxiv.org/abs/2304.14374v3","updated":"2024-01-02T09:44:46Z","published":"2023-04-27T17:46:00Z","title":"Pseudo-Hamiltonian neural networks for learning partial differential\n  equations","summary":"  Pseudo-Hamiltonian neural networks (PHNN) were recently introduced for\nlearning dynamical systems that can be modelled by ordinary differential\nequations. In this paper, we extend the method to partial differential\nequations. The resulting model is comprised of up to three neural networks,\nmodelling terms representing conservation, dissipation and external forces, and\ndiscrete convolution operators that can either be learned or be given as input.\nWe demonstrate numerically the superior performance of PHNN compared to a\nbaseline model that models the full dynamics by a single neural network.\nMoreover, since the PHNN model consists of three parts with different physical\ninterpretations, these can be studied separately to gain insight into the\nsystem, and the learned model is applicable also if external forces are removed\nor changed.\n","authors":["Sølve Eidnes","Kjetil Olsen Lye"],"pdf_url":"https://arxiv.org/pdf/2304.14374v3.pdf","comment":"39 pages, 18 figures; v3: expanded text and added numerical\n  experiments, new subsections: 5.1, 6.3, 6.4"},{"id":"http://arxiv.org/abs/2401.01124v1","updated":"2024-01-02T09:40:02Z","published":"2024-01-02T09:40:02Z","title":"Explainable Adaptive Tree-based Model Selection for Time Series\n  Forecasting","summary":"  Tree-based models have been successfully applied to a wide variety of tasks,\nincluding time series forecasting. They are increasingly in demand and widely\naccepted because of their comparatively high level of interpretability.\nHowever, many of them suffer from the overfitting problem, which limits their\napplication in real-world decision-making. This problem becomes even more\nsevere in online-forecasting settings where time series observations are\nincrementally acquired, and the distributions from which they are drawn may\nkeep changing over time. In this context, we propose a novel method for the\nonline selection of tree-based models using the TreeSHAP explainability method\nin the task of time series forecasting. We start with an arbitrary set of\ndifferent tree-based models. Then, we outline a performance-based ranking with\na coherent design to make TreeSHAP able to specialize the tree-based\nforecasters across different regions in the input time series. In this\nframework, adequate model selection is performed online, adaptively following\ndrift detection in the time series. In addition, explainability is supported on\nthree levels, namely online input importance, model selection, and model output\nexplanation. An extensive empirical study on various real-world datasets\ndemonstrates that our method achieves excellent or on-par results in comparison\nto the state-of-the-art approaches as well as several baselines.\n","authors":["Matthias Jakobs","Amal Saadallah"],"pdf_url":"https://arxiv.org/pdf/2401.01124v1.pdf","comment":"Accepted and presented at ICDM 2023"},{"id":"http://arxiv.org/abs/2307.16164v3","updated":"2024-01-02T09:32:23Z","published":"2023-07-30T08:18:39Z","title":"Adaptive learning of density ratios in RKHS","summary":"  Estimating the ratio of two probability densities from finitely many\nobservations of the densities is a central problem in machine learning and\nstatistics with applications in two-sample testing, divergence estimation,\ngenerative modeling, covariate shift adaptation, conditional density\nestimation, and novelty detection. In this work, we analyze a large class of\ndensity ratio estimation methods that minimize a regularized Bregman divergence\nbetween the true density ratio and a model in a reproducing kernel Hilbert\nspace (RKHS). We derive new finite-sample error bounds, and we propose a\nLepskii type parameter choice principle that minimizes the bounds without\nknowledge of the regularity of the density ratio. In the special case of\nquadratic loss, our method adaptively achieves a minimax optimal error rate. A\nnumerical illustration is provided.\n","authors":["Werner Zellinger","Stefan Kindermann","Sergei V. Pereverzyev"],"pdf_url":"https://arxiv.org/pdf/2307.16164v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.15224v2","updated":"2024-01-02T09:32:23Z","published":"2023-09-26T19:43:14Z","title":"Collaborative Watermarking for Adversarial Speech Synthesis","summary":"  Advances in neural speech synthesis have brought us technology that is not\nonly close to human naturalness, but is also capable of instant voice cloning\nwith little data, and is highly accessible with pre-trained models available.\nNaturally, the potential flood of generated content raises the need for\nsynthetic speech detection and watermarking. Recently, considerable research\neffort in synthetic speech detection has been related to the Automatic Speaker\nVerification and Spoofing Countermeasure Challenge (ASVspoof), which focuses on\npassive countermeasures. This paper takes a complementary view to generated\nspeech detection: a synthesis system should make an active effort to watermark\nthe generated speech in a way that aids detection by another machine, but\nremains transparent to a human listener. We propose a collaborative training\nscheme for synthetic speech watermarking and show that a HiFi-GAN neural\nvocoder collaborating with the ASVspoof 2021 baseline countermeasure models\nconsistently improves detection performance over conventional classifier\ntraining. Furthermore, we demonstrate how collaborative training can be paired\nwith augmentation strategies for added robustness against noise and\ntime-stretching. Finally, listening tests demonstrate that collaborative\ntraining has little adverse effect on perceptual quality of vocoded speech.\n","authors":["Lauri Juvela","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2309.15224v2.pdf","comment":"Accepted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2401.01119v1","updated":"2024-01-02T09:31:14Z","published":"2024-01-02T09:31:14Z","title":"Utilizing Autoregressive Networks for Full Lifecycle Data Generation of\n  Rolling Bearings for RUL Prediction","summary":"  The prediction of rolling bearing lifespan is of significant importance in\nindustrial production. However, the scarcity of high-quality, full lifecycle\ndata has been a major constraint in achieving precise predictions. To address\nthis challenge, this paper introduces the CVGAN model, a novel framework\ncapable of generating one-dimensional vibration signals in both horizontal and\nvertical directions, conditioned on historical vibration data and remaining\nuseful life. In addition, we propose an autoregressive generation method that\ncan iteratively utilize previously generated vibration information to guide the\ngeneration of current signals. The effectiveness of the CVGAN model is\nvalidated through experiments conducted on the PHM 2012 dataset. Our findings\ndemonstrate that the CVGAN model, in terms of both MMD and FID metrics,\noutperforms many advanced methods in both autoregressive and non-autoregressive\ngeneration modes. Notably, training using the full lifecycle data generated by\nthe CVGAN model significantly improves the performance of the predictive model.\nThis result highlights the effectiveness of the data generated by CVGans in\nenhancing the predictive power of these models.\n","authors":["Junliang Wang","Qinghua Zhang","Guanhua Zhu","Guoxi Sun"],"pdf_url":"https://arxiv.org/pdf/2401.01119v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.06920v2","updated":"2024-01-02T09:26:08Z","published":"2023-05-09T15:22:05Z","title":"Pseudo-Hamiltonian system identification","summary":"  Identifying the underlying dynamics of physical systems can be challenging\nwhen only provided with observational data. In this work, we consider systems\nthat can be modelled as first-order ordinary differential equations. By\nassuming a certain pseudo-Hamiltonian formulation, we are able to learn the\nanalytic terms of internal dynamics even if the model is trained on data where\nthe system is affected by unknown damping and external disturbances. In cases\nwhere it is difficult to find analytic terms for the disturbances, a hybrid\nmodel that uses a neural network to learn these can still accurately identify\nthe dynamics of the system as if under ideal conditions. This makes the models\napplicable in some situations where other system identification models fail.\nFurthermore, we propose to use a fourth-order symmetric integration scheme in\nthe loss function and avoid actual integration in the training, and demonstrate\non varied examples how this leads to increased performance on noisy data.\n","authors":["Sigurd Holmsen","Sølve Eidnes","Signe Riemer-Sørensen"],"pdf_url":"https://arxiv.org/pdf/2305.06920v2.pdf","comment":"33 pages, 21 figures, including the appendix; v2: restructured and\n  modified the text, added Section 6"},{"id":"http://arxiv.org/abs/2401.01100v1","updated":"2024-01-02T08:43:06Z","published":"2024-01-02T08:43:06Z","title":"Scalable manifold learning by uniform landmark sampling and constrained\n  locally linear embedding","summary":"  As a pivotal approach in machine learning and data science, manifold learning\naims to uncover the intrinsic low-dimensional structure within complex\nnonlinear manifolds in high-dimensional space. By exploiting the manifold\nhypothesis, various techniques for nonlinear dimension reduction have been\ndeveloped to facilitate visualization, classification, clustering, and gaining\nkey insights. Although existing manifold learning methods have achieved\nremarkable successes, they still suffer from extensive distortions incurred in\nthe global structure, which hinders the understanding of underlying patterns.\nScalability issues also limit their applicability for handling large-scale\ndata. Here, we propose a scalable manifold learning (scML) method that can\nmanipulate large-scale and high-dimensional data in an efficient manner. It\nstarts by seeking a set of landmarks to construct the low-dimensional skeleton\nof the entire data and then incorporates the non-landmarks into the landmark\nspace based on the constrained locally linear embedding (CLLE). We empirically\nvalidated the effectiveness of scML on synthetic datasets and real-world\nbenchmarks of different types, and applied it to analyze the single-cell\ntranscriptomics and detect anomalies in electrocardiogram (ECG) signals. scML\nscales well with increasing data sizes and exhibits promising performance in\npreserving the global structure. The experiments demonstrate notable robustness\nin embedding quality as the sample rate decreases.\n","authors":["Dehua Peng","Zhipeng Gui","Wenzhang Wei","Huayi Wu"],"pdf_url":"https://arxiv.org/pdf/2401.01100v1.pdf","comment":"33 pages, 10 figures"},{"id":"http://arxiv.org/abs/2401.01099v1","updated":"2024-01-02T08:42:48Z","published":"2024-01-02T08:42:48Z","title":"Efficient Parallel Audio Generation using Group Masked Language Modeling","summary":"  We present a fast and high-quality codec language model for parallel audio\ngeneration. While SoundStorm, a state-of-the-art parallel audio generation\nmodel, accelerates inference speed compared to autoregressive models, it still\nsuffers from slow inference due to iterative sampling. To resolve this problem,\nwe propose Group-Masked Language Modeling~(G-MLM) and Group Iterative Parallel\nDecoding~(G-IPD) for efficient parallel audio generation. Both the training and\nsampling schemes enable the model to synthesize high-quality audio with a small\nnumber of iterations by effectively modeling the group-wise conditional\ndependencies. In addition, our model employs a cross-attention-based\narchitecture to capture the speaker style of the prompt voice and improves\ncomputational efficiency. Experimental results demonstrate that our proposed\nmodel outperforms the baselines in prompt-based audio generation.\n","authors":["Myeonghun Jeong","Minchan Kim","Joun Yeop Lee","Nam Soo Kim"],"pdf_url":"https://arxiv.org/pdf/2401.01099v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2401.00744v2","updated":"2024-01-02T08:36:58Z","published":"2024-01-01T12:57:15Z","title":"Harmonizing Covariance and Expressiveness for Deep Hamiltonian\n  Regression in Crystalline Material Research: a Hybrid Cascaded Regression\n  Framework","summary":"  Deep learning for Hamiltonian regression of quantum systems in material\nresearch necessitates satisfying the covariance laws, among which achieving\nSO(3)-equivariance without sacrificing the expressiveness of networks remains\nan elusive challenge due to the restriction to non-linear mappings on\nguaranteeing theoretical equivariance. To alleviate the\ncovariance-expressiveness dilemma, we propose a hybrid framework with two\ncascaded regression stages. The first stage, with a theoretically-guaranteed\ncovariant neural network modeling symmetry properties of 3D atom systems,\nyields theoretically covariant features and baseline Hamiltonian predictions,\nassisting the second stage in learning covariance. Meanwhile, the second stage,\npowered by a non-linear 3D graph Transformer network we propose for structural\nmodeling of 3D atomic systems, refines the first stage's output as a\nfine-grained prediction of Hamiltonians with better expressiveness capability.\nThe combination of a theoretically covariant yet inevitably less expressive\nmodel with a highly expressive non-linear network enables precise,\ngeneralizable predictions while maintaining robust covariance under coordinate\ntransformations. Our method achieves state-of-the-art performance in\nHamiltonian prediction for electronic structure calculations, confirmed through\nexperiments on five crystalline material databases.\n","authors":["Shi Yin","Xudong Zhu","Tianyu Gao","Haochong Zhang","Feng Wu","Lixin He"],"pdf_url":"https://arxiv.org/pdf/2401.00744v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.02164v2","updated":"2024-01-02T08:08:10Z","published":"2022-06-05T12:35:52Z","title":"Estimating and Mitigating the Congestion Effect of Curbside Pick-ups and\n  Drop-offs: A Causal Inference Approach","summary":"  Curb space is one of the busiest areas in urban road networks. Especially in\nrecent years, the rapid increase of ride-hailing trips and commercial\ndeliveries has induced massive pick-ups/drop-offs (PUDOs), which occupy the\nlimited curb space that was designed and built decades ago. These PUDOs could\njam curbside utilization and disturb the mainline traffic flow, evidently\nleading to significant negative societal externalities. However, there is a\nlack of an analytical framework that rigorously quantifies and mitigates the\ncongestion effect of PUDOs in the system view, particularly with little data\nsupport and involvement of confounding effects. To bridge this research gap,\nthis paper develops a rigorous causal inference approach to estimate the\ncongestion effect of PUDOs on general regional networks. A causal graph is set\nto represent the spatio-temporal relationship between PUDOs and traffic speed,\nand a double and separated machine learning (DSML) method is proposed to\nquantify how PUDOs affect traffic congestion. Additionally, a re-routing\nformulation is developed and solved to encourage passenger walking and traffic\nflow re-routing to achieve system optimization. Numerical experiments are\nconducted using real-world data in the Manhattan area. On average, 100\nadditional units of PUDOs in a region could reduce the traffic speed by 3.70\nand 4.54 mph on weekdays and weekends, respectively. Re-routing trips with\nPUDOs on curb space could respectively reduce the system-wide total travel time\nby 2.44% and 2.12% in Midtown and Central Park on weekdays. Sensitivity\nanalysis is also conducted to demonstrate the effectiveness and robustness of\nthe proposed framework.\n","authors":["Xiaohui Liu","Sean Qian","Hock-Hai Teo","Wei Ma"],"pdf_url":"https://arxiv.org/pdf/2206.02164v2.pdf","comment":"Accepted at Transportation Science"},{"id":"http://arxiv.org/abs/2401.01085v1","updated":"2024-01-02T07:57:04Z","published":"2024-01-02T07:57:04Z","title":"Imperio: Language-Guided Backdoor Attacks for Arbitrary Model Control","summary":"  Revolutionized by the transformer architecture, natural language processing\n(NLP) has received unprecedented attention. While advancements in NLP models\nhave led to extensive research into their backdoor vulnerabilities, the\npotential for these advancements to introduce new backdoor threats remains\nunexplored. This paper proposes Imperio, which harnesses the language\nunderstanding capabilities of NLP models to enrich backdoor attacks. Imperio\nprovides a new model control experience. It empowers the adversary to control\nthe victim model with arbitrary output through language-guided instructions.\nThis is achieved using a language model to fuel a conditional trigger\ngenerator, with optimizations designed to extend its language understanding\ncapabilities to backdoor instruction interpretation and execution. Our\nexperiments across three datasets, five attacks, and nine defenses confirm\nImperio's effectiveness. It can produce contextually adaptive triggers from\ntext descriptions and control the victim model with desired outputs, even in\nscenarios not encountered during training. The attack maintains a high success\nrate across complex datasets without compromising the accuracy of clean inputs\nand also exhibits resilience against representative defenses. The source code\nis available at \\url{https://khchow.com/Imperio}.\n","authors":["Ka-Ho Chow","Wenqi Wei","Lei Yu"],"pdf_url":"https://arxiv.org/pdf/2401.01085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01084v1","updated":"2024-01-02T07:56:17Z","published":"2024-01-02T07:56:17Z","title":"Global Convergence of Natural Policy Gradient with Hessian-aided\n  Momentum Variance Reduction","summary":"  Natural policy gradient (NPG) and its variants are widely-used policy search\nmethods in reinforcement learning. Inspired by prior work, a new NPG variant\ncoined NPG-HM is developed in this paper, which utilizes the Hessian-aided\nmomentum technique for variance reduction, while the sub-problem is solved via\nthe stochastic gradient descent method. It is shown that NPG-HM can achieve the\nglobal last iterate $\\epsilon$-optimality with a sample complexity of\n$\\mathcal{O}(\\epsilon^{-2})$, which is the best known result for natural policy\ngradient type methods under the generic Fisher non-degenerate policy\nparameterizations. The convergence analysis is built upon a relaxed weak\ngradient dominance property tailored for NPG under the compatible function\napproximation framework, as well as a neat way to decompose the error when\nhandling the sub-problem. Moreover, numerical experiments on Mujoco-based\nenvironments demonstrate the superior performance of NPG-HM over other\nstate-of-the-art policy gradient methods.\n","authors":["Jie Feng","Ke Wei","Jinchi Chen"],"pdf_url":"https://arxiv.org/pdf/2401.01084v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01083v1","updated":"2024-01-02T07:56:05Z","published":"2024-01-02T07:56:05Z","title":"Aircraft Landing Time Prediction with Deep Learning on Trajectory Images","summary":"  Aircraft landing time (ALT) prediction is crucial for air traffic management,\nespecially for arrival aircraft sequencing on the runway. In this study, a\ntrajectory image-based deep learning method is proposed to predict ALTs for the\naircraft entering the research airspace that covers the Terminal Maneuvering\nArea (TMA). Specifically, the trajectories of all airborne arrival aircraft\nwithin the temporal capture window are used to generate an image with the\ntarget aircraft trajectory labeled as red and all background aircraft\ntrajectory labeled as blue. The trajectory images contain various information,\nincluding the aircraft position, speed, heading, relative distances, and\narrival traffic flows. It enables us to use state-of-the-art deep convolution\nneural networks for ALT modeling. We also use real-time runway usage obtained\nfrom the trajectory data and the external information such as aircraft types\nand weather conditions as additional inputs. Moreover, a convolution neural\nnetwork (CNN) based module is designed for automatic holding-related\nfeaturizing, which takes the trajectory images, the leading aircraft holding\nstatus, and their time and speed gap at the research airspace boundary as its\ninputs. Its output is further fed into the final end-to-end ALT prediction. The\nproposed ALT prediction approach is applied to Singapore Changi Airport (ICAO\nCode: WSSS) using one-month Automatic Dependent Surveillance-Broadcast (ADS-B)\ndata from November 1 to November 30, 2022. Experimental results show that by\nintegrating the holding featurization, we can reduce the mean absolute error\n(MAE) from 82.23 seconds to 43.96 seconds, and achieve an average accuracy of\n96.1\\%, with 79.4\\% of the predictions errors being less than 60 seconds.\n","authors":["Liping Huang","Sheng Zhang","Yicheng Zhang","Yi Zhang","Yifang Yin"],"pdf_url":"https://arxiv.org/pdf/2401.01083v1.pdf","comment":"In 2023 13th SESAR Innovation Days (SIDS2023)"},{"id":"http://arxiv.org/abs/2310.10477v4","updated":"2024-01-02T07:51:33Z","published":"2023-10-16T14:59:10Z","title":"Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake\n  Analysis","summary":"  The rapid development of large language models (LLMs) has not only provided\nnumerous opportunities but also presented significant challenges. This becomes\nparticularly evident when LLMs inadvertently generate harmful or toxic content,\neither unintentionally or because of intentional inducement. Existing alignment\nmethods usually direct LLMs toward the favorable outcomes by utilizing\nhuman-annotated, flawless instruction-response pairs. Conversely, this study\nproposes a novel alignment technique based on mistake analysis, which\ndeliberately exposes LLMs to erroneous content to learn the reasons for\nmistakes and how to avoid them. In this case, mistakes are repurposed into\nvaluable data for alignment, effectively helping to avoid the production of\nerroneous responses. Without external models or human annotations, our method\nleverages a model's intrinsic ability to discern undesirable mistakes and\nimproves the safety of its generated responses. Experimental results reveal\nthat our method outperforms existing alignment approaches in enhancing model\nsafety while maintaining the overall utility.\n","authors":["Kai Chen","Chunwei Wang","Kuo Yang","Jianhua Han","Lanqing Hong","Fei Mi","Hang Xu","Zhengying Liu","Wenyong Huang","Zhenguo Li","Dit-Yan Yeung","Lifeng Shang","Xin Jiang","Qun Liu"],"pdf_url":"https://arxiv.org/pdf/2310.10477v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01077v1","updated":"2024-01-02T07:46:33Z","published":"2024-01-02T07:46:33Z","title":"Constrained Online Two-stage Stochastic Optimization: Algorithm with\n  (and without) Predictions","summary":"  We consider an online two-stage stochastic optimization with long-term\nconstraints over a finite horizon of $T$ periods. At each period, we take the\nfirst-stage action, observe a model parameter realization and then take the\nsecond-stage action from a feasible set that depends both on the first-stage\ndecision and the model parameter. We aim to minimize the cumulative objective\nvalue while guaranteeing that the long-term average second-stage decision\nbelongs to a set. We develop online algorithms for the online two-stage problem\nfrom adversarial learning algorithms. Also, the regret bound of our algorithm\ncan be reduced to the regret bound of embedded adversarial learning algorithms.\nBased on this framework, we obtain new results under various settings. When the\nmodel parameters are drawn from unknown non-stationary distributions and we are\ngiven machine-learned predictions of the distributions, we develop a new\nalgorithm from our framework with a regret $O(W_T+\\sqrt{T})$, where $W_T$\nmeasures the total inaccuracy of the machine-learned predictions. We then\ndevelop another algorithm that works when no machine-learned predictions are\ngiven and show the performances.\n","authors":["Piao Hu","Jiashuo Jiang","Guodong Lyu","Hao Su"],"pdf_url":"https://arxiv.org/pdf/2401.01077v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2302.00997"},{"id":"http://arxiv.org/abs/2308.13815v2","updated":"2024-01-02T07:34:33Z","published":"2023-08-26T08:39:16Z","title":"Arbitrary Distributions Mapping via SyMOT-Flow: A Flow-based Approach\n  Integrating Maximum Mean Discrepancy and Optimal Transport","summary":"  Finding a transformation between two unknown probability distributions from\nfinite samples is crucial for modeling complex data distributions and\nperforming tasks such as sample generation, domain adaptation and statistical\ninference. One powerful framework for such transformations is normalizing flow,\nwhich transforms an unknown distribution into a standard normal distribution\nusing an invertible network. In this paper, we introduce a novel model called\nSyMOT-Flow that trains an invertible transformation by minimizing the symmetric\nmaximum mean discrepancy between samples from two unknown distributions, and an\noptimal transport cost is incorporated as regularization to obtain a\nshort-distance and interpretable transformation. The resulted transformation\nleads to more stable and accurate sample generation. Several theoretical\nresults are established for the proposed model and its effectiveness is\nvalidated with low-dimensional illustrative examples as well as\nhigh-dimensional bi-modality medical image generation through the forward and\nreverse flows.\n","authors":["Zhe Xiong","Qiaoqiao Ding","Xiaoqun Zhang"],"pdf_url":"https://arxiv.org/pdf/2308.13815v2.pdf","comment":"12 pages article, 1 pages supplementary"},{"id":"http://arxiv.org/abs/2302.00878v4","updated":"2024-01-02T06:59:55Z","published":"2023-02-02T05:00:29Z","title":"The Contextual Lasso: Sparse Linear Models via Deep Neural Networks","summary":"  Sparse linear models are one of several core tools for interpretable machine\nlearning, a field of emerging importance as predictive models permeate\ndecision-making in many domains. Unfortunately, sparse linear models are far\nless flexible as functions of their input features than black-box models like\ndeep neural networks. With this capability gap in mind, we study a not-uncommon\nsituation where the input features dichotomize into two groups: explanatory\nfeatures, which are candidates for inclusion as variables in an interpretable\nmodel, and contextual features, which select from the candidate variables and\ndetermine their effects. This dichotomy leads us to the contextual lasso, a new\nstatistical estimator that fits a sparse linear model to the explanatory\nfeatures such that the sparsity pattern and coefficients vary as a function of\nthe contextual features. The fitting process learns this function\nnonparametrically via a deep neural network. To attain sparse coefficients, we\ntrain the network with a novel lasso regularizer in the form of a projection\nlayer that maps the network's output onto the space of $\\ell_1$-constrained\nlinear models. An extensive suite of experiments on real and synthetic data\nsuggests that the learned models, which remain highly transparent, can be\nsparser than the regular lasso without sacrificing the predictive power of a\nstandard deep neural network.\n","authors":["Ryan Thompson","Amir Dezfouli","Robert Kohn"],"pdf_url":"https://arxiv.org/pdf/2302.00878v4.pdf","comment":"To appear in Advances in Neural Information Processing Systems"},{"id":"http://arxiv.org/abs/2211.15542v3","updated":"2024-01-02T06:36:38Z","published":"2022-11-28T16:48:24Z","title":"Autonomous Assessment of Demonstration Sufficiency via Bayesian Inverse\n  Reinforcement Learning","summary":"  We examine the problem of determining demonstration sufficiency: how can a\nrobot self-assess whether it has received enough demonstrations from an expert\nto ensure a desired level of performance? To address this problem, we propose a\nnovel self-assessment approach based on Bayesian inverse reinforcement learning\nand value-at-risk, enabling learning-from-demonstration (\"LfD\") robots to\ncompute high-confidence bounds on their performance and use these bounds to\ndetermine when they have a sufficient number of demonstrations. We propose and\nevaluate two definitions of sufficiency: (1) normalized expected value\ndifference, which measures regret with respect to the human's unobserved reward\nfunction, and (2) percent improvement over a baseline policy. We demonstrate\nhow to formulate high-confidence bounds on both of these metrics. We evaluate\nour approach in simulation for both discrete and continuous state-space domains\nand illustrate the feasibility of developing a robotic system that can\naccurately evaluate demonstration sufficiency. We also show that the robot can\nutilize active learning in asking for demonstrations from specific states which\nresults in fewer demos needed for the robot to still maintain high confidence\nin its policy. Finally, via a user study, we show that our approach\nsuccessfully enables robots to perform at users' desired performance levels,\nwithout needing too many or perfectly optimal demonstrations.\n","authors":["Tu Trinh","Haoyu Chen","Daniel S. Brown"],"pdf_url":"https://arxiv.org/pdf/2211.15542v3.pdf","comment":"Prior version appears in proceedings of AAAI FSS-22 Symposium\n  \"Lessons Learned for Autonomous Assessment of Machine Abilities (LLAAMA)\".\n  Current version appears in proceedings of HRI '24, March 11-14, 2024,\n  Boulder, CO, USA"},{"id":"http://arxiv.org/abs/2401.01056v1","updated":"2024-01-02T06:31:24Z","published":"2024-01-02T06:31:24Z","title":"Enhancing Automatic Modulation Recognition through Robust Global Feature\n  Extraction","summary":"  Automatic Modulation Recognition (AMR) plays a crucial role in wireless\ncommunication systems. Deep learning AMR strategies have achieved tremendous\nsuccess in recent years. Modulated signals exhibit long temporal dependencies,\nand extracting global features is crucial in identifying modulation schemes.\nTraditionally, human experts analyze patterns in constellation diagrams to\nclassify modulation schemes. Classical convolutional-based networks, due to\ntheir limited receptive fields, excel at extracting local features but struggle\nto capture global relationships. To address this limitation, we introduce a\nnovel hybrid deep framework named TLDNN, which incorporates the architectures\nof the transformer and long short-term memory (LSTM). We utilize the\nself-attention mechanism of the transformer to model the global correlations in\nsignal sequences while employing LSTM to enhance the capture of temporal\ndependencies. To mitigate the impact like RF fingerprint features and channel\ncharacteristics on model generalization, we propose data augmentation\nstrategies known as segment substitution (SS) to enhance the model's robustness\nto modulation-related features. Experimental results on widely-used datasets\ndemonstrate that our method achieves state-of-the-art performance and exhibits\nsignificant advantages in terms of complexity. Our proposed framework serves as\na foundational backbone that can be extended to different datasets. We have\nverified the effectiveness of our augmentation approach in enhancing the\ngeneralization of the models, particularly in few-shot scenarios. Code is\navailable at \\url{https://github.com/AMR-Master/TLDNN}.\n","authors":["Yunpeng Qu","Zhilin Lu","Rui Zeng","Jintao Wang","Jian Wang"],"pdf_url":"https://arxiv.org/pdf/2401.01056v1.pdf","comment":"submitted to IEEE Transactions on Vehicular Technology, 14 pages, 11\n  figures"},{"id":"http://arxiv.org/abs/2401.01054v1","updated":"2024-01-02T06:26:25Z","published":"2024-01-02T06:26:25Z","title":"Elastic Multi-Gradient Descent for Parallel Continual Learning","summary":"  The goal of Continual Learning (CL) is to continuously learn from new data\nstreams and accomplish the corresponding tasks. Previously studied CL assumes\nthat data are given in sequence nose-to-tail for different tasks, thus indeed\nbelonging to Serial Continual Learning (SCL). This paper studies the novel\nparadigm of Parallel Continual Learning (PCL) in dynamic multi-task scenarios,\nwhere a diverse set of tasks is encountered at different time points. PCL\npresents challenges due to the training of an unspecified number of tasks with\nvarying learning progress, leading to the difficulty of guaranteeing effective\nmodel updates for all encountered tasks. In our previous conference work, we\nfocused on measuring and reducing the discrepancy among gradients in a\nmulti-objective optimization problem, which, however, may still contain\nnegative transfers in every model update. To address this issue, in the dynamic\nmulti-objective optimization problem, we introduce task-specific elastic\nfactors to adjust the descent direction towards the Pareto front. The proposed\nmethod, called Elastic Multi-Gradient Descent (EMGD), ensures that each update\nfollows an appropriate Pareto descent direction, minimizing any negative impact\non previously learned tasks. To balance the training between old and new tasks,\nwe also propose a memory editing mechanism guided by the gradient computed\nusing EMGD. This editing process updates the stored data points, reducing\ninterference in the Pareto descent direction from previous tasks. Experiments\non public datasets validate the effectiveness of our EMGD in the PCL setting.\n","authors":["Fan Lyu","Wei Feng","Yuepan Li","Qing Sun","Fanhua Shang","Liang Wan","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2401.01054v1.pdf","comment":"Submited to IEEE TPAMI"},{"id":"http://arxiv.org/abs/2401.01048v1","updated":"2024-01-02T06:00:47Z","published":"2024-01-02T06:00:47Z","title":"PAC-Bayesian Domain Adaptation Bounds for Multi-view learning","summary":"  This paper presents a series of new results for domain adaptation in the\nmulti-view learning setting. The incorporation of multiple views in the domain\nadaptation was paid little attention in the previous studies. In this way, we\npropose an analysis of generalization bounds with Pac-Bayesian theory to\nconsolidate the two paradigms, which are currently treated separately. Firstly,\nbuilding on previous work by Germain et al., we adapt the distance between\ndistribution proposed by Germain et al. for domain adaptation with the concept\nof multi-view learning. Thus, we introduce a novel distance that is tailored\nfor the multi-view domain adaptation setting. Then, we give Pac-Bayesian bounds\nfor estimating the introduced divergence. Finally, we compare the different new\nbounds with the previous studies.\n","authors":["Mehdi Hennequin","Khalid Benabdeslem","Haytham Elghazel"],"pdf_url":"https://arxiv.org/pdf/2401.01048v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2004.11829 by other authors"},{"id":"http://arxiv.org/abs/2401.01047v1","updated":"2024-01-02T05:55:27Z","published":"2024-01-02T05:55:27Z","title":"Sharp Analysis of Power Iteration for Tensor PCA","summary":"  We investigate the power iteration algorithm for the tensor PCA model\nintroduced in Richard and Montanari (2014). Previous work studying the\nproperties of tensor power iteration is either limited to a constant number of\niterations, or requires a non-trivial data-independent initialization. In this\npaper, we move beyond these limitations and analyze the dynamics of randomly\ninitialized tensor power iteration up to polynomially many steps. Our\ncontributions are threefold: First, we establish sharp bounds on the number of\niterations required for power method to converge to the planted signal, for a\nbroad range of the signal-to-noise ratios. Second, our analysis reveals that\nthe actual algorithmic threshold for power iteration is smaller than the one\nconjectured in literature by a polylog(n) factor, where n is the ambient\ndimension. Finally, we propose a simple and effective stopping criterion for\npower iteration, which provably outputs a solution that is highly correlated\nwith the true signal. Extensive numerical experiments verify our theoretical\nresults.\n","authors":["Yuchen Wu","Kangjie Zhou"],"pdf_url":"https://arxiv.org/pdf/2401.01047v1.pdf","comment":"40 pages, 8 figures"},{"id":"http://arxiv.org/abs/2312.04021v3","updated":"2024-01-02T05:10:27Z","published":"2023-12-07T03:37:39Z","title":"A Study on the Calibration of In-context Learning","summary":"  Accurate uncertainty quantification is crucial for the safe deployment of\nlanguage models (LMs), and prior research has demonstrated improvements in the\ncalibration of modern LMs. Our study focuses on in-context learning (ICL), a\nprevalent method for adapting static LMs through tailored prompts, and examines\nthe balance between performance and calibration across a broad spectrum of\nnatural language understanding and reasoning tasks. Through comprehensive\nexperiments, we observe that, with an increasing number of ICL examples, models\ninitially exhibit increased miscalibration before achieving better calibration\nand miscalibration tends to arise in low-shot settings. Moreover, we find that\nmethods aimed at improving usability, such as fine-tuning and chain-of-thought\n(CoT) prompting, can lead to miscalibration and unreliable natural language\nexplanations, suggesting that new methods may be required for scenarios where\nmodels are expected to be reliable.\n","authors":["Hanlin Zhang","Yi-Fan Zhang","Yaodong Yu","Dhruv Madeka","Dean Foster","Eric Xing","Himabindu Lakkaraju","Sham Kakade"],"pdf_url":"https://arxiv.org/pdf/2312.04021v3.pdf","comment":"Spotlight Talk at NeurIPS 2023 Workshop on Failure Modes in the Age\n  of Foundation Models"},{"id":"http://arxiv.org/abs/2401.00713v2","updated":"2024-01-02T05:01:25Z","published":"2024-01-01T09:53:24Z","title":"A Survey on Graph Neural Networks in Intelligent Transportation Systems","summary":"  Intelligent Transportation System (ITS) is vital in improving traffic\ncongestion, reducing traffic accidents, optimizing urban planning, etc.\nHowever, due to the complexity of the traffic network, traditional machine\nlearning and statistical methods are relegated to the background. With the\nadvent of the artificial intelligence era, many deep learning frameworks have\nmade remarkable progress in various fields and are now considered effective\nmethods in many areas. As a deep learning method, Graph Neural Networks (GNNs)\nhave emerged as a highly competitive method in the ITS field since 2019 due to\ntheir strong ability to model graph-related problems. As a result, more and\nmore scholars pay attention to the applications of GNNs in transportation\ndomains, which have shown excellent performance. However, most of the research\nin this area is still concentrated on traffic forecasting, while other ITS\ndomains, such as autonomous vehicles and urban planning, still require more\nattention. This paper aims to review the applications of GNNs in six\nrepresentative and emerging ITS domains: traffic forecasting, autonomous\nvehicles, traffic signal control, transportation safety, demand prediction, and\nparking management. We have reviewed extensive graph-related studies from 2018\nto 2023, summarized their methods, features, and contributions, and presented\nthem in informative tables or lists. Finally, we have identified the challenges\nof applying GNNs to ITS and suggested potential future directions.\n","authors":["Hourun Li","Yusheng Zhao","Zhengyang Mao","Yifang Qin","Zhiping Xiao","Jiaqi Feng","Yiyang Gu","Wei Ju","Xiao Luo","Ming Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.00713v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.10875v3","updated":"2024-01-02T04:15:02Z","published":"2023-07-20T13:47:30Z","title":"Risk-optimized Outlier Removal for Robust 3D Point Cloud Classification","summary":"  With the growth of 3D sensing technology, deep learning system for 3D point\nclouds has become increasingly important, especially in applications like\nautonomous vehicles where safety is a primary concern. However, there are also\ngrowing concerns about the reliability of these systems when they encounter\nnoisy point clouds, whether occurring naturally or introduced with malicious\nintent. This paper highlights the challenges of point cloud classification\nposed by various forms of noise, from simple background noise to malicious\nbackdoor attacks that can intentionally skew model predictions. While there's\nan urgent need for optimized point cloud denoising, current point outlier\nremoval approaches, an essential step for denoising, rely heavily on\nhandcrafted strategies and are not adapted for higher-level tasks, such as\nclassification. To address this issue, we introduce an innovative point outlier\ncleansing method that harnesses the power of downstream classification models.\nBy employing gradient-based attribution analysis, we define a novel concept:\npoint risk. Drawing inspiration from tail risk minimization in finance, we\nrecast the outlier removal process as an optimization problem, named PointCVaR.\nExtensive experiments show that our proposed technique not only robustly\nfilters diverse point cloud outliers but also consistently and significantly\nenhances existing robust methods for point cloud classification.\n","authors":["Xinke Li","Junchi Lu","Henghui Ding","Changsheng Sun","Joey Tianyi Zhou","Chee Yeow Meng"],"pdf_url":"https://arxiv.org/pdf/2307.10875v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01023v1","updated":"2024-01-02T04:14:16Z","published":"2024-01-02T04:14:16Z","title":"CautionSuicide: A Deep Learning Based Approach for Detecting Suicidal\n  Ideation in Real Time Chatbot Conversation","summary":"  Suicide is recognized as one of the most serious concerns in the modern\nsociety. Suicide causes tragedy that affects countries, communities, and\nfamilies. There are many factors that lead to suicidal ideations. Early\ndetection of suicidal ideations can help to prevent suicide occurrence by\nproviding the victim with the required professional support, especially when\nthe victim does not recognize the danger of having suicidal ideations. As\ntechnology usage has increased, people share and express their ideations\ndigitally via social media, chatbots, and other digital platforms. In this\npaper, we proposed a novel, simple deep learning-based model to detect suicidal\nideations in digital content, mainly focusing on chatbots as the primary data\nsource. In addition, we provide a framework that employs the proposed suicide\ndetection integration with a chatbot-based support system.\n","authors":["Nelly Elsayed","Zag ElSayed","Murat Ozer"],"pdf_url":"https://arxiv.org/pdf/2401.01023v1.pdf","comment":"5 pages, 6 figures, 4 tables, Under review in IEEE conference"},{"id":"http://arxiv.org/abs/2312.11460v3","updated":"2024-01-02T04:11:12Z","published":"2023-12-18T18:59:06Z","title":"Hybrid Internal Model: Learning Agile Legged Locomotion with Simulated\n  Robot Response","summary":"  Robust locomotion control depends on accurate state estimations. However, the\nsensors of most legged robots can only provide partial and noisy observations,\nmaking the estimation particularly challenging, especially for external states\nlike terrain frictions and elevation maps. Inspired by the classical Internal\nModel Control principle, we consider these external states as disturbances and\nintroduce Hybrid Internal Model (HIM) to estimate them according to the\nresponse of the robot. The response, which we refer to as the hybrid internal\nembedding, contains the robot's explicit velocity and implicit stability\nrepresentation, corresponding to two primary goals for locomotion tasks:\nexplicitly tracking velocity and implicitly maintaining stability. We use\ncontrastive learning to optimize the embedding to be close to the robot's\nsuccessor state, in which the response is naturally embedded. HIM has several\nappealing benefits: It only needs the robot's proprioceptions, i.e., those from\njoint encoders and IMU as observations. It innovatively maintains consistent\nobservations between simulation reference and reality that avoids information\nloss in mimicking learning. It exploits batch-level information that is more\nrobust to noises and keeps better sample efficiency. It only requires 1 hour of\ntraining on an RTX 4090 to enable a quadruped robot to traverse any terrain\nunder any disturbances. A wealth of real-world experiments demonstrates its\nagility, even in high-difficulty tasks and cases never occurred during the\ntraining process, revealing remarkable open-world generalizability.\n","authors":["Junfeng Long","Zirui Wang","Quanyi Li","Jiawei Gao","Liu Cao","Jiangmiao Pang"],"pdf_url":"https://arxiv.org/pdf/2312.11460v3.pdf","comment":"Use 1 hour to train a quadruped robot capable of traversing any\n  terrain under any disturbances in the open world, Project Page:\n  https://github.com/OpenRobotLab/HIMLoco"},{"id":"http://arxiv.org/abs/2401.01013v1","updated":"2024-01-02T04:00:48Z","published":"2024-01-02T04:00:48Z","title":"Boosting Transformer's Robustness and Efficacy in PPG Signal Artifact\n  Detection with Self-Supervised Learning","summary":"  Recent research at CHU Sainte Justine's Pediatric Critical Care Unit (PICU)\nhas revealed that traditional machine learning methods, such as semi-supervised\nlabel propagation and K-nearest neighbors, outperform Transformer-based models\nin artifact detection from PPG signals, mainly when data is limited. This study\naddresses the underutilization of abundant unlabeled data by employing\nself-supervised learning (SSL) to extract latent features from these data,\nfollowed by fine-tuning on labeled data. Our experiments demonstrate that SSL\nsignificantly enhances the Transformer model's ability to learn\nrepresentations, improving its robustness in artifact classification tasks.\nAmong various SSL techniques, including masking, contrastive learning, and DINO\n(self-distillation with no labels)-contrastive learning exhibited the most\nstable and superior performance in small PPG datasets. Further, we delve into\noptimizing contrastive loss functions, which are crucial for contrastive SSL.\nInspired by InfoNCE, we introduce a novel contrastive loss function that\nfacilitates smoother training and better convergence, thereby enhancing\nperformance in artifact classification. In summary, this study establishes the\nefficacy of SSL in leveraging unlabeled data, particularly in enhancing the\ncapabilities of the Transformer model. This approach holds promise for broader\napplications in PICU environments, where annotated data is often limited.\n","authors":["Thanh-Dung Le"],"pdf_url":"https://arxiv.org/pdf/2401.01013v1.pdf","comment":"Under preparation to submit to IEEE for possible publications"},{"id":"http://arxiv.org/abs/2401.01010v1","updated":"2024-01-02T03:37:11Z","published":"2024-01-02T03:37:11Z","title":"Unsupervised Continual Anomaly Detection with Contrastively-learned\n  Prompt","summary":"  Unsupervised Anomaly Detection (UAD) with incremental training is crucial in\nindustrial manufacturing, as unpredictable defects make obtaining sufficient\nlabeled data infeasible. However, continual learning methods primarily rely on\nsupervised annotations, while the application in UAD is limited due to the\nabsence of supervision. Current UAD methods train separate models for different\nclasses sequentially, leading to catastrophic forgetting and a heavy\ncomputational burden. To address this issue, we introduce a novel Unsupervised\nContinual Anomaly Detection framework called UCAD, which equips the UAD with\ncontinual learning capability through contrastively-learned prompts. In the\nproposed UCAD, we design a Continual Prompting Module (CPM) by utilizing a\nconcise key-prompt-knowledge memory bank to guide task-invariant `anomaly'\nmodel predictions using task-specific `normal' knowledge. Moreover,\nStructure-based Contrastive Learning (SCL) is designed with the Segment\nAnything Model (SAM) to improve prompt learning and anomaly segmentation\nresults. Specifically, by treating SAM's masks as structure, we draw features\nwithin the same mask closer and push others apart for general feature\nrepresentations. We conduct comprehensive experiments and set the benchmark on\nunsupervised continual anomaly detection and segmentation, demonstrating that\nour method is significantly better than anomaly detection methods, even with\nrehearsal training. The code will be available at\nhttps://github.com/shirowalker/UCAD.\n","authors":["Jiaqi Liu","Kai Wu","Qiang Nie","Ying Chen","Bin-Bin Gao","Yong Liu","Jinbao Wang","Chengjie Wang","Feng Zheng"],"pdf_url":"https://arxiv.org/pdf/2401.01010v1.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2201.01942v2","updated":"2024-01-02T03:23:43Z","published":"2022-01-06T07:12:36Z","title":"Efficiently Disentangle Causal Representations","summary":"  This paper proposes an efficient approach to learning disentangled\nrepresentations with causal mechanisms based on the difference of conditional\nprobabilities in original and new distributions. We approximate the difference\nwith models' generalization abilities so that it fits in the standard machine\nlearning framework and can be efficiently computed. In contrast to the\nstate-of-the-art approach, which relies on the learner's adaptation speed to\nnew distribution, the proposed approach only requires evaluating the model's\ngeneralization ability. We provide a theoretical explanation for the advantage\nof the proposed method, and our experiments show that the proposed technique is\n1.9--11.0$\\times$ more sample efficient and 9.4--32.4 times quicker than the\nprevious method on various tasks. The source code is available at\n\\url{https://github.com/yuanpeng16/EDCR}.\n","authors":["Yuanpeng Li","Joel Hestness","Mohamed Elhoseiny","Liang Zhao","Kenneth Church"],"pdf_url":"https://arxiv.org/pdf/2201.01942v2.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2401.00544v2","updated":"2024-01-02T03:03:18Z","published":"2023-12-31T17:15:25Z","title":"A Reliable Knowledge Processing Framework for Combustion Science using\n  Foundation Models","summary":"  This research explores the integration of large language models (LLMs) into\nscientific data assimilation, focusing on combustion science as a case study.\nLeveraging foundational models integrated with Retrieval-Augmented Generation\n(RAG) framework, the study introduces an approach to process diverse combustion\nresearch data, spanning experimental studies, simulations, and literature. The\nmultifaceted nature of combustion research emphasizes the critical role of\nknowledge processing in navigating and extracting valuable information from a\nvast and diverse pool of sources. The developed approach minimizes\ncomputational and economic expenses while optimizing data privacy and accuracy.\nIt incorporates prompt engineering and offline open-source LLMs, offering user\nautonomy in selecting base models. The study provides a thorough examination of\ntext segmentation strategies, conducts comparative studies between LLMs, and\nexplores various optimized prompts to demonstrate the effectiveness of the\nframework. By incorporating an external database, the framework outperforms a\nconventional LLM in generating accurate responses and constructing robust\narguments. Additionally, the study delves into the investigation of optimized\nprompt templates for the purpose of efficient extraction of scientific\nliterature. The research addresses concerns related to hallucinations and false\nresearch articles by introducing a custom workflow developed with a detection\nalgorithm to filter out inaccuracies. Despite identified areas for improvement,\nthe framework consistently delivers accurate domain-specific responses with\nminimal human oversight. The prompt-agnostic approach introduced holds promise\nfor future deliberations. The study underscores the significance of integrating\nLLMs and knowledge processing techniques in scientific research, providing a\nfoundation for advancements in data assimilation and utilization.\n","authors":["Vansh Sharma","Venkat Raman"],"pdf_url":"https://arxiv.org/pdf/2401.00544v2.pdf","comment":"38 pages and 10 figures; Fixed figure resolution"},{"id":"http://arxiv.org/abs/2312.01187v2","updated":"2024-01-02T02:05:01Z","published":"2023-12-02T17:25:30Z","title":"SASSL: Enhancing Self-Supervised Learning via Neural Style Transfer","summary":"  Self-supervised learning relies heavily on data augmentation to extract\nmeaningful representations from unlabeled images. While existing\nstate-of-the-art augmentation pipelines incorporate a wide range of primitive\ntransformations, these often disregard natural image structure. Thus, augmented\nsamples can exhibit degraded semantic information and low stylistic diversity,\naffecting downstream performance of self-supervised representations. To\novercome this, we propose SASSL: Style Augmentations for Self Supervised\nLearning, a novel augmentation technique based on Neural Style Transfer. The\nmethod decouples semantic and stylistic attributes in images and applies\ntransformations exclusively to the style while preserving content, generating\ndiverse augmented samples that better retain their semantic properties.\nExperimental results show our technique achieves a top-1 classification\nperformance improvement of more than 2% on ImageNet compared to the\nwell-established MoCo v2. We also measure transfer learning performance across\nfive diverse datasets, observing significant improvements of up to 3.75%. Our\nexperiments indicate that decoupling style from content information and\ntransferring style across datasets to diversify augmentations can significantly\nimprove downstream performance of self-supervised representations.\n","authors":["Renan A. Rojas-Gomez","Karan Singhal","Ali Etemad","Alex Bijamov","Warren R. Morningstar","Philip Andrew Mansfield"],"pdf_url":"https://arxiv.org/pdf/2312.01187v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00981v1","updated":"2024-01-02T00:55:10Z","published":"2024-01-02T00:55:10Z","title":"Machine Learning Classification of Alzheimer's Disease Stages Using\n  Cerebrospinal Fluid Biomarkers Alone","summary":"  Early diagnosis of Alzheimer's disease is a challenge because the existing\nmethodologies do not identify the patients in their preclinical stage, which\ncan last up to a decade prior to the onset of clinical symptoms. Several\nresearch studies demonstrate the potential of cerebrospinal fluid biomarkers,\namyloid beta 1-42, T-tau, and P-tau, in early diagnosis of Alzheimer's disease\nstages. In this work, we used machine learning models to classify different\nstages of Alzheimer's disease based on the cerebrospinal fluid biomarker levels\nalone. An electronic health record of patients from the National Alzheimer's\nCoordinating Centre database was analyzed and the patients were subdivided\nbased on mini-mental state scores and clinical dementia ratings. Statistical\nand correlation analyses were performed to identify significant differences\nbetween the Alzheimer's stages. Afterward, machine learning classifiers\nincluding K-Nearest Neighbors, Ensemble Boosted Tree, Ensemble Bagged Tree,\nSupport Vector Machine, Logistic Regression, and Naive Bayes classifiers were\nemployed to classify the Alzheimer's disease stages. The results demonstrate\nthat Ensemble Boosted Tree (84.4%) and Logistic Regression (73.4%) provide the\nhighest accuracy for binary classification, while Ensemble Bagged Tree (75.4%)\ndemonstrates better accuracy for multiclassification. The findings from this\nresearch are expected to help clinicians in making an informed decision\nregarding the early diagnosis of Alzheimer's from the cerebrospinal fluid\nbiomarkers alone, monitoring of the disease progression, and implementation of\nappropriate intervention measures.\n","authors":["Vivek Kumar Tiwari","Premananda Indic","Shawana Tabassum"],"pdf_url":"https://arxiv.org/pdf/2401.00981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.17353v2","updated":"2024-01-02T00:47:59Z","published":"2023-12-28T20:41:24Z","title":"Towards Auto-Modeling of Formal Verification for NextG Protocols: A\n  Multimodal cross- and self-attention Large Language Model Approach","summary":"  This paper introduces Auto-modeling of Formal Verification with Real-world\nPrompting for 5G and NextG protocols (AVRE), a novel system designed for the\nformal verification of Next Generation (NextG) communication protocols,\naddressing the increasing complexity and scalability challenges in network\nprotocol design and verification. Utilizing Large Language Models (LLMs), AVRE\ntransforms protocol descriptions into dependency graphs and formal models,\nefficiently resolving ambiguities and capturing design intent. The system\nintegrates a transformer model with LLMs to autonomously establish quantifiable\ndependency relationships through cross- and self-attention mechanisms. Enhanced\nby iterative feedback from the HyFuzz experimental platform, AVRE significantly\nadvances the accuracy and relevance of formal verification in complex\ncommunication protocols, offering a groundbreaking approach to validating\nsophisticated communication systems. We compare CAL's performance with\nstate-of-the-art LLM-based models and traditional time sequence models,\ndemonstrating its superiority in accuracy and robustness, achieving an accuracy\nof 95.94\\% and an AUC of 0.98. This NLP-based approach enables, for the first\ntime, the creation of exploits directly from design documents, making\nremarkable progress in scalable system verification and validation.\n","authors":["Jingda Yang","Ying Wang"],"pdf_url":"https://arxiv.org/pdf/2312.17353v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01460v1","updated":"2024-01-02T23:26:33Z","published":"2024-01-02T23:26:33Z","title":"Point Cloud Classification via Deep Set Linearized Optimal Transport","summary":"  We introduce Deep Set Linearized Optimal Transport, an algorithm designed for\nthe efficient simultaneous embedding of point clouds into an $L^2-$space. This\nembedding preserves specific low-dimensional structures within the Wasserstein\nspace while constructing a classifier to distinguish between various classes of\npoint clouds. Our approach is motivated by the observation that $L^2-$distances\nbetween optimal transport maps for distinct point clouds, originating from a\nshared fixed reference distribution, provide an approximation of the\nWasserstein-2 distance between these point clouds, under certain assumptions.\nTo learn approximations of these transport maps, we employ input convex neural\nnetworks (ICNNs) and establish that, under specific conditions, Euclidean\ndistances between samples from these ICNNs closely mirror Wasserstein-2\ndistances between the true distributions. Additionally, we train a\ndiscriminator network that attaches weights these samples and creates a\npermutation invariant classifier to differentiate between different classes of\npoint clouds. We showcase the advantages of our algorithm over the standard\ndeep set approach through experiments on a flow cytometry dataset with a\nlimited number of labeled point clouds.\n","authors":["Scott Mahan","Caroline Moosmüller","Alexander Cloninger"],"pdf_url":"https://arxiv.org/pdf/2401.01460v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.19274v3","updated":"2024-01-02T23:10:49Z","published":"2023-10-30T05:13:58Z","title":"Prediction of Effective Elastic Moduli of Rocks using Graph Neural\n  Networks","summary":"  This study presents a Graph Neural Networks (GNNs)-based approach for\npredicting the effective elastic moduli of rocks from their digital CT-scan\nimages. We use the Mapper algorithm to transform 3D digital rock images into\ngraph datasets, encapsulating essential geometrical information. These graphs,\nafter training, prove effective in predicting elastic moduli. Our GNN model\nshows robust predictive capabilities across various graph sizes derived from\nvarious subcube dimensions. Not only does it perform well on the test dataset,\nbut it also maintains high prediction accuracy for unseen rocks and unexplored\nsubcube sizes. Comparative analysis with Convolutional Neural Networks (CNNs)\nreveals the superior performance of GNNs in predicting unseen rock properties.\nMoreover, the graph representation of microstructures significantly reduces GPU\nmemory requirements (compared to the grid representation for CNNs), enabling\ngreater flexibility in the batch size selection. This work demonstrates the\npotential of GNN models in enhancing the prediction accuracy of rock properties\nand boosting the efficiency of digital rock analysis.\n","authors":["Jaehong Chung","Rasool Ahmad","WaiChing Sun","Wei Cai","Tapan Mukerji"],"pdf_url":"https://arxiv.org/pdf/2310.19274v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01458v1","updated":"2024-01-02T23:05:07Z","published":"2024-01-02T23:05:07Z","title":"Concurrent Self-testing of Neural Networks Using Uncertainty Fingerprint","summary":"  Neural networks (NNs) are increasingly used in always-on safety-critical\napplications deployed on hardware accelerators (NN-HAs) employing various\nmemory technologies. Reliable continuous operation of NN is essential for\nsafety-critical applications. During online operation, NNs are susceptible to\nsingle and multiple permanent and soft errors due to factors such as radiation,\naging, and thermal effects. Explicit NN-HA testing methods cannot detect\ntransient faults during inference, are unsuitable for always-on applications,\nand require extensive test vector generation and storage. Therefore, in this\npaper, we propose the \\emph{uncertainty fingerprint} approach representing the\nonline fault status of NN. Furthermore, we propose a dual head NN topology\nspecifically designed to produce uncertainty fingerprints and the primary\nprediction of the NN in \\emph{a single shot}. During the online operation, by\nmatching the uncertainty fingerprint, we can concurrently self-test NNs with up\nto $100\\%$ coverage with a low false positive rate while maintaining a similar\nperformance of the primary task. Compared to existing works, memory overhead is\nreduced by up to $243.7$ MB, multiply and accumulate (MAC) operation is reduced\nby up to $10000\\times$, and false-positive rates are reduced by up to $89\\%$.\n","authors":["Soyed Tuhin Ahmed","Mehdi B. tahoori"],"pdf_url":"https://arxiv.org/pdf/2401.01458v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00077v3","updated":"2024-01-02T22:54:55Z","published":"2023-09-29T18:33:10Z","title":"Optimizing with Low Budgets: a Comparison on the Black-box Optimization\n  Benchmarking Suite and OpenAI Gym","summary":"  The growing ubiquity of machine learning (ML) has led it to enter various\nareas of computer science, including black-box optimization (BBO). Recent\nresearch is particularly concerned with Bayesian optimization (BO). BO-based\nalgorithms are popular in the ML community, as they are used for hyperparameter\noptimization and more generally for algorithm configuration. However, their\nefficiency decreases as the dimensionality of the problem and the budget of\nevaluations increase. Meanwhile, derivative-free optimization methods have\nevolved independently in the optimization community. Therefore, we urge to\nunderstand whether cross-fertilization is possible between the two communities,\nML and BBO, i.e., whether algorithms that are heavily used in ML also work well\nin BBO and vice versa. Comparative experiments often involve rather small\nbenchmarks and show visible problems in the experimental setup, such as poor\ninitialization of baselines, overfitting due to problem-specific setting of\nhyperparameters, and low statistical significance.\n  With this paper, we update and extend a comparative study presented by Hutter\net al. in 2013. We compare BBO tools for ML with more classical heuristics,\nfirst on the well-known BBOB benchmark suite from the COCO environment and then\non Direct Policy Search for OpenAI Gym, a reinforcement learning benchmark. Our\nresults confirm that BO-based optimizers perform well on both benchmarks when\nbudgets are limited, albeit with a higher computational cost, while they are\noften outperformed by algorithms from other families when the evaluation budget\nbecomes larger. We also show that some algorithms from the BBO community\nperform surprisingly well on ML tasks.\n","authors":["Elena Raponi","Nathanael Rakotonirina Carraz","Jérémy Rapin","Carola Doerr","Olivier Teytaud"],"pdf_url":"https://arxiv.org/pdf/2310.00077v3.pdf","comment":"To appear in IEEE Transactions on Evolutionary Computation"},{"id":"http://arxiv.org/abs/2210.13702v2","updated":"2024-01-02T22:33:42Z","published":"2022-10-25T01:51:36Z","title":"DeXtreme: Transfer of Agile In-hand Manipulation from Simulation to\n  Reality","summary":"  Recent work has demonstrated the ability of deep reinforcement learning (RL)\nalgorithms to learn complex robotic behaviours in simulation, including in the\ndomain of multi-fingered manipulation. However, such models can be challenging\nto transfer to the real world due to the gap between simulation and reality. In\nthis paper, we present our techniques to train a) a policy that can perform\nrobust dexterous manipulation on an anthropomorphic robot hand and b) a robust\npose estimator suitable for providing reliable real-time information on the\nstate of the object being manipulated. Our policies are trained to adapt to a\nwide range of conditions in simulation. Consequently, our vision-based policies\nsignificantly outperform the best vision policies in the literature on the same\nreorientation task and are competitive with policies that are given privileged\nstate information via motion capture systems. Our work reaffirms the\npossibilities of sim-to-real transfer for dexterous manipulation in diverse\nkinds of hardware and simulator setups, and in our case, with the Allegro Hand\nand Isaac Gym GPU-based simulation. Furthermore, it opens up possibilities for\nresearchers to achieve such results with commonly-available, affordable robot\nhands and cameras. Videos of the resulting policy and supplementary\ninformation, including experiments and demos, can be found at\nhttps://dextreme.org/\n","authors":["Ankur Handa","Arthur Allshire","Viktor Makoviychuk","Aleksei Petrenko","Ritvik Singh","Jingzhou Liu","Denys Makoviichuk","Karl Van Wyk","Alexander Zhurkevich","Balakumar Sundaralingam","Yashraj Narang","Jean-Francois Lafleche","Dieter Fox","Gavriel State"],"pdf_url":"https://arxiv.org/pdf/2210.13702v2.pdf","comment":"28 pages. A smaller version of this paper is accepted to ICRA 2023"},{"id":"http://arxiv.org/abs/2305.19555v3","updated":"2024-01-02T22:30:00Z","published":"2023-05-31T04:50:29Z","title":"Large Language Models Are Not Strong Abstract Reasoners","summary":"  Large Language Models have shown tremendous performance on a large variety of\nnatural language processing tasks, ranging from text comprehension to common\nsense reasoning. However, the mechanisms responsible for this success remain\nopaque, and it is unclear whether LLMs can achieve human-like cognitive\ncapabilities or whether these models are still fundamentally circumscribed.\nAbstract reasoning is a fundamental task for cognition, consisting of finding\nand applying a general pattern from few data. Evaluating deep neural\narchitectures on this task could give insight into their potential limitations\nregarding reasoning and their broad generalisation abilities, yet this is\ncurrently an under-explored area. In this paper, we introduce a new benchmark\nfor evaluating language models beyond memorization on abstract reasoning tasks.\nWe perform extensive evaluations of state-of-the-art LLMs, showing that they\ncurrently achieve very limited performance in contrast with other natural\nlanguage tasks, even when applying techniques that have been shown to improve\nperformance on other NLP tasks. We argue that guiding LLM generation to follow\ncausal paths could help improve the generalisation and reasoning abilities of\nLLMs.\n","authors":["Gaël Gendron","Qiming Bao","Michael Witbrock","Gillian Dobbie"],"pdf_url":"https://arxiv.org/pdf/2305.19555v3.pdf","comment":"50 pages, 14 pages for the main paper and 36 pages for the\n  supplement, 35 figures, 17 tables. V3: performed additional experiments"},{"id":"http://arxiv.org/abs/2401.01448v1","updated":"2024-01-02T22:15:20Z","published":"2024-01-02T22:15:20Z","title":"ProbMCL: Simple Probabilistic Contrastive Learning for Multi-label\n  Visual Classification","summary":"  Multi-label image classification presents a challenging task in many domains,\nincluding computer vision and medical imaging. Recent advancements have\nintroduced graph-based and transformer-based methods to improve performance and\ncapture label dependencies. However, these methods often include complex\nmodules that entail heavy computation and lack interpretability. In this paper,\nwe propose Probabilistic Multi-label Contrastive Learning (ProbMCL), a novel\nframework to address these challenges in multi-label image classification\ntasks. Our simple yet effective approach employs supervised contrastive\nlearning, in which samples that share enough labels with an anchor image based\non a decision threshold are introduced as a positive set. This structure\ncaptures label dependencies by pulling positive pair embeddings together and\npushing away negative samples that fall below the threshold. We enhance\nrepresentation learning by incorporating a mixture density network into\ncontrastive learning and generating Gaussian mixture distributions to explore\nthe epistemic uncertainty of the feature encoder. We validate the effectiveness\nof our framework through experimentation with datasets from the computer vision\nand medical imaging domains. Our method outperforms the existing\nstate-of-the-art methods while achieving a low computational footprint on both\ndatasets. Visualization analyses also demonstrate that ProbMCL-learned\nclassifiers maintain a meaningful semantic topology.\n","authors":["Ahmad Sajedi","Samir Khaki","Yuri A. Lawryshyn","Konstantinos N. Plataniotis"],"pdf_url":"https://arxiv.org/pdf/2401.01448v1.pdf","comment":"This paper has been accepted for the IEEE International Conference on\n  Acoustics, Speech and Signal Processing (ICASSP) 2024"},{"id":"http://arxiv.org/abs/2401.01442v1","updated":"2024-01-02T21:43:01Z","published":"2024-01-02T21:43:01Z","title":"Hierarchical Over-the-Air Federated Learning with Awareness of\n  Interference and Data Heterogeneity","summary":"  When implementing hierarchical federated learning over wireless networks,\nscalability assurance and the ability to handle both interference and device\ndata heterogeneity are crucial. This work introduces a learning method designed\nto address these challenges, along with a scalable transmission scheme that\nefficiently uses a single wireless resource through over-the-air computation.\nTo provide resistance against data heterogeneity, we employ gradient\naggregations. Meanwhile, the impact of interference is minimized through\noptimized receiver normalizing factors. For this, we model a multi-cluster\nwireless network using stochastic geometry, and characterize the mean squared\nerror of the aggregation estimations as a function of the network parameters.\nWe show that despite the interference and the data heterogeneity, the proposed\nscheme achieves high learning accuracy and can significantly outperform the\nconventional hierarchical algorithm.\n","authors":["Seyed Mohammad Azimi-Abarghouyi","Viktoria Fodor"],"pdf_url":"https://arxiv.org/pdf/2401.01442v1.pdf","comment":"To appear at IEEE WCNC 2024. Overlap with arXiv:2211.16162"},{"id":"http://arxiv.org/abs/2307.05134v2","updated":"2024-01-02T21:18:48Z","published":"2023-07-11T09:23:05Z","title":"TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation","summary":"  The progress in the generation of synthetic images has made it crucial to\nassess their quality. While several metrics have been proposed to assess the\nrendering of images, it is crucial for Text-to-Image (T2I) models, which\ngenerate images based on a prompt, to consider additional aspects such as to\nwhich extent the generated image matches the important content of the prompt.\nMoreover, although the generated images usually result from a random starting\npoint, the influence of this one is generally not considered. In this article,\nwe propose a new metric based on prompt templates to study the alignment\nbetween the content specified in the prompt and the corresponding generated\nimages. It allows us to better characterize the alignment in terms of the type\nof the specified objects, their number, and their color. We conducted a study\non several recent T2I models about various aspects. An additional interesting\nresult we obtained with our approach is that image quality can vary drastically\ndepending on the noise used as a seed for the images. We also quantify the\ninfluence of the number of concepts in the prompt, their order as well as their\n(color) attributes. Finally, our method allows us to identify some seeds that\nproduce better images than others, opening novel directions of research on this\nunderstudied topic.\n","authors":["Paul Grimal","Hervé Le Borgne","Olivier Ferret","Julien Tourille"],"pdf_url":"https://arxiv.org/pdf/2307.05134v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.03932v4","updated":"2024-01-02T20:41:07Z","published":"2022-12-07T19:56:11Z","title":"Low Variance Off-policy Evaluation with State-based Importance Sampling","summary":"  In off-policy reinforcement learning, a behaviour policy performs exploratory\ninteractions with the environment to obtain state-action-reward samples which\nare then used to learn a target policy that optimises the expected return. This\nleads to a problem of off-policy evaluation, where one needs to evaluate the\ntarget policy from samples collected by the often unrelated behaviour policy.\nImportance sampling is a traditional statistical technique that is often\napplied to off-policy evaluation. While importance sampling estimators are\nunbiased, their variance increases exponentially with the horizon of the\ndecision process due to computing the importance weight as a product of action\nprobability ratios, yielding estimates with low accuracy for domains involving\nlong-term planning. This paper proposes state-based importance sampling, which\ndrops the action probability ratios of sub-trajectories with ``negligible\nstates'' -- roughly speaking, those for which the chosen actions have no impact\non the return estimate -- from the computation of the importance weight.\nTheoretical results show this reduces the ordinary importance sampling variance\nfrom $O(\\exp(H))$ to $O(\\exp(X))$ where $X < H$ is the largest subtrajectory\nwith non-negligible states. To identify negligible states, two search\nalgorithms are proposed, one based on covariance testing and one based on\nstate-action values. We formulate state-based variants of ordinary importance\nsampling, weighted importance sampling, per-decision importance sampling,\nincremental importance sampling, doubly robust off-policy evaluation, and\nstationary density ratio estimation. Experiments in four distinct domains show\nthat state-based methods consistently yield reduced variance and improved\naccuracy compared to their traditional counterparts.\n","authors":["David M. Bossens","Philip S. Thomas"],"pdf_url":"https://arxiv.org/pdf/2212.03932v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01426v1","updated":"2024-01-02T20:31:15Z","published":"2024-01-02T20:31:15Z","title":"Modular Learning of Deep Causal Generative Models for High-dimensional\n  Causal Inference","summary":"  Pearl's causal hierarchy establishes a clear separation between\nobservational, interventional, and counterfactual questions. Researchers\nproposed sound and complete algorithms to compute identifiable causal queries\nat a given level of the hierarchy using the causal structure and data from the\nlower levels of the hierarchy. However, most of these algorithms assume that we\ncan accurately estimate the probability distribution of the data, which is an\nimpractical assumption for high-dimensional variables such as images. On the\nother hand, modern generative deep learning architectures can be trained to\nlearn how to accurately sample from such high-dimensional distributions.\nEspecially with the recent rise of foundation models for images, it is\ndesirable to leverage pre-trained models to answer causal queries with such\nhigh-dimensional data. To address this, we propose a sequential training\nalgorithm that, given the causal structure and a pre-trained conditional\ngenerative model, can train a deep causal generative model, which utilizes the\npre-trained model and can provably sample from identifiable interventional and\ncounterfactual distributions. Our algorithm, called Modular-DCM, uses\nadversarial training to learn the network weights, and to the best of our\nknowledge, is the first algorithm that can make use of pre-trained models and\nprovably sample from any identifiable causal query in the presence of latent\nconfounders with high-dimensional data. We demonstrate the utility of our\nalgorithm using semi-synthetic and real-world datasets containing images as\nvariables in the causal structure.\n","authors":["Md Musfiqur Rahman","Murat Kocaoglu"],"pdf_url":"https://arxiv.org/pdf/2401.01426v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01425v1","updated":"2024-01-02T20:28:06Z","published":"2024-01-02T20:28:06Z","title":"SwapTransformer: highway overtaking tactical planner model via imitation\n  learning on OSHA dataset","summary":"  This paper investigates the high-level decision-making problem in highway\nscenarios regarding lane changing and over-taking other slower vehicles. In\nparticular, this paper aims to improve the Travel Assist feature for automatic\novertaking and lane changes on highways. About 9 million samples including lane\nimages and other dynamic objects are collected in simulation. This data;\nOvertaking on Simulated HighwAys (OSHA) dataset is released to tackle this\nchallenge. To solve this problem, an architecture called SwapTransformer is\ndesigned and implemented as an imitation learning approach on the OSHA dataset.\nMoreover, auxiliary tasks such as future points and car distance network\npredictions are proposed to aid the model in better understanding the\nsurrounding environment. The performance of the proposed solution is compared\nwith a multi-layer perceptron (MLP) and multi-head self-attention networks as\nbaselines in a simulation environment. We also demonstrate the performance of\nthe model with and without auxiliary tasks. All models are evaluated based on\ndifferent metrics such as time to finish each lap, number of overtakes, and\nspeed difference with speed limit. The evaluation shows that the\nSwapTransformer model outperforms other models in different traffic densities\nin the inference phase.\n","authors":["Alireza Shamsoshoara","Safin B Salih","Pedram Aghazadeh"],"pdf_url":"https://arxiv.org/pdf/2401.01425v1.pdf","comment":"19 pages, 12 Figures, 1 Algorithm, 2 Tables"},{"id":"http://arxiv.org/abs/2212.05987v2","updated":"2024-01-02T20:03:19Z","published":"2022-12-12T15:45:23Z","title":"Selective classification using a robust meta-learning approach","summary":"  Predictive uncertainty-a model's self awareness regarding its accuracy on an\ninput-is key for both building robust models via training interventions and for\ntest-time applications such as selective classification. We propose a novel\ninstance-conditioned reweighting approach that captures predictive uncertainty\nusing an auxiliary network and unifies these train- and test-time applications.\nThe auxiliary network is trained using a meta-objective in a bilevel\noptimization framework. A key contribution of our proposal is the\nmeta-objective of minimizing the dropout variance, an approximation of Bayesian\nPredictive uncertainty. We show in controlled experiments that we effectively\ncapture the diverse specific notions of uncertainty through this\nmeta-objective, while previous approaches only capture certain aspects. These\nresults translate to significant gains in real-world settings-selective\nclassification, label noise, domain adaptation, calibration-and across\ndatasets-Imagenet, Cifar100, diabetic retinopathy, Camelyon, WILDs,\nImagenet-C,-A,-R, Clothing1M, etc. For Diabetic Retinopathy, we see upto\n3.4%/3.3% accuracy and AUC gains over SOTA in selective classification. We also\nimprove upon large-scale pretrained models such as PLEX.\n","authors":["Nishant Jain","Karthikeyan Shanmugam","Pradeep Shenoy"],"pdf_url":"https://arxiv.org/pdf/2212.05987v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00490v2","updated":"2024-01-02T19:52:24Z","published":"2023-12-31T13:19:27Z","title":"Kernel Density Estimation for Multiclass Quantification","summary":"  Several disciplines, like the social sciences, epidemiology, sentiment\nanalysis, or market research, are interested in knowing the distribution of the\nclasses in a population rather than the individual labels of the members\nthereof. Quantification is the supervised machine learning task concerned with\nobtaining accurate predictors of class prevalence, and to do so particularly in\nthe presence of label shift. The distribution-matching (DM) approaches\nrepresent one of the most important families among the quantification methods\nthat have been proposed in the literature so far. Current DM approaches model\nthe involved populations by means of histograms of posterior probabilities. In\nthis paper, we argue that their application to the multiclass setting is\nsuboptimal since the histograms become class-specific, thus missing the\nopportunity to model inter-class information that may exist in the data. We\npropose a new representation mechanism based on multivariate densities that we\nmodel via kernel density estimation (KDE). The experiments we have carried out\nshow our method, dubbed KDEy, yields superior quantification performance with\nrespect to previous DM approaches. We also investigate the KDE-based\nrepresentation within the maximum likelihood framework and show KDEy often\nshows superior performance with respect to the expectation-maximization method\nfor quantification, arguably the strongest contender in the quantification\narena to date.\n","authors":["Alejandro Moreo","Pablo González","Juan José del Coz"],"pdf_url":"https://arxiv.org/pdf/2401.00490v2.pdf","comment":"fixed broken references to appendices"},{"id":"http://arxiv.org/abs/2401.01414v1","updated":"2024-01-02T19:51:49Z","published":"2024-01-02T19:51:49Z","title":"VALD-MD: Visual Attribution via Latent Diffusion for Medical Diagnostics","summary":"  Visual attribution in medical imaging seeks to make evident the\ndiagnostically-relevant components of a medical image, in contrast to the more\ncommon detection of diseased tissue deployed in standard machine vision\npipelines (which are less straightforwardly interpretable/explainable to\nclinicians). We here present a novel generative visual attribution technique,\none that leverages latent diffusion models in combination with domain-specific\nlarge language models, in order to generate normal counterparts of abnormal\nimages. The discrepancy between the two hence gives rise to a mapping\nindicating the diagnostically-relevant image components. To achieve this, we\ndeploy image priors in conjunction with appropriate conditioning mechanisms in\norder to control the image generative process, including natural language text\nprompts acquired from medical science and applied radiology. We perform\nexperiments and quantitatively evaluate our results on the COVID-19 Radiography\nDatabase containing labelled chest X-rays with differing pathologies via the\nFrechet Inception Distance (FID), Structural Similarity (SSIM) and Multi Scale\nStructural Similarity Metric (MS-SSIM) metrics obtained between real and\ngenerated images. The resulting system also exhibits a range of latent\ncapabilities including zero-shot localized disease induction, which are\nevaluated with real examples from the cheXpert dataset.\n","authors":["Ammar A. Siddiqui","Santosh Tirunagari","Tehseen Zia","David Windridge"],"pdf_url":"https://arxiv.org/pdf/2401.01414v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01404v1","updated":"2024-01-02T19:00:13Z","published":"2024-01-02T19:00:13Z","title":"Scalable network reconstruction in subquadratic time","summary":"  Network reconstruction consists in determining the unobserved pairwise\ncouplings between $N$ nodes given only observational data on the resulting\nbehavior that is conditioned on those couplings -- typically a time-series or\nindependent samples from a graphical model. A major obstacle to the scalability\nof algorithms proposed for this problem is a seemingly unavoidable quadratic\ncomplexity of $O(N^2)$, corresponding to the requirement of each possible\npairwise coupling being contemplated at least once, despite the fact that most\nnetworks of interest are sparse, with a number of non-zero couplings that is\nonly $O(N)$. Here we present a general algorithm applicable to a broad range of\nreconstruction problems that achieves its result in subquadratic time, with a\ndata-dependent complexity loosely upper bounded by $O(N^{3/2}\\log N)$, but with\na more typical log-linear complexity of $O(N\\log^2N)$. Our algorithm relies on\na stochastic second neighbor search that produces the best edge candidates with\nhigh probability, thus bypassing an exhaustive quadratic search. In practice,\nour algorithm achieves a performance that is many orders of magnitude faster\nthan the quadratic baseline, allows for easy parallelization, and thus enables\nthe reconstruction of networks with hundreds of thousands and even millions of\nnodes and edges.\n","authors":["Tiago P. Peixoto"],"pdf_url":"https://arxiv.org/pdf/2401.01404v1.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2401.01398v1","updated":"2024-01-02T18:34:29Z","published":"2024-01-02T18:34:29Z","title":"Accelerating Black-Box Molecular Property Optimization by Adaptively\n  Learning Sparse Subspaces","summary":"  Molecular property optimization (MPO) problems are inherently challenging\nsince they are formulated over discrete, unstructured spaces and the labeling\nprocess involves expensive simulations or experiments, which fundamentally\nlimits the amount of available data. Bayesian optimization (BO) is a powerful\nand popular framework for efficient optimization of noisy, black-box objective\nfunctions (e.g., measured property values), thus is a potentially attractive\nframework for MPO. To apply BO to MPO problems, one must select a structured\nmolecular representation that enables construction of a probabilistic surrogate\nmodel. Many molecular representations have been developed, however, they are\nall high-dimensional, which introduces important challenges in the BO process\n-- mainly because the curse of dimensionality makes it difficult to define and\nperform inference over a suitable class of surrogate models. This challenge has\nbeen recently addressed by learning a lower-dimensional encoding of a SMILE or\ngraph representation of a molecule in an unsupervised manner and then\nperforming BO in the encoded space. In this work, we show that such methods\nhave a tendency to \"get stuck,\" which we hypothesize occurs since the mapping\nfrom the encoded space to property values is not necessarily well-modeled by a\nGaussian process. We argue for an alternative approach that combines numerical\nmolecular descriptors with a sparse axis-aligned Gaussian process model, which\nis capable of rapidly identifying sparse subspaces that are most relevant to\nmodeling the unknown property function. We demonstrate that our proposed method\nsubstantially outperforms existing MPO methods on a variety of benchmark and\nreal-world problems. Specifically, we show that our method can routinely find\nnear-optimal molecules out of a set of more than $>100$k alternatives within\n100 or fewer expensive queries.\n","authors":["Farshud Sorourifar","Thomas Banker","Joel A. Paulson"],"pdf_url":"https://arxiv.org/pdf/2401.01398v1.pdf","comment":"9 pages, 2 figures consisting of 6 and 4 plots, accepted to NeurIPS\n  2023 Workshop on Adaptive Experimental Design and Active Learning in the Real\n  World"},{"id":"http://arxiv.org/abs/2401.01395v1","updated":"2024-01-02T18:03:57Z","published":"2024-01-02T18:03:57Z","title":"Deep autoregressive modeling for land use land cover","summary":"  Land use / land cover (LULC) modeling is a challenging task due to long-range\ndependencies between geographic features and distinct spatial patterns related\nto topography, ecology, and human development. We identify a close connection\nbetween modeling of spatial patterns of land use and the task of image\ninpainting from computer vision and conduct a study of a modified PixelCNN\narchitecture with approximately 19 million parameters for modeling LULC. In\ncomparison with a benchmark spatial statistical model, we find that the former\nis capable of capturing much richer spatial correlation patterns such as roads\nand water bodies but does not produce a calibrated predictive distribution,\nsuggesting the need for additional tuning. We find evidence of predictive\nunderdispersion with regard to important ecologically-relevant land use\nstatistics such as patch count and adjacency which can be ameliorated to some\nextent by manipulating sampling variability.\n","authors":["Christopher Krapu","Mark Borsuk","Ryan Calder"],"pdf_url":"https://arxiv.org/pdf/2401.01395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01393v1","updated":"2024-01-02T15:37:47Z","published":"2024-01-02T15:37:47Z","title":"Backtracking New Q-Newton's method, Newton's flow, Voronoi's diagram and\n  Stochastic root finding","summary":"  A new variant of Newton's method - named Backtracking New Q-Newton's method\n(BNQN) - which has strong theoretical guarantee, is easy to implement, and has\ngood experimental performance, was recently introduced by the third author.\n  Experiments performed previously showed some remarkable properties of the\nbasins of attractions for finding roots of polynomials and meromorphic\nfunctions, with BNQN. In general, they look more smooth than that of Newton's\nmethod.\n  In this paper, we continue to experimentally explore in depth this remarkable\nphenomenon, and connect BNQN to Newton's flow and Voronoi's diagram. This link\nposes a couple of challenging puzzles to be explained. Experiments also\nindicate that BNQN is more robust against random perturbations than Newton's\nmethod and Random Relaxed Newton's method.\n","authors":["John Erik Fornaess","Mi Hu","Tuyen Trung Truong","Takayuki Watanabe"],"pdf_url":"https://arxiv.org/pdf/2401.01393v1.pdf","comment":"48 pages. Comments are welcome! arXiv admin note: text overlap with\n  arXiv:2312.12166"}],"Multimedia":[{"id":"http://arxiv.org/abs/2306.11300v5","updated":"2024-01-02T14:18:02Z","published":"2023-06-20T05:30:59Z","title":"RS5M and GeoRSCLIP: A Large Scale Vision-Language Dataset and A Large\n  Vision-Language Model for Remote Sensing","summary":"  Pre-trained Vision-Language Models (VLMs) utilizing extensive image-text\npaired data have demonstrated unprecedented image-text association\ncapabilities, achieving remarkable results across various downstream tasks. A\ncritical challenge is how to make use of existing large-scale pre-trained VLMs,\nwhich are trained on common objects, to perform the domain-specific transfer\nfor accomplishing domain-related downstream tasks. A critical challenge is how\nto make use of existing large-scale pre-trained VLMs, which are trained on\ncommon objects, to perform the domain-specific transfer for accomplishing\ndomain-related downstream tasks. In this paper, we propose a new framework that\nincludes the Domain pre-trained Vision-Language Model (DVLM), bridging the gap\nbetween the General Vision-Language Model (GVLM) and domain-specific downstream\ntasks. Moreover, we present an image-text paired dataset in the field of remote\nsensing (RS), RS5M, which has 5 million RS images with English descriptions.\nThe dataset is obtained from filtering publicly available image-text paired\ndatasets and captioning label-only RS datasets with pre-trained VLM. These\nconstitute the first large-scale RS image-text paired dataset. Additionally, we\nfine-tuned the CLIP model and tried several Parameter-Efficient Fine-Tuning\nmethods on RS5M to implement the DVLM. Experimental results show that our\nproposed dataset is highly effective for various tasks, and our model GeoRSCLIP\nimproves upon the baseline or previous state-of-the-art model by $3\\%\\sim20\\%$\nin Zero-shot Classification (ZSC), $3\\%\\sim6\\%$ in Remote Sensing Cross-Modal\nText-Image Retrieval (RSCTIR) and $4\\%\\sim5\\%$ in Semantic Localization (SeLo)\ntasks. Dataset and models have been released in:\n\\url{https://github.com/om-ai-lab/RS5M}.\n","authors":["Zilun Zhang","Tiancheng Zhao","Yulong Guo","Jianwei Yin"],"pdf_url":"https://arxiv.org/pdf/2306.11300v5.pdf","comment":"RS5M dataset v5"},{"id":"http://arxiv.org/abs/2401.01163v1","updated":"2024-01-02T11:46:42Z","published":"2024-01-02T11:46:42Z","title":"NU-Class Net: A Novel Deep Learning-based Approach for Video Quality\n  Enhancement","summary":"  Video content has experienced a surge in popularity, asserting its dominance\nover internet traffic and Internet of Things (IoT) networks. Video compression\nhas long been regarded as the primary means of efficiently managing the\nsubstantial multimedia traffic generated by video-capturing devices.\nNevertheless, video compression algorithms entail significant computational\ndemands in order to achieve substantial compression ratios. This complexity\npresents a formidable challenge when implementing efficient video coding\nstandards in resource-constrained embedded systems, such as IoT edge node\ncameras. To tackle this challenge, this paper introduces NU-Class Net, an\ninnovative deep-learning model designed to mitigate compression artifacts\nstemming from lossy compression codecs. This enhancement significantly elevates\nthe perceptible quality of low-bit-rate videos. By employing the NU-Class Net,\nthe video encoder within the video-capturing node can reduce output quality,\nthereby generating low-bit-rate videos and effectively curtailing both\ncomputation and bandwidth requirements at the edge. On the decoder side, which\nis typically less encumbered by resource limitations, NU-Class Net is applied\nafter the video decoder to compensate for artifacts and approximate the quality\nof the original video. Experimental results affirm the efficacy of the proposed\nmodel in enhancing the perceptible quality of videos, especially those streamed\nat low bit rates.\n","authors":["Parham Zilouchian Moghaddam","Mehdi Modarressi","MohammadAmin Sadeghi"],"pdf_url":"https://arxiv.org/pdf/2401.01163v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10307v2","updated":"2024-01-02T02:36:22Z","published":"2023-12-16T03:50:13Z","title":"MusER: Musical Element-Based Regularization for Generating Symbolic\n  Music with Emotion","summary":"  Generating music with emotion is an important task in automatic music\ngeneration, in which emotion is evoked through a variety of musical elements\n(such as pitch and duration) that change over time and collaborate with each\nother. However, prior research on deep learning-based emotional music\ngeneration has rarely explored the contribution of different musical elements\nto emotions, let alone the deliberate manipulation of these elements to alter\nthe emotion of music, which is not conducive to fine-grained element-level\ncontrol over emotions. To address this gap, we present a novel approach\nemploying musical element-based regularization in the latent space to\ndisentangle distinct elements, investigate their roles in distinguishing\nemotions, and further manipulate elements to alter musical emotions.\nSpecifically, we propose a novel VQ-VAE-based model named MusER. MusER\nincorporates a regularization loss to enforce the correspondence between the\nmusical element sequences and the specific dimensions of latent variable\nsequences, providing a new solution for disentangling discrete sequences.\nTaking advantage of the disentangled latent vectors, a two-level decoding\nstrategy that includes multiple decoders attending to latent vectors with\ndifferent semantics is devised to better predict the elements. By visualizing\nlatent space, we conclude that MusER yields a disentangled and interpretable\nlatent space and gain insights into the contribution of distinct elements to\nthe emotional dimensions (i.e., arousal and valence). Experimental results\ndemonstrate that MusER outperforms the state-of-the-art models for generating\nemotional music in both objective and subjective evaluation. Besides, we\nrearrange music through element transfer and attempt to alter the emotion of\nmusic by transferring emotion-distinguishable elements.\n","authors":["Shulei Ji","Xinyu Yang"],"pdf_url":"https://arxiv.org/pdf/2312.10307v2.pdf","comment":"Accepted by AAAI 2024"}]},"2024-01-01T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2309.12269v4","updated":"2024-01-01T23:48:58Z","published":"2023-09-21T17:24:40Z","title":"The Cambridge Law Corpus: A Dataset for Legal AI Research","summary":"  We introduce the Cambridge Law Corpus (CLC), a dataset for legal AI research.\nIt consists of over 250 000 court cases from the UK. Most cases are from the\n21st century, but the corpus includes cases as old as the 16th century. This\npaper presents the first release of the corpus, containing the raw text and\nmeta-data. Together with the corpus, we provide annotations on case outcomes\nfor 638 cases, done by legal experts. Using our annotated data, we have trained\nand evaluated case outcome extraction with GPT-3, GPT-4 and RoBERTa models to\nprovide benchmarks. We include an extensive legal and ethical discussion to\naddress the potentially sensitive nature of this material. As a consequence,\nthe corpus will only be released for research purposes under certain\nrestrictions.\n","authors":["Andreas Östling","Holli Sargeant","Huiyuan Xie","Ludwig Bull","Alexander Terenin","Leif Jonsson","Måns Magnusson","Felix Steffek"],"pdf_url":"https://arxiv.org/pdf/2309.12269v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04469v2","updated":"2024-01-01T23:18:29Z","published":"2023-12-07T17:41:44Z","title":"On the Learnability of Watermarks for Language Models","summary":"  Watermarking of language model outputs enables statistical detection of\nmodel-generated text, which has many applications in the responsible deployment\nof language models. Existing watermarking strategies operate by altering the\ndecoder of an existing language model, and the ability for a language model to\ndirectly learn to generate the watermark would have significant implications\nfor the real-world deployment of watermarks. First, learned watermarks could be\nused to build open models that naturally generate watermarked text, allowing\nfor open models to benefit from watermarking. Second, if watermarking is used\nto determine the provenance of generated text, an adversary can hurt the\nreputation of a victim model by spoofing its watermark and generating damaging\nwatermarked text. To investigate the learnability of watermarks, we propose\nwatermark distillation, which trains a student model to behave like a teacher\nmodel that uses decoding-based watermarking. We test our approach on three\ndistinct decoding-based watermarking strategies and various hyperparameter\nsettings, finding that models can learn to generate watermarked text with high\ndetectability. We also find limitations to learnability, including the loss of\nwatermarking capabilities under fine-tuning on normal text and high sample\ncomplexity when learning low-distortion watermarks.\n","authors":["Chenchen Gu","Xiang Lisa Li","Percy Liang","Tatsunori Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2312.04469v2.pdf","comment":"22 pages, 6 figures"},{"id":"http://arxiv.org/abs/2305.17760v6","updated":"2024-01-01T21:06:57Z","published":"2023-05-28T16:04:48Z","title":"Language Models are Bounded Pragmatic Speakers: Understanding RLHF from\n  a Bayesian Cognitive Modeling Perspective","summary":"  How do language models \"think\"? This paper formulates a probabilistic\ncognitive model called the bounded pragmatic speaker, which can characterize\nthe operation of different variations of language models. Specifically, we\ndemonstrate that large language models fine-tuned with reinforcement learning\nfrom human feedback (Ouyang et al., 2022) embody a model of thought that\nconceptually resembles a fast-and-slow model (Kahneman, 2011), which\npsychologists have attributed to humans. We discuss the limitations of\nreinforcement learning from human feedback as a fast-and-slow model of thought\nand propose avenues for expanding this framework. In essence, our research\nhighlights the value of adopting a cognitive probabilistic modeling approach to\ngain insights into the comprehension, evaluation, and advancement of language\nmodels.\n","authors":["Khanh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2305.17760v6.pdf","comment":"Proceedings of the First Workshop on Theory of Mind in Communicating\n  Agents at (TOM @ ICML 2023)"},{"id":"http://arxiv.org/abs/2401.00824v1","updated":"2024-01-01T17:48:25Z","published":"2024-01-01T17:48:25Z","title":"Graph-Convolutional Autoencoder Ensembles for the Humanities,\n  Illustrated with a Study of the American Slave Trade","summary":"  We introduce a graph-aware autoencoder ensemble framework, with associated\nformalisms and tooling, designed to facilitate deep learning for scholarship in\nthe humanities. By composing sub-architectures to produce a model isomorphic to\na humanistic domain we maintain interpretability while providing function\nsignatures for each sub-architectural choice, allowing both traditional and\ncomputational researchers to collaborate without disrupting established\npractices. We illustrate a practical application of our approach to a\nhistorical study of the American post-Atlantic slave trade, and make several\nspecific technical contributions: a novel hybrid graph-convolutional\nautoencoder mechanism, batching policies for common graph topologies, and\nmasking techniques for particular use-cases. The effectiveness of the framework\nfor broadening participation of diverse domains is demonstrated by a growing\nsuite of two dozen studies, both collaborations with humanists and established\ntasks from machine learning literature, spanning a variety of fields and data\nmodalities. We make performance comparisons of several different architectural\nchoices and conclude with an ambitious list of imminent next steps for this\nresearch.\n","authors":["Tom Lippincott"],"pdf_url":"https://arxiv.org/pdf/2401.00824v1.pdf","comment":"More in-depth technical companion to \"A general neural ensemble\n  technique to support traditional scholarship\", Digital Humanities 2020"},{"id":"http://arxiv.org/abs/2401.00820v1","updated":"2024-01-01T17:32:28Z","published":"2024-01-01T17:32:28Z","title":"A Computational Framework for Behavioral Assessment of LLM Therapists","summary":"  The emergence of ChatGPT and other large language models (LLMs) has greatly\nincreased interest in utilizing LLMs as therapists to support individuals\nstruggling with mental health challenges. However, due to the lack of\nsystematic studies, our understanding of how LLM therapists behave, i.e., ways\nin which they respond to clients, is significantly limited. Understanding their\nbehavior across a wide range of clients and situations is crucial to accurately\nassess their capabilities and limitations in the high-risk setting of mental\nhealth, where undesirable behaviors can lead to severe consequences. In this\npaper, we propose BOLT, a novel computational framework to study the\nconversational behavior of LLMs when employed as therapists. We develop an\nin-context learning method to quantitatively measure the behavior of LLMs based\non 13 different psychotherapy techniques including reflections, questions,\nsolutions, normalizing, and psychoeducation. Subsequently, we compare the\nbehavior of LLM therapists against that of high- and low-quality human therapy,\nand study how their behavior can be modulated to better reflect behaviors\nobserved in high-quality therapy. Our analysis of GPT and Llama-variants\nreveals that these LLMs often resemble behaviors more commonly exhibited in\nlow-quality therapy rather than high-quality therapy, such as offering a higher\ndegree of problem-solving advice when clients share emotions, which is against\ntypical recommendations. At the same time, unlike low-quality therapy, LLMs\nreflect significantly more upon clients' needs and strengths. Our analysis\nframework suggests that despite the ability of LLMs to generate anecdotal\nexamples that appear similar to human therapists, LLM therapists are currently\nnot fully consistent with high-quality care, and thus require additional\nresearch to ensure quality care.\n","authors":["Yu Ying Chiu","Ashish Sharma","Inna Wanyin Lin","Tim Althoff"],"pdf_url":"https://arxiv.org/pdf/2401.00820v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00812v1","updated":"2024-01-01T16:51:20Z","published":"2024-01-01T16:51:20Z","title":"If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code\n  Empowers Large Language Models to Serve as Intelligent Agents","summary":"  The prominent large language models (LLMs) of today differ from past language\nmodels not only in size, but also in the fact that they are trained on a\ncombination of natural language and formal language (code). As a medium between\nhumans and computers, code translates high-level goals into executable steps,\nfeaturing standard syntax, logical consistency, abstraction, and modularity. In\nthis survey, we present an overview of the various benefits of integrating code\ninto LLMs' training data. Specifically, beyond enhancing LLMs in code\ngeneration, we observe that these unique properties of code help (i) unlock the\nreasoning ability of LLMs, enabling their applications to a range of more\ncomplex natural language tasks; (ii) steer LLMs to produce structured and\nprecise intermediate steps, which can then be connected to external execution\nends through function calls; and (iii) take advantage of code compilation and\nexecution environment, which also provides diverse feedback for model\nimprovement. In addition, we trace how these profound capabilities of LLMs,\nbrought by code, have led to their emergence as intelligent agents (IAs) in\nsituations where the ability to understand instructions, decompose goals, plan\nand execute actions, and refine from feedback are crucial to their success on\ndownstream tasks. Finally, we present several key challenges and future\ndirections of empowering LLMs with code.\n","authors":["Ke Yang","Jiateng Liu","John Wu","Chaoqi Yang","Yi R. Fung","Sha Li","Zixuan Huang","Xu Cao","Xingyao Wang","Yiquan Wang","Heng Ji","Chengxiang Zhai"],"pdf_url":"https://arxiv.org/pdf/2401.00812v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00811v1","updated":"2024-01-01T16:42:56Z","published":"2024-01-01T16:42:56Z","title":"PerSHOP -- A Persian dataset for shopping dialogue systems modeling","summary":"  Nowadays, dialogue systems are used in many fields of industry and research.\nThere are successful instances of these systems, such as Apple Siri, Google\nAssistant, and IBM Watson. Task-oriented dialogue system is a category of\nthese, that are used in specific tasks. They can perform tasks such as booking\nplane tickets or making restaurant reservations. Shopping is one of the most\npopular areas on these systems. The bot replaces the human salesperson and\ninteracts with the customers by speaking. To train the models behind the scenes\nof these systems, annotated data is needed. In this paper, we developed a\ndataset of dialogues in the Persian language through crowd-sourcing. We\nannotated these dialogues to train a model. This dataset contains nearly 22k\nutterances in 15 different domains and 1061 dialogues. This is the largest\nPersian dataset in this field, which is provided freely so that future\nresearchers can use it. Also, we proposed some baseline models for natural\nlanguage understanding (NLU) tasks. These models perform two tasks for NLU:\nintent classification and entity extraction. The F-1 score metric obtained for\nintent classification is around 91% and for entity extraction is around 93%,\nwhich can be a baseline for future research.\n","authors":["Keyvan Mahmoudi","Heshaam Faili"],"pdf_url":"https://arxiv.org/pdf/2401.00811v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00793v1","updated":"2024-01-01T15:40:35Z","published":"2024-01-01T15:40:35Z","title":"SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for\n  Large Language Models","summary":"  With the growing use of large language models hosted on cloud platforms to\noffer inference services, privacy concerns are escalating, especially\nconcerning sensitive data like investment plans and bank account details.\nSecure Multi-Party Computing (SMPC) emerges as a promising solution to protect\nthe privacy of inference data and model parameters. However, the application of\nSMPC in Privacy-Preserving Inference (PPI) for large language models,\nparticularly those based on the Transformer architecture, often leads to\nconsiderable slowdowns or declines in performance. This is largely due to the\nmultitude of nonlinear operations in the Transformer architecture, which are\nnot well-suited to SMPC and are difficult to circumvent or optimize\neffectively. To address this concern, we introduce an advanced optimization\nframework called SecFormer, designed to strike an optimal balance between\nperformance and efficiency in PPI for Transformer models. By implementing\nknowledge distillation techniques, we successfully eliminate the high-cost\nexponential and maximum operations in PPI without sacrificing model\nperformance. Additionally, we have developed a suite of efficient SMPC\nprotocols that utilize segmented polynomials and Goldschmidt's method to handle\nother complex nonlinear functions within PPI, such as GeLU, LayerNorm, and\nSoftmax. Our extensive experiments reveal that SecFormer outperforms MPCFormer\nin performance, showing improvements of $5.6\\%$ and $24.2\\%$ for\nBERT$_{\\text{BASE}}$ and BERT$_{\\text{LARGE}}$, respectively. In terms of\nefficiency, SecFormer is 3.4 and 3.2 times faster than Puma, demonstrating its\neffectiveness and speed.\n","authors":["Jinglong Luo","Yehong Zhang","Jiaqi Zhang","Xin Mu","Hui Wang","Yue Yu","Zenglin Xu"],"pdf_url":"https://arxiv.org/pdf/2401.00793v1.pdf","comment":"12pages, 15figures"},{"id":"http://arxiv.org/abs/2401.00788v1","updated":"2024-01-01T15:30:19Z","published":"2024-01-01T15:30:19Z","title":"Astraios: Parameter-Efficient Instruction Tuning Code Large Language\n  Models","summary":"  The high cost of full-parameter fine-tuning (FFT) of Large Language Models\n(LLMs) has led to a series of parameter-efficient fine-tuning (PEFT) methods.\nHowever, it remains unclear which methods provide the best cost-performance\ntrade-off at different model scales. We introduce Astraios, a suite of 28\ninstruction-tuned OctoCoder models using 7 tuning methods and 4 model sizes up\nto 16 billion parameters. Through investigations across 5 tasks and 8 different\ndatasets encompassing both code comprehension and code generation tasks, we\nfind that FFT generally leads to the best downstream performance across all\nscales, and PEFT methods differ significantly in their efficacy based on the\nmodel scale. LoRA usually offers the most favorable trade-off between cost and\nperformance. Further investigation into the effects of these methods on both\nmodel robustness and code security reveals that larger models tend to\ndemonstrate reduced robustness and less security. At last, we explore the\nrelationships among updated parameters, cross-entropy loss, and task\nperformance. We find that the tuning effectiveness observed in small models\ngeneralizes well to larger models, and the validation loss in instruction\ntuning can be a reliable indicator of overall downstream performance.\n","authors":["Terry Yue Zhuo","Armel Zebaze","Nitchakarn Suppattarachai","Leandro von Werra","Harm de Vries","Qian Liu","Niklas Muennighoff"],"pdf_url":"https://arxiv.org/pdf/2401.00788v1.pdf","comment":"25 pages (12 main), 19 figures, 8 tables"},{"id":"http://arxiv.org/abs/2401.00779v1","updated":"2024-01-01T14:58:53Z","published":"2024-01-01T14:58:53Z","title":"Temporal Validity Change Prediction","summary":"  Temporal validity is an important property of text that is useful for many\ndownstream applications, such as recommender systems, conversational AI, or\nstory understanding. Existing benchmarking tasks often require models to\nidentify the temporal validity duration of a single statement. However, in many\ncases, additional contextual information, such as sentences in a story or posts\non a social media profile, can be collected from the available text stream.\nThis contextual information may greatly alter the duration for which a\nstatement is expected to be valid. We propose Temporal Validity Change\nPrediction, a natural language processing task benchmarking the capability of\nmachine learning models to detect contextual statements that induce such\nchange. We create a dataset consisting of temporal target statements sourced\nfrom Twitter and crowdsource sample context statements. We then benchmark a set\nof transformer-based language models on our dataset. Finally, we experiment\nwith temporal validity duration prediction as an auxiliary task to improve the\nperformance of the state-of-the-art model.\n","authors":["Georg Wenzel","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2401.00779v1.pdf","comment":"9 pages, 9 figures, 3 tables"},{"id":"http://arxiv.org/abs/2401.00763v1","updated":"2024-01-01T14:06:55Z","published":"2024-01-01T14:06:55Z","title":"New Job, New Gender? Measuring the Social Bias in Image Generation\n  Models","summary":"  Image generation models can generate or edit images from a given text. Recent\nadvancements in image generation technology, exemplified by DALL-E and\nMidjourney, have been groundbreaking. These advanced models, despite their\nimpressive capabilities, are often trained on massive Internet datasets, making\nthem susceptible to generating content that perpetuates social stereotypes and\nbiases, which can lead to severe consequences. Prior research on assessing bias\nwithin image generation models suffers from several shortcomings, including\nlimited accuracy, reliance on extensive human labor, and lack of comprehensive\nanalysis. In this paper, we propose BiasPainter, a novel metamorphic testing\nframework that can accurately, automatically and comprehensively trigger social\nbias in image generation models. BiasPainter uses a diverse range of seed\nimages of individuals and prompts the image generation models to edit these\nimages using gender, race, and age-neutral queries. These queries span 62\nprofessions, 39 activities, 57 types of objects, and 70 personality traits. The\nframework then compares the edited images to the original seed images, focusing\non any changes related to gender, race, and age. BiasPainter adopts a testing\noracle that these characteristics should not be modified when subjected to\nneutral prompts. Built upon this design, BiasPainter can trigger the social\nbias and evaluate the fairness of image generation models. To evaluate the\neffectiveness of BiasPainter, we use BiasPainter to test five widely-used\ncommercial image generation software and models, such as stable diffusion and\nMidjourney. Experimental results show that 100\\% of the generated test cases\ncan successfully trigger social bias in image generation models.\n","authors":["Wenxuan Wang","Haonan Bai","Jen-tse Huang","Yuxuan Wan","Youliang Yuan","Haoyi Qiu","Nanyun Peng","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2401.00763v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00761v1","updated":"2024-01-01T14:02:27Z","published":"2024-01-01T14:02:27Z","title":"The Earth is Flat? Unveiling Factual Errors in Large Language Models","summary":"  Large Language Models (LLMs) like ChatGPT are foundational in various\napplications due to their extensive knowledge from pre-training and\nfine-tuning. Despite this, they are prone to generating factual and commonsense\nerrors, raising concerns in critical areas like healthcare, journalism, and\neducation to mislead users. Current methods for evaluating LLMs' veracity are\nlimited by test data leakage or the need for extensive human labor, hindering\nefficient and accurate error detection. To tackle this problem, we introduce a\nnovel, automatic testing framework, FactChecker, aimed at uncovering factual\ninaccuracies in LLMs. This framework involves three main steps: First, it\nconstructs a factual knowledge graph by retrieving fact triplets from a\nlarge-scale knowledge database. Then, leveraging the knowledge graph,\nFactChecker employs a rule-based approach to generates three types of questions\n(Yes-No, Multiple-Choice, and WH questions) that involve single-hop and\nmulti-hop relations, along with correct answers. Lastly, it assesses the LLMs'\nresponses for accuracy using tailored matching strategies for each question\ntype. Our extensive tests on six prominent LLMs, including text-davinci-002,\ntext-davinci-003, ChatGPT~(gpt-3.5-turbo, gpt-4), Vicuna, and LLaMA-2, reveal\nthat FactChecker can trigger factual errors in up to 45\\% of questions in these\nmodels. Moreover, we demonstrate that FactChecker's test cases can improve\nLLMs' factual accuracy through in-context learning and fine-tuning (e.g.,\nllama-2-13b-chat's accuracy increase from 35.3\\% to 68.5\\%). We are making all\ncode, data, and results available for future research endeavors.\n","authors":["Wenxuan Wang","Juluan Shi","Zhaopeng Tu","Youliang Yuan","Jen-tse Huang","Wenxiang Jiao","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2401.00761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00757v1","updated":"2024-01-01T13:53:53Z","published":"2024-01-01T13:53:53Z","title":"A & B == B & A: Triggering Logical Reasoning Failures in Large Language\n  Models","summary":"  Recent advancements in large language models (LLMs) have propelled Artificial\nIntelligence (AI) to new heights, enabling breakthroughs in various tasks such\nas writing assistance, code generation, and machine translation. A significant\ndistinction of advanced LLMs, such as ChatGPT, is their demonstrated ability to\n\"reason.\" However, evaluating the reasoning ability of LLMs remains a challenge\nas most existing evaluations focus on their accuracy on the downstream tasks\nrather than directly assessing their reasoning processes. Efforts have been\nmade to develop benchmarks and metrics to assess reasoning in LLMs, but they\nsuffer from data leakage or limited scope. In this paper, we introduce\nLogicAsker, an automatic approach that comprehensively evaluates and improves\nthe logical reasoning abilities of LLMs under a set of atomic reasoning skills\nbased on propositional and predicate logic. The results provide insights into\nLLMs' reasoning abilities and reveal the logical rules the LLMs did not learn\nwell. We evaluate LogicAsker on six widely deployed LLMs, including GPT-3,\nChatGPT, GPT-4, Bard, Vicuna, and Guanaco. The results show that test cases\nfrom LogicAsker can find logical reasoning failures in different LLMs with a\nrate of 25\\% - 94\\%. In addition, the test cases of LogicAsker can be further\nused to design demonstration examples for in-context learning, which\neffectively improves the logical reasoning ability of LLMs, e.g., 10\\% for\nGPT-4. As far as we know, our work is the first to create prompts based on\ntesting results to improve LLMs' formal reasoning ability effectively. All the\ncode, data, and results will be released for reproduction and future research.\n","authors":["Yuxuan Wan","Wenxuan Wang","Yiliu Yang","Youliang Yuan","Jen-tse Huang","Pinjia He","Wenxiang Jiao","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2401.00757v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00751v1","updated":"2024-01-01T13:28:46Z","published":"2024-01-01T13:28:46Z","title":"Machine Translation Testing via Syntactic Tree Pruning","summary":"  Machine translation systems have been widely adopted in our daily life,\nmaking life easier and more convenient. Unfortunately, erroneous translations\nmay result in severe consequences, such as financial losses. This requires to\nimprove the accuracy and the reliability of machine translation systems.\nHowever, it is challenging to test machine translation systems because of the\ncomplexity and intractability of the underlying neural models. To tackle these\nchallenges, we propose a novel metamorphic testing approach by syntactic tree\npruning (STP) to validate machine translation systems. Our key insight is that\na pruned sentence should have similar crucial semantics compared with the\noriginal sentence. Specifically, STP (1) proposes a core semantics-preserving\npruning strategy by basic sentence structure and dependency relations on the\nlevel of syntactic tree representation; (2) generates source sentence pairs\nbased on the metamorphic relation; (3) reports suspicious issues whose\ntranslations break the consistency property by a bag-of-words model. We further\nevaluate STP on two state-of-the-art machine translation systems (i.e., Google\nTranslate and Bing Microsoft Translator) with 1,200 source sentences as inputs.\nThe results show that STP can accurately find 5,073 unique erroneous\ntranslations in Google Translate and 5,100 unique erroneous translations in\nBing Microsoft Translator (400% more than state-of-the-art techniques), with\n64.5% and 65.4% precision, respectively. The reported erroneous translations\nvary in types and more than 90% of them cannot be found by state-of-the-art\ntechniques. There are 9,393 erroneous translations unique to STP, which is\n711.9% more than state-of-the-art techniques. Moreover, STP is quite effective\nto detect translation errors for the original sentences with a recall reaching\n74.0%, improving state-of-the-art techniques by 55.1% on average.\n","authors":["Quanjun Zhang","Juan Zhai","Chunrong Fang","Jiawei Liu","Weisong Sun","Haichuan Hu","Qingyu Wang"],"pdf_url":"https://arxiv.org/pdf/2401.00751v1.pdf","comment":"Accepted to ACM Transactions on Software Engineering and Methodology\n  2024 (TOSEM'24)"},{"id":"http://arxiv.org/abs/2401.00741v1","updated":"2024-01-01T12:49:36Z","published":"2024-01-01T12:49:36Z","title":"ToolEyes: Fine-Grained Evaluation for Tool Learning Capabilities of\n  Large Language Models in Real-world Scenarios","summary":"  Existing evaluations of tool learning primarily focus on validating the\nalignment of selected tools for large language models (LLMs) with expected\noutcomes. However, these approaches rely on a limited set of scenarios where\nanswers can be pre-determined, diverging from genuine needs. Furthermore, a\nsole emphasis on outcomes disregards the intricate capabilities essential for\nLLMs to effectively utilize tools. To tackle this issue, we propose ToolEyes, a\nfine-grained system tailored for the evaluation of the LLMs' tool learning\ncapabilities in authentic scenarios. The system meticulously examines seven\nreal-world scenarios, analyzing five dimensions crucial to LLMs in tool\nlearning: format alignment, intent comprehension, behavior planning, tool\nselection, and answer organization. Additionally, ToolEyes incorporates a tool\nlibrary boasting approximately 600 tools, serving as an intermediary between\nLLMs and the physical world. Evaluations involving ten LLMs across three\ncategories reveal a preference for specific scenarios and limited cognitive\nabilities in tool learning. Intriguingly, expanding the model size even\nexacerbates the hindrance to tool learning. These findings offer instructive\ninsights aimed at advancing the field of tool learning. The data is available\natt https://github.com/Junjie-Ye/ToolEyes.git.\n","authors":["Junjie Ye","Guanyu Li","Songyang Gao","Caishuang Huang","Yilong Wu","Sixian Li","Xiaoran Fan","Shihan Dou","Qi Zhang","Tao Gui","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2401.00741v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14557v2","updated":"2024-01-01T09:24:47Z","published":"2023-12-22T09:30:41Z","title":"Aurora:Activating Chinese chat capability for Mixtral-8x7B sparse\n  Mixture-of-Experts through Instruction-Tuning","summary":"  Existing research has demonstrated that refining large language models (LLMs)\nthrough the utilization of machine-generated instruction-following data\nempowers these models to exhibit impressive zero-shot capabilities for novel\ntasks, without requiring human-authored instructions. In this paper, we\nsystematically investigate, preprocess, and integrate three Chinese\ninstruction-following datasets with the aim of enhancing the Chinese\nconversational capabilities of Mixtral-8x7B sparse Mixture-of-Experts model.\nThrough instruction fine-tuning on this carefully processed dataset, we\nsuccessfully construct the Mixtral-8x7B sparse Mixture-of-Experts model named\n\"Aurora.\" To assess the performance of Aurora, we utilize three widely\nrecognized benchmark tests: C-Eval, MMLU, and CMMLU. Empirical studies validate\nthe effectiveness of instruction fine-tuning applied to Mixtral-8x7B sparse\nMixture-of-Experts model. This work is pioneering in the execution of\ninstruction fine-tuning on a sparse expert-mixed model, marking a significant\nbreakthrough in enhancing the capabilities of this model architecture. Our\ncode, data and model are publicly available at\n  https://github.com/WangRongsheng/Aurora\n","authors":["Rongsheng Wang","Haoming Chen","Ruizhe Zhou","Yaofei Duan","Kunyan Cai","Han Ma","Jiaxi Cui","Jian Li","Patrick Cheong-Iao Pang","Yapeng Wang","Tao Tan"],"pdf_url":"https://arxiv.org/pdf/2312.14557v2.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2312.17660v2","updated":"2024-01-01T08:59:40Z","published":"2023-12-29T15:56:24Z","title":"Normalization of Lithuanian Text Using Regular Expressions","summary":"  Text Normalization is an integral part of any text-to-speech synthesis\nsystem. In a natural language text, there are elements such as numbers, dates,\nabbreviations, etc. that belong to other semiotic classes. They are called\nnon-standard words (NSW) and need to be expanded into ordinary words. For this\npurpose, it is necessary to identify the semiotic class of each NSW. The\ntaxonomy of semiotic classes adapted to the Lithuanian language is presented in\nthe work. Sets of rules are created for detecting and expanding NSWs based on\nregular expressions. Experiments with three completely different data sets were\nperformed and the accuracy was assessed. Causes of errors are explained and\nrecommendations are given for the development of text normalization rules.\n","authors":["Pijus Kasparaitis"],"pdf_url":"https://arxiv.org/pdf/2312.17660v2.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2401.00698v1","updated":"2024-01-01T08:32:50Z","published":"2024-01-01T08:32:50Z","title":"Large Language Models aren't all that you need","summary":"  This paper describes the architecture and systems built towards solving the\nSemEval 2023 Task 2: MultiCoNER II (Multilingual Complex Named Entity\nRecognition) [1]. We evaluate two approaches (a) a traditional Conditional\nRandom Fields model and (b) a Large Language Model (LLM) fine-tuned with a\ncustomized head and compare the two approaches. The novel ideas explored are:\n1) Decaying auxiliary loss (with residual) - where we train the model on an\nauxiliary task of Coarse-Grained NER and include this task as a part of the\nloss function 2) Triplet token blending - where we explore ways of blending the\nembeddings of neighboring tokens in the final NER layer prior to prediction 3)\nTask-optimal heads - where we explore a variety of custom heads and learning\nrates for the final layer of the LLM. We also explore multiple LLMs including\nGPT-3 and experiment with a variety of dropout and other hyperparameter\nsettings before arriving at our final model which achieves micro & macro f1 of\n0.85/0.84 (on dev) and 0.67/0.61 on the test data . We show that while\npre-trained LLMs, by themselves, bring about a large improvement in scores as\ncompared to traditional models, we also demonstrate that tangible improvements\nto the Macro-F1 score can be made by augmenting the LLM with additional\nfeature/loss/model engineering techniques described above.\n","authors":["Kiran Voderhobli Holla","Chaithanya Kumar","Aryan Singh"],"pdf_url":"https://arxiv.org/pdf/2401.00698v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00690v1","updated":"2024-01-01T07:35:31Z","published":"2024-01-01T07:35:31Z","title":"Benchmarking Large Language Models on Controllable Generation under\n  Diversified Instructions","summary":"  While large language models (LLMs) have exhibited impressive\ninstruction-following capabilities, it is still unclear whether and to what\nextent they can respond to explicit constraints that might be entailed in\nvarious instructions. As a significant aspect of LLM alignment, it is thus\nimportant to formulate such a specialized set of instructions as well as\ninvestigate the resulting behavior of LLMs. To address this vacancy, we propose\na new benchmark CoDI-Eval to systematically and comprehensively evaluate LLMs'\nresponses to instructions with various constraints. We construct a large\ncollection of constraints-attributed instructions as a test suite focused on\nboth generalization and coverage. Specifically, we advocate an instruction\ndiversification process to synthesize diverse forms of constraint expression\nand also deliberate the candidate task taxonomy with even finer-grained\nsub-categories. Finally, we automate the entire evaluation process to\nfacilitate further developments. Different from existing studies on\ncontrollable text generation, CoDI-Eval extends the scope to the prevalent\ninstruction-following paradigm for the first time. We provide extensive\nevaluations of representative LLMs (e.g., ChatGPT, Vicuna) on CoDI-Eval,\nrevealing their limitations in following instructions with specific constraints\nand there is still a significant gap between open-source and commercial\nclosed-source LLMs. We believe this benchmark will facilitate research into\nimproving the controllability of LLMs' responses to instructions. Our data and\ncode are available at https://github.com/Xt-cyh/CoDI-Eval.\n","authors":["Yihan Chen","Benfeng Xu","Quan Wang","Yi Liu","Zhendong Mao"],"pdf_url":"https://arxiv.org/pdf/2401.00690v1.pdf","comment":"Accepted to AAAI 2024"},{"id":"http://arxiv.org/abs/2401.00689v1","updated":"2024-01-01T07:35:29Z","published":"2024-01-01T07:35:29Z","title":"Large language model for Bible sentiment analysis: Sermon on the Mount","summary":"  The revolution of natural language processing via large language models has\nmotivated its use in multidisciplinary areas that include social sciences and\nhumanities and more specifically, comparative religion. Sentiment analysis\nprovides a mechanism to study the emotions expressed in text. Recently,\nsentiment analysis has been used to study and compare translations of the\nBhagavad Gita, which is a fundamental and sacred Hindu text. In this study, we\nuse sentiment analysis for studying selected chapters of the Bible. These\nchapters are known as the Sermon on the Mount. We utilize a pre-trained\nlanguage model for sentiment analysis by reviewing five translations of the\nSermon on the Mount, which include the King James version, the New\nInternational Version, the New Revised Standard Version, the Lamsa Version, and\nthe Basic English Version. We provide a chapter-by-chapter and verse-by-verse\ncomparison using sentiment and semantic analysis and review the major\nsentiments expressed. Our results highlight the varying sentiments across the\nchapters and verses. We found that the vocabulary of the respective\ntranslations is significantly different. We detected different levels of\nhumour, optimism, and empathy in the respective chapters that were used by\nJesus to deliver his message.\n","authors":["Mahek Vora","Tom Blau","Vansh Kachhwal","Ashu M. G. Solo","Rohitash Chandra"],"pdf_url":"https://arxiv.org/pdf/2401.00689v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00676v1","updated":"2024-01-01T06:04:52Z","published":"2024-01-01T06:04:52Z","title":"Digger: Detecting Copyright Content Mis-usage in Large Language Model\n  Training","summary":"  Pre-training, which utilizes extensive and varied datasets, is a critical\nfactor in the success of Large Language Models (LLMs) across numerous\napplications. However, the detailed makeup of these datasets is often not\ndisclosed, leading to concerns about data security and potential misuse. This\nis particularly relevant when copyrighted material, still under legal\nprotection, is used inappropriately, either intentionally or unintentionally,\ninfringing on the rights of the authors.\n  In this paper, we introduce a detailed framework designed to detect and\nassess the presence of content from potentially copyrighted books within the\ntraining datasets of LLMs. This framework also provides a confidence estimation\nfor the likelihood of each content sample's inclusion. To validate our\napproach, we conduct a series of simulated experiments, the results of which\naffirm the framework's effectiveness in identifying and addressing instances of\ncontent misuse in LLM training processes. Furthermore, we investigate the\npresence of recognizable quotes from famous literary works within these\ndatasets. The outcomes of our study have significant implications for ensuring\nthe ethical use of copyrighted materials in the development of LLMs,\nhighlighting the need for more transparent and responsible data management\npractices in this field.\n","authors":["Haodong Li","Gelei Deng","Yi Liu","Kailong Wang","Yuekang Li","Tianwei Zhang","Yang Liu","Guoai Xu","Guosheng Xu","Haoyu Wang"],"pdf_url":"https://arxiv.org/pdf/2401.00676v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00642v1","updated":"2024-01-01T03:04:14Z","published":"2024-01-01T03:04:14Z","title":"Predicting Anti-microbial Resistance using Large Language Models","summary":"  During times of increasing antibiotic resistance and the spread of infectious\ndiseases like COVID-19, it is important to classify genes related to antibiotic\nresistance. As natural language processing has advanced with transformer-based\nlanguage models, many language models that learn characteristics of nucleotide\nsequences have also emerged. These models show good performance in classifying\nvarious features of nucleotide sequences. When classifying nucleotide\nsequences, not only the sequence itself, but also various background knowledge\nis utilized. In this study, we use not only a nucleotide sequence-based\nlanguage model but also a text language model based on PubMed articles to\nreflect more biological background knowledge in the model. We propose a method\nto fine-tune the nucleotide sequence language model and the text language model\nbased on various databases of antibiotic resistance genes. We also propose an\nLLM-based augmentation technique to supplement the data and an ensemble method\nto effectively combine the two models. We also propose a benchmark for\nevaluating the model. Our method achieved better performance than the\nnucleotide sequence language model in the drug resistance class prediction.\n","authors":["Hyunwoo Yoo","Bahrad Sokhansanj","James R. Brown","Gail Rosen"],"pdf_url":"https://arxiv.org/pdf/2401.00642v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02478v2","updated":"2024-01-01T02:10:43Z","published":"2023-03-10T14:36:47Z","title":"Exploring AI-Generated Text in Student Writing: How Does AI Help?","summary":"  English as foreign language_EFL_students' use of text generated from\nartificial intelligence_AI_natural language generation_NLG_tools may improve\ntheir writing quality. However, it remains unclear to what extent AI-generated\ntext in these students' writing might lead to higher-quality writing. We\nexplored 23 Hong Kong secondary school students' attempts to write stories\ncomprising their own words and AI-generated text. Human experts scored the\nstories for dimensions of content, language and organization. We analyzed the\nbasic organization and structure and syntactic complexity of the stories'\nAI-generated text and performed multiple linear regression and cluster\nanalyses. The results show the number of human words and the number of\nAI-generated words contribute significantly to scores. Besides, students can be\ngrouped into competent and less competent writers who use more AI-generated\ntext or less AI-generated text compared to their peers. Comparisons of clusters\nreveal some benefit of AI-generated text in improving the quality of both\nhigh-scoring students' and low-scoring students' writing. The findings can\ninform pedagogical strategies to use AI-generated text for EFL students'\nwriting and to address digital divides. This study contributes designs of NLG\ntools and writing activities to implement AI-generated text in schools.\n","authors":["David James Woo","Hengky Susanto","Chi Ho Yeung","Kai Guo","April Ka Yeng Fung"],"pdf_url":"https://arxiv.org/pdf/2304.02478v2.pdf","comment":"45 pages, 11 figures, 3 tables"},{"id":"http://arxiv.org/abs/2208.09709v2","updated":"2024-01-01T01:35:08Z","published":"2022-08-20T15:21:35Z","title":"BSpell: A CNN-Blended BERT Based Bangla Spell Checker","summary":"  Bangla typing is mostly performed using English keyboard and can be highly\nerroneous due to the presence of compound and similarly pronounced letters.\nSpelling correction of a misspelled word requires understanding of word typing\npattern as well as the context of the word usage. A specialized BERT model\nnamed BSpell has been proposed in this paper targeted towards word for word\ncorrection in sentence level. BSpell contains an end-to-end trainable CNN\nsub-model named SemanticNet along with specialized auxiliary loss. This allows\nBSpell to specialize in highly inflected Bangla vocabulary in the presence of\nspelling errors. Furthermore, a hybrid pretraining scheme has been proposed for\nBSpell that combines word level and character level masking. Comparison on two\nBangla and one Hindi spelling correction dataset shows the superiority of our\nproposed approach. BSpell is available as a Bangla spell checking tool via\nGitHub: https://github.com/Hasiburshanto/Bangla-Spell-Checker\n","authors":["Chowdhury Rafeed Rahman","MD. Hasibur Rahman","Samiha Zakir","Mohammad Rafsan","Mohammed Eunus Ali"],"pdf_url":"https://arxiv.org/pdf/2208.09709v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.16033v2","updated":"2024-01-01T23:50:31Z","published":"2023-10-24T17:48:04Z","title":"ViCrop: Perceiving Small Visual Details in Zero-shot Visual Question\n  Answering with Multimodal Large Language Models","summary":"  Multimodal Large Language Models (MLLMs) have recently achieved promising\nzero-shot accuracy on visual question answering (VQA) -- a fundamental task\naffecting various downstream applications and domains. Given the great\npotential for the broad use of these models, it is important to investigate\ntheir limitations in dealing with different image and question properties. In\nthis work, we investigate whether MLLMs can perceive details as well as larger\ncomponents in images. In particular, we show that their zero-shot accuracy in\nanswering visual questions is very sensitive to the size of the visual subject\nrelated to the question, declining up to $45.91\\%$ with size. Furthermore, we\nshow that this effect is causal by observing that human visual cropping can\nsignificantly mitigate their sensitivity to size. To scale up the usefulness of\nhuman cropping, we propose ViCrop, a general framework that utilizes automatic\nvisual cropping to enhance zero-shot VQA of MLLMs. We construct five variants\nof ViCrop leveraging either external localization models or the decision\nprocess of the given MLLM itself. Our results show that ViCrop improves MLLMs'\nzero-shot accuracy across different VQA datasets, for example, enhances\nBLIP2-T5's performance by $32.23\\%$ on the TextVQA test set. To facilitate\nfurther investigation of MLLMs' behaviors, our code is publicly released.\n","authors":["Jiarui Zhang","Mahyar Khayatkhoei","Prateek Chhikara","Filip Ilievski"],"pdf_url":"https://arxiv.org/pdf/2310.16033v2.pdf","comment":"18 pages, 11 figures, 6 tables"},{"id":"http://arxiv.org/abs/2401.02938v1","updated":"2024-01-01T23:10:23Z","published":"2024-01-01T23:10:23Z","title":"Fast and Optimal Weight Update for Pruned Large Language Models","summary":"  Pruning large language models (LLMs) is a challenging task due to their\nenormous size. The primary difficulty is fine-tuning the model after pruning,\nwhich is needed to recover the lost performance caused by dropping weights.\nRecent approaches have either ignored fine-tuning entirely, focusing on\nefficient pruning criteria, or attempted layer-wise weight updates, preserving\nthe behavior of each layer. However, even layer-wise weight updates can be\ncostly for LLMs, and previous works have resorted to various approximations.\n  In our paper, we propose a fast and optimal weight update algorithm for\npruned layers based on the Alternating Direction Method of Multipliers (ADMM).\nCoupled with a simple iterative pruning mask selection, our algorithm achieves\nstate-of-the-art pruning performance across a wide range of LLMs. Code is\navailable at https://github.com/fmfi-compbio/admm-pruning.\n","authors":["Vladimír Boža"],"pdf_url":"https://arxiv.org/pdf/2401.02938v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2401.00971v1","updated":"2024-01-01T23:01:40Z","published":"2024-01-01T23:01:40Z","title":"Efficient Multi-domain Text Recognition Deep Neural Network\n  Parameterization with Residual Adapters","summary":"  Recent advancements in deep neural networks have markedly enhanced the\nperformance of computer vision tasks, yet the specialized nature of these\nnetworks often necessitates extensive data and high computational power.\nAddressing these requirements, this study presents a novel neural network model\nadept at optical character recognition (OCR) across diverse domains, leveraging\nthe strengths of multi-task learning to improve efficiency and generalization.\nThe model is designed to achieve rapid adaptation to new domains, maintain a\ncompact size conducive to reduced computational resource demand, ensure high\naccuracy, retain knowledge from previous learning experiences, and allow for\ndomain-specific performance improvements without the need to retrain entirely.\nRigorous evaluation on open datasets has validated the model's ability to\nsignificantly lower the number of trainable parameters without sacrificing\nperformance, indicating its potential as a scalable and adaptable solution in\nthe field of computer vision, particularly for applications in optical text\nrecognition.\n","authors":["Jiayou Chao","Wei Zhu"],"pdf_url":"https://arxiv.org/pdf/2401.00971v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00964v1","updated":"2024-01-01T22:27:59Z","published":"2024-01-01T22:27:59Z","title":"Data Augmentation Techniques for Cross-Domain WiFi CSI-based Human\n  Activity Recognition","summary":"  The recognition of human activities based on WiFi Channel State Information\n(CSI) enables contactless and visual privacy-preserving sensing in indoor\nenvironments. However, poor model generalization, due to varying environmental\nconditions and sensing hardware, is a well-known problem in this space. To\naddress this issue, in this work, data augmentation techniques commonly used in\nimage-based learning are applied to WiFi CSI to investigate their effects on\nmodel generalization performance in cross-scenario and cross-system settings.\nIn particular, we focus on the generalization between line-of-sight (LOS) and\nnon-line-of-sight (NLOS) through-wall scenarios, as well as on the\ngeneralization between different antenna systems, which remains under-explored.\nWe collect and make publicly available a dataset of CSI amplitude spectrograms\nof human activities. Utilizing this data, an ablation study is conducted in\nwhich activity recognition models based on the EfficientNetV2 architecture are\ntrained, allowing us to assess the effects of each augmentation on model\ngeneralization performance. The gathered results show that specific\ncombinations of simple data augmentation techniques applied to CSI amplitude\ndata can significantly improve cross-scenario and cross-system generalization.\n","authors":["Julian Strohmayer","Martin Kampel"],"pdf_url":"https://arxiv.org/pdf/2401.00964v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00935v1","updated":"2024-01-01T19:00:55Z","published":"2024-01-01T19:00:55Z","title":"Boundary Attention: Learning to Find Faint Boundaries at Any Resolution","summary":"  We present a differentiable model that explicitly models boundaries --\nincluding contours, corners and junctions -- using a new mechanism that we call\nboundary attention. We show that our model provides accurate results even when\nthe boundary signal is very weak or is swamped by noise. Compared to previous\nclassical methods for finding faint boundaries, our model has the advantages of\nbeing differentiable; being scalable to larger images; and automatically\nadapting to an appropriate level of geometric detail in each part of an image.\nCompared to previous deep methods for finding boundaries via end-to-end\ntraining, it has the advantages of providing sub-pixel precision, being more\nresilient to noise, and being able to process any image at its native\nresolution and aspect ratio.\n","authors":["Mia Gaia Polansky","Charles Herrmann","Junhwa Hur","Deqing Sun","Dor Verbin","Todd Zickler"],"pdf_url":"https://arxiv.org/pdf/2401.00935v1.pdf","comment":"Project website at boundaryattention.github.io:\n  http://boundaryattention.github.io"},{"id":"http://arxiv.org/abs/2401.00850v1","updated":"2024-01-01T18:59:33Z","published":"2024-01-01T18:59:33Z","title":"Refining Pre-Trained Motion Models","summary":"  Given the difficulty of manually annotating motion in video, the current best\nmotion estimation methods are trained with synthetic data, and therefore\nstruggle somewhat due to a train/test gap. Self-supervised methods hold the\npromise of training directly on real video, but typically perform worse. These\ninclude methods trained with warp error (i.e., color constancy) combined with\nsmoothness terms, and methods that encourage cycle-consistency in the estimates\n(i.e., tracking backwards should yield the opposite trajectory as tracking\nforwards). In this work, we take on the challenge of improving state-of-the-art\nsupervised models with self-supervised training. We find that when the\ninitialization is supervised weights, most existing self-supervision techniques\nactually make performance worse instead of better, which suggests that the\nbenefit of seeing the new data is overshadowed by the noise in the training\nsignal. Focusing on obtaining a ``clean'' training signal from real-world\nunlabelled video, we propose to separate label-making and training into two\ndistinct stages. In the first stage, we use the pre-trained model to estimate\nmotion in a video, and then select the subset of motion estimates which we can\nverify with cycle-consistency. This produces a sparse but accurate\npseudo-labelling of the video. In the second stage, we fine-tune the model to\nreproduce these outputs, while also applying augmentations on the input. We\ncomplement this boot-strapping method with simple techniques that densify and\nre-balance the pseudo-labels, ensuring that we do not merely train on ``easy''\ntracks. We show that our method yields reliable gains over fully-supervised\nmethods in real videos, for both short-term (flow-based) and long-range\n(multi-frame) pixel tracking.\n","authors":["Xinglong Sun","Adam W. Harley","Leonidas J. Guibas"],"pdf_url":"https://arxiv.org/pdf/2401.00850v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00849v1","updated":"2024-01-01T18:58:42Z","published":"2024-01-01T18:58:42Z","title":"COSMO: COntrastive Streamlined MultimOdal Model with Interleaved\n  Pre-Training","summary":"  In the evolution of Vision-Language Pre-training, shifting from short-text\ncomprehension to encompassing extended textual contexts is pivotal. Recent\nautoregressive vision-language models like \\cite{flamingo, palme}, leveraging\nthe long-context capability of Large Language Models, have excelled in few-shot\ntext generation tasks but face challenges in alignment tasks. Addressing this\ngap, we introduce the contrastive loss into text generation models, presenting\nthe COntrastive-Streamlined MultimOdal framework (\\ModelName), strategically\npartitioning the language model into dedicated unimodal text processing and\nadept multimodal data handling components. \\ModelName, our unified framework,\nmerges unimodal and multimodal elements, enhancing model performance for tasks\ninvolving textual and visual data while notably reducing learnable parameters.\nHowever, these models demand extensive long-text datasets, yet the availability\nof high-quality long-text video datasets remains limited. To bridge this gap,\nthis work introduces \\VideoDatasetName, an inaugural interleaved video-text\ndataset featuring comprehensive captions, marking a significant step forward.\nDemonstrating its impact, we illustrate how \\VideoDatasetName{} enhances model\nperformance in image-text tasks. With 34% learnable parameters and utilizing\n72\\% of the available data, our model demonstrates significant superiority over\nOpenFlamingo~\\cite{openflamingo}. For instance, in the 4-shot flickr captioning\ntask, performance notably improves from 57.2% to 65.\\%. The contributions of\n\\ModelName{} and \\VideoDatasetName{} are underscored by notable performance\ngains across 14 diverse downstream datasets encompassing both image-text and\nvideo-text tasks.\n","authors":["Alex Jinpeng Wang","Linjie Li","Kevin Qinghong Lin","Jianfeng Wang","Kevin Lin","Zhengyuan Yang","Lijuan Wang","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2401.00849v1.pdf","comment":"16 pages; Website: http://fingerrec.github.io/cosmo"},{"id":"http://arxiv.org/abs/2401.00847v1","updated":"2024-01-01T18:56:54Z","published":"2024-01-01T18:56:54Z","title":"Mocap Everyone Everywhere: Lightweight Motion Capture With Smartwatches\n  and a Head-Mounted Camera","summary":"  We present a lightweight and affordable motion capture method based on two\nsmartwatches and a head-mounted camera. In contrast to the existing approaches\nthat use six or more expert-level IMU devices, our approach is much more\ncost-effective and convenient. Our method can make wearable motion capture\naccessible to everyone everywhere, enabling 3D full-body motion capture in\ndiverse environments. As a key idea to overcome the extreme sparsity and\nambiguities of sensor inputs, we integrate 6D head poses obtained from the\nhead-mounted cameras for motion estimation. To enable capture in expansive\nindoor and outdoor scenes, we propose an algorithm to track and update floor\nlevel changes to define head poses, coupled with a multi-stage\nTransformer-based regression module. We also introduce novel strategies\nleveraging visual cues of egocentric images to further enhance the motion\ncapture quality while reducing ambiguities. We demonstrate the performance of\nour method on various challenging scenarios, including complex outdoor\nenvironments and everyday motions including object interactions and social\ninteractions among multiple individuals.\n","authors":["Jiye Lee","Hanbyul Joo"],"pdf_url":"https://arxiv.org/pdf/2401.00847v1.pdf","comment":"Project page: https://jiyewise.github.io/projects/MocapEvery/"},{"id":"http://arxiv.org/abs/2310.16542v2","updated":"2024-01-01T18:26:05Z","published":"2023-10-25T10:45:38Z","title":"ParisLuco3D: A high-quality target dataset for domain generalization of\n  LiDAR perception","summary":"  LiDAR is an essential sensor for autonomous driving by collecting precise\ngeometric information regarding a scene. As the performance of various LiDAR\nperception tasks has improved, generalizations to new environments and sensors\nhas emerged to test these optimized models in real-world conditions.\nUnfortunately, the various annotation strategies of data providers complicate\nthe computation of cross-domain performances.\n  This paper provides a novel dataset, ParisLuco3D, specifically designed for\ncross-domain evaluation to make it easier to evaluate the performance utilizing\nvarious source datasets. Alongside the dataset, online benchmarks for LiDAR\nsemantic segmentation, LiDAR object detection, and LiDAR tracking are provided\nto ensure a fair comparison across methods.\n  The ParisLuco3D dataset, evaluation scripts, and links to benchmarks can be\nfound at the following website: https://npm3d.fr/parisluco3d\n","authors":["Jules Sanchez","Louis Soum-Fontez","Jean-Emmanuel Deschaud","Francois Goulette"],"pdf_url":"https://arxiv.org/pdf/2310.16542v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00834v1","updated":"2024-01-01T18:23:51Z","published":"2024-01-01T18:23:51Z","title":"Deblurring 3D Gaussian Splatting","summary":"  Recent studies in Radiance Fields have paved the robust way for novel view\nsynthesis with their photorealistic rendering quality. Nevertheless, they\nusually employ neural networks and volumetric rendering, which are costly to\ntrain and impede their broad use in various real-time applications due to the\nlengthy rendering time. Lately 3D Gaussians splatting-based approach has been\nproposed to model the 3D scene, and it achieves remarkable visual quality while\nrendering the images in real-time. However, it suffers from severe degradation\nin the rendering quality if the training images are blurry. Blurriness commonly\noccurs due to the lens defocusing, object motion, and camera shake, and it\ninevitably intervenes in clean image acquisition. Several previous studies have\nattempted to render clean and sharp images from blurry input images using\nneural fields. The majority of those works, however, are designed only for\nvolumetric rendering-based neural radiance fields and are not straightforwardly\napplicable to rasterization-based 3D Gaussian splatting methods. Thus, we\npropose a novel real-time deblurring framework, deblurring 3D Gaussian\nSplatting, using a small Multi-Layer Perceptron (MLP) that manipulates the\ncovariance of each 3D Gaussian to model the scene blurriness. While deblurring\n3D Gaussian Splatting can still enjoy real-time rendering, it can reconstruct\nfine and sharp details from blurry images. A variety of experiments have been\nconducted on the benchmark, and the results have revealed the effectiveness of\nour approach for deblurring. Qualitative results are available at\nhttps://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/\n","authors":["Byeonghyeon Lee","Howoong Lee","Xiangyu Sun","Usman Ali","Eunbyung Park"],"pdf_url":"https://arxiv.org/pdf/2401.00834v1.pdf","comment":"19 pages, 8 figures"},{"id":"http://arxiv.org/abs/2401.00833v1","updated":"2024-01-01T18:23:39Z","published":"2024-01-01T18:23:39Z","title":"Rethinking RAFT for Efficient Optical Flow","summary":"  Despite significant progress in deep learning-based optical flow methods,\naccurately estimating large displacements and repetitive patterns remains a\nchallenge. The limitations of local features and similarity search patterns\nused in these algorithms contribute to this issue. Additionally, some existing\nmethods suffer from slow runtime and excessive graphic memory consumption. To\naddress these problems, this paper proposes a novel approach based on the RAFT\nframework. The proposed Attention-based Feature Localization (AFL) approach\nincorporates the attention mechanism to handle global feature extraction and\naddress repetitive patterns. It introduces an operator for matching pixels with\ncorresponding counterparts in the second frame and assigning accurate flow\nvalues. Furthermore, an Amorphous Lookup Operator (ALO) is proposed to enhance\nconvergence speed and improve RAFTs ability to handle large displacements by\nreducing data redundancy in its search operator and expanding the search space\nfor similarity extraction. The proposed method, Efficient RAFT\n(Ef-RAFT),achieves significant improvements of 10% on the Sintel dataset and 5%\non the KITTI dataset over RAFT. Remarkably, these enhancements are attained\nwith a modest 33% reduction in speed and a mere 13% increase in memory usage.\nThe code is available at: https://github.com/n3slami/Ef-RAFT\n","authors":["Navid Eslami","Farnoosh Arefi","Amir M. Mansourian","Shohreh Kasaei"],"pdf_url":"https://arxiv.org/pdf/2401.00833v1.pdf","comment":"7 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2401.00929v1","updated":"2024-01-01T18:20:43Z","published":"2024-01-01T18:20:43Z","title":"GenH2R: Learning Generalizable Human-to-Robot Handover via Scalable\n  Simulation, Demonstration, and Imitation","summary":"  This paper presents GenH2R, a framework for learning generalizable\nvision-based human-to-robot (H2R) handover skills. The goal is to equip robots\nwith the ability to reliably receive objects with unseen geometry handed over\nby humans in various complex trajectories. We acquire such generalizability by\nlearning H2R handover at scale with a comprehensive solution including\nprocedural simulation assets creation, automated demonstration generation, and\neffective imitation learning. We leverage large-scale 3D model repositories,\ndexterous grasp generation methods, and curve-based 3D animation to create an\nH2R handover simulation environment named \\simabbns, surpassing the number of\nscenes in existing simulators by three orders of magnitude. We further\nintroduce a distillation-friendly demonstration generation method that\nautomatically generates a million high-quality demonstrations suitable for\nlearning. Finally, we present a 4D imitation learning method augmented by a\nfuture forecasting objective to distill demonstrations into a visuo-motor\nhandover policy. Experimental evaluations in both simulators and the real world\ndemonstrate significant improvements (at least +10\\% success rate) over\nbaselines in all cases. The project page is https://GenH2R.github.io/.\n","authors":["Zifan Wang","Junyu Chen","Ziqing Chen","Pengwei Xie","Rui Chen","Li Yi"],"pdf_url":"https://arxiv.org/pdf/2401.00929v1.pdf","comment":"The project page is https://GenH2R.github.io/"},{"id":"http://arxiv.org/abs/2401.00825v1","updated":"2024-01-01T17:48:38Z","published":"2024-01-01T17:48:38Z","title":"Sharp-NeRF: Grid-based Fast Deblurring Neural Radiance Fields Using\n  Sharpness Prior","summary":"  Neural Radiance Fields (NeRF) have shown remarkable performance in neural\nrendering-based novel view synthesis. However, NeRF suffers from severe visual\nquality degradation when the input images have been captured under imperfect\nconditions, such as poor illumination, defocus blurring, and lens aberrations.\nEspecially, defocus blur is quite common in the images when they are normally\ncaptured using cameras. Although few recent studies have proposed to render\nsharp images of considerably high-quality, yet they still face many key\nchallenges. In particular, those methods have employed a Multi-Layer Perceptron\n(MLP) based NeRF, which requires tremendous computational time. To overcome\nthese shortcomings, this paper proposes a novel technique Sharp-NeRF -- a\ngrid-based NeRF that renders clean and sharp images from the input blurry\nimages within half an hour of training. To do so, we used several grid-based\nkernels to accurately model the sharpness/blurriness of the scene. The\nsharpness level of the pixels is computed to learn the spatially varying blur\nkernels. We have conducted experiments on the benchmarks consisting of blurry\nimages and have evaluated full-reference and non-reference metrics. The\nqualitative and quantitative results have revealed that our approach renders\nthe sharp novel views with vivid colors and fine details, and it has\nconsiderably faster training time than the previous works. Our project page is\navailable at https://benhenryl.github.io/SharpNeRF/\n","authors":["Byeonghyeon Lee","Howoong Lee","Usman Ali","Eunbyung Park"],"pdf_url":"https://arxiv.org/pdf/2401.00825v1.pdf","comment":"Accepted to WACV 2024"},{"id":"http://arxiv.org/abs/2401.00816v1","updated":"2024-01-01T17:15:42Z","published":"2024-01-01T17:15:42Z","title":"GLIMPSE: Generalized Local Imaging with MLPs","summary":"  Deep learning is the current de facto state of the art in tomographic\nimaging. A common approach is to feed the result of a simple inversion, for\nexample the backprojection, to a convolutional neural network (CNN) which then\ncomputes the reconstruction. Despite strong results on 'in-distribution' test\ndata similar to the training data, backprojection from sparse-view data\ndelocalizes singularities, so these approaches require a large receptive field\nto perform well. As a consequence, they overfit to certain global structures\nwhich leads to poor generalization on out-of-distribution (OOD) samples.\nMoreover, their memory complexity and training time scale unfavorably with\nimage resolution, making them impractical for application at realistic clinical\nresolutions, especially in 3D: a standard U-Net requires a substantial 140GB of\nmemory and 2600 seconds per epoch on a research-grade GPU when training on\n1024x1024 images. In this paper, we introduce GLIMPSE, a local processing\nneural network for computed tomography which reconstructs a pixel value by\nfeeding only the measurements associated with the neighborhood of the pixel to\na simple MLP. While achieving comparable or better performance with successful\nCNNs like the U-Net on in-distribution test data, GLIMPSE significantly\noutperforms them on OOD samples while maintaining a memory footprint almost\nindependent of image resolution; 5GB memory suffices to train on 1024x1024\nimages. Further, we built GLIMPSE to be fully differentiable, which enables\nfeats such as recovery of accurate projection angles if they are out of\ncalibration.\n","authors":["AmirEhsan Khorashadizadeh","Valentin Debarnot","Tianlin Liu","Ivan Dokmanić"],"pdf_url":"https://arxiv.org/pdf/2401.00816v1.pdf","comment":"12 pages, 10 figures"},{"id":"http://arxiv.org/abs/2401.00926v1","updated":"2024-01-01T16:28:30Z","published":"2024-01-01T16:28:30Z","title":"Accurate Leukocyte Detection Based on Deformable-DETR and Multi-Level\n  Feature Fusion for Aiding Diagnosis of Blood Diseases","summary":"  In standard hospital blood tests, the traditional process requires doctors to\nmanually isolate leukocytes from microscopic images of patients' blood using\nmicroscopes. These isolated leukocytes are then categorized via automatic\nleukocyte classifiers to determine the proportion and volume of different types\nof leukocytes present in the blood samples, aiding disease diagnosis. This\nmethodology is not only time-consuming and labor-intensive, but it also has a\nhigh propensity for errors due to factors such as image quality and\nenvironmental conditions, which could potentially lead to incorrect subsequent\nclassifications and misdiagnosis. To address these issues, this paper proposes\nan innovative method of leukocyte detection: the Multi-level Feature Fusion and\nDeformable Self-attention DETR (MFDS-DETR). To tackle the issue of leukocyte\nscale disparity, we designed the High-level Screening-feature Fusion Pyramid\n(HS-FPN), enabling multi-level fusion. This model uses high-level features as\nweights to filter low-level feature information via a channel attention module\nand then merges the screened information with the high-level features, thus\nenhancing the model's feature expression capability. Further, we address the\nissue of leukocyte feature scarcity by incorporating a multi-scale deformable\nself-attention module in the encoder and using the self-attention and\ncross-deformable attention mechanisms in the decoder, which aids in the\nextraction of the global features of the leukocyte feature maps. The\neffectiveness, superiority, and generalizability of the proposed MFDS-DETR\nmethod are confirmed through comparisons with other cutting-edge leukocyte\ndetection models using the private WBCDD, public LISC and BCCD datasets. Our\nsource code and private WBCCD dataset are available at\nhttps://github.com/JustlfC03/MFDS-DETR.\n","authors":["Yifei Chen","Chenyan Zhang","Ben Chen","Yiyu Huang","Yifei Sun","Changmiao Wang","Xianjun Fu","Yuxing Dai","Feiwei Qin","Yong Peng","Yu Gao"],"pdf_url":"https://arxiv.org/pdf/2401.00926v1.pdf","comment":"15 pages, 11 figures, accept Computers in Biology and Medicine"},{"id":"http://arxiv.org/abs/2401.00789v1","updated":"2024-01-01T15:31:06Z","published":"2024-01-01T15:31:06Z","title":"Retrieval-Augmented Egocentric Video Captioning","summary":"  Understanding human actions from videos of first-person view poses\nsignificant challenges. Most prior approaches explore representation learning\non egocentric videos only, while overlooking the potential benefit of\nexploiting existing large-scale third-person videos. In this paper, (1) we\ndevelop EgoInstructor, a retrieval-augmented multimodal captioning model that\nautomatically retrieves semantically relevant third-person instructional videos\nto enhance the video captioning of egocentric videos. (2) For training the\ncross-view retrieval module, we devise an automatic pipeline to discover\nego-exo video pairs from distinct large-scale egocentric and exocentric\ndatasets. (3) We train the cross-view retrieval module with a novel EgoExoNCE\nloss that pulls egocentric and exocentric video features closer by aligning\nthem to shared text features that describe similar actions. (4) Through\nextensive experiments, our cross-view retrieval module demonstrates superior\nperformance across seven benchmarks. Regarding egocentric video captioning,\nEgoInstructor exhibits significant improvements by leveraging third-person\nvideos as references.\n","authors":["Jilan Xu","Yifei Huang","Junlin Hou","Guo Chen","Yuejie Zhang","Rui Feng","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2401.00789v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14181v3","updated":"2024-01-01T14:48:48Z","published":"2023-09-25T14:43:43Z","title":"Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level\n  Vision","summary":"  The rapid evolution of Multi-modality Large Language Models (MLLMs) has\ncatalyzed a shift in computer vision from specialized models to general-purpose\nfoundation models. Nevertheless, there is still an inadequacy in assessing the\nabilities of MLLMs on low-level visual perception and understanding. To address\nthis gap, we present Q-Bench, a holistic benchmark crafted to systematically\nevaluate potential abilities of MLLMs on three realms: low-level visual\nperception, low-level visual description, and overall visual quality\nassessment. a) To evaluate the low-level perception ability, we construct the\nLLVisionQA dataset, consisting of 2,990 diverse-sourced images, each equipped\nwith a human-asked question focusing on its low-level attributes. We then\nmeasure the correctness of MLLMs on answering these questions. b) To examine\nthe description ability of MLLMs on low-level information, we propose the\nLLDescribe dataset consisting of long expert-labelled golden low-level text\ndescriptions on 499 images, and a GPT-involved comparison pipeline between\noutputs of MLLMs and the golden descriptions. c) Besides these two tasks, we\nfurther measure their visual quality assessment ability to align with human\nopinion scores. Specifically, we design a softmax-based strategy that enables\nMLLMs to predict quantifiable quality scores, and evaluate them on various\nexisting image quality assessment (IQA) datasets. Our evaluation across the\nthree abilities confirms that MLLMs possess preliminary low-level visual\nskills. However, these skills are still unstable and relatively imprecise,\nindicating the need for specific enhancements on MLLMs towards these abilities.\nWe hope that our benchmark can encourage the research community to delve deeper\nto discover and enhance these untapped potentials of MLLMs. Project Page:\nhttps://q-future.github.io/Q-Bench.\n","authors":["Haoning Wu","Zicheng Zhang","Erli Zhang","Chaofeng Chen","Liang Liao","Annan Wang","Chunyi Li","Wenxiu Sun","Qiong Yan","Guangtao Zhai","Weisi Lin"],"pdf_url":"https://arxiv.org/pdf/2309.14181v3.pdf","comment":"27 pages, 11 tables, with updated results"},{"id":"http://arxiv.org/abs/2312.17163v2","updated":"2024-01-01T14:40:50Z","published":"2023-12-28T17:52:09Z","title":"FENet: Focusing Enhanced Network for Lane Detection","summary":"  Inspired by human driving focus, this research pioneers networks augmented\nwith Focusing Sampling, Partial Field of View Evaluation, Enhanced FPN\narchitecture and Directional IoU Loss - targeted innovations addressing\nobstacles to precise lane detection for autonomous driving. Experiments\ndemonstrate our Focusing Sampling strategy, emphasizing vital distant details\nunlike uniform approaches, significantly boosts both benchmark and practical\ncurved/distant lane recognition accuracy essential for safety. While FENetV1\nachieves state-of-the-art conventional metric performance via enhancements\nisolating perspective-aware contexts mimicking driver vision, FENetV2 proves\nmost reliable on the proposed Partial Field analysis. Hence we specifically\nrecommend V2 for practical lane navigation despite fractional degradation on\nstandard entire-image measures. Future directions include collecting on-road\ndata and integrating complementary dual frameworks to further breakthroughs\nguided by human perception principles. Code will be made available.\n","authors":["Liman Wang","Hanyang Zhong"],"pdf_url":"https://arxiv.org/pdf/2312.17163v2.pdf","comment":"12 pages including appendix. The website will be released soon"},{"id":"http://arxiv.org/abs/2305.14669v3","updated":"2024-01-01T14:40:33Z","published":"2023-05-24T03:23:35Z","title":"NegVSR: Augmenting Negatives for Generalized Noise Modeling in\n  Real-World Video Super-Resolution","summary":"  The capability of video super-resolution (VSR) to synthesize high-resolution\n(HR) video from ideal datasets has been demonstrated in many works. However,\napplying the VSR model to real-world video with unknown and complex degradation\nremains a challenging task. First, existing degradation metrics in most VSR\nmethods are not able to effectively simulate real-world noise and blur. On the\ncontrary, simple combinations of classical degradation are used for real-world\nnoise modeling, which led to the VSR model often being violated by\nout-of-distribution noise. Second, many SR models focus on noise simulation and\ntransfer. Nevertheless, the sampled noise is monotonous and limited. To address\nthe aforementioned problems, we propose a Negatives augmentation strategy for\ngeneralized noise modeling in Video Super-Resolution (NegVSR) task.\nSpecifically, we first propose sequential noise generation toward real-world\ndata to extract practical noise sequences. Then, the degeneration domain is\nwidely expanded by negative augmentation to build up various yet challenging\nreal-world noise sets. We further propose the augmented negative guidance loss\nto learn robust features among augmented negatives effectively. Extensive\nexperiments on real-world datasets (e.g., VideoLQ and FLIR) show that our\nmethod outperforms state-of-the-art methods with clear margins, especially in\nvisual quality. Project page is available at: https://negvsr.github.io/.\n","authors":["Yexing Song","Meilin Wang","Zhijing Yang","Xiaoyu Xian","Yukai Shi"],"pdf_url":"https://arxiv.org/pdf/2305.14669v3.pdf","comment":"Accepted by AAAI2024, a effective data augmentation framework for\n  real-world video super-resolution, see our demo at: https://negvsr.github.io/"},{"id":"http://arxiv.org/abs/2312.13108v2","updated":"2024-01-01T14:26:39Z","published":"2023-12-20T15:28:38Z","title":"ASSISTGUI: Task-Oriented Desktop Graphical User Interface Automation","summary":"  Graphical User Interface (GUI) automation holds significant promise for\nassisting users with complex tasks, thereby boosting human productivity.\nExisting works leveraging Large Language Model (LLM) or LLM-based AI agents\nhave shown capabilities in automating tasks on Android and Web platforms.\nHowever, these tasks are primarily aimed at simple device usage and\nentertainment operations. This paper presents a novel benchmark, AssistGUI, to\nevaluate whether models are capable of manipulating the mouse and keyboard on\nthe Windows platform in response to user-requested tasks. We carefully\ncollected a set of 100 tasks from nine widely-used software applications, such\nas, After Effects and MS Word, each accompanied by the necessary project files\nfor better evaluation. Moreover, we propose an advanced Actor-Critic Embodied\nAgent framework, which incorporates a sophisticated GUI parser driven by an\nLLM-agent and an enhanced reasoning mechanism adept at handling lengthy\nprocedural tasks. Our experimental results reveal that our GUI Parser and\nReasoning mechanism outshine existing methods in performance. Nevertheless, the\npotential remains substantial, with the best model attaining only a 46% success\nrate on our benchmark. We conclude with a thorough analysis of the current\nmethods' limitations, setting the stage for future breakthroughs in this\ndomain.\n","authors":["Difei Gao","Lei Ji","Zechen Bai","Mingyu Ouyang","Peiran Li","Dongxing Mao","Qinchen Wu","Weichen Zhang","Peiyi Wang","Xiangwu Guo","Hengxu Wang","Luowei Zhou","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2312.13108v2.pdf","comment":"Project Page: https://showlab.github.io/assistgui/"},{"id":"http://arxiv.org/abs/2401.00766v1","updated":"2024-01-01T14:14:35Z","published":"2024-01-01T14:14:35Z","title":"Bracketing is All You Need: Unifying Image Restoration and Enhancement\n  Tasks with Multi-Exposure Images","summary":"  It is challenging but highly desired to acquire high-quality photos with\nclear content in low-light environments. Although multi-image processing\nmethods (using burst, dual-exposure, or multi-exposure images) have made\nsignificant progress in addressing this issue, they typically focus exclusively\non specific restoration or enhancement tasks, being insufficient in exploiting\nmulti-image. Motivated by that multi-exposure images are complementary in\ndenoising, deblurring, high dynamic range imaging, and super-resolution, we\npropose to utilize bracketing photography to unify restoration and enhancement\ntasks in this work. Due to the difficulty in collecting real-world pairs, we\nsuggest a solution that first pre-trains the model with synthetic paired data\nand then adapts it to real-world unlabeled images. In particular, a temporally\nmodulated recurrent network (TMRNet) and self-supervised adaptation method are\nproposed. Moreover, we construct a data simulation pipeline to synthesize pairs\nand collect real-world images from 200 nighttime scenarios. Experiments on both\ndatasets show that our method performs favorably against the state-of-the-art\nmulti-image processing ones. The dataset, code, and pre-trained models are\navailable at https://github.com/cszhilu1998/BracketIRE.\n","authors":["Zhilu Zhang","Shuohao Zhang","Renlong Wu","Zifei Yan","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2401.00766v1.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2401.00763v1","updated":"2024-01-01T14:06:55Z","published":"2024-01-01T14:06:55Z","title":"New Job, New Gender? Measuring the Social Bias in Image Generation\n  Models","summary":"  Image generation models can generate or edit images from a given text. Recent\nadvancements in image generation technology, exemplified by DALL-E and\nMidjourney, have been groundbreaking. These advanced models, despite their\nimpressive capabilities, are often trained on massive Internet datasets, making\nthem susceptible to generating content that perpetuates social stereotypes and\nbiases, which can lead to severe consequences. Prior research on assessing bias\nwithin image generation models suffers from several shortcomings, including\nlimited accuracy, reliance on extensive human labor, and lack of comprehensive\nanalysis. In this paper, we propose BiasPainter, a novel metamorphic testing\nframework that can accurately, automatically and comprehensively trigger social\nbias in image generation models. BiasPainter uses a diverse range of seed\nimages of individuals and prompts the image generation models to edit these\nimages using gender, race, and age-neutral queries. These queries span 62\nprofessions, 39 activities, 57 types of objects, and 70 personality traits. The\nframework then compares the edited images to the original seed images, focusing\non any changes related to gender, race, and age. BiasPainter adopts a testing\noracle that these characteristics should not be modified when subjected to\nneutral prompts. Built upon this design, BiasPainter can trigger the social\nbias and evaluate the fairness of image generation models. To evaluate the\neffectiveness of BiasPainter, we use BiasPainter to test five widely-used\ncommercial image generation software and models, such as stable diffusion and\nMidjourney. Experimental results show that 100\\% of the generated test cases\ncan successfully trigger social bias in image generation models.\n","authors":["Wenxuan Wang","Haonan Bai","Jen-tse Huang","Yuxuan Wan","Youliang Yuan","Haoyi Qiu","Nanyun Peng","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2401.00763v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.08842v3","updated":"2024-01-01T13:56:49Z","published":"2023-04-18T09:13:52Z","title":"UDTIRI: An Online Open-Source Intelligent Road Inspection Benchmark\n  Suite","summary":"  In the nascent domain of urban digital twins (UDT), the prospects for\nleveraging cutting-edge deep learning techniques are vast and compelling.\nParticularly within the specialized area of intelligent road inspection (IRI),\na noticeable gap exists, underscored by the current dearth of dedicated\nresearch efforts and the lack of large-scale well-annotated datasets. To foster\nadvancements in this burgeoning field, we have launched an online open-source\nbenchmark suite, referred to as UDTIRI. Along with this article, we introduce\nthe road pothole detection task, the first online competition published within\nthis benchmark suite. This task provides a well-annotated dataset, comprising\n1,000 RGB images and their pixel/instance-level ground-truth annotations,\ncaptured in diverse real-world scenarios under different illumination and\nweather conditions. Our benchmark provides a systematic and thorough evaluation\nof state-of-the-art object detection, semantic segmentation, and instance\nsegmentation networks, developed based on either convolutional neural networks\nor Transformers. We anticipate that our benchmark will serve as a catalyst for\nthe integration of advanced UDT techniques into IRI. By providing algorithms\nwith a more comprehensive understanding of diverse road conditions, we seek to\nunlock their untapped potential and foster innovation in this critical domain.\n","authors":["Sicen Guo","Jiahang Li","Yi Feng","Dacheng Zhou","Denghuang Zhang","Chen Chen","Shuai Su","Xingyi Zhu","Qijun Chen","Rui Fan"],"pdf_url":"https://arxiv.org/pdf/2304.08842v3.pdf","comment":"Database webpage: https://www.udtiri.com/, Kaggle webpage:\n  https://www.kaggle.com/datasets/jiahangli617/udtiri"},{"id":"http://arxiv.org/abs/2312.01324v2","updated":"2024-01-01T13:27:15Z","published":"2023-12-03T09:00:31Z","title":"MABViT -- Modified Attention Block Enhances Vision Transformers","summary":"  Recent studies have demonstrated the effectiveness of Gated Linear Units\n(GLU) in enhancing transformer models, particularly in Large Language Models\n(LLMs). Additionally, utilizing a parallel configuration within each\nTransformer block rather than the conventional serialized method has been\nrevealed to accelerate the training of LLMs without significantly impacting\nperformance. However, when the MLP and attention block were run in parallel for\nthe image classification task, we observed a noticeable decline in performance.\nWe propose a novel transformer variant that integrates non-linearity within the\nattention block to tackle this problem. We implemented the GLU-based activation\nfunction on the Value tensor, and this new technique surpasses the current\nstate-of-the-art S/16 variant of Vision Transformers by 0.6% on the ImageNet-1K\ndataset while utilizing fewer parameters. It also supersedes the B/16 variant\nwhile using only half the parameters. Furthermore, we provide results with the\nGELU activation function variant to confirm our assertions. Lastly, we showcase\nthat the MABViT variants exhibit greater potential when utilized in deep\ntransformers compared to the standard architecture.\n","authors":["Mahesh Ramesh","Aswinkumar Ramkumar"],"pdf_url":"https://arxiv.org/pdf/2312.01324v2.pdf","comment":"Accepted at Deployable AI Workshop, AAAI Conference"},{"id":"http://arxiv.org/abs/2401.00740v1","updated":"2024-01-01T12:48:23Z","published":"2024-01-01T12:48:23Z","title":"Beyond Subspace Isolation: Many-to-Many Transformer for Light Field\n  Image Super-resolution","summary":"  The effective extraction of spatial-angular features plays a crucial role in\nlight field image super-resolution (LFSR) tasks, and the introduction of\nconvolution and Transformers leads to significant improvement in this area.\nNevertheless, due to the large 4D data volume of light field images, many\nexisting methods opted to decompose the data into a number of lower-dimensional\nsubspaces and perform Transformers in each sub-space individually. As a side\neffect, these methods inadvertently restrict the self-attention mechanisms to a\nOne-to-One scheme accessing only a limited subset of LF data, explicitly\npreventing comprehensive optimization on all spatial and angular cues. In this\npaper, we identify this limitation as subspace isolation and introduce a novel\nMany-to-Many Transformer (M2MT) to address it. M2MT aggregates angular\ninformation in the spatial subspace before performing the self-attention\nmechanism. It enables complete access to all information across all\nsub-aperture images (SAIs) in a light field image. Consequently, M2MT is\nenabled to comprehensively capture long-range correlation dependencies. With\nM2MT as the pivotal component, we develop a simple yet effective M2MT network\nfor LFSR. Our experimental results demonstrate that M2MT achieves\nstate-of-the-art performance across various public datasets. We further conduct\nin-depth analysis using local attribution maps (LAM) to obtain visual\ninterpretability, and the results validate that M2MT is empowered with a truly\nnon-local context in both spatial and angular subspaces to mitigate subspace\nisolation and acquire effective spatial-angular representation.\n","authors":["Zeke Zexi Hu","Xiaoming Chen","Vera Yuk Ying Chung","Yiran Shen"],"pdf_url":"https://arxiv.org/pdf/2401.00740v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00739v1","updated":"2024-01-01T12:42:32Z","published":"2024-01-01T12:42:32Z","title":"DiffMorph: Text-less Image Morphing with Diffusion Models","summary":"  Text-conditioned image generation models are a prevalent use of AI image\nsynthesis, yet intuitively controlling output guided by an artist remains\nchallenging. Current methods require multiple images and textual prompts for\neach object to specify them as concepts to generate a single customized image.\n  On the other hand, our work, \\verb|DiffMorph|, introduces a novel approach\nthat synthesizes images that mix concepts without the use of textual prompts.\nOur work integrates a sketch-to-image module to incorporate user sketches as\ninput. \\verb|DiffMorph| takes an initial image with conditioning artist-drawn\nsketches to generate a morphed image.\n  We employ a pre-trained text-to-image diffusion model and fine-tune it to\nreconstruct each image faithfully. We seamlessly merge images and concepts from\nsketches into a cohesive composition. The image generation capability of our\nwork is demonstrated through our results and a comparison of these with\nprompt-based image generation.\n","authors":["Shounak Chatterjee"],"pdf_url":"https://arxiv.org/pdf/2401.00739v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16754v2","updated":"2024-01-01T12:27:23Z","published":"2023-11-28T12:52:49Z","title":"Towards Full-scene Domain Generalization in Multi-agent Collaborative\n  Bird's Eye View Segmentation for Connected and Autonomous Driving","summary":"  Collaborative perception has recently gained significant attention in\nautonomous driving, improving perception quality by enabling the exchange of\nadditional information among vehicles. However, deploying collaborative\nperception systems can lead to domain shifts due to diverse environmental\nconditions and data heterogeneity among connected and autonomous vehicles\n(CAVs). To address these challenges, we propose a unified domain generalization\nframework applicable in both training and inference stages of collaborative\nperception. In the training phase, we introduce an Amplitude Augmentation\n(AmpAug) method to augment low-frequency image variations, broadening the\nmodel's ability to learn across various domains. We also employ a\nmeta-consistency training scheme to simulate domain shifts, optimizing the\nmodel with a carefully designed consistency loss to encourage domain-invariant\nrepresentations. In the inference phase, we introduce an intra-system domain\nalignment mechanism to reduce or potentially eliminate the domain discrepancy\namong CAVs prior to inference. Comprehensive experiments substantiate the\neffectiveness of our method in comparison with the existing state-of-the-art\nworks. Code will be released at https://github.com/DG-CAVs/DG-CoPerception.git.\n","authors":["Senkang Hu","Zhengru Fang","Xianhao Chen","Yuguang Fang","Sam Kwong"],"pdf_url":"https://arxiv.org/pdf/2311.16754v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00736v1","updated":"2024-01-01T12:25:57Z","published":"2024-01-01T12:25:57Z","title":"Diffusion Models, Image Super-Resolution And Everything: A Survey","summary":"  Diffusion Models (DMs) represent a significant advancement in image\nSuper-Resolution (SR), aligning technical image quality more closely with human\npreferences and expanding SR applications. DMs address critical limitations of\nprevious methods, enhancing overall realism and details in SR images. However,\nDMs suffer from color-shifting issues, and their high computational costs call\nfor efficient sampling alternatives, underscoring the challenge of balancing\ncomputational efficiency and image quality. This survey gives an overview of\nDMs applied to image SR and offers a detailed analysis that underscores the\nunique characteristics and methodologies within this domain, distinct from\nbroader existing reviews in the field. It presents a unified view of DM\nfundamentals and explores research directions, including alternative input\ndomains, conditioning strategies, guidance, corruption spaces, and zero-shot\nmethods. This survey provides insights into the evolution of image SR with DMs,\naddressing current trends, challenges, and future directions in this rapidly\nevolving field.\n","authors":["Brian B. Moser","Arundhati S. Shanbhag","Federico Raue","Stanislav Frolov","Sebastian Palacio","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2401.00736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00921v1","updated":"2024-01-01T12:08:35Z","published":"2024-01-01T12:08:35Z","title":"Skeleton2vec: A Self-supervised Learning Framework with Contextualized\n  Target Representations for Skeleton Sequence","summary":"  Self-supervised pre-training paradigms have been extensively explored in the\nfield of skeleton-based action recognition. In particular, methods based on\nmasked prediction have pushed the performance of pre-training to a new height.\nHowever, these methods take low-level features, such as raw joint coordinates\nor temporal motion, as prediction targets for the masked regions, which is\nsuboptimal. In this paper, we show that using high-level contextualized\nfeatures as prediction targets can achieve superior performance. Specifically,\nwe propose Skeleton2vec, a simple and efficient self-supervised 3D action\nrepresentation learning framework, which utilizes a transformer-based teacher\nencoder taking unmasked training samples as input to create latent\ncontextualized representations as prediction targets. Benefiting from the\nself-attention mechanism, the latent representations generated by the teacher\nencoder can incorporate the global context of the entire training samples,\nleading to a richer training task. Additionally, considering the high temporal\ncorrelations in skeleton sequences, we propose a motion-aware tube masking\nstrategy which divides the skeleton sequence into several tubes and performs\npersistent masking within each tube based on motion priors, thus forcing the\nmodel to build long-range spatio-temporal connections and focus on\naction-semantic richer regions. Extensive experiments on NTU-60, NTU-120, and\nPKU-MMD datasets demonstrate that our proposed Skeleton2vec outperforms\nprevious methods and achieves state-of-the-art results.\n","authors":["Ruizhuo Xu","Linzhi Huang","Mei Wang","Jiani Hu","Weihong Deng"],"pdf_url":"https://arxiv.org/pdf/2401.00921v1.pdf","comment":"Submitted to CVPR 2024"},{"id":"http://arxiv.org/abs/2401.00729v1","updated":"2024-01-01T11:54:51Z","published":"2024-01-01T11:54:51Z","title":"NightRain: Nighttime Video Deraining via Adaptive-Rain-Removal and\n  Adaptive-Correction","summary":"  Existing deep-learning-based methods for nighttime video deraining rely on\nsynthetic data due to the absence of real-world paired data. However, the\nintricacies of the real world, particularly with the presence of light effects\nand low-light regions affected by noise, create significant domain gaps,\nhampering synthetic-trained models in removing rain streaks properly and\nleading to over-saturation and color shifts. Motivated by this, we introduce\nNightRain, a novel nighttime video deraining method with adaptive-rain-removal\nand adaptive-correction. Our adaptive-rain-removal uses unlabeled rain videos\nto enable our model to derain real-world rain videos, particularly in regions\naffected by complex light effects. The idea is to allow our model to obtain\nrain-free regions based on the confidence scores. Once rain-free regions and\nthe corresponding regions from our input are obtained, we can have region-based\npaired real data. These paired data are used to train our model using a\nteacher-student framework, allowing the model to iteratively learn from less\nchallenging regions to more challenging regions. Our adaptive-correction aims\nto rectify errors in our model's predictions, such as over-saturation and color\nshifts. The idea is to learn from clear night input training videos based on\nthe differences or distance between those input videos and their corresponding\npredictions. Our model learns from these differences, compelling our model to\ncorrect the errors. From extensive experiments, our method demonstrates\nstate-of-the-art performance. It achieves a PSNR of 26.73dB, surpassing\nexisting nighttime video deraining methods by a substantial margin of 13.7%.\n","authors":["Beibei Lin","Yeying Jin","Wending Yan","Wei Ye","Yuan Yuan","Shunli Zhang","Robby Tan"],"pdf_url":"https://arxiv.org/pdf/2401.00729v1.pdf","comment":"Accepted by AAAI24"},{"id":"http://arxiv.org/abs/2401.00728v1","updated":"2024-01-01T11:50:01Z","published":"2024-01-01T11:50:01Z","title":"MultiFusionNet: Multilayer Multimodal Fusion of Deep Neural Networks for\n  Chest X-Ray Image Classification","summary":"  Chest X-ray imaging is a critical diagnostic tool for identifying pulmonary\ndiseases. However, manual interpretation of these images is time-consuming and\nerror-prone. Automated systems utilizing convolutional neural networks (CNNs)\nhave shown promise in improving the accuracy and efficiency of chest X-ray\nimage classification. While previous work has mainly focused on using feature\nmaps from the final convolution layer, there is a need to explore the benefits\nof leveraging additional layers for improved disease classification. Extracting\nrobust features from limited medical image datasets remains a critical\nchallenge. In this paper, we propose a novel deep learning-based multilayer\nmultimodal fusion model that emphasizes extracting features from different\nlayers and fusing them. Our disease detection model considers the\ndiscriminatory information captured by each layer. Furthermore, we propose the\nfusion of different-sized feature maps (FDSFM) module to effectively merge\nfeature maps from diverse layers. The proposed model achieves a significantly\nhigher accuracy of 97.21% and 99.60% for both three-class and two-class\nclassifications, respectively. The proposed multilayer multimodal fusion model,\nalong with the FDSFM module, holds promise for accurate disease classification\nand can also be extended to other disease classifications in chest X-ray\nimages.\n","authors":["Saurabh Agarwal","K. V. Arya","Yogesh Kumar Meena"],"pdf_url":"https://arxiv.org/pdf/2401.00728v1.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2401.00722v1","updated":"2024-01-01T10:49:09Z","published":"2024-01-01T10:49:09Z","title":"BRAU-Net++: U-Shaped Hybrid CNN-Transformer Network for Medical Image\n  Segmentation","summary":"  Accurate medical image segmentation is essential for clinical quantification,\ndisease diagnosis, treatment planning and many other applications. Both\nconvolution-based and transformer-based u-shaped architectures have made\nsignificant success in various medical image segmentation tasks. The former can\nefficiently learn local information of images while requiring much more\nimage-specific inductive biases inherent to convolution operation. The latter\ncan effectively capture long-range dependency at different feature scales using\nself-attention, whereas it typically encounters the challenges of quadratic\ncompute and memory requirements with sequence length increasing. To address\nthis problem, through integrating the merits of these two paradigms in a\nwell-designed u-shaped architecture, we propose a hybrid yet effective\nCNN-Transformer network, named BRAU-Net++, for an accurate medical image\nsegmentation task. Specifically, BRAU-Net++ uses bi-level routing attention as\nthe core building block to design our u-shaped encoder-decoder structure, in\nwhich both encoder and decoder are hierarchically constructed, so as to learn\nglobal semantic information while reducing computational complexity.\nFurthermore, this network restructures skip connection by incorporating\nchannel-spatial attention which adopts convolution operations, aiming to\nminimize local spatial information loss and amplify global\ndimension-interaction of multi-scale features. Extensive experiments on three\npublic benchmark datasets demonstrate that our proposed approach surpasses\nother state-of-the-art methods including its baseline: BRAU-Net under almost\nall evaluation metrics. We achieve the average Dice-Similarity Coefficient\n(DSC) of 82.47, 90.10, and 92.94 on Synapse multi-organ segmentation, ISIC-2018\nChallenge, and CVC-ClinicDB, as well as the mIoU of 84.01 and 88.17 on\nISIC-2018 Challenge and CVC-ClinicDB, respectively.\n","authors":["Libin Lan","Pengzhou Cai","Lu Jiang","Xiaojuan Liu","Yongmei Li","Yudong Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.00722v1.pdf","comment":"12 pages, 6 figures, 9 tables code:\n  https://github.com/Caipengzhou/BRAU-Netplusplus"},{"id":"http://arxiv.org/abs/2401.00719v1","updated":"2024-01-01T10:46:42Z","published":"2024-01-01T10:46:42Z","title":"Depth Map Denoising Network and Lightweight Fusion Network for Enhanced\n  3D Face Recognition","summary":"  With the increasing availability of consumer depth sensors, 3D face\nrecognition (FR) has attracted more and more attention. However, the data\nacquired by these sensors are often coarse and noisy, making them impractical\nto use directly. In this paper, we introduce an innovative Depth map denoising\nnetwork (DMDNet) based on the Denoising Implicit Image Function (DIIF) to\nreduce noise and enhance the quality of facial depth images for low-quality 3D\nFR. After generating clean depth faces using DMDNet, we further design a\npowerful recognition network called Lightweight Depth and Normal Fusion network\n(LDNFNet), which incorporates a multi-branch fusion block to learn unique and\ncomplementary features between different modalities such as depth and normal\nimages. Comprehensive experiments conducted on four distinct low-quality\ndatabases demonstrate the effectiveness and robustness of our proposed methods.\nFurthermore, when combining DMDNet and LDNFNet, we achieve state-of-the-art\nresults on the Lock3DFace database.\n","authors":["Ruizhuo Xu","Ke Wang","Chao Deng","Mei Wang","Xi Chen","Wenhui Huang","Junlan Feng","Weihong Deng"],"pdf_url":"https://arxiv.org/pdf/2401.00719v1.pdf","comment":"Accepted by Pattern Recognition"},{"id":"http://arxiv.org/abs/2312.09168v2","updated":"2024-01-01T10:15:46Z","published":"2023-12-14T17:34:53Z","title":"DiffusionLight: Light Probes for Free by Painting a Chrome Ball","summary":"  We present a simple yet effective technique to estimate lighting in a single\ninput image. Current techniques rely heavily on HDR panorama datasets to train\nneural networks to regress an input with limited field-of-view to a full\nenvironment map. However, these approaches often struggle with real-world,\nuncontrolled settings due to the limited diversity and size of their datasets.\nTo address this problem, we leverage diffusion models trained on billions of\nstandard images to render a chrome ball into the input image. Despite its\nsimplicity, this task remains challenging: the diffusion models often insert\nincorrect or inconsistent objects and cannot readily generate images in HDR\nformat. Our research uncovers a surprising relationship between the appearance\nof chrome balls and the initial diffusion noise map, which we utilize to\nconsistently generate high-quality chrome balls. We further fine-tune an LDR\ndifusion model (Stable Diffusion XL) with LoRA, enabling it to perform exposure\nbracketing for HDR light estimation. Our method produces convincing light\nestimates across diverse settings and demonstrates superior generalization to\nin-the-wild scenarios.\n","authors":["Pakkapon Phongthawee","Worameth Chinchuthakun","Nontaphat Sinsunthithet","Amit Raj","Varun Jampani","Pramook Khungurn","Supasorn Suwajanakorn"],"pdf_url":"https://arxiv.org/pdf/2312.09168v2.pdf","comment":"For more info and code, please visit our website\n  https://diffusionlight.github.io/"},{"id":"http://arxiv.org/abs/2401.00711v1","updated":"2024-01-01T09:39:57Z","published":"2024-01-01T09:39:57Z","title":"Text2Avatar: Text to 3D Human Avatar Generation with Codebook-Driven\n  Body Controllable Attribute","summary":"  Generating 3D human models directly from text helps reduce the cost and time\nof character modeling. However, achieving multi-attribute controllable and\nrealistic 3D human avatar generation is still challenging due to feature\ncoupling and the scarcity of realistic 3D human avatar datasets. To address\nthese issues, we propose Text2Avatar, which can generate realistic-style 3D\navatars based on the coupled text prompts. Text2Avatar leverages a discrete\ncodebook as an intermediate feature to establish a connection between text and\navatars, enabling the disentanglement of features. Furthermore, to alleviate\nthe scarcity of realistic style 3D human avatar data, we utilize a pre-trained\nunconditional 3D human avatar generation model to obtain a large amount of 3D\navatar pseudo data, which allows Text2Avatar to achieve realistic style\ngeneration. Experimental results demonstrate that our method can generate\nrealistic 3D avatars from coupled textual data, which is challenging for other\nexisting methods in this field.\n","authors":["Chaoqun Gong","Yuqin Dai","Ronghui Li","Achun Bao","Jun Li","Jian Yang","Yachao Zhang","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2401.00711v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.03380v2","updated":"2024-01-01T09:26:56Z","published":"2023-11-02T08:18:37Z","title":"An attempt to generate new bridge types from latent space of variational\n  autoencoder","summary":"  Try to generate new bridge types using generative artificial intelligence\ntechnology. The grayscale images of the bridge facade with the change of\ncomponent width was rendered by 3dsMax animation software, and then the OpenCV\nmodule performed an appropriate amount of geometric transformation (rotation,\nhorizontal scale, vertical scale) to obtain the image dataset of three-span\nbeam bridge, arch bridge, cable-stayed bridge and suspension bridge. Based on\nPython programming language, TensorFlow and Keras deep learning platform\nframework, variational autoencoder was constructed and trained, and\nlow-dimensional bridge-type latent space that is convenient for vector\noperations was obtained. Variational autoencoder can combine two bridge types\non the basis of the original of human into one that is a new bridge type.\nGenerative artificial intelligence technology can assist bridge designers in\nbridge-type innovation, and can be used as copilot.\n","authors":["Hongjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.03380v2.pdf","comment":"9 pages, 8 figures"},{"id":"http://arxiv.org/abs/2401.00708v1","updated":"2024-01-01T09:25:03Z","published":"2024-01-01T09:25:03Z","title":"Revisiting Nonlocal Self-Similarity from Continuous Representation","summary":"  Nonlocal self-similarity (NSS) is an important prior that has been\nsuccessfully applied in multi-dimensional data processing tasks, e.g., image\nand video recovery. However, existing NSS-based methods are solely suitable for\nmeshgrid data such as images and videos, but are not suitable for emerging\noff-meshgrid data, e.g., point cloud and climate data. In this work, we revisit\nthe NSS from the continuous representation perspective and propose a novel\nContinuous Representation-based NonLocal method (termed as CRNL), which has two\ninnovative features as compared with classical nonlocal methods. First, based\non the continuous representation, our CRNL unifies the measure of\nself-similarity for on-meshgrid and off-meshgrid data and thus is naturally\nsuitable for both of them. Second, the nonlocal continuous groups can be more\ncompactly and efficiently represented by the coupled low-rank function\nfactorization, which simultaneously exploits the similarity within each group\nand across different groups, while classical nonlocal methods neglect the\nsimilarity across groups. This elaborately designed coupled mechanism allows\nour method to enjoy favorable performance over conventional NSS methods in\nterms of both effectiveness and efficiency. Extensive multi-dimensional data\nprocessing experiments on-meshgrid (e.g., image inpainting and image denoising)\nand off-meshgrid (e.g., climate data prediction and point cloud recovery)\nvalidate the versatility, effectiveness, and efficiency of our CRNL as compared\nwith state-of-the-art methods.\n","authors":["Yisi Luo","Xile Zhao","Deyu Meng"],"pdf_url":"https://arxiv.org/pdf/2401.00708v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00701v1","updated":"2024-01-01T08:54:18Z","published":"2024-01-01T08:54:18Z","title":"Towards Efficient and Effective Text-to-Video Retrieval with\n  Coarse-to-Fine Visual Representation Learning","summary":"  In recent years, text-to-video retrieval methods based on CLIP have\nexperienced rapid development. The primary direction of evolution is to exploit\nthe much wider gamut of visual and textual cues to achieve alignment.\nConcretely, those methods with impressive performance often design a heavy\nfusion block for sentence (words)-video (frames) interaction, regardless of the\nprohibitive computation complexity. Nevertheless, these approaches are not\noptimal in terms of feature utilization and retrieval efficiency. To address\nthis issue, we adopt multi-granularity visual feature learning, ensuring the\nmodel's comprehensiveness in capturing visual content features spanning from\nabstract to detailed levels during the training phase. To better leverage the\nmulti-granularity features, we devise a two-stage retrieval architecture in the\nretrieval phase. This solution ingeniously balances the coarse and fine\ngranularity of retrieval content. Moreover, it also strikes a harmonious\nequilibrium between retrieval effectiveness and efficiency. Specifically, in\ntraining phase, we design a parameter-free text-gated interaction block (TIB)\nfor fine-grained video representation learning and embed an extra Pearson\nConstraint to optimize cross-modal representation learning. In retrieval phase,\nwe use coarse-grained video representations for fast recall of top-k\ncandidates, which are then reranked by fine-grained video representations.\nExtensive experiments on four benchmarks demonstrate the efficiency and\neffectiveness. Notably, our method achieves comparable performance with the\ncurrent state-of-the-art methods while being nearly 50 times faster.\n","authors":["Kaibin Tian","Yanhua Cheng","Yi Liu","Xinglin Hou","Quan Chen","Han Li"],"pdf_url":"https://arxiv.org/pdf/2401.00701v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.06424v3","updated":"2024-01-01T08:50:45Z","published":"2022-03-12T12:39:41Z","title":"VariabilityTrack:Multi-Object Tracking with Variable Speed Object\n  Movement","summary":"  Multi-object tracking (MOT) aims at estimating bounding boxes and identities\nof objects in videos. Most methods can be roughly classified as\ntracking-by-detection and joint-detection-association paradigms. Although the\nlatter has elicited more attention and demonstrates comparable performance\nrelative than the former, we claim that the tracking-by-detection paradigm is\nstill the optimal solution in terms of tracking accuracy,such as\nByteTrack,which achieves 80.3 MOTA, 77.3 IDF1 and 63.1 HOTA on the test set of\nMOT17 with 30 FPS running speed on a single V100 GPU.However, under complex\nperspectives such as vehicle and UAV acceleration, the performance of such a\ntracker using uniform Kalman filter will be greatly affected, resulting in\ntracking loss.In this paper, we propose a variable speed Kalman filter\nalgorithm based on environmental feedback and improve the matching process,\nwhich can greatly improve the tracking effect in complex variable speed scenes\nwhile maintaining high tracking accuracy in relatively static scenes.\nEventually, higher MOTA and IDF1 results can be achieved on MOT17 test set than\nByteTrack\n","authors":["Run Luo","JinLin Wei","Qiao Lin"],"pdf_url":"https://arxiv.org/pdf/2203.06424v3.pdf","comment":"find some mistake in this paper"},{"id":"http://arxiv.org/abs/2401.00700v1","updated":"2024-01-01T08:46:29Z","published":"2024-01-01T08:46:29Z","title":"An attempt to generate new bridge types from latent space of generative\n  adversarial network","summary":"  Try to generate new bridge types using generative artificial intelligence\ntechnology. Symmetric structured image dataset of three-span beam bridge, arch\nbridge, cable-stayed bridge and suspension bridge are used . Based on Python\nprogramming language, TensorFlow and Keras deep learning platform framework ,\nas well as Wasserstein loss function and Lipschitz constraints, generative\nadversarial network is constructed and trained. From the obtained low\ndimensional bridge-type latent space sampling, new bridge types with asymmetric\nstructures can be generated. Generative adversarial network can create new\nbridge types by organically combining different structural components on the\nbasis of human original bridge types. It has a certain degree of human original\nability. Generative artificial intelligence technology can open up imagination\nspace and inspire humanity.\n","authors":["Hongjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.00700v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2401.00695v1","updated":"2024-01-01T08:19:21Z","published":"2024-01-01T08:19:21Z","title":"Credible Teacher for Semi-Supervised Object Detection in Open Scene","summary":"  Semi-Supervised Object Detection (SSOD) has achieved resounding success by\nleveraging unlabeled data to improve detection performance. However, in Open\nScene Semi-Supervised Object Detection (O-SSOD), unlabeled data may contains\nunknown objects not observed in the labeled data, which will increase\nuncertainty in the model's predictions for known objects. It is detrimental to\nthe current methods that mainly rely on self-training, as more uncertainty\nleads to the lower localization and classification precision of pseudo labels.\nTo this end, we propose Credible Teacher, an end-to-end framework. Credible\nTeacher adopts an interactive teaching mechanism using flexible labels to\nprevent uncertain pseudo labels from misleading the model and gradually reduces\nits uncertainty through the guidance of other credible pseudo labels. Empirical\nresults have demonstrated our method effectively restrains the adverse effect\ncaused by O-SSOD and significantly outperforms existing counterparts.\n","authors":["Jingyu Zhuang","Kuo Wang","Liang Lin","Guanbin Li"],"pdf_url":"https://arxiv.org/pdf/2401.00695v1.pdf","comment":"Accpet by ICASSP 2024"},{"id":"http://arxiv.org/abs/2401.00692v1","updated":"2024-01-01T08:11:38Z","published":"2024-01-01T08:11:38Z","title":"Self-supervised learning for skin cancer diagnosis with limited training\n  data","summary":"  Cancer diagnosis is a well-studied problem in machine learning since early\ndetection of cancer is often the determining factor in prognosis. Supervised\ndeep learning achieves excellent results in cancer image classification,\nusually through transfer learning. However, these models require large amounts\nof labelled data and for several types of cancer, large labelled datasets do\nnot exist. In this paper, we demonstrate that a model pre-trained using a\nself-supervised learning algorithm known as Barlow Twins can outperform the\nconventional supervised transfer learning pipeline. We juxtapose two base\nmodels: i) pretrained in a supervised fashion on ImageNet; ii) pretrained in a\nself-supervised fashion on ImageNet. Both are subsequently fine tuned on a\nsmall labelled skin lesion dataset and evaluated on a large test set. We\nachieve a mean test accuracy of 70\\% for self-supervised transfer in comparison\nto 66\\% for supervised transfer. Interestingly, boosting performance further is\npossible by self-supervised pretraining a second time (on unlabelled skin\nlesion images) before subsequent fine tuning. This hints at an alternative path\nto collecting more labelled data in settings where this is challenging - namely\njust collecting more unlabelled images. Our framework is applicable to cancer\nimage classification models in the low-labelled data regime.\n","authors":["Hamish Haggerty","Rohitash Chandra"],"pdf_url":"https://arxiv.org/pdf/2401.00692v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.16846v3","updated":"2024-01-01T06:25:08Z","published":"2023-06-29T10:37:29Z","title":"Lightweight texture transfer based on texture feature preset","summary":"  In the task of texture transfer, reference texture images typically exhibit\nhighly repetitive texture features, and the texture transfer results from\ndifferent content images under the same style also share remarkably similar\ntexture patterns. Encoding such highly similar texture features often requires\ndeep layers and a large number of channels, making it is also the main source\nof the entire model's parameter count and computational load, and inference\ntime. We propose a lightweight texture transfer based on texture feature preset\n(TFP). TFP takes full advantage of the high repetitiveness of texture features\nby providing preset universal texture feature maps for a given style. These\npreset feature maps can be fused and decoded directly with shallow color\ntransfer feature maps of any content to generate texture transfer results,\nthereby avoiding redundant texture information from being encoded repeatedly.\nThe texture feature map we preset is encoded through noise input images with\nconsistent distribution (standard normal distribution). This consistent input\ndistribution can completely avoid the problem of texture transfer\ndifferentiation, and by randomly sampling different noise inputs, we can obtain\ndifferent texture features and texture transfer results under the same\nreference style. Compared to state-of-the-art techniques, our TFP not only\nproduces visually superior results but also reduces the model size by 3.2-3538\ntimes and speeds up the process by 1.8-5.6 times.\n","authors":["ShiQi Jiang"],"pdf_url":"https://arxiv.org/pdf/2306.16846v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.10772v5","updated":"2024-01-01T05:40:17Z","published":"2022-12-21T05:08:37Z","title":"Low-Light Image and Video Enhancement: A Comprehensive Survey and Beyond","summary":"  This paper presents a comprehensive survey of low-light image and video\nenhancement, addressing two primary challenges in the field. The first\nchallenge is the prevalence of mixed over-/under-exposed images, which are not\nadequately addressed by existing methods. In response, this work introduces two\nenhanced variants of the SICE dataset: SICE_Grad and SICE_Mix, designed to\nbetter represent these complexities. The second challenge is the scarcity of\nsuitable low-light video datasets for training and testing. To address this,\nthe paper introduces the Night Wenzhou dataset, a large-scale, high-resolution\nvideo collection that features challenging fast-moving aerial scenes and\nstreetscapes with varied illuminations and degradation. This study also\nconducts an extensive analysis of key techniques and performs comparative\nexperiments using the proposed and current benchmark datasets. The survey\nconcludes by highlighting emerging applications, discussing unresolved\nchallenges, and suggesting future research directions within the LLIE\ncommunity. The datasets are available at\nhttps://github.com/ShenZheng2000/LLIE_Survey.\n","authors":["Shen Zheng","Yiling Ma","Jinqian Pan","Changjie Lu","Gaurav Gupta"],"pdf_url":"https://arxiv.org/pdf/2212.10772v5.pdf","comment":"21 pages, 10 tables, and 17 figures"},{"id":"http://arxiv.org/abs/2312.07586v2","updated":"2024-01-01T04:25:04Z","published":"2023-12-11T02:40:40Z","title":"Characteristic Guidance: Non-linear Correction for Diffusion Model at\n  Large Guidance Scale","summary":"  Popular guidance for denoising diffusion probabilistic model (DDPM) linearly\ncombines distinct conditional models together to provide enhanced control over\nsamples. However, this approach overlooks nonlinear effects that become\nsignificant when guidance scale is large. To address this issue, we propose\ncharacteristic guidance, a sampling method that provides first-principle\nnon-linear correction for classifier-free guided DDPMs. Such correction forces\nthe guided DDPMs to respect the Fokker-Planck equation of their underlying\ndiffusion process, in a way that is training-free, derivative-free, and\ncompatible with existing sampling methods. Experiments show that characteristic\nguidance enhances control and reduces color and exposure issues in image\ngeneration, proving effective in diverse applications ranging from latent space\nsampling to solving physics problems like magnet phase transitions.\n","authors":["Candi Zheng","Yuan Lan"],"pdf_url":"https://arxiv.org/pdf/2312.07586v2.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2401.00663v1","updated":"2024-01-01T04:24:48Z","published":"2024-01-01T04:24:48Z","title":"1st Place Solution for 5th LSVOS Challenge: Referring Video Object\n  Segmentation","summary":"  The recent transformer-based models have dominated the Referring Video Object\nSegmentation (RVOS) task due to the superior performance. Most prior works\nadopt unified DETR framework to generate segmentation masks in\nquery-to-instance manner. In this work, we integrate strengths of that leading\nRVOS models to build up an effective paradigm. We first obtain binary mask\nsequences from the RVOS models. To improve the consistency and quality of\nmasks, we propose Two-Stage Multi-Model Fusion strategy. Each stage rationally\nensembles RVOS models based on framework design as well as training strategy,\nand leverages different video object segmentation (VOS) models to enhance mask\ncoherence by object propagation mechanism. Our method achieves 75.7% J&F on\nRef-Youtube-VOS validation set and 70% J&F on test set, which ranks 1st place\non 5th Large-scale Video Object Segmentation Challenge (ICCV 2023) track 3.\nCode is available at https://github.com/RobertLuo1/iccv2023_RVOS_Challenge.\n","authors":["Zhuoyan Luo","Yicheng Xiao","Yong Liu","Yitong Wang","Yansong Tang","Xiu Li","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2401.00663v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00657v1","updated":"2024-01-01T04:01:40Z","published":"2024-01-01T04:01:40Z","title":"Optimizing ADMM and Over-Relaxed ADMM Parameters for Linear Quadratic\n  Problems","summary":"  The Alternating Direction Method of Multipliers (ADMM) has gained significant\nattention across a broad spectrum of machine learning applications.\nIncorporating the over-relaxation technique shows potential for enhancing the\nconvergence rate of ADMM. However, determining optimal algorithmic parameters,\nincluding both the associated penalty and relaxation parameters, often relies\non empirical approaches tailored to specific problem domains and contextual\nscenarios. Incorrect parameter selection can significantly hinder ADMM's\nconvergence rate. To address this challenge, in this paper we first propose a\ngeneral approach to optimize the value of penalty parameter, followed by a\nnovel closed-form formula to compute the optimal relaxation parameter in the\ncontext of linear quadratic problems (LQPs). We then experimentally validate\nour parameter selection methods through random instantiations and diverse\nimaging applications, encompassing diffeomorphic image registration, image\ndeblurring, and MRI reconstruction.\n","authors":["Jintao Song","Wenqi Lu","Yunwen Lei","Yuchao Tang","Zhenkuan Pan","Jinming Duan"],"pdf_url":"https://arxiv.org/pdf/2401.00657v1.pdf","comment":"Accepted to AAAI 2024"},{"id":"http://arxiv.org/abs/2401.00653v1","updated":"2024-01-01T03:45:07Z","published":"2024-01-01T03:45:07Z","title":"PROMPT-IML: Image Manipulation Localization with Pre-trained Foundation\n  Models Through Prompt Tuning","summary":"  Deceptive images can be shared in seconds with social networking services,\nposing substantial risks. Tampering traces, such as boundary artifacts and\nhigh-frequency information, have been significantly emphasized by massive\nnetworks in the Image Manipulation Localization (IML) field. However, they are\nprone to image post-processing operations, which limit the generalization and\nrobustness of existing methods. We present a novel Prompt-IML framework. We\nobserve that humans tend to discern the authenticity of an image based on both\nsemantic and high-frequency information, inspired by which, the proposed\nframework leverages rich semantic knowledge from pre-trained visual foundation\nmodels to assist IML. We are the first to design a framework that utilizes\nvisual foundation models specially for the IML task. Moreover, we design a\nFeature Alignment and Fusion module to align and fuse features of semantic\nfeatures with high-frequency features, which aims at locating tampered regions\nfrom multiple perspectives. Experimental results demonstrate that our model can\nachieve better performance on eight typical fake image datasets and outstanding\nrobustness.\n","authors":["Xuntao Liu","Yuzhou Yang","Qichao Ying","Zhenxing Qian","Xinpeng Zhang","Sheng Li"],"pdf_url":"https://arxiv.org/pdf/2401.00653v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2401.00652v1","updated":"2024-01-01T03:40:07Z","published":"2024-01-01T03:40:07Z","title":"From Covert Hiding to Visual Editing: Robust Generative Video\n  Steganography","summary":"  Traditional video steganography methods are based on modifying the covert\nspace for embedding, whereas we propose an innovative approach that embeds\nsecret message within semantic feature for steganography during the video\nediting process. Although existing traditional video steganography methods\ndisplay a certain level of security and embedding capacity, they lack adequate\nrobustness against common distortions in online social networks (OSNs). In this\npaper, we introduce an end-to-end robust generative video steganography network\n(RoGVS), which achieves visual editing by modifying semantic feature of videos\nto embed secret message. We employ face-swapping scenario to showcase the\nvisual editing effects. We first design a secret message embedding module to\nadaptively hide secret message into the semantic feature of videos. Extensive\nexperiments display that the proposed RoGVS method applied to facial video\ndatasets demonstrate its superiority over existing video and image\nsteganography techniques in terms of both robustness and capacity.\n","authors":["Xueying Mao","Xiaoxiao Hu","Wanli Peng","Zhenliang Gan","Qichao Ying","Zhenxing Qian","Sheng Li","Xinpeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.00652v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2401.00639v1","updated":"2024-01-01T02:35:13Z","published":"2024-01-01T02:35:13Z","title":"Geometry Depth Consistency in RGBD Relative Pose Estimation","summary":"  Relative pose estimation for RGBD cameras is crucial in a number of\napplications. Previous approaches either rely on the RGB aspect of the images\nto estimate pose thus not fully making use of depth in the estimation process\nor estimate pose from the 3D cloud of points that each image produces, thus not\nmaking full use of RGB information. This paper shows that if one pair of\ncorrespondences is hypothesized from the RGB-based ranked-ordered\ncorrespondence list, then the space of remaining correspondences is restricted\nto corresponding pairs of curves nested around the hypothesized correspondence,\nimplicitly capturing depth consistency. This simple Geometric Depth Constraint\n(GDC) significantly reduces potential matches. In effect this becomes a filter\non possible correspondences that helps reduce the number of outliers and thus\nexpedites RANSAC significantly. As such, the same budget of time allows for\nmore RANSAC iterations and therefore additional robustness and a significant\nspeedup. In addition, the paper proposed a Nested RANSAC approach that also\nspeeds up the process, as shown through experiments on TUM, ICL-NUIM, and RGBD\nScenes v2 datasets.\n","authors":["Sourav Kumar","Chiang-Heng Chien","Benjamin Kimia"],"pdf_url":"https://arxiv.org/pdf/2401.00639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00912v1","updated":"2024-01-01T02:29:59Z","published":"2024-01-01T02:29:59Z","title":"ScatterFormer: Efficient Voxel Transformer with Scattered Linear\n  Attention","summary":"  Window-based transformers have demonstrated strong ability in large-scale\npoint cloud understanding by capturing context-aware representations with\naffordable attention computation in a more localized manner. However, because\nof the sparse nature of point clouds, the number of voxels per window varies\nsignificantly. Current methods partition the voxels in each window into\nmultiple subsets of equal size, which cost expensive overhead in sorting and\npadding the voxels, making them run slower than sparse convolution based\nmethods. In this paper, we present ScatterFormer, which, for the first time to\nour best knowledge, could directly perform attention on voxel sets with\nvariable length. The key of ScatterFormer lies in the innovative Scatter Linear\nAttention (SLA) module, which leverages the linear attention mechanism to\nprocess in parallel all voxels scattered in different windows. Harnessing the\nhierarchical computation units of the GPU and matrix blocking algorithm, we\nreduce the latency of the proposed SLA module to less than 1 ms on moderate\nGPUs. Besides, we develop a cross-window interaction module to simultaneously\nenhance the local representation and allow the information flow across windows,\neliminating the need for window shifting. Our proposed ScatterFormer\ndemonstrates 73 mAP (L2) on the large-scale Waymo Open Dataset and 70.5 NDS on\nthe NuScenes dataset, running at an outstanding detection rate of 28 FPS. Code\nis available at https://github.com/skyhehe123/ScatterFormer\n","authors":["Chenhang He","Ruihuang Li","Guowen Zhang","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.00912v1.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2401.00617v1","updated":"2024-01-01T00:10:58Z","published":"2024-01-01T00:10:58Z","title":"Towards Improved Proxy-based Deep Metric Learning via Data-Augmented\n  Domain Adaptation","summary":"  Deep Metric Learning (DML) plays an important role in modern computer vision\nresearch, where we learn a distance metric for a set of image representations.\nRecent DML techniques utilize the proxy to interact with the corresponding\nimage samples in the embedding space. However, existing proxy-based DML methods\nfocus on learning individual proxy-to-sample distance while the overall\ndistribution of samples and proxies lacks attention. In this paper, we present\na novel proxy-based DML framework that focuses on aligning the sample and proxy\ndistributions to improve the efficiency of proxy-based DML losses.\nSpecifically, we propose the Data-Augmented Domain Adaptation (DADA) method to\nadapt the domain gap between the group of samples and proxies. To the best of\nour knowledge, we are the first to leverage domain adaptation to boost the\nperformance of proxy-based DML. We show that our method can be easily plugged\ninto existing proxy-based DML losses. Our experiments on benchmarks, including\nthe popular CUB-200-2011, CARS196, Stanford Online Products, and In-Shop\nClothes Retrieval, show that our learning algorithm significantly improves the\nexisting proxy losses and achieves superior results compared to the existing\nmethods.\n","authors":["Li Ren","Chen Chen","Liqiang Wang","Kien Hua"],"pdf_url":"https://arxiv.org/pdf/2401.00617v1.pdf","comment":"Accepted by AAAI 2024"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2207.00109v2","updated":"2024-01-01T21:27:44Z","published":"2022-06-30T21:38:00Z","title":"Ranking In Generalized Linear Bandits","summary":"  We study the ranking problem in generalized linear bandits. At each time, the\nlearning agent selects an ordered list of items and observes stochastic\noutcomes. In recommendation systems, displaying an ordered list of the most\nattractive items is not always optimal as both position and item dependencies\nresult in a complex reward function. A very naive example is the lack of\ndiversity when all the most attractive items are from the same category. We\nmodel the position and item dependencies in the ordered list and design UCB and\nThompson Sampling type algorithms for this problem. Our work generalizes\nexisting studies in several directions, including position dependencies where\nposition discount is a particular case, and connecting the ranking problem to\ngraph theory.\n","authors":["Amitis Shidani","George Deligiannidis","Arnaud Doucet"],"pdf_url":"https://arxiv.org/pdf/2207.00109v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.15790v2","updated":"2024-01-01T20:21:21Z","published":"2023-10-24T12:39:30Z","title":"A statistical significance testing approach for measuring term\n  burstiness with applications to domain-specific terminology extraction","summary":"  A term in a corpus is said to be ``bursty'' (or overdispersed) when its\noccurrences are concentrated in few out of many documents. In this paper, we\npropose Residual Inverse Collection Frequency (RICF), a statistical\nsignificance test inspired heuristic for quantifying term burstiness. The\nchi-squared test is, to our knowledge, the sole test of statistical\nsignificance among existing term burstiness measures. Chi-squared test term\nburstiness scores are computed from the collection frequency statistic (i.e.,\nthe proportion that a specified term constitutes in relation to all terms\nwithin a corpus). However, the document frequency of a term (i.e., the\nproportion of documents within a corpus in which a specific term occurs) is\nexploited by certain other widely used term burstiness measures. RICF addresses\nthis shortcoming of the chi-squared test by virtue of its term burstiness\nscores systematically incorporating both the collection frequency and document\nfrequency statistics. We evaluate the RICF measure on a domain-specific\ntechnical terminology extraction task using the GENIA Term corpus benchmark,\nwhich comprises 2,000 annotated biomedical article abstracts. RICF generally\noutperformed the chi-squared test in terms of precision at k score with percent\nimprovements of 0.00% (P@10), 6.38% (P@50), 6.38% (P@100), 2.27% (P@500), 2.61%\n(P@1000), and 1.90% (P@5000). Furthermore, RICF performance was competitive\nwith the performances of other well-established measures of term burstiness.\nBased on these findings, we consider our contributions in this paper as a\npromising starting point for future exploration in leveraging statistical\nsignificance testing in text analysis.\n","authors":["Samuel Sarria Hurtado","Todd Mullen","Taku Onodera","Paul Sheridan"],"pdf_url":"https://arxiv.org/pdf/2310.15790v2.pdf","comment":"19 pages, 1 figure, 6 tables"},{"id":"http://arxiv.org/abs/2401.00797v1","updated":"2024-01-01T15:57:15Z","published":"2024-01-01T15:57:15Z","title":"Distillation is All You Need for Practically Using Different Pre-trained\n  Recommendation Models","summary":"  Pre-trained recommendation models (PRMs) have attracted widespread attention\nrecently. However, their totally different model structure, huge model size and\ncomputation cost hinder their application in practical recommender systems.\nHence, it is highly essential to explore how to practically utilize PRMs in\nreal-world recommendations. In this paper, we propose a novel joint knowledge\ndistillation from different pre-trained recommendation models named PRM-KD for\nrecommendation, which takes full advantages of diverse PRMs as teacher models\nfor enhancing student models efficiently. Specifically, PRM-KD jointly distills\ndiverse informative knowledge from multiple representative PRMs such as\nUniSRec, Recformer, and UniM^2Rec. The knowledge from the above PRMs are then\nsmartly integrated into the student recommendation model considering their\nconfidence and consistency. We further verify the universality of PRM-KD with\nvarious types of student models, including sequential recommendation, feature\ninteraction, and graph-based models. Extensive experiments on five real-world\ndatasets demonstrate the effectiveness and efficacy of PRM-KD, which could be\nviewed as an economical shortcut in practically and conveniently making full\nuse of different PRMs in online systems.\n","authors":["Wenqi Sun","Ruobing Xie","Junjie Zhang","Wayne Xin Zhao","Leyu Lin","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2401.00797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00775v1","updated":"2024-01-01T14:41:10Z","published":"2024-01-01T14:41:10Z","title":"Recent Advances in Text Analysis","summary":"  Text analysis is an interesting research area in data science and has various\napplications, such as in artificial intelligence, biomedical research, and\nengineering. We review popular methods for text analysis, ranging from topic\nmodeling to the recent neural language models. In particular, we review\nTopic-SCORE, a statistical approach to topic modeling, and discuss how to use\nit to analyze MADStat - a dataset on statistical publications that we collected\nand cleaned.\n  The application of Topic-SCORE and other methods on MADStat leads to\ninteresting findings. For example, $11$ representative topics in statistics are\nidentified. For each journal, the evolution of topic weights over time can be\nvisualized, and these results are used to analyze the trends in statistical\nresearch. In particular, we propose a new statistical model for ranking the\ncitation impacts of $11$ topics, and we also build a cross-topic citation graph\nto illustrate how research results on different topics spread to one another.\n  The results on MADStat provide a data-driven picture of the statistical\nresearch in $1975$--$2015$, from a text analysis perspective.\n","authors":["Zheng Tracy Ke","Pengsheng Ji","Jiashun Jin","Wanshan Li"],"pdf_url":"https://arxiv.org/pdf/2401.00775v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00737v1","updated":"2024-01-01T12:30:46Z","published":"2024-01-01T12:30:46Z","title":"Searching, fast and slow, through product catalogs","summary":"  String matching algorithms in the presence of abbreviations, such as in Stock\nKeeping Unit (SKU) product catalogs, remains a relatively unexplored topic. In\nthis paper, we present a unified architecture for SKU search that provides both\na real-time suggestion system (based on a Trie data structure) as well as a\nlower latency search system (making use of character level TF-IDF in\ncombination with language model vector embeddings) where users initiate the\nsearch process explicitly. We carry out ablation studies that justify designing\na complex search system composed of multiple components to address the delicate\ntrade-off between speed and accuracy. Using SKU search in the Dynamics CRM as\nan example, we show how our system vastly outperforms, in all aspects, the\nresults provided by the default search engine. Finally, we show how SKU\ndescriptions may be enhanced via generative text models (using gpt-3.5-turbo)\nso that the consumers of the search results may get more context and a\ngenerally better experience when presented with the results of their SKU\nsearch.\n","authors":["Dayananda Ubrangala","Juhi Sharma","Sharath Kumar Rangappa","Kiran R","Ravi Prasad Kondapalli","Laurent Boué"],"pdf_url":"https://arxiv.org/pdf/2401.00737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10661v2","updated":"2024-01-01T06:42:06Z","published":"2023-12-17T09:31:47Z","title":"Wikiformer: Pre-training with Structured Information of Wikipedia for\n  Ad-hoc Retrieval","summary":"  With the development of deep learning and natural language processing\ntechniques, pre-trained language models have been widely used to solve\ninformation retrieval (IR) problems. Benefiting from the pre-training and\nfine-tuning paradigm, these models achieve state-of-the-art performance. In\nprevious works, plain texts in Wikipedia have been widely used in the\npre-training stage. However, the rich structured information in Wikipedia, such\nas the titles, abstracts, hierarchical heading (multi-level title) structure,\nrelationship between articles, references, hyperlink structures, and the\nwriting organizations, has not been fully explored. In this paper, we devise\nfour pre-training objectives tailored for IR tasks based on the structured\nknowledge of Wikipedia. Compared to existing pre-training methods, our approach\ncan better capture the semantic knowledge in the training corpus by leveraging\nthe human-edited structured data from Wikipedia. Experimental results on\nmultiple IR benchmark datasets show the superior performance of our model in\nboth zero-shot and fine-tuning settings compared to existing strong retrieval\nbaselines. Besides, experimental results in biomedical and legal domains\ndemonstrate that our approach achieves better performance in vertical domains\ncompared to previous models, especially in scenarios where long text similarity\nmatching is needed.\n","authors":["Weihang Su","Qingyao Ai","Xiangsheng Li","Jia Chen","Yiqun Liu","Xiaolong Wu","Shengluan Hou"],"pdf_url":"https://arxiv.org/pdf/2312.10661v2.pdf","comment":"Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2401.00974v1","updated":"2024-01-01T23:33:56Z","published":"2024-01-01T23:33:56Z","title":"Downstream Task-Oriented Generative Model Selections on Synthetic Data\n  Training for Fraud Detection Models","summary":"  Devising procedures for downstream task-oriented generative model selections\nis an unresolved problem of practical importance. Existing studies focused on\nthe utility of a single family of generative models. They provided limited\ninsights on how synthetic data practitioners select the best family generative\nmodels for synthetic training tasks given a specific combination of machine\nlearning model class and performance metric. In this paper, we approach the\ndownstream task-oriented generative model selections problem in the case of\ntraining fraud detection models and investigate the best practice given\ndifferent combinations of model interpretability and model performance\nconstraints. Our investigation supports that, while both Neural\nNetwork(NN)-based and Bayesian Network(BN)-based generative models are both\ngood to complete synthetic training task under loose model interpretability\nconstrain, the BN-based generative models is better than NN-based when\nsynthetic training fraud detection model under strict model interpretability\nconstrain. Our results provides practical guidance for machine learning\npractitioner who is interested in replacing their training dataset from real to\nsynthetic, and shed lights on more general downstream task-oriented generative\nmodel selection problems.\n","authors":["Yinan Cheng","Chi-Hua Wang","Vamsi K. Potluru","Tucker Balch","Guang Cheng"],"pdf_url":"https://arxiv.org/pdf/2401.00974v1.pdf","comment":"The following article has been accepted by ICAIF22, Synthetic Data\n  for AI in Finance; see\n  https://sites.google.com/view/icaif-synthetic-2022/program"},{"id":"http://arxiv.org/abs/2401.00973v1","updated":"2024-01-01T23:30:31Z","published":"2024-01-01T23:30:31Z","title":"Facebook Report on Privacy of fNIRS data","summary":"  The primary goal of this project is to develop privacy-preserving machine\nlearning model training techniques for fNIRS data. This project will build a\nlocal model in a centralized setting with both differential privacy (DP) and\ncertified robustness. It will also explore collaborative federated learning to\ntrain a shared model between multiple clients without sharing local fNIRS\ndatasets. To prevent unintentional private information leakage of such clients'\nprivate datasets, we will also implement DP in the federated learning setting.\n","authors":["Md Imran Hossen","Sai Venkatesh Chilukoti","Liqun Shan","Vijay Srinivas Tida","Xiali Hei"],"pdf_url":"https://arxiv.org/pdf/2401.00973v1.pdf","comment":"15 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2401.00972v1","updated":"2024-01-01T23:25:48Z","published":"2024-01-01T23:25:48Z","title":"Robust Meta-Model for Predicting the Need for Blood Transfusion in\n  Non-traumatic ICU Patients","summary":"  Objective: Blood transfusions, crucial in managing anemia and coagulopathy in\nICU settings, require accurate prediction for effective resource allocation and\npatient risk assessment. However, existing clinical decision support systems\nhave primarily targeted a particular patient demographic with unique medical\nconditions and focused on a single type of blood transfusion. This study aims\nto develop an advanced machine learning-based model to predict the probability\nof transfusion necessity over the next 24 hours for a diverse range of\nnon-traumatic ICU patients.\n  Methods: We conducted a retrospective cohort study on 72,072 adult\nnon-traumatic ICU patients admitted to a high-volume US metropolitan academic\nhospital between 2016 and 2020. We developed a meta-learner and various machine\nlearning models to serve as predictors, training them annually with four-year\ndata and evaluating on the fifth, unseen year, iteratively over five years.\n  Results: The experimental results revealed that the meta-model surpasses the\nother models in different development scenarios. It achieved notable\nperformance metrics, including an Area Under the Receiver Operating\nCharacteristic (AUROC) curve of 0.97, an accuracy rate of 0.93, and an F1-score\nof 0.89 in the best scenario.\n  Conclusion: This study pioneers the use of machine learning models for\npredicting blood transfusion needs in a diverse cohort of critically ill\npatients. The findings of this evaluation confirm that our model not only\npredicts transfusion requirements effectively but also identifies key\nbiomarkers for making transfusion decisions.\n","authors":["Alireza Rafiei","Ronald Moore","Tilendra Choudhary","Curtis Marshall","Geoffrey Smith","John D. Roback","Ravi M. Patel","Cassandra D. Josephson","Rishikesan Kamaleswaran"],"pdf_url":"https://arxiv.org/pdf/2401.00972v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04469v2","updated":"2024-01-01T23:18:29Z","published":"2023-12-07T17:41:44Z","title":"On the Learnability of Watermarks for Language Models","summary":"  Watermarking of language model outputs enables statistical detection of\nmodel-generated text, which has many applications in the responsible deployment\nof language models. Existing watermarking strategies operate by altering the\ndecoder of an existing language model, and the ability for a language model to\ndirectly learn to generate the watermark would have significant implications\nfor the real-world deployment of watermarks. First, learned watermarks could be\nused to build open models that naturally generate watermarked text, allowing\nfor open models to benefit from watermarking. Second, if watermarking is used\nto determine the provenance of generated text, an adversary can hurt the\nreputation of a victim model by spoofing its watermark and generating damaging\nwatermarked text. To investigate the learnability of watermarks, we propose\nwatermark distillation, which trains a student model to behave like a teacher\nmodel that uses decoding-based watermarking. We test our approach on three\ndistinct decoding-based watermarking strategies and various hyperparameter\nsettings, finding that models can learn to generate watermarked text with high\ndetectability. We also find limitations to learnability, including the loss of\nwatermarking capabilities under fine-tuning on normal text and high sample\ncomplexity when learning low-distortion watermarks.\n","authors":["Chenchen Gu","Xiang Lisa Li","Percy Liang","Tatsunori Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2312.04469v2.pdf","comment":"22 pages, 6 figures"},{"id":"http://arxiv.org/abs/2304.14274v4","updated":"2024-01-01T22:49:19Z","published":"2023-04-25T09:40:47Z","title":"When Do Graph Neural Networks Help with Node Classification?\n  Investigating the Impact of Homophily Principle on Node Distinguishability","summary":"  Homophily principle, i.e., nodes with the same labels are more likely to be\nconnected, has been believed to be the main reason for the performance\nsuperiority of Graph Neural Networks (GNNs) over Neural Networks on node\nclassification tasks. Recent research suggests that, even in the absence of\nhomophily, the advantage of GNNs still exists as long as nodes from the same\nclass share similar neighborhood patterns. However, this argument only\nconsiders intra-class Node Distinguishability (ND) but neglects inter-class ND,\nwhich provides incomplete understanding of homophily on GNNs. In this paper, we\nfirst demonstrate such deficiency with examples and argue that an ideal\nsituation for ND is to have smaller intra-class ND than inter-class ND. To\nformulate this idea and study ND deeply, we propose Contextual Stochastic Block\nModel for Homophily (CSBM-H) and define two metrics, Probabilistic Bayes Error\n(PBE) and negative generalized Jeffreys divergence, to quantify ND. With the\nmetrics, we visualize and analyze how graph filters, node degree distributions\nand class variances influence ND, and investigate the combined effect of intra-\nand inter-class ND. Besides, we discovered the mid-homophily pitfall, which\noccurs widely in graph datasets. Furthermore, we verified that, in real-work\ntasks, the superiority of GNNs is indeed closely related to both intra- and\ninter-class ND regardless of homophily levels. Grounded in this observation, we\npropose a new hypothesis-testing based performance metric beyond homophily,\nwhich is non-linear, feature-based and can provide statistical threshold value\nfor GNNs' the superiority. Experiments indicate that it is significantly more\neffective than the existing homophily metrics on revealing the advantage and\ndisadvantage of graph-aware modes on both synthetic and benchmark real-world\ndatasets.\n","authors":["Sitao Luan","Chenqing Hua","Minkai Xu","Qincheng Lu","Jiaqi Zhu","Xiao-Wen Chang","Jie Fu","Jure Leskovec","Doina Precup"],"pdf_url":"https://arxiv.org/pdf/2304.14274v4.pdf","comment":"Accepted by 37th Conference on Neural Information Processing Systems\n  (NeurIPS 2023)"},{"id":"http://arxiv.org/abs/2401.00965v1","updated":"2024-01-01T22:34:14Z","published":"2024-01-01T22:34:14Z","title":"Improve Fidelity and Utility of Synthetic Credit Card Transaction Time\n  Series from Data-centric Perspective","summary":"  Exploring generative model training for synthetic tabular data, specifically\nin sequential contexts such as credit card transaction data, presents\nsignificant challenges. This paper addresses these challenges, focusing on\nattaining both high fidelity to actual data and optimal utility for machine\nlearning tasks. We introduce five pre-processing schemas to enhance the\ntraining of the Conditional Probabilistic Auto-Regressive Model (CPAR),\ndemonstrating incremental improvements in the synthetic data's fidelity and\nutility. Upon achieving satisfactory fidelity levels, our attention shifts to\ntraining fraud detection models tailored for time-series data, evaluating the\nutility of the synthetic data. Our findings offer valuable insights and\npractical guidelines for synthetic data practitioners in the finance sector,\ntransitioning from real to synthetic datasets for training purposes, and\nilluminating broader methodologies for synthesizing credit card transaction\ntime series.\n","authors":["Din-Yin Hsieh","Chi-Hua Wang","Guang Cheng"],"pdf_url":"https://arxiv.org/pdf/2401.00965v1.pdf","comment":"The following article has been accepted by 2nd Workshop on Synthetic\n  Data for AI in Finance; see\n  https://sites.google.com/view/icaif-synthetic/home"},{"id":"http://arxiv.org/abs/2401.00964v1","updated":"2024-01-01T22:27:59Z","published":"2024-01-01T22:27:59Z","title":"Data Augmentation Techniques for Cross-Domain WiFi CSI-based Human\n  Activity Recognition","summary":"  The recognition of human activities based on WiFi Channel State Information\n(CSI) enables contactless and visual privacy-preserving sensing in indoor\nenvironments. However, poor model generalization, due to varying environmental\nconditions and sensing hardware, is a well-known problem in this space. To\naddress this issue, in this work, data augmentation techniques commonly used in\nimage-based learning are applied to WiFi CSI to investigate their effects on\nmodel generalization performance in cross-scenario and cross-system settings.\nIn particular, we focus on the generalization between line-of-sight (LOS) and\nnon-line-of-sight (NLOS) through-wall scenarios, as well as on the\ngeneralization between different antenna systems, which remains under-explored.\nWe collect and make publicly available a dataset of CSI amplitude spectrograms\nof human activities. Utilizing this data, an ablation study is conducted in\nwhich activity recognition models based on the EfficientNetV2 architecture are\ntrained, allowing us to assess the effects of each augmentation on model\ngeneralization performance. The gathered results show that specific\ncombinations of simple data augmentation techniques applied to CSI amplitude\ndata can significantly improve cross-scenario and cross-system generalization.\n","authors":["Julian Strohmayer","Martin Kampel"],"pdf_url":"https://arxiv.org/pdf/2401.00964v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00961v1","updated":"2024-01-01T21:41:20Z","published":"2024-01-01T21:41:20Z","title":"Automated Model Selection for Tabular Data","summary":"  Structured data in the form of tabular datasets contain features that are\ndistinct and discrete, with varying individual and relative importances to the\ntarget. Combinations of one or more features may be more predictive and\nmeaningful than simple individual feature contributions. R's mixed effect\nlinear models library allows users to provide such interactive feature\ncombinations in the model design. However, given many features and possible\ninteractions to select from, model selection becomes an exponentially difficult\ntask. We aim to automate the model selection process for predictions on tabular\ndatasets incorporating feature interactions while keeping computational costs\nsmall. The framework includes two distinct approaches for feature selection: a\nPriority-based Random Grid Search and a Greedy Search method. The\nPriority-based approach efficiently explores feature combinations using prior\nprobabilities to guide the search. The Greedy method builds the solution\niteratively by adding or removing features based on their impact. Experiments\non synthetic demonstrate the ability to effectively capture predictive feature\ncombinations.\n","authors":["Avinash Amballa","Anmol Mekala","Gayathri Akkinapalli","Manas Madine","Naga Pavana Priya Yarrabolu","Przemyslaw A. Grabowicz"],"pdf_url":"https://arxiv.org/pdf/2401.00961v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2308.15640v2","updated":"2024-01-01T21:35:10Z","published":"2023-08-29T21:25:24Z","title":"Identifying Constitutive Parameters for Complex Hyperelastic Materials\n  using Physics-Informed Neural Networks","summary":"  Identifying constitutive parameters in engineering and biological materials,\nparticularly those with intricate geometries and mechanical behaviors, remains\na longstanding challenge. The recent advent of Physics-Informed Neural Networks\n(PINNs) offers promising solutions, but current frameworks are often limited to\nbasic constitutive laws and encounter practical constraints when combined with\nexperimental data. In this paper, we introduce a robust PINN-based framework\ndesigned to identify material parameters for soft materials, specifically those\nexhibiting complex constitutive behaviors, under large deformation in plane\nstress conditions. Distinctively, our model emphasizes training PINNs with\nmulti-modal synthetic experimental datasets consisting of full-field\ndeformation and loading history, ensuring algorithm robustness even with noisy\ndata. Our results reveal that the PINNs framework can accurately identify\nconstitutive parameters of the incompressible Arruda-Boyce model for samples\nwith intricate geometries, maintaining an error below 5%, even with an\nexperimental noise level of 5%. We believe our framework provides a robust\nmodulus identification approach for complex solids, especially for those with\ngeometrical and constitutive complexity.\n","authors":["Siyuan Song","Hanxun Jin"],"pdf_url":"https://arxiv.org/pdf/2308.15640v2.pdf","comment":"28 pages, 5 figures, 1 table"},{"id":"http://arxiv.org/abs/2207.00109v2","updated":"2024-01-01T21:27:44Z","published":"2022-06-30T21:38:00Z","title":"Ranking In Generalized Linear Bandits","summary":"  We study the ranking problem in generalized linear bandits. At each time, the\nlearning agent selects an ordered list of items and observes stochastic\noutcomes. In recommendation systems, displaying an ordered list of the most\nattractive items is not always optimal as both position and item dependencies\nresult in a complex reward function. A very naive example is the lack of\ndiversity when all the most attractive items are from the same category. We\nmodel the position and item dependencies in the ordered list and design UCB and\nThompson Sampling type algorithms for this problem. Our work generalizes\nexisting studies in several directions, including position dependencies where\nposition discount is a particular case, and connecting the ranking problem to\ngraph theory.\n","authors":["Amitis Shidani","George Deligiannidis","Arnaud Doucet"],"pdf_url":"https://arxiv.org/pdf/2207.00109v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.17760v6","updated":"2024-01-01T21:06:57Z","published":"2023-05-28T16:04:48Z","title":"Language Models are Bounded Pragmatic Speakers: Understanding RLHF from\n  a Bayesian Cognitive Modeling Perspective","summary":"  How do language models \"think\"? This paper formulates a probabilistic\ncognitive model called the bounded pragmatic speaker, which can characterize\nthe operation of different variations of language models. Specifically, we\ndemonstrate that large language models fine-tuned with reinforcement learning\nfrom human feedback (Ouyang et al., 2022) embody a model of thought that\nconceptually resembles a fast-and-slow model (Kahneman, 2011), which\npsychologists have attributed to humans. We discuss the limitations of\nreinforcement learning from human feedback as a fast-and-slow model of thought\nand propose avenues for expanding this framework. In essence, our research\nhighlights the value of adopting a cognitive probabilistic modeling approach to\ngain insights into the comprehension, evaluation, and advancement of language\nmodels.\n","authors":["Khanh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2305.17760v6.pdf","comment":"Proceedings of the First Workshop on Theory of Mind in Communicating\n  Agents at (TOM @ ICML 2023)"},{"id":"http://arxiv.org/abs/2401.00953v1","updated":"2024-01-01T20:33:27Z","published":"2024-01-01T20:33:27Z","title":"Families of costs with zero and nonnegative MTW tensor in optimal\n  transport","summary":"  We compute explicitly the MTW tensor (or cross curvature) for the optimal\ntransport problem on $\\mathbb{R}^n$ with a cost function of form $\\mathsf{c}(x,\ny) = \\mathsf{u}(x^{\\mathfrak{t}}y)$, where $\\mathsf{u}$ is a scalar function\nwith inverse $\\mathsf{s}$, $x^{\\ft}y$ is a nondegenerate bilinear pairing of\nvectors $x, y$ belonging to an open subset of $\\mathbb{R}^n$. The condition\nthat the MTW-tensor vanishes on null vectors under the Kim-McCann metric is a\nfourth-order nonlinear ODE, which could be reduced to a linear ODE of the form\n$\\mathsf{s}^{(2)} - S\\mathsf{s}^{(1)} + P\\mathsf{s} = 0$ with constant\ncoefficients $P$ and $S$. The resulting inverse functions include {\\it Lambert}\nand {\\it generalized inverse hyperbolic\\slash trigonometric} functions. The\nsquare Euclidean metric and $\\log$-type costs are equivalent to instances of\nthese solutions. The optimal map for the family is also explicit. For cost\nfunctions of a similar form on a hyperboloid model of the hyperbolic space and\nunit sphere, we also express this tensor in terms of algebraic expressions in\nderivatives of $\\mathsf{s}$ using the Gauss-Codazzi equation, obtaining new\nfamilies of strictly regular costs for these manifolds, including new families\nof {\\it power function costs}. We analyze the $\\sinh$-type hyperbolic cost,\nproviding examples of $\\mathsf{c}$-convex functions and divergence.\n","authors":["Du Nguyen"],"pdf_url":"https://arxiv.org/pdf/2401.00953v1.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2307.12083v3","updated":"2024-01-01T18:46:22Z","published":"2023-07-22T14:15:29Z","title":"Active Control of Flow over Rotating Cylinder by Multiple Jets using\n  Deep Reinforcement Learning","summary":"  The real power of artificial intelligence appears in reinforcement learning,\nwhich is computationally and physically more sophisticated due to its dynamic\nnature. Rotation and injection are some of the proven ways in active flow\ncontrol for drag reduction on blunt bodies. In this paper, rotation will be\nadded to the cylinder alongside the deep reinforcement learning (DRL)\nalgorithm, which uses multiple controlled jets to reach the maximum possible\ndrag suppression. Characteristics of the DRL code, including controlling\nparameters, their limitations, and optimization of the DRL network for use with\nrotation will be presented. This work will focus on optimizing the number and\npositions of the jets, the sensors location, and the maximum allowed flow rate\nto jets in the form of the maximum allowed flow rate of each actuation and the\ntotal number of them per episode. It is found that combining the rotation and\nDRL is promising since it suppresses the vortex shedding, stabilizes the Karman\nvortex street, and reduces the drag coefficient by up to 49.75%. Also, it will\nbe shown that having more sensors at more locations is not always a good choice\nand the sensor number and location should be determined based on the need of\nthe user and corresponding configuration. Also, allowing the agent to have\naccess to higher flow rates, mostly reduces the performance, except when the\ncylinder rotates. In all cases, the agent can keep the lift coefficient at a\nvalue near zero, or stabilize it at a smaller number.\n","authors":["Kamyar Dobakhti","Jafar Ghazanfarian"],"pdf_url":"https://arxiv.org/pdf/2307.12083v3.pdf","comment":"1st Edit: Parts of the introduction, simulation environment, and the\n  network and reinforcement learning framework have been revised. --- 2nd Edit:\n  Added real-world scenario, possible design of rotating cylinder, sensors\n  limited to the body of the cylinder, Re = 200, compared the results and\n  runtime of a shallow network with a higher number of neurons vs deeper\n  network but lower number of neurons"},{"id":"http://arxiv.org/abs/2302.04062v6","updated":"2024-01-01T18:11:24Z","published":"2023-02-08T13:59:31Z","title":"Machine Learning for Synthetic Data Generation: A Review","summary":"  Machine learning heavily relies on data, but real-world applications often\nencounter various data-related issues. These include data of poor quality,\ninsufficient data points leading to under-fitting of machine learning models,\nand difficulties in data access due to concerns surrounding privacy, safety,\nand regulations. In light of these challenges, the concept of synthetic data\ngeneration emerges as a promising alternative that allows for data sharing and\nutilization in ways that real-world data cannot facilitate. This paper presents\na comprehensive systematic review of existing studies that employ machine\nlearning models for the purpose of generating synthetic data. The review\nencompasses various perspectives, starting with the applications of synthetic\ndata generation, spanning computer vision, speech, natural language processing,\nhealthcare, and business domains. Additionally, it explores different machine\nlearning methods, with particular emphasis on neural network architectures and\ndeep generative models. The paper also addresses the crucial aspects of privacy\nand fairness concerns related to synthetic data generation. Furthermore, this\nstudy identifies the challenges and opportunities prevalent in this emerging\nfield, shedding light on the potential avenues for future research. By delving\ninto the intricacies of synthetic data generation, this paper aims to\ncontribute to the advancement of knowledge and inspire further exploration in\nsynthetic data generation.\n","authors":["Yingzhou Lu","Minjie Shen","Huazheng Wang","Xiao Wang","Capucine van Rechem","Wenqi Wei"],"pdf_url":"https://arxiv.org/pdf/2302.04062v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00828v1","updated":"2024-01-01T17:56:24Z","published":"2024-01-01T17:56:24Z","title":"Multi-Lattice Sampling of Quantum Field Theories via Neural Operators","summary":"  We consider the problem of sampling discrete field configurations $\\phi$ from\nthe Boltzmann distribution $[d\\phi] Z^{-1} e^{-S[\\phi]}$, where $S$ is the\nlattice-discretization of the continuous Euclidean action $\\mathcal S$ of some\nquantum field theory. Since such densities arise as the approximation of the\nunderlying functional density $[\\mathcal D\\phi(x)] \\mathcal Z^{-1} e^{-\\mathcal\nS[\\phi(x)]}$, we frame the task as an instance of operator learning. In\nparticular, we propose to approximate a time-dependent operator $\\mathcal V_t$\nwhose time integral provides a mapping between the functional distributions of\nthe free theory $[\\mathcal D\\phi(x)] \\mathcal Z_0^{-1} e^{-\\mathcal\nS_{0}[\\phi(x)]}$ and of the target theory $[\\mathcal D\\phi(x)]\\mathcal\nZ^{-1}e^{-\\mathcal S[\\phi(x)]}$. Whenever a particular lattice is chosen, the\noperator $\\mathcal V_t$ can be discretized to a finite dimensional,\ntime-dependent vector field $V_t$ which in turn induces a continuous\nnormalizing flow between finite dimensional distributions over the chosen\nlattice. This flow can then be trained to be a diffeormorphism between the\ndiscretized free and target theories $[d\\phi] Z_0^{-1} e^{-S_{0}[\\phi]}$,\n$[d\\phi] Z^{-1}e^{-S[\\phi]}$. We run experiments on the $\\phi^4$-theory to\nexplore to what extent such operator-based flow architectures generalize to\nlattice sizes they were not trained on and show that pretraining on smaller\nlattices can lead to speedup over training only a target lattice size.\n","authors":["Bálint Máté","François Fleuret"],"pdf_url":"https://arxiv.org/pdf/2401.00828v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.09894v3","updated":"2024-01-01T17:55:25Z","published":"2022-08-21T14:39:30Z","title":"Byzantines can also Learn from History: Fall of Centered Clipping in\n  Federated Learning","summary":"  The increasing popularity of the federated learning (FL) framework due to its\nsuccess in a wide range of collaborative learning tasks also induces certain\nsecurity concerns. Among many vulnerabilities, the risk of Byzantine attacks is\nof particular concern, which refers to the possibility of malicious clients\nparticipating in the learning process. Hence, a crucial objective in FL is to\nneutralize the potential impact of Byzantine attacks and to ensure that the\nfinal model is trustable. It has been observed that the higher the variance\namong the clients' models/updates, the more space there is for Byzantine\nattacks to be hidden. As a consequence, by utilizing momentum, and thus,\nreducing the variance, it is possible to weaken the strength of known Byzantine\nattacks. The centered clipping (CC) framework has further shown that the\nmomentum term from the previous iteration, besides reducing the variance, can\nbe used as a reference point to neutralize Byzantine attacks better. In this\nwork, we first expose vulnerabilities of the CC framework, and introduce a\nnovel attack strategy that can circumvent the defences of CC and other robust\naggregators and reduce their test accuracy up to %33 on best-case scenarios in\nimage classification tasks. Then, we propose a new robust and fast defence\nmechanism that is effective against the proposed and other existing Byzantine\nattacks.\n","authors":["Kerem Ozfatura","Emre Ozfatura","Alptekin Kupcu","Deniz Gunduz"],"pdf_url":"https://arxiv.org/pdf/2208.09894v3.pdf","comment":"IEEE Transactions on Information Forensics and Security 2023"},{"id":"http://arxiv.org/abs/2401.00824v1","updated":"2024-01-01T17:48:25Z","published":"2024-01-01T17:48:25Z","title":"Graph-Convolutional Autoencoder Ensembles for the Humanities,\n  Illustrated with a Study of the American Slave Trade","summary":"  We introduce a graph-aware autoencoder ensemble framework, with associated\nformalisms and tooling, designed to facilitate deep learning for scholarship in\nthe humanities. By composing sub-architectures to produce a model isomorphic to\na humanistic domain we maintain interpretability while providing function\nsignatures for each sub-architectural choice, allowing both traditional and\ncomputational researchers to collaborate without disrupting established\npractices. We illustrate a practical application of our approach to a\nhistorical study of the American post-Atlantic slave trade, and make several\nspecific technical contributions: a novel hybrid graph-convolutional\nautoencoder mechanism, batching policies for common graph topologies, and\nmasking techniques for particular use-cases. The effectiveness of the framework\nfor broadening participation of diverse domains is demonstrated by a growing\nsuite of two dozen studies, both collaborations with humanists and established\ntasks from machine learning literature, spanning a variety of fields and data\nmodalities. We make performance comparisons of several different architectural\nchoices and conclude with an ambitious list of imminent next steps for this\nresearch.\n","authors":["Tom Lippincott"],"pdf_url":"https://arxiv.org/pdf/2401.00824v1.pdf","comment":"More in-depth technical companion to \"A general neural ensemble\n  technique to support traditional scholarship\", Digital Humanities 2020"},{"id":"http://arxiv.org/abs/2401.00816v1","updated":"2024-01-01T17:15:42Z","published":"2024-01-01T17:15:42Z","title":"GLIMPSE: Generalized Local Imaging with MLPs","summary":"  Deep learning is the current de facto state of the art in tomographic\nimaging. A common approach is to feed the result of a simple inversion, for\nexample the backprojection, to a convolutional neural network (CNN) which then\ncomputes the reconstruction. Despite strong results on 'in-distribution' test\ndata similar to the training data, backprojection from sparse-view data\ndelocalizes singularities, so these approaches require a large receptive field\nto perform well. As a consequence, they overfit to certain global structures\nwhich leads to poor generalization on out-of-distribution (OOD) samples.\nMoreover, their memory complexity and training time scale unfavorably with\nimage resolution, making them impractical for application at realistic clinical\nresolutions, especially in 3D: a standard U-Net requires a substantial 140GB of\nmemory and 2600 seconds per epoch on a research-grade GPU when training on\n1024x1024 images. In this paper, we introduce GLIMPSE, a local processing\nneural network for computed tomography which reconstructs a pixel value by\nfeeding only the measurements associated with the neighborhood of the pixel to\na simple MLP. While achieving comparable or better performance with successful\nCNNs like the U-Net on in-distribution test data, GLIMPSE significantly\noutperforms them on OOD samples while maintaining a memory footprint almost\nindependent of image resolution; 5GB memory suffices to train on 1024x1024\nimages. Further, we built GLIMPSE to be fully differentiable, which enables\nfeats such as recovery of accurate projection angles if they are out of\ncalibration.\n","authors":["AmirEhsan Khorashadizadeh","Valentin Debarnot","Tianlin Liu","Ivan Dokmanić"],"pdf_url":"https://arxiv.org/pdf/2401.00816v1.pdf","comment":"12 pages, 10 figures"},{"id":"http://arxiv.org/abs/2306.13746v2","updated":"2024-01-01T17:11:34Z","published":"2023-06-23T19:06:50Z","title":"Revisiting inference after prediction","summary":"  Recent work has focused on the very common practice of prediction-based\ninference: that is, (i) using a pre-trained machine learning model to predict\nan unobserved response variable, and then (ii) conducting inference on the\nassociation between that predicted response and some covariates. As pointed out\nby Wang et al. (2020), applying a standard inferential approach in (ii) does\nnot accurately quantify the association between the unobserved (as opposed to\nthe predicted) response and the covariates. In recent work, Wang et al. (2020)\nand Angelopoulos et al. (2023) propose corrections to step (ii) in order to\nenable valid inference on the association between the unobserved response and\nthe covariates. Here, we show that the method proposed by Angelopoulos et al.\n(2023) successfully controls the type 1 error rate and provides confidence\nintervals with correct nominal coverage, regardless of the quality of the\npre-trained machine learning model used to predict the unobserved response.\nHowever, the method proposed by Wang et al. (2020) provides valid inference\nonly under very strong conditions that rarely hold in practice: for instance,\nif the machine learning model perfectly estimates the true regression function\nin the study population of interest.\n","authors":["Keshav Motwani","Daniela Witten"],"pdf_url":"https://arxiv.org/pdf/2306.13746v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.09126v3","updated":"2024-01-01T17:04:58Z","published":"2023-05-16T03:13:55Z","title":"Transfer Learning for Causal Effect Estimation","summary":"  We present a Transfer Causal Learning (TCL) framework when target and source\ndomains share the same covariate/feature spaces, aiming to improve causal\neffect estimation accuracy in limited data. Limited data is very common in\nmedical applications, where some rare medical conditions, such as sepsis, are\nof interest. Our proposed method, named \\texttt{$\\ell_1$-TCL}, incorporates\n$\\ell_1$ regularized TL for nuisance models (e.g., propensity score model); the\nTL estimator of the nuisance parameters is plugged into downstream average\ncausal/treatment effect estimators (e.g., inverse probability weighted\nestimator). We establish non-asymptotic recovery guarantees for the\n\\texttt{$\\ell_1$-TCL} with generalized linear model (GLM) under the sparsity\nassumption in the high-dimensional setting, and demonstrate the empirical\nbenefits of \\texttt{$\\ell_1$-TCL} through extensive numerical simulation for\nGLM and recent neural network nuisance models. Our method is subsequently\nextended to real data and generates meaningful insights consistent with medical\nliterature, a case where all baseline methods fail.\n","authors":["Song Wei","Hanyu Zhang","Ronald Moore","Rishikesan Kamaleswaran","Yao Xie"],"pdf_url":"https://arxiv.org/pdf/2305.09126v3.pdf","comment":"Preliminary version, titled \"Transfer causal learning: Causal effect\n  estimation with knowledge transfer\", has been presented in ICML 3rd Workshop\n  on Interpretable Machine Learning in Healthcare (IMLH), 2023; see the arXiv\n  version in v2"},{"id":"http://arxiv.org/abs/2304.03365v2","updated":"2024-01-01T16:45:55Z","published":"2023-04-06T20:47:09Z","title":"Decision-Focused Model-based Reinforcement Learning for Reward Transfer","summary":"  Decision-focused (DF) model-based reinforcement learning has recently been\nintroduced as a powerful algorithm that can focus on learning the MDP dynamics\nthat are most relevant for obtaining high returns. While this approach\nincreases the agent's performance by directly optimizing the reward, it does so\nby learning less accurate dynamics from a maximum likelihood perspective. We\ndemonstrate that when the reward function is defined by preferences over\nmultiple objectives, the DF model may be sensitive to changes in the objective\npreferences.In this work, we develop the robust decision-focused (RDF)\nalgorithm, which leverages the non-identifiability of DF solutions to learn\nmodels that maximize expected returns while simultaneously learning models that\ntransfer to changes in the preference over multiple objectives. We demonstrate\nthe effectiveness of RDF on two synthetic domains and two healthcare\nsimulators, showing that it significantly improves the robustness of DF model\nlearning to changes in the reward function without compromising training-time\nreturn.\n","authors":["Abhishek Sharma","Sonali Parbhoo","Omer Gottesman","Finale Doshi-Velez"],"pdf_url":"https://arxiv.org/pdf/2304.03365v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00809v1","updated":"2024-01-01T16:34:00Z","published":"2024-01-01T16:34:00Z","title":"A review on different techniques used to combat the non-IID and\n  heterogeneous nature of data in FL","summary":"  Federated Learning (FL) is a machine-learning approach enabling collaborative\nmodel training across multiple decentralized edge devices that hold local data\nsamples, all without exchanging these samples. This collaborative process\noccurs under the supervision of a central server orchestrating the training or\nvia a peer-to-peer network. The significance of FL is particularly pronounced\nin industries such as healthcare and finance, where data privacy holds\nparamount importance. However, training a model under the Federated learning\nsetting brings forth several challenges, with one of the most prominent being\nthe heterogeneity of data distribution among the edge devices. The data is\ntypically non-independently and non-identically distributed (non-IID), thereby\npresenting challenges to model convergence. This report delves into the issues\narising from non-IID and heterogeneous data and explores current algorithms\ndesigned to address these challenges.\n","authors":["Venkataraman Natarajan Iyer"],"pdf_url":"https://arxiv.org/pdf/2401.00809v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12210v2","updated":"2024-01-01T15:52:04Z","published":"2023-08-23T15:50:51Z","title":"ULDP-FL: Federated Learning with Across Silo User-Level Differential\n  Privacy","summary":"  Differentially Private Federated Learning (DP-FL) has garnered attention as a\ncollaborative machine learning approach that ensures formal privacy. Most DP-FL\napproaches ensure DP at the record-level within each silo for cross-silo FL.\nHowever, a single user's data may extend across multiple silos, and the desired\nuser-level DP guarantee for such a setting remains unknown. In this study, we\npresent Uldp-FL, a novel FL framework designed to guarantee user-level DP in\ncross-silo FL where a single user's data may belong to multiple silos. Our\nproposed algorithm directly ensures user-level DP through per-user weighted\nclipping, departing from group-privacy approaches. We provide a theoretical\nanalysis of the algorithm's privacy and utility. Additionally, we enhance the\nutility of the proposed algorithm with an enhanced weighting strategy based on\nuser record distribution and design a novel private protocol that ensures no\nadditional information is revealed to the silos and the server. Experiments on\nreal-world datasets show substantial improvements in our methods in\nprivacy-utility trade-offs under user-level DP compared to baseline methods. To\nthe best of our knowledge, our work is the first FL framework that effectively\nprovides user-level DP in the general cross-silo FL setting.\n","authors":["Fumiyuki Kato","Li Xiong","Shun Takagi","Yang Cao","Masatoshi Yoshikawa"],"pdf_url":"https://arxiv.org/pdf/2308.12210v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00793v1","updated":"2024-01-01T15:40:35Z","published":"2024-01-01T15:40:35Z","title":"SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for\n  Large Language Models","summary":"  With the growing use of large language models hosted on cloud platforms to\noffer inference services, privacy concerns are escalating, especially\nconcerning sensitive data like investment plans and bank account details.\nSecure Multi-Party Computing (SMPC) emerges as a promising solution to protect\nthe privacy of inference data and model parameters. However, the application of\nSMPC in Privacy-Preserving Inference (PPI) for large language models,\nparticularly those based on the Transformer architecture, often leads to\nconsiderable slowdowns or declines in performance. This is largely due to the\nmultitude of nonlinear operations in the Transformer architecture, which are\nnot well-suited to SMPC and are difficult to circumvent or optimize\neffectively. To address this concern, we introduce an advanced optimization\nframework called SecFormer, designed to strike an optimal balance between\nperformance and efficiency in PPI for Transformer models. By implementing\nknowledge distillation techniques, we successfully eliminate the high-cost\nexponential and maximum operations in PPI without sacrificing model\nperformance. Additionally, we have developed a suite of efficient SMPC\nprotocols that utilize segmented polynomials and Goldschmidt's method to handle\nother complex nonlinear functions within PPI, such as GeLU, LayerNorm, and\nSoftmax. Our extensive experiments reveal that SecFormer outperforms MPCFormer\nin performance, showing improvements of $5.6\\%$ and $24.2\\%$ for\nBERT$_{\\text{BASE}}$ and BERT$_{\\text{LARGE}}$, respectively. In terms of\nefficiency, SecFormer is 3.4 and 3.2 times faster than Puma, demonstrating its\neffectiveness and speed.\n","authors":["Jinglong Luo","Yehong Zhang","Jiaqi Zhang","Xin Mu","Hui Wang","Yue Yu","Zenglin Xu"],"pdf_url":"https://arxiv.org/pdf/2401.00793v1.pdf","comment":"12pages, 15figures"},{"id":"http://arxiv.org/abs/2312.09086v2","updated":"2024-01-01T15:21:05Z","published":"2023-12-14T16:17:20Z","title":"COMBHelper: A Neural Approach to Reduce Search Space for Graph\n  Combinatorial Problems","summary":"  Combinatorial Optimization (CO) problems over graphs appear routinely in many\napplications such as in optimizing traffic, viral marketing in social networks,\nand matching for job allocation. Due to their combinatorial nature, these\nproblems are often NP-hard. Existing approximation algorithms and heuristics\nrely on the search space to find the solutions and become time-consuming when\nthis space is large. In this paper, we design a neural method called COMBHelper\nto reduce this space and thus improve the efficiency of the traditional CO\nalgorithms based on node selection. Specifically, it employs a Graph Neural\nNetwork (GNN) to identify promising nodes for the solution set. This pruned\nsearch space is then fed to the traditional CO algorithms. COMBHelper also uses\na Knowledge Distillation (KD) module and a problem-specific boosting module to\nbring further efficiency and efficacy. Our extensive experiments show that the\ntraditional CO algorithms with COMBHelper are at least 2 times faster than\ntheir original versions.\n","authors":["Hao Tian","Sourav Medya","Wei Ye"],"pdf_url":"https://arxiv.org/pdf/2312.09086v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00781v1","updated":"2024-01-01T15:03:14Z","published":"2024-01-01T15:03:14Z","title":"Inferring Heterogeneous Treatment Effects of Crashes on Highway Traffic:\n  A Doubly Robust Causal Machine Learning Approach","summary":"  Highway traffic crashes exert a considerable impact on both transportation\nsystems and the economy. In this context, accurate and dependable emergency\nresponses are crucial for effective traffic management. However, the influence\nof crashes on traffic status varies across diverse factors and may be biased\ndue to selection bias. Therefore, there arises a necessity to accurately\nestimate the heterogeneous causal effects of crashes, thereby providing\nessential insights to facilitate individual-level emergency decision-making.\nThis paper proposes a novel causal machine learning framework to estimate the\ncausal effect of different types of crashes on highway speed. The Neyman-Rubin\nCausal Model (RCM) is employed to formulate this problem from a causal\nperspective. The Conditional Shapley Value Index (CSVI) is proposed based on\ncausal graph theory to filter adverse variables, and the Structural Causal\nModel (SCM) is then adopted to define the statistical estimand for causal\neffects. The treatment effects are estimated by Doubly Robust Learning (DRL)\nmethods, which combine doubly robust causal inference with classification and\nregression machine learning models. Experimental results from 4815 crashes on\nHighway Interstate 5 in Washington State reveal the heterogeneous treatment\neffects of crashes at varying distances and durations. The rear-end crashes\ncause more severe congestion and longer durations than other types of crashes,\nand the sideswipe crashes have the longest delayed impact. Additionally, the\nfindings show that rear-end crashes affect traffic greater at night, while\ncrash to objects has the most significant influence during peak hours.\nStatistical hypothesis tests, error metrics based on matched \"counterfactual\noutcomes\", and sensitive analyses are employed for assessment, and the results\nvalidate the accuracy and effectiveness of our method.\n","authors":["Shuang Li","Ziyuan Pu","Zhiyong Cui","Seunghyeon Lee","Xiucheng Guo","Dong Ngoduy"],"pdf_url":"https://arxiv.org/pdf/2401.00781v1.pdf","comment":"38 pages, 13 figures, 8 tables"},{"id":"http://arxiv.org/abs/2401.00776v1","updated":"2024-01-01T14:45:19Z","published":"2024-01-01T14:45:19Z","title":"Edge Computing based Human-Robot Cognitive Fusion: A Medical Case Study\n  in the Autism Spectrum Disorder Therapy","summary":"  In recent years, edge computing has served as a paradigm that enables many\nfuture technologies like AI, Robotics, IoT, and high-speed wireless sensor\nnetworks (like 5G) by connecting cloud computing facilities and services to the\nend users. Especially in medical and healthcare applications, it provides\nremote patient monitoring and increases voluminous multimedia. From the\nrobotics angle, robot-assisted therapy (RAT) is an active-assistive robotic\ntechnology in rehabilitation robotics, attracting many researchers to study and\nbenefit people with disability like autism spectrum disorder (ASD) children.\nHowever, the main challenge of RAT is that the model capable of detecting the\naffective states of ASD people exists and can recall individual preferences.\nMoreover, involving expert diagnosis and recommendations to guide robots in\nupdating the therapy approach to adapt to different statuses and scenarios is a\ncrucial part of the ASD therapy process. This paper proposes the architecture\nof edge cognitive computing by combining human experts and assisted robots\ncollaborating in the same framework to help ASD patients with long-term\nsupport. By integrating the real-time computing and analysis of a new cognitive\nrobotic model for ASD therapy, the proposed architecture can achieve a seamless\nremote diagnosis, round-the-clock symptom monitoring, emergency warning,\ntherapy alteration, and advanced assistance.\n","authors":["Qin Yang"],"pdf_url":"https://arxiv.org/pdf/2401.00776v1.pdf","comment":"This paper was accepted by the 38th AAAI 2024 workshop: \"Cooperative\n  Multi-Agent Systems Decision-Making and Learning: From Individual Needs to\n  Swarm Intelligence\""},{"id":"http://arxiv.org/abs/2401.00773v1","updated":"2024-01-01T14:34:11Z","published":"2024-01-01T14:34:11Z","title":"Unsupervised Outlier Detection using Random Subspace and Subsampling\n  Ensembles of Dirichlet Process Mixtures","summary":"  Probabilistic mixture models are acknowledged as a valuable tool for\nunsupervised outlier detection owing to their interpretability and intuitive\ngrounding in statistical principles. Within this framework, Dirichlet process\nmixture models emerge as a compelling alternative to conventional finite\nmixture models for both clustering and outlier detection tasks. However,\ndespite their evident advantages, the widespread adoption of Dirichlet process\nmixture models in unsupervised outlier detection has been hampered by\nchallenges related to computational inefficiency and sensitivity to outliers\nduring the construction of detectors. To tackle these challenges, we propose a\nnovel outlier detection method based on ensembles of Dirichlet process Gaussian\nmixtures. The proposed method is a fully unsupervised algorithm that\ncapitalizes on random subspace and subsampling ensembles, not only ensuring\nefficient computation but also enhancing the robustness of the resulting\noutlier detector. Moreover, the proposed method leverages variational inference\nfor Dirichlet process mixtures to ensure efficient and fast computation.\nEmpirical studies with benchmark datasets demonstrate that our method\noutperforms existing approaches for unsupervised outlier detection.\n","authors":["Dongwook Kim","Juyeon Park","Hee Cheol Chung","Seonghyun Jeong"],"pdf_url":"https://arxiv.org/pdf/2401.00773v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.04102v3","updated":"2024-01-01T14:16:18Z","published":"2023-08-08T07:33:49Z","title":"Asynchronous Evolution of Deep Neural Network Architectures","summary":"  Many evolutionary algorithms (EAs) take advantage of parallel evaluation of\ncandidates. However, if evaluation times vary significantly, many worker nodes\n(i.e.,\\ compute clients) are idle much of the time, waiting for the next\ngeneration to be created. Evolutionary neural architecture search (ENAS), a\nclass of EAs that optimizes the architecture and hyperparameters of deep neural\nnetworks, is particularly vulnerable to this issue. This paper proposes a\ngeneric asynchronous evaluation strategy (AES) that is then adapted to work\nwith ENAS. AES increases throughput by maintaining a queue of up to $K$\nindividuals ready to be sent to the workers for evaluation and proceeding to\nthe next generation as soon as $M<<K$ individuals have been evaluated. A\nsuitable value for $M$ is determined experimentally, balancing diversity and\nefficiency. To showcase the generality and power of AES, it was first evaluated\nin eight-line sorting network design (a single-population optimization task\nwith limited evaluation-time variability), achieving an over two-fold speedup.\nNext, it was evaluated in 11-bit multiplexer design (a single-population\ndiscovery task with extended variability), where a 14-fold speedup was\nobserved. It was then scaled up to ENAS for image captioning (a\nmulti-population open-ended-optimization task), resulting in an over two-fold\nspeedup. In all problems, a multifold performance improvement was observed,\nsuggesting that AES is a promising method for parallelizing the evolution of\ncomplex systems with long and variable evaluation times, such as those in ENAS.\n","authors":["Jason Liang","Hormoz Shahrzad","Risto Miikkulainen"],"pdf_url":"https://arxiv.org/pdf/2308.04102v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.08842v3","updated":"2024-01-01T13:56:49Z","published":"2023-04-18T09:13:52Z","title":"UDTIRI: An Online Open-Source Intelligent Road Inspection Benchmark\n  Suite","summary":"  In the nascent domain of urban digital twins (UDT), the prospects for\nleveraging cutting-edge deep learning techniques are vast and compelling.\nParticularly within the specialized area of intelligent road inspection (IRI),\na noticeable gap exists, underscored by the current dearth of dedicated\nresearch efforts and the lack of large-scale well-annotated datasets. To foster\nadvancements in this burgeoning field, we have launched an online open-source\nbenchmark suite, referred to as UDTIRI. Along with this article, we introduce\nthe road pothole detection task, the first online competition published within\nthis benchmark suite. This task provides a well-annotated dataset, comprising\n1,000 RGB images and their pixel/instance-level ground-truth annotations,\ncaptured in diverse real-world scenarios under different illumination and\nweather conditions. Our benchmark provides a systematic and thorough evaluation\nof state-of-the-art object detection, semantic segmentation, and instance\nsegmentation networks, developed based on either convolutional neural networks\nor Transformers. We anticipate that our benchmark will serve as a catalyst for\nthe integration of advanced UDT techniques into IRI. By providing algorithms\nwith a more comprehensive understanding of diverse road conditions, we seek to\nunlock their untapped potential and foster innovation in this critical domain.\n","authors":["Sicen Guo","Jiahang Li","Yi Feng","Dacheng Zhou","Denghuang Zhang","Chen Chen","Shuai Su","Xingyi Zhu","Qijun Chen","Rui Fan"],"pdf_url":"https://arxiv.org/pdf/2304.08842v3.pdf","comment":"Database webpage: https://www.udtiri.com/, Kaggle webpage:\n  https://www.kaggle.com/datasets/jiahangli617/udtiri"},{"id":"http://arxiv.org/abs/2401.00756v1","updated":"2024-01-01T13:52:05Z","published":"2024-01-01T13:52:05Z","title":"MPRE: Multi-perspective Patient Representation Extractor for Disease\n  Prediction","summary":"  Patient representation learning based on electronic health records (EHR) is a\ncritical task for disease prediction. This task aims to effectively extract\nuseful information on dynamic features. Although various existing works have\nachieved remarkable progress, the model performance can be further improved by\nfully extracting the trends, variations, and the correlation between the trends\nand variations in dynamic features. In addition, sparse visit records limit the\nperformance of deep learning models. To address these issues, we propose the\nMulti-perspective Patient Representation Extractor (MPRE) for disease\nprediction. Specifically, we propose Frequency Transformation Module (FTM) to\nextract the trend and variation information of dynamic features in the\ntime-frequency domain, which can enhance the feature representation. In the 2D\nMulti-Extraction Network (2D MEN), we form the 2D temporal tensor based on\ntrend and variation. Then, the correlations between trend and variation are\ncaptured by the proposed dilated operation. Moreover, we propose the\nFirst-Order Difference Attention Mechanism (FODAM) to calculate the\ncontributions of differences in adjacent variations to the disease diagnosis\nadaptively. To evaluate the performance of MPRE and baseline methods, we\nconduct extensive experiments on two real-world public datasets. The experiment\nresults show that MPRE outperforms state-of-the-art baseline methods in terms\nof AUROC and AUPRC.\n","authors":["Ziyue Yu","Jiayi Wang","Wuman Luo","Rita Tse","Giovanni Pau"],"pdf_url":"https://arxiv.org/pdf/2401.00756v1.pdf","comment":"Accepted by ICDM 2023"},{"id":"http://arxiv.org/abs/2401.00755v1","updated":"2024-01-01T13:44:16Z","published":"2024-01-01T13:44:16Z","title":"Saliency-Aware Regularized Graph Neural Network","summary":"  The crux of graph classification lies in the effective representation\nlearning for the entire graph. Typical graph neural networks focus on modeling\nthe local dependencies when aggregating features of neighboring nodes, and\nobtain the representation for the entire graph by aggregating node features.\nSuch methods have two potential limitations: 1) the global node saliency w.r.t.\ngraph classification is not explicitly modeled, which is crucial since\ndifferent nodes may have different semantic relevance to graph classification;\n2) the graph representation directly aggregated from node features may have\nlimited effectiveness to reflect graph-level information. In this work, we\npropose the Saliency-Aware Regularized Graph Neural Network (SAR-GNN) for graph\nclassification, which consists of two core modules: 1) a traditional graph\nneural network serving as the backbone for learning node features and 2) the\nGraph Neural Memory designed to distill a compact graph representation from\nnode features of the backbone. We first estimate the global node saliency by\nmeasuring the semantic similarity between the compact graph representation and\nnode features. Then the learned saliency distribution is leveraged to\nregularize the neighborhood aggregation of the backbone, which facilitates the\nmessage passing of features for salient nodes and suppresses the less relevant\nnodes. Thus, our model can learn more effective graph representation. We\ndemonstrate the merits of SAR-GNN by extensive experiments on seven datasets\nacross various types of graph data. Code will be released.\n","authors":["Wenjie Pei","Weina Xu","Zongze Wu","Weichao Li","Jinfan Wang","Guangming Lu","Xiangrong Wang"],"pdf_url":"https://arxiv.org/pdf/2401.00755v1.pdf","comment":"Accepted by Artificial Intelligence Journal with minor revision"},{"id":"http://arxiv.org/abs/2312.01324v2","updated":"2024-01-01T13:27:15Z","published":"2023-12-03T09:00:31Z","title":"MABViT -- Modified Attention Block Enhances Vision Transformers","summary":"  Recent studies have demonstrated the effectiveness of Gated Linear Units\n(GLU) in enhancing transformer models, particularly in Large Language Models\n(LLMs). Additionally, utilizing a parallel configuration within each\nTransformer block rather than the conventional serialized method has been\nrevealed to accelerate the training of LLMs without significantly impacting\nperformance. However, when the MLP and attention block were run in parallel for\nthe image classification task, we observed a noticeable decline in performance.\nWe propose a novel transformer variant that integrates non-linearity within the\nattention block to tackle this problem. We implemented the GLU-based activation\nfunction on the Value tensor, and this new technique surpasses the current\nstate-of-the-art S/16 variant of Vision Transformers by 0.6% on the ImageNet-1K\ndataset while utilizing fewer parameters. It also supersedes the B/16 variant\nwhile using only half the parameters. Furthermore, we provide results with the\nGELU activation function variant to confirm our assertions. Lastly, we showcase\nthat the MABViT variants exhibit greater potential when utilized in deep\ntransformers compared to the standard architecture.\n","authors":["Mahesh Ramesh","Aswinkumar Ramkumar"],"pdf_url":"https://arxiv.org/pdf/2312.01324v2.pdf","comment":"Accepted at Deployable AI Workshop, AAAI Conference"},{"id":"http://arxiv.org/abs/2306.15546v2","updated":"2024-01-01T13:07:10Z","published":"2023-06-27T15:15:55Z","title":"When Foundation Model Meets Federated Learning: Motivations, Challenges,\n  and Future Directions","summary":"  The intersection of the Foundation Model (FM) and Federated Learning (FL)\nprovides mutual benefits, presents a unique opportunity to unlock new\npossibilities in AI research, and address critical challenges in AI and\nreal-world applications. FL expands the availability of data for FMs and\nenables computation sharing, distributing the training process and reducing the\nburden on FL participants. It promotes collaborative FM development,\ndemocratizing the process and fostering inclusivity and innovation. On the\nother hand, FM, with its enormous size, pre-trained knowledge, and exceptional\nperformance, serves as a robust starting point for FL, facilitating faster\nconvergence and better performance under non-iid data. Additionally, leveraging\nFM to generate synthetic data enriches data diversity, reduces overfitting, and\npreserves privacy. By examining the interplay between FL and FM, this paper\naims to deepen the understanding of their synergistic relationship,\nhighlighting the motivations, challenges, and future directions. Through an\nexploration of the challenges faced by FL and FM individually and their\ninterconnections, we aim to inspire future research directions that can further\nenhance both fields, driving advancements and propelling the development of\nprivacy-preserving and scalable AI systems.\n","authors":["Weiming Zhuang","Chen Chen","Lingjuan Lyu"],"pdf_url":"https://arxiv.org/pdf/2306.15546v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00737v1","updated":"2024-01-01T12:30:46Z","published":"2024-01-01T12:30:46Z","title":"Searching, fast and slow, through product catalogs","summary":"  String matching algorithms in the presence of abbreviations, such as in Stock\nKeeping Unit (SKU) product catalogs, remains a relatively unexplored topic. In\nthis paper, we present a unified architecture for SKU search that provides both\na real-time suggestion system (based on a Trie data structure) as well as a\nlower latency search system (making use of character level TF-IDF in\ncombination with language model vector embeddings) where users initiate the\nsearch process explicitly. We carry out ablation studies that justify designing\na complex search system composed of multiple components to address the delicate\ntrade-off between speed and accuracy. Using SKU search in the Dynamics CRM as\nan example, we show how our system vastly outperforms, in all aspects, the\nresults provided by the default search engine. Finally, we show how SKU\ndescriptions may be enhanced via generative text models (using gpt-3.5-turbo)\nso that the consumers of the search results may get more context and a\ngenerally better experience when presented with the results of their SKU\nsearch.\n","authors":["Dayananda Ubrangala","Juhi Sharma","Sharath Kumar Rangappa","Kiran R","Ravi Prasad Kondapalli","Laurent Boué"],"pdf_url":"https://arxiv.org/pdf/2401.00737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00736v1","updated":"2024-01-01T12:25:57Z","published":"2024-01-01T12:25:57Z","title":"Diffusion Models, Image Super-Resolution And Everything: A Survey","summary":"  Diffusion Models (DMs) represent a significant advancement in image\nSuper-Resolution (SR), aligning technical image quality more closely with human\npreferences and expanding SR applications. DMs address critical limitations of\nprevious methods, enhancing overall realism and details in SR images. However,\nDMs suffer from color-shifting issues, and their high computational costs call\nfor efficient sampling alternatives, underscoring the challenge of balancing\ncomputational efficiency and image quality. This survey gives an overview of\nDMs applied to image SR and offers a detailed analysis that underscores the\nunique characteristics and methodologies within this domain, distinct from\nbroader existing reviews in the field. It presents a unified view of DM\nfundamentals and explores research directions, including alternative input\ndomains, conditioning strategies, guidance, corruption spaces, and zero-shot\nmethods. This survey provides insights into the evolution of image SR with DMs,\naddressing current trends, challenges, and future directions in this rapidly\nevolving field.\n","authors":["Brian B. Moser","Arundhati S. Shanbhag","Federico Raue","Stanislav Frolov","Sebastian Palacio","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2401.00736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00728v1","updated":"2024-01-01T11:50:01Z","published":"2024-01-01T11:50:01Z","title":"MultiFusionNet: Multilayer Multimodal Fusion of Deep Neural Networks for\n  Chest X-Ray Image Classification","summary":"  Chest X-ray imaging is a critical diagnostic tool for identifying pulmonary\ndiseases. However, manual interpretation of these images is time-consuming and\nerror-prone. Automated systems utilizing convolutional neural networks (CNNs)\nhave shown promise in improving the accuracy and efficiency of chest X-ray\nimage classification. While previous work has mainly focused on using feature\nmaps from the final convolution layer, there is a need to explore the benefits\nof leveraging additional layers for improved disease classification. Extracting\nrobust features from limited medical image datasets remains a critical\nchallenge. In this paper, we propose a novel deep learning-based multilayer\nmultimodal fusion model that emphasizes extracting features from different\nlayers and fusing them. Our disease detection model considers the\ndiscriminatory information captured by each layer. Furthermore, we propose the\nfusion of different-sized feature maps (FDSFM) module to effectively merge\nfeature maps from diverse layers. The proposed model achieves a significantly\nhigher accuracy of 97.21% and 99.60% for both three-class and two-class\nclassifications, respectively. The proposed multilayer multimodal fusion model,\nalong with the FDSFM module, holds promise for accurate disease classification\nand can also be extended to other disease classifications in chest X-ray\nimages.\n","authors":["Saurabh Agarwal","K. V. Arya","Yogesh Kumar Meena"],"pdf_url":"https://arxiv.org/pdf/2401.00728v1.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2312.09168v2","updated":"2024-01-01T10:15:46Z","published":"2023-12-14T17:34:53Z","title":"DiffusionLight: Light Probes for Free by Painting a Chrome Ball","summary":"  We present a simple yet effective technique to estimate lighting in a single\ninput image. Current techniques rely heavily on HDR panorama datasets to train\nneural networks to regress an input with limited field-of-view to a full\nenvironment map. However, these approaches often struggle with real-world,\nuncontrolled settings due to the limited diversity and size of their datasets.\nTo address this problem, we leverage diffusion models trained on billions of\nstandard images to render a chrome ball into the input image. Despite its\nsimplicity, this task remains challenging: the diffusion models often insert\nincorrect or inconsistent objects and cannot readily generate images in HDR\nformat. Our research uncovers a surprising relationship between the appearance\nof chrome balls and the initial diffusion noise map, which we utilize to\nconsistently generate high-quality chrome balls. We further fine-tune an LDR\ndifusion model (Stable Diffusion XL) with LoRA, enabling it to perform exposure\nbracketing for HDR light estimation. Our method produces convincing light\nestimates across diverse settings and demonstrates superior generalization to\nin-the-wild scenarios.\n","authors":["Pakkapon Phongthawee","Worameth Chinchuthakun","Nontaphat Sinsunthithet","Amit Raj","Varun Jampani","Pramook Khungurn","Supasorn Suwajanakorn"],"pdf_url":"https://arxiv.org/pdf/2312.09168v2.pdf","comment":"For more info and code, please visit our website\n  https://diffusionlight.github.io/"},{"id":"http://arxiv.org/abs/2312.10841v2","updated":"2024-01-01T09:39:03Z","published":"2023-12-17T23:10:39Z","title":"Online Boosting Adaptive Learning under Concept Drift for Multistream\n  Classification","summary":"  Multistream classification poses significant challenges due to the necessity\nfor rapid adaptation in dynamic streaming processes with concept drift. Despite\nthe growing research outcomes in this area, there has been a notable oversight\nregarding the temporal dynamic relationships between these streams, leading to\nthe issue of negative transfer arising from irrelevant data. In this paper, we\npropose a novel Online Boosting Adaptive Learning (OBAL) method that\neffectively addresses this limitation by adaptively learning the dynamic\ncorrelation among different streams. Specifically, OBAL operates in a\ndual-phase mechanism, in the first of which we design an Adaptive COvariate\nShift Adaptation (AdaCOSA) algorithm to construct an initialized ensemble model\nusing archived data from various source streams, thus mitigating the covariate\nshift while learning the dynamic correlations via an adaptive re-weighting\nstrategy. During the online process, we employ a Gaussian Mixture Model-based\nweighting mechanism, which is seamlessly integrated with the acquired\ncorrelations via AdaCOSA to effectively handle asynchronous drift. This\napproach significantly improves the predictive performance and stability of the\ntarget stream. We conduct comprehensive experiments on several synthetic and\nreal-world data streams, encompassing various drifting scenarios and types. The\nresults clearly demonstrate that OBAL achieves remarkable advancements in\naddressing multistream classification problems by effectively leveraging\npositive knowledge derived from multiple sources.\n","authors":["En Yu","Jie Lu","Bin Zhang","Guangquan Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.10841v2.pdf","comment":"AAAI 2024"},{"id":"http://arxiv.org/abs/2311.03380v2","updated":"2024-01-01T09:26:56Z","published":"2023-11-02T08:18:37Z","title":"An attempt to generate new bridge types from latent space of variational\n  autoencoder","summary":"  Try to generate new bridge types using generative artificial intelligence\ntechnology. The grayscale images of the bridge facade with the change of\ncomponent width was rendered by 3dsMax animation software, and then the OpenCV\nmodule performed an appropriate amount of geometric transformation (rotation,\nhorizontal scale, vertical scale) to obtain the image dataset of three-span\nbeam bridge, arch bridge, cable-stayed bridge and suspension bridge. Based on\nPython programming language, TensorFlow and Keras deep learning platform\nframework, variational autoencoder was constructed and trained, and\nlow-dimensional bridge-type latent space that is convenient for vector\noperations was obtained. Variational autoencoder can combine two bridge types\non the basis of the original of human into one that is a new bridge type.\nGenerative artificial intelligence technology can assist bridge designers in\nbridge-type innovation, and can be used as copilot.\n","authors":["Hongjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.03380v2.pdf","comment":"9 pages, 8 figures"},{"id":"http://arxiv.org/abs/2401.00700v1","updated":"2024-01-01T08:46:29Z","published":"2024-01-01T08:46:29Z","title":"An attempt to generate new bridge types from latent space of generative\n  adversarial network","summary":"  Try to generate new bridge types using generative artificial intelligence\ntechnology. Symmetric structured image dataset of three-span beam bridge, arch\nbridge, cable-stayed bridge and suspension bridge are used . Based on Python\nprogramming language, TensorFlow and Keras deep learning platform framework ,\nas well as Wasserstein loss function and Lipschitz constraints, generative\nadversarial network is constructed and trained. From the obtained low\ndimensional bridge-type latent space sampling, new bridge types with asymmetric\nstructures can be generated. Generative adversarial network can create new\nbridge types by organically combining different structural components on the\nbasis of human original bridge types. It has a certain degree of human original\nability. Generative artificial intelligence technology can open up imagination\nspace and inspire humanity.\n","authors":["Hongjun Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.00700v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2401.00698v1","updated":"2024-01-01T08:32:50Z","published":"2024-01-01T08:32:50Z","title":"Large Language Models aren't all that you need","summary":"  This paper describes the architecture and systems built towards solving the\nSemEval 2023 Task 2: MultiCoNER II (Multilingual Complex Named Entity\nRecognition) [1]. We evaluate two approaches (a) a traditional Conditional\nRandom Fields model and (b) a Large Language Model (LLM) fine-tuned with a\ncustomized head and compare the two approaches. The novel ideas explored are:\n1) Decaying auxiliary loss (with residual) - where we train the model on an\nauxiliary task of Coarse-Grained NER and include this task as a part of the\nloss function 2) Triplet token blending - where we explore ways of blending the\nembeddings of neighboring tokens in the final NER layer prior to prediction 3)\nTask-optimal heads - where we explore a variety of custom heads and learning\nrates for the final layer of the LLM. We also explore multiple LLMs including\nGPT-3 and experiment with a variety of dropout and other hyperparameter\nsettings before arriving at our final model which achieves micro & macro f1 of\n0.85/0.84 (on dev) and 0.67/0.61 on the test data . We show that while\npre-trained LLMs, by themselves, bring about a large improvement in scores as\ncompared to traditional models, we also demonstrate that tangible improvements\nto the Macro-F1 score can be made by augmenting the LLM with additional\nfeature/loss/model engineering techniques described above.\n","authors":["Kiran Voderhobli Holla","Chaithanya Kumar","Aryan Singh"],"pdf_url":"https://arxiv.org/pdf/2401.00698v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00692v1","updated":"2024-01-01T08:11:38Z","published":"2024-01-01T08:11:38Z","title":"Self-supervised learning for skin cancer diagnosis with limited training\n  data","summary":"  Cancer diagnosis is a well-studied problem in machine learning since early\ndetection of cancer is often the determining factor in prognosis. Supervised\ndeep learning achieves excellent results in cancer image classification,\nusually through transfer learning. However, these models require large amounts\nof labelled data and for several types of cancer, large labelled datasets do\nnot exist. In this paper, we demonstrate that a model pre-trained using a\nself-supervised learning algorithm known as Barlow Twins can outperform the\nconventional supervised transfer learning pipeline. We juxtapose two base\nmodels: i) pretrained in a supervised fashion on ImageNet; ii) pretrained in a\nself-supervised fashion on ImageNet. Both are subsequently fine tuned on a\nsmall labelled skin lesion dataset and evaluated on a large test set. We\nachieve a mean test accuracy of 70\\% for self-supervised transfer in comparison\nto 66\\% for supervised transfer. Interestingly, boosting performance further is\npossible by self-supervised pretraining a second time (on unlabelled skin\nlesion images) before subsequent fine tuning. This hints at an alternative path\nto collecting more labelled data in settings where this is challenging - namely\njust collecting more unlabelled images. Our framework is applicable to cancer\nimage classification models in the low-labelled data regime.\n","authors":["Hamish Haggerty","Rohitash Chandra"],"pdf_url":"https://arxiv.org/pdf/2401.00692v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00691v1","updated":"2024-01-01T08:03:52Z","published":"2024-01-01T08:03:52Z","title":"Stochastic Gradient Descent for Additive Nonparametric Regression","summary":"  This paper introduces an iterative algorithm designed to train additive\nmodels with favorable memory storage and computational requirements. The\nalgorithm can be viewed as the functional counterpart of stochastic gradient\ndescent, applied to the coefficients of a truncated basis expansion of the\ncomponent functions. We show that the resulting estimator satisfies an oracle\ninequality that allows for model mispecification. In the well-specified\nsetting, by choosing the learning rate carefully across three distinct stages\nof training, we prove that its risk is minimax optimal in terms of the\ndependence on the dimensionality of the data and the size of the training\nsample.\n","authors":["Xin Chen","Jason M. Klusowski"],"pdf_url":"https://arxiv.org/pdf/2401.00691v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00688v1","updated":"2024-01-01T07:31:32Z","published":"2024-01-01T07:31:32Z","title":"Inferring community structure in attributed hypergraphs using stochastic\n  block models","summary":"  Hypergraphs are a representation of complex systems involving interactions\namong more than two entities and allow to investigation of higher-order\nstructure and dynamics in real-world complex systems. Community structure is a\ncommon property observed in empirical networks in various domains. Stochastic\nblock models have been employed to investigate community structure in networks.\nNode attribute data, often accompanying network data, has been found to\npotentially enhance the learning of community structure in dyadic networks. In\nthis study, we develop a statistical framework that incorporates node attribute\ndata into the learning of community structure in a hypergraph, employing a\nstochastic block model. We demonstrate that our model, which we refer to as\nHyperNEO, enhances the learning of community structure in synthetic and\nempirical hypergraphs when node attributes are sufficiently associated with the\ncommunities. Furthermore, we found that applying a dimensionality reduction\nmethod, UMAP, to the learned representations obtained using stochastic block\nmodels, including our model, maps nodes into a two-dimensional vector space\nwhile largely preserving community structure in empirical hypergraphs. We\nexpect that our framework will broaden the investigation and understanding of\nhigher-order community structure in real-world complex systems.\n","authors":["Kazuki Nakajima","Takeaki Uno"],"pdf_url":"https://arxiv.org/pdf/2401.00688v1.pdf","comment":"28 pages, 11 figures, 8 tables"},{"id":"http://arxiv.org/abs/2401.00685v1","updated":"2024-01-01T07:07:27Z","published":"2024-01-01T07:07:27Z","title":"Communication-Efficient Federated Learning for LEO Constellations\n  Integrated with HAPs Using Hybrid NOMA-OFDM","summary":"  Space AI has become increasingly important and sometimes even necessary for\ngovernment, businesses, and society. An active research topic under this\nmission is integrating federated learning (FL) with satellite communications\n(SatCom) so that numerous low Earth orbit (LEO) satellites can collaboratively\ntrain a machine learning model. However, the special communication environment\nof SatCom leads to a very slow FL training process up to days and weeks. This\npaper proposes NomaFedHAP, a novel FL-SatCom approach tailored to LEO\nsatellites, that (1) utilizes high-altitude platforms (HAPs) as distributed\nparameter servers (PS) to enhance satellite visibility, and (2) introduces\nnon-orthogonal multiple access (NOMA) into LEO to enable fast and\nbandwidth-efficient model transmissions. In addition, NomaFedHAP includes (3) a\nnew communication topology that exploits HAPs to bridge satellites among\ndifferent orbits to mitigate the Doppler shift, and (4) a new FL model\naggregation scheme that optimally balances models between different orbits and\nshells. Moreover, we (5) derive a closed-form expression of the outage\nprobability for satellites in near and far shells, as well as for the entire\nsystem. Our extensive simulations have validated the mathematical analysis and\ndemonstrated the superior performance of NomaFedHAP in achieving fast and\nefficient FL model convergence with high accuracy as compared to the\nstate-of-the-art.\n","authors":["Mohamed Elmahallawy","Tie Luo","Khaled Ramadan"],"pdf_url":"https://arxiv.org/pdf/2401.00685v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00684v1","updated":"2024-01-01T07:04:20Z","published":"2024-01-01T07:04:20Z","title":"A Temporal Filter to Extract Doped Conducting Polymer Information\n  Features from an Electronic Nose","summary":"  Identifying relevant machine-learning features for multi-sensing platforms is\nboth an applicative limitation to recognize environments and a necessity to\ninterpret the physical relevance of transducers' complementarity in their\ninformation processing. Particularly for long acquisitions, feature extraction\nmust be fully automatized without human intervention and resilient to\nperturbations without increasing significantly the computational cost of a\nclassifier. In this study, we investigate on the relative resistance and\ncurrent modulation of a 24-dimensional conductimetric electronic nose, which\nuses the exponential moving average as a floating reference in a low-cost\ninformation descriptor for environment recognition. In particular, we\nidentified that depending on the structure of a linear classifier, the 'modema'\ndescriptor is optimized for different material sensing elements' contributions\nto classify information patterns. The low-pass filtering optimization leads to\nopposite behaviors between unsupervised and supervised learning: the latter one\nfavors longer integration of the reference, allowing to recognize five\ndifferent classes over 90%, while the first one prefers using the latest events\nas its reference to clusterize patterns by environment nature. Its electronic\nimplementation shall greatly diminish the computational requirements of\nconductimetric electronic noses for on-board environment recognition without\nhuman supervision.\n","authors":["Wiem Haj Ammar","Aicha Boujnah","Antoine Baron","Aimen Boubaker","Adel Kalboussi","Kamal Lmimouni","Sebastien Pecqueur"],"pdf_url":"https://arxiv.org/pdf/2401.00684v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00916v1","updated":"2024-01-01T06:53:36Z","published":"2024-01-01T06:53:36Z","title":"Data Assimilation in Chaotic Systems Using Deep Reinforcement Learning","summary":"  Data assimilation (DA) plays a pivotal role in diverse applications, ranging\nfrom climate predictions and weather forecasts to trajectory planning for\nautonomous vehicles. A prime example is the widely used ensemble Kalman filter\n(EnKF), which relies on linear updates to minimize variance among the ensemble\nof forecast states. Recent advancements have seen the emergence of deep\nlearning approaches in this domain, primarily within a supervised learning\nframework. However, the adaptability of such models to untrained scenarios\nremains a challenge. In this study, we introduce a novel DA strategy that\nutilizes reinforcement learning (RL) to apply state corrections using full or\npartial observations of the state variables. Our investigation focuses on\ndemonstrating this approach to the chaotic Lorenz '63 system, where the agent's\nobjective is to minimize the root-mean-squared error between the observations\nand corresponding forecast states. Consequently, the agent develops a\ncorrection strategy, enhancing model forecasts based on available system state\nobservations. Our strategy employs a stochastic action policy, enabling a Monte\nCarlo-based DA framework that relies on randomly sampling the policy to\ngenerate an ensemble of assimilated realizations. Results demonstrate that the\ndeveloped RL algorithm performs favorably when compared to the EnKF.\nAdditionally, we illustrate the agent's capability to assimilate non-Gaussian\ndata, addressing a significant limitation of the EnKF.\n","authors":["Mohamad Abed El Rahman Hammoud","Naila Raboudi","Edriss S. Titi","Omar Knio","Ibrahim Hoteit"],"pdf_url":"https://arxiv.org/pdf/2401.00916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00678v1","updated":"2024-01-01T06:15:16Z","published":"2024-01-01T06:15:16Z","title":"General-purpose foundation models for increased autonomy in\n  robot-assisted surgery","summary":"  The dominant paradigm for end-to-end robot learning focuses on optimizing\ntask-specific objectives that solve a single robotic problem such as picking up\nan object or reaching a target position. However, recent work on high-capacity\nmodels in robotics has shown promise toward being trained on large collections\nof diverse and task-agnostic datasets of video demonstrations. These models\nhave shown impressive levels of generalization to unseen circumstances,\nespecially as the amount of data and the model complexity scale. Surgical robot\nsystems that learn from data have struggled to advance as quickly as other\nfields of robot learning for a few reasons: (1) there is a lack of existing\nlarge-scale open-source data to train models, (2) it is challenging to model\nthe soft-body deformations that these robots work with during surgery because\nsimulation cannot match the physical and visual complexity of biological\ntissue, and (3) surgical robots risk harming patients when tested in clinical\ntrials and require more extensive safety measures. This perspective article\naims to provide a path toward increasing robot autonomy in robot-assisted\nsurgery through the development of a multi-modal, multi-task,\nvision-language-action model for surgical robots. Ultimately, we argue that\nsurgical robots are uniquely positioned to benefit from general-purpose models\nand provide three guiding actions toward increased autonomy in robot-assisted\nsurgery.\n","authors":["Samuel Schmidgall","Ji Woong Kim","Alan Kuntz","Ahmed Ezzat Ghazi","Axel Krieger"],"pdf_url":"https://arxiv.org/pdf/2401.00678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00676v1","updated":"2024-01-01T06:04:52Z","published":"2024-01-01T06:04:52Z","title":"Digger: Detecting Copyright Content Mis-usage in Large Language Model\n  Training","summary":"  Pre-training, which utilizes extensive and varied datasets, is a critical\nfactor in the success of Large Language Models (LLMs) across numerous\napplications. However, the detailed makeup of these datasets is often not\ndisclosed, leading to concerns about data security and potential misuse. This\nis particularly relevant when copyrighted material, still under legal\nprotection, is used inappropriately, either intentionally or unintentionally,\ninfringing on the rights of the authors.\n  In this paper, we introduce a detailed framework designed to detect and\nassess the presence of content from potentially copyrighted books within the\ntraining datasets of LLMs. This framework also provides a confidence estimation\nfor the likelihood of each content sample's inclusion. To validate our\napproach, we conduct a series of simulated experiments, the results of which\naffirm the framework's effectiveness in identifying and addressing instances of\ncontent misuse in LLM training processes. Furthermore, we investigate the\npresence of recognizable quotes from famous literary works within these\ndatasets. The outcomes of our study have significant implications for ensuring\nthe ethical use of copyrighted materials in the development of LLMs,\nhighlighting the need for more transparent and responsible data management\npractices in this field.\n","authors":["Haodong Li","Gelei Deng","Yi Liu","Kailong Wang","Yuekang Li","Tianwei Zhang","Yang Liu","Guoai Xu","Guosheng Xu","Haoyu Wang"],"pdf_url":"https://arxiv.org/pdf/2401.00676v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.10483v3","updated":"2024-01-01T05:33:50Z","published":"2023-10-16T15:03:55Z","title":"Passive Inference Attacks on Split Learning via Adversarial\n  Regularization","summary":"  Split Learning (SL) has emerged as a practical and efficient alternative to\ntraditional federated learning. While previous attempts to attack SL have often\nrelied on overly strong assumptions or targeted easily exploitable models, we\nseek to develop more practical attacks. We introduce SDAR, a novel attack\nframework against SL with an honest-but-curious server. SDAR leverages\nauxiliary data and adversarial regularization to learn a decodable simulator of\nthe client's private model, which can effectively infer the client's private\nfeatures under the vanilla SL, and both features and labels under the U-shaped\nSL. We perform extensive experiments in both configurations to validate the\neffectiveness of our proposed attacks. Notably, in challenging but practical\nscenarios where existing passive attacks struggle to reconstruct the client's\nprivate data effectively, SDAR consistently achieves attack performance\ncomparable to active attacks. On CIFAR-10, at the deep split level of 7, SDAR\nachieves private feature reconstruction with less than 0.025 mean squared error\nin both the vanilla and the U-shaped SL, and attains a label inference accuracy\nof over 98% in the U-shaped setting, while existing attacks fail to produce\nnon-trivial results.\n","authors":["Xiaochen Zhu","Xinjian Luo","Yuncheng Wu","Yangfan Jiang","Xiaokui Xiao","Beng Chin Ooi"],"pdf_url":"https://arxiv.org/pdf/2310.10483v3.pdf","comment":"17 pages, 20 figures"},{"id":"http://arxiv.org/abs/2401.00664v1","updated":"2024-01-01T04:35:53Z","published":"2024-01-01T04:35:53Z","title":"New Sample Complexity Bounds for (Regularized) Sample Average\n  Approximation in Several Heavy-Tailed, Non-Lipschitzian, and High-Dimensional\n  Cases","summary":"  We study the sample complexity of sample average approximation (SAA) and its\nsimple variations, referred to as the regularized SAA (RSAA), in solving convex\nand strongly convex stochastic programming (SP) problems under\nheavy-tailed-ness, non-Lipschitz-ness, and/or high dimensionality. The presence\nof such irregularities underscores critical vacua in the literature. In\nresponse, this paper presents three sets of results: First, we show that the\n(R)SAA is effective even if the objective function is not necessarily Lipschitz\nand the underlying distribution admits some bounded central moments only at\n(near-)optimal solutions. Second, when the SP's objective function is the sum\nof a smooth term and a Lipschitz term, we prove that the (R)SAA's sample\ncomplexity is completely independent from any complexity measures (e.g., the\ncovering number) of the feasible region. Third, we explicate the (R)SAA's\nsample complexities with regard to the dependence on dimensionality $d$: When\nsome $p$th ($p\\geq 2$) central moment of the underlying distribution is\nbounded, we show that the required sample size grows at a rate no worse than\n$\\mathcal O\\left(p d^{2/p}\\right)$ under any one of the three structural\nassumptions: (i) strong convexity w.r.t. the $q$-norm ($q\\geq 1$); (ii) the\ncombination of restricted strong convexity and sparsity; and (iii) a\ndimension-insensitive $q$-norm of an optimal solution. In both cases of (i) and\n(iii), it is further required that $p\\leq q/(q-1)$. As a direct implication,\nthe (R)SAA's complexity becomes (poly-)logarithmic in $d$, whenever $p\\geq\nc\\cdot \\ln d$ is admissible for some constant $c>0$. These new results deviate\nfrom the SAA's typical sample complexities that grow polynomially with $d$.\nPart of our proof is based on the average-replace-one (RO) stability, which\nappears to be novel for the (R)SAA's analyses.\n","authors":["Hongcheng Liu","Jindong Tong"],"pdf_url":"https://arxiv.org/pdf/2401.00664v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07586v2","updated":"2024-01-01T04:25:04Z","published":"2023-12-11T02:40:40Z","title":"Characteristic Guidance: Non-linear Correction for Diffusion Model at\n  Large Guidance Scale","summary":"  Popular guidance for denoising diffusion probabilistic model (DDPM) linearly\ncombines distinct conditional models together to provide enhanced control over\nsamples. However, this approach overlooks nonlinear effects that become\nsignificant when guidance scale is large. To address this issue, we propose\ncharacteristic guidance, a sampling method that provides first-principle\nnon-linear correction for classifier-free guided DDPMs. Such correction forces\nthe guided DDPMs to respect the Fokker-Planck equation of their underlying\ndiffusion process, in a way that is training-free, derivative-free, and\ncompatible with existing sampling methods. Experiments show that characteristic\nguidance enhances control and reduces color and exposure issues in image\ngeneration, proving effective in diverse applications ranging from latent space\nsampling to solving physics problems like magnet phase transitions.\n","authors":["Candi Zheng","Yuan Lan"],"pdf_url":"https://arxiv.org/pdf/2312.07586v2.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2401.00658v1","updated":"2024-01-01T04:11:55Z","published":"2024-01-01T04:11:55Z","title":"Point Cloud in the Air","summary":"  Acquisition and processing of point clouds (PCs) is a crucial enabler for\nmany emerging applications reliant on 3D spatial data, such as robot\nnavigation, autonomous vehicles, and augmented reality. In most scenarios, PCs\nacquired by remote sensors must be transmitted to an edge server for fusion,\nsegmentation, or inference. Wireless transmission of PCs not only puts on\nincreased burden on the already congested wireless spectrum, but also confronts\na unique set of challenges arising from the irregular and unstructured nature\nof PCs. In this paper, we meticulously delineate these challenges and offer a\ncomprehensive examination of existing solutions while candidly acknowledging\ntheir inherent limitations. In response to these intricacies, we proffer four\npragmatic solution frameworks, spanning advanced techniques, hybrid schemes,\nand distributed data aggregation approaches. In doing so, our goal is to chart\na path toward efficient, reliable, and low-latency wireless PC transmission.\n","authors":["Yulin Shao","Chenghong Bian","Li Yang","Qianqian Yang","Zhaoyang Zhang","Deniz Gunduz"],"pdf_url":"https://arxiv.org/pdf/2401.00658v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.15112v2","updated":"2024-01-01T03:57:57Z","published":"2023-12-22T23:16:13Z","title":"Less or More From Teacher: Exploiting Trilateral Geometry For Knowledge\n  Distillation","summary":"  Knowledge distillation aims to train a compact student network using soft\nsupervision from a larger teacher network and hard supervision from ground\ntruths. However, determining an optimal knowledge fusion ratio that balances\nthese supervisory signals remains challenging. Prior methods generally resort\nto a constant or heuristic-based fusion ratio, which often falls short of a\nproper balance. In this study, we introduce a novel adaptive method for\nlearning a sample-wise knowledge fusion ratio, exploiting both the correctness\nof teacher and student, as well as how well the student mimics the teacher on\neach sample. Our method naturally leads to the intra-sample trilateral\ngeometric relations among the student prediction ($S$), teacher prediction\n($T$), and ground truth ($G$). To counterbalance the impact of outliers, we\nfurther extend to the inter-sample relations, incorporating the teacher's\nglobal average prediction $\\bar{T}$ for samples within the same class. A simple\nneural network then learns the implicit mapping from the intra- and\ninter-sample relations to an adaptive, sample-wise knowledge fusion ratio in a\nbilevel-optimization manner. Our approach provides a simple, practical, and\nadaptable solution for knowledge distillation that can be employed across\nvarious architectures and model sizes. Extensive experiments demonstrate\nconsistent improvements over other loss re-weighting methods on image\nclassification, attack detection, and click-through rate prediction.\n","authors":["Chengming Hu","Haolun Wu","Xuan Li","Chen Ma","Xi Chen","Jun Yan","Boyu Wang","Xue Liu"],"pdf_url":"https://arxiv.org/pdf/2312.15112v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.11250v2","updated":"2024-01-01T03:43:41Z","published":"2023-06-20T03:03:04Z","title":"InRank: Incremental Low-Rank Learning","summary":"  The theory of greedy low-rank learning (GLRL) aims to explain the impressive\ngeneralization capabilities of deep learning. It proves that stochastic\ngradient-based training implicitly regularizes neural networks towards low-rank\nsolutions through a gradual increase of the rank during training. However,\nthere is a gap between theory and practice since GLRL requires an infinitesimal\ninitialization of the weights, which is not practical due to the fact that it\nis a saddle point. In this work, we remove the assumption of infinitesimal\ninitialization by focusing on cumulative weight updates. We prove the\ncumulative weight updates follow an incremental low-rank trajectory for\narbitrary orthogonal initialization of weights in a three-layer linear network.\nEmpirically, we demonstrate that our theory holds on a broad range of neural\nnetworks (e.g., transformers) and standard training algorithms (e.g., SGD,\nAdam). However, existing training algorithms do not exploit the low-rank\nproperty to improve computational efficiency as the networks are not\nparameterized in low-rank. To remedy this, we design a new training algorithm\nIncremental Low-Rank Learning (InRank), which explicitly expresses cumulative\nweight updates as low-rank matrices while incrementally augmenting their ranks\nduring training. We evaluate InRank on GPT-2, and our results indicate that\nInRank achieves comparable prediction performance as the full-rank counterpart\nwhile requiring at most 33% of the total ranks throughout training. We also\npropose an efficient version of InRank that achieves a reduction of 37% in\ntotal training time and 36% in model size when training GPT-medium on\nWikiText-103 from scratch.\n","authors":["Jiawei Zhao","Yifei Zhang","Beidi Chen","Florian Schäfer","Anima Anandkumar"],"pdf_url":"https://arxiv.org/pdf/2306.11250v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2112.08364v2","updated":"2024-01-01T03:11:53Z","published":"2021-12-15T02:42:28Z","title":"Data Valuation for Vertical Federated Learning: A Model-free and\n  Privacy-preserving Method","summary":"  Vertical Federated learning (VFL) is a promising paradigm for predictive\nanalytics, empowering an organization (i.e., task party) to enhance its\npredictive models through collaborations with multiple data suppliers (i.e.,\ndata parties) in a decentralized and privacy-preserving way. Despite the\nfast-growing interest in VFL, the lack of effective and secure tools for\nassessing the value of data owned by data parties hinders the application of\nVFL in business contexts. In response, we propose FedValue, a\nprivacy-preserving, task-specific but model-free data valuation method for VFL,\nwhich consists of a data valuation metric and a federated computation method.\nSpecifically, we first introduce a novel data valuation metric, namely\nMShapley-CMI. The metric evaluates a data party's contribution to a predictive\nanalytics task without the need of executing a machine learning model, making\nit well-suited for real-world applications of VFL. Next, we develop an\ninnovative federated computation method that calculates the MShapley-CMI value\nfor each data party in a privacy-preserving manner. Extensive experiments\nconducted on six public datasets validate the efficacy of FedValue for data\nvaluation in the context of VFL. In addition, we illustrate the practical\nutility of FedValue with a case study involving federated movie\nrecommendations.\n","authors":["Xiao Han","Leye Wang","Junjie Wu","Xiao Fang"],"pdf_url":"https://arxiv.org/pdf/2112.08364v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00633v1","updated":"2024-01-01T02:03:35Z","published":"2024-01-01T02:03:35Z","title":"On Discprecncies between Perturbation Evaluations of Graph Neural\n  Network Attributions","summary":"  Neural networks are increasingly finding their way into the realm of graphs\nand modeling relationships between features. Concurrently graph neural network\nexplanation approaches are being invented to uncover relationships between the\nnodes of the graphs. However, there is a disparity between the existing\nattribution methods, and it is unclear which attribution to trust. Therefore\nresearch has introduced evaluation experiments that assess them from different\nperspectives. In this work, we assess attribution methods from a perspective\nnot previously explored in the graph domain: retraining. The core idea is to\nretrain the network on important (or not important) relationships as identified\nby the attributions and evaluate how networks can generalize based on these\nrelationships. We reformulate the retraining framework to sidestep issues\nlurking in the previous formulation and propose guidelines for correct\nanalysis. We run our analysis on four state-of-the-art GNN attribution methods\nand five synthetic and real-world graph classification datasets. The analysis\nreveals that attributions perform variably depending on the dataset and the\nnetwork. Most importantly, we observe that the famous GNNExplainer performs\nsimilarly to an arbitrary designation of edge importance. The study concludes\nthat the retraining evaluation cannot be used as a generalized benchmark and\nrecommends it as a toolset to evaluate attributions on a specifically addressed\nnetwork, dataset, and sparsity.\n","authors":["Razieh Rezaei","Alireza Dizaji","Ashkan Khakzar","Anees Kazi","Nassir Navab","Daniel Rueckert"],"pdf_url":"https://arxiv.org/pdf/2401.00633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00629v1","updated":"2024-01-01T01:44:58Z","published":"2024-01-01T01:44:58Z","title":"Adversarially Trained Actor Critic for offline CMDPs","summary":"  We propose a Safe Adversarial Trained Actor Critic (SATAC) algorithm for\noffline reinforcement learning (RL) with general function approximation in the\npresence of limited data coverage. SATAC operates as a two-player Stackelberg\ngame featuring a refined objective function. The actor (leader player)\noptimizes the policy against two adversarially trained value critics (follower\nplayers), who focus on scenarios where the actor's performance is inferior to\nthe behavior policy. Our framework provides both theoretical guarantees and a\nrobust deep-RL implementation. Theoretically, we demonstrate that when the\nactor employs a no-regret optimization oracle, SATAC achieves two guarantees:\n(i) For the first time in the offline RL setting, we establish that SATAC can\nproduce a policy that outperforms the behavior policy while maintaining the\nsame level of safety, which is critical to designing an algorithm for offline\nRL. (ii) We demonstrate that the algorithm guarantees policy improvement across\na broad range of hyperparameters, indicating its practical robustness.\nAdditionally, we offer a practical version of SATAC and compare it with\nexisting state-of-the-art offline safe-RL algorithms in continuous control\nenvironments. SATAC outperforms all baselines across a range of tasks, thus\nvalidating the theoretical performance.\n","authors":["Honghao Wei","Xiyue Peng","Xin Liu","Arnob Ghosh"],"pdf_url":"https://arxiv.org/pdf/2401.00629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00625v1","updated":"2024-01-01T01:12:42Z","published":"2024-01-01T01:12:42Z","title":"Beyond Efficiency: A Systematic Survey of Resource-Efficient Large\n  Language Models","summary":"  The burgeoning field of Large Language Models (LLMs), exemplified by\nsophisticated models like OpenAI's ChatGPT, represents a significant\nadvancement in artificial intelligence. These models, however, bring forth\nsubstantial challenges in the high consumption of computational, memory,\nenergy, and financial resources, especially in environments with limited\nresource capabilities. This survey aims to systematically address these\nchallenges by reviewing a broad spectrum of techniques designed to enhance the\nresource efficiency of LLMs. We categorize methods based on their optimization\nfocus: computational, memory, energy, financial, and network resources and\ntheir applicability across various stages of an LLM's lifecycle, including\narchitecture design, pretraining, finetuning, and system design. Additionally,\nthe survey introduces a nuanced categorization of resource efficiency\ntechniques by their specific resource types, which uncovers the intricate\nrelationships and mappings between various resources and corresponding\noptimization techniques. A standardized set of evaluation metrics and datasets\nis also presented to facilitate consistent and fair comparisons across\ndifferent models and techniques. By offering a comprehensive overview of the\ncurrent sota and identifying open research avenues, this survey serves as a\nfoundational reference for researchers and practitioners, aiding them in\ndeveloping more sustainable and efficient LLMs in a rapidly evolving landscape.\n","authors":["Guangji Bai","Zheng Chai","Chen Ling","Shiyu Wang","Jiaying Lu","Nan Zhang","Tingwei Shi","Ziyang Yu","Mengdan Zhu","Yifei Zhang","Carl Yang","Yue Cheng","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2401.00625v1.pdf","comment":"Preprint. GitHub repo:\n  https://github.com/tiingweii-shii/Awesome-Resource-Efficient-LLM-Papers"},{"id":"http://arxiv.org/abs/2401.00622v1","updated":"2024-01-01T00:54:02Z","published":"2024-01-01T00:54:02Z","title":"Federated Class-Incremental Learning with New-Class Augmented\n  Self-Distillation","summary":"  Federated Learning (FL) enables collaborative model training among\nparticipants while guaranteeing the privacy of raw data. Mainstream FL\nmethodologies overlook the dynamic nature of real-world data, particularly its\ntendency to grow in volume and diversify in classes over time. This oversight\nresults in FL methods suffering from catastrophic forgetting, where models\ninadvertently discard previously learned information upon assimilating new\ndata. In response to this challenge, we propose a novel Federated\nClass-Incremental Learning (FCIL) method, named FCIL with New-Class Augmented\nSelf-Distillation (FedNASD). FedNASD combines new class scores, which are\ninferred from current models, with historical models' predictions. Based on the\ncombined past and present knowledge, it incorporates self-distillation over\nmodels on clients, aiming to achieve effective knowledge transfer from\nhistorical models to current models. Theoretical analysis demonstrates that\nFedNASD is equivalent to modeling old class scores as conditional probabilities\nin the absence of new classes. Additionally, it reconciles the predictions of\nnew classes with current models to refine the conditional probabilities of\nhistorical scores where new classes do not exist. Empirical experiments\ndemonstrate the superiority of FedNASD over four baseline algorithms in\nreducing the average forgetting rate and boosting global accuracy.\n","authors":["Zhiyuan Wu","Tianliu He","Sheng Sun","Yuwei Wang","Min Liu","Bo Gao","Xuefeng Jiang"],"pdf_url":"https://arxiv.org/pdf/2401.00622v1.pdf","comment":"9 pages, 2 figures, 4 tables"},{"id":"http://arxiv.org/abs/2302.00293v3","updated":"2024-01-01T00:41:39Z","published":"2023-02-01T07:47:26Z","title":"A Survey of Methods, Challenges and Perspectives in Causality","summary":"  Deep Learning models have shown success in a large variety of tasks by\nextracting correlation patterns from high-dimensional data but still struggle\nwhen generalizing out of their initial distribution. As causal engines aim to\nlearn mechanisms independent from a data distribution, combining Deep Learning\nwith Causality can have a great impact on the two fields. In this paper, we\nfurther motivate this assumption. We perform an extensive overview of the\ntheories and methods for Causality from different perspectives, with an\nemphasis on Deep Learning and the challenges met by the two domains. We show\nearly attempts to bring the fields together and the possible perspectives for\nthe future. We finish by providing a large variety of applications for\ntechniques from Causality.\n","authors":["Gaël Gendron","Michael Witbrock","Gillian Dobbie"],"pdf_url":"https://arxiv.org/pdf/2302.00293v3.pdf","comment":"40 pages, 37 pages for the main paper and 3 pages for the supplement,\n  8 figures, submitted to ACM Computing Surveys"}],"Multimedia":[{"id":"http://arxiv.org/abs/2309.14181v3","updated":"2024-01-01T14:48:48Z","published":"2023-09-25T14:43:43Z","title":"Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level\n  Vision","summary":"  The rapid evolution of Multi-modality Large Language Models (MLLMs) has\ncatalyzed a shift in computer vision from specialized models to general-purpose\nfoundation models. Nevertheless, there is still an inadequacy in assessing the\nabilities of MLLMs on low-level visual perception and understanding. To address\nthis gap, we present Q-Bench, a holistic benchmark crafted to systematically\nevaluate potential abilities of MLLMs on three realms: low-level visual\nperception, low-level visual description, and overall visual quality\nassessment. a) To evaluate the low-level perception ability, we construct the\nLLVisionQA dataset, consisting of 2,990 diverse-sourced images, each equipped\nwith a human-asked question focusing on its low-level attributes. We then\nmeasure the correctness of MLLMs on answering these questions. b) To examine\nthe description ability of MLLMs on low-level information, we propose the\nLLDescribe dataset consisting of long expert-labelled golden low-level text\ndescriptions on 499 images, and a GPT-involved comparison pipeline between\noutputs of MLLMs and the golden descriptions. c) Besides these two tasks, we\nfurther measure their visual quality assessment ability to align with human\nopinion scores. Specifically, we design a softmax-based strategy that enables\nMLLMs to predict quantifiable quality scores, and evaluate them on various\nexisting image quality assessment (IQA) datasets. Our evaluation across the\nthree abilities confirms that MLLMs possess preliminary low-level visual\nskills. However, these skills are still unstable and relatively imprecise,\nindicating the need for specific enhancements on MLLMs towards these abilities.\nWe hope that our benchmark can encourage the research community to delve deeper\nto discover and enhance these untapped potentials of MLLMs. Project Page:\nhttps://q-future.github.io/Q-Bench.\n","authors":["Haoning Wu","Zicheng Zhang","Erli Zhang","Chaofeng Chen","Liang Liao","Annan Wang","Chunyi Li","Wenxiu Sun","Qiong Yan","Guangtao Zhai","Weisi Lin"],"pdf_url":"https://arxiv.org/pdf/2309.14181v3.pdf","comment":"27 pages, 11 tables, with updated results"},{"id":"http://arxiv.org/abs/2401.00763v1","updated":"2024-01-01T14:06:55Z","published":"2024-01-01T14:06:55Z","title":"New Job, New Gender? Measuring the Social Bias in Image Generation\n  Models","summary":"  Image generation models can generate or edit images from a given text. Recent\nadvancements in image generation technology, exemplified by DALL-E and\nMidjourney, have been groundbreaking. These advanced models, despite their\nimpressive capabilities, are often trained on massive Internet datasets, making\nthem susceptible to generating content that perpetuates social stereotypes and\nbiases, which can lead to severe consequences. Prior research on assessing bias\nwithin image generation models suffers from several shortcomings, including\nlimited accuracy, reliance on extensive human labor, and lack of comprehensive\nanalysis. In this paper, we propose BiasPainter, a novel metamorphic testing\nframework that can accurately, automatically and comprehensively trigger social\nbias in image generation models. BiasPainter uses a diverse range of seed\nimages of individuals and prompts the image generation models to edit these\nimages using gender, race, and age-neutral queries. These queries span 62\nprofessions, 39 activities, 57 types of objects, and 70 personality traits. The\nframework then compares the edited images to the original seed images, focusing\non any changes related to gender, race, and age. BiasPainter adopts a testing\noracle that these characteristics should not be modified when subjected to\nneutral prompts. Built upon this design, BiasPainter can trigger the social\nbias and evaluate the fairness of image generation models. To evaluate the\neffectiveness of BiasPainter, we use BiasPainter to test five widely-used\ncommercial image generation software and models, such as stable diffusion and\nMidjourney. Experimental results show that 100\\% of the generated test cases\ncan successfully trigger social bias in image generation models.\n","authors":["Wenxuan Wang","Haonan Bai","Jen-tse Huang","Yuxuan Wan","Youliang Yuan","Haoyi Qiu","Nanyun Peng","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2401.00763v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00736v1","updated":"2024-01-01T12:25:57Z","published":"2024-01-01T12:25:57Z","title":"Diffusion Models, Image Super-Resolution And Everything: A Survey","summary":"  Diffusion Models (DMs) represent a significant advancement in image\nSuper-Resolution (SR), aligning technical image quality more closely with human\npreferences and expanding SR applications. DMs address critical limitations of\nprevious methods, enhancing overall realism and details in SR images. However,\nDMs suffer from color-shifting issues, and their high computational costs call\nfor efficient sampling alternatives, underscoring the challenge of balancing\ncomputational efficiency and image quality. This survey gives an overview of\nDMs applied to image SR and offers a detailed analysis that underscores the\nunique characteristics and methodologies within this domain, distinct from\nbroader existing reviews in the field. It presents a unified view of DM\nfundamentals and explores research directions, including alternative input\ndomains, conditioning strategies, guidance, corruption spaces, and zero-shot\nmethods. This survey provides insights into the evolution of image SR with DMs,\naddressing current trends, challenges, and future directions in this rapidly\nevolving field.\n","authors":["Brian B. Moser","Arundhati S. Shanbhag","Federico Raue","Stanislav Frolov","Sebastian Palacio","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2401.00736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00658v1","updated":"2024-01-01T04:11:55Z","published":"2024-01-01T04:11:55Z","title":"Point Cloud in the Air","summary":"  Acquisition and processing of point clouds (PCs) is a crucial enabler for\nmany emerging applications reliant on 3D spatial data, such as robot\nnavigation, autonomous vehicles, and augmented reality. In most scenarios, PCs\nacquired by remote sensors must be transmitted to an edge server for fusion,\nsegmentation, or inference. Wireless transmission of PCs not only puts on\nincreased burden on the already congested wireless spectrum, but also confronts\na unique set of challenges arising from the irregular and unstructured nature\nof PCs. In this paper, we meticulously delineate these challenges and offer a\ncomprehensive examination of existing solutions while candidly acknowledging\ntheir inherent limitations. In response to these intricacies, we proffer four\npragmatic solution frameworks, spanning advanced techniques, hybrid schemes,\nand distributed data aggregation approaches. In doing so, our goal is to chart\na path toward efficient, reliable, and low-latency wireless PC transmission.\n","authors":["Yulin Shao","Chenghong Bian","Li Yang","Qianqian Yang","Zhaoyang Zhang","Deniz Gunduz"],"pdf_url":"https://arxiv.org/pdf/2401.00658v1.pdf","comment":null}]},"2023-12-31T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2401.00609v1","updated":"2023-12-31T23:41:41Z","published":"2023-12-31T23:41:41Z","title":"A Survey of Personality, Persona, and Profile in Conversational Agents\n  and Chatbots","summary":"  We present a review of personality in neural conversational agents (CAs),\nalso called chatbots. First, we define Personality, Persona, and Profile. We\nexplain all personality schemes which have been used in CAs, and list models\nunder the scheme(s) which they use. Second we describe 21 datasets which have\nbeen developed in recent CA personality research. Third, we define the methods\nused to embody personality in a CA, and review recent models using them.\nFourth, we survey some relevant reviews on CAs, personality, and related\ntopics. Finally, we draw conclusions and identify some research challenges for\nthis important emerging field.\n","authors":["Richard Sutcliffe"],"pdf_url":"https://arxiv.org/pdf/2401.00609v1.pdf","comment":"25 pages, 6 tables, 207 references"},{"id":"http://arxiv.org/abs/2401.00908v1","updated":"2023-12-31T22:37:52Z","published":"2023-12-31T22:37:52Z","title":"DocLLM: A layout-aware generative language model for multimodal document\n  understanding","summary":"  Enterprise documents such as forms, invoices, receipts, reports, contracts,\nand other similar records, often carry rich semantics at the intersection of\ntextual and spatial modalities. The visual cues offered by their complex\nlayouts play a crucial role in comprehending these documents effectively. In\nthis paper, we present DocLLM, a lightweight extension to traditional large\nlanguage models (LLMs) for reasoning over visual documents, taking into account\nboth textual semantics and spatial layout. Our model differs from existing\nmultimodal LLMs by avoiding expensive image encoders and focuses exclusively on\nbounding box information to incorporate the spatial layout structure.\nSpecifically, the cross-alignment between text and spatial modalities is\ncaptured by decomposing the attention mechanism in classical transformers to a\nset of disentangled matrices. Furthermore, we devise a pre-training objective\nthat learns to infill text segments. This approach allows us to address\nirregular layouts and heterogeneous content frequently encountered in visual\ndocuments. The pre-trained model is fine-tuned using a large-scale instruction\ndataset, covering four core document intelligence tasks. We demonstrate that\nour solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks,\nand generalizes well to 4 out of 5 previously unseen datasets.\n","authors":["Dongsheng Wang","Natraj Raman","Mathieu Sibue","Zhiqiang Ma","Petr Babkin","Simerjot Kaur","Yulong Pei","Armineh Nourbakhsh","Xiaomo Liu"],"pdf_url":"https://arxiv.org/pdf/2401.00908v1.pdf","comment":"16 pages, 4 figures"},{"id":"http://arxiv.org/abs/2312.14335v2","updated":"2023-12-31T22:31:48Z","published":"2023-12-21T23:42:13Z","title":"Context-aware Decoding Reduces Hallucination in Query-focused\n  Summarization","summary":"  Query-focused summarization (QFS) aims to provide a summary of a single\ndocument/multi documents that can satisfy the information needs of a given\nquery. It is useful for various real-world applications, such as abstractive\nsnippet generation or more recent retrieval augmented generation (RAG). A\nprototypical QFS pipeline consists of a retriever (sparse or dense retrieval)\nand a generator (usually a large language model). However, applying large\nlanguage models (LLM) potentially leads to hallucinations, especially when the\nevidence contradicts the prior belief of LLMs. There has been growing interest\nin developing new decoding methods to improve generation quality and reduce\nhallucination. In this work, we conduct a large-scale reproducibility study on\none recently proposed decoding method -- Context-aware Decoding (CAD). In\naddition to replicating CAD's experiments on news summarization datasets, we\ninclude experiments on QFS datasets, and conduct more rigorous analysis on\ncomputational complexity and hyperparameter sensitivity. Experiments with eight\ndifferent language models show that performance-wise, CAD improves QFS quality\nby (1) reducing factuality errors/hallucinations while (2) mostly retaining the\nmatch of lexical patterns, measured by ROUGE scores, while also at a cost of\nincreased inference-time FLOPs and reduced decoding speed. The code\nimplementation based on Huggingface Library is made available\nhttps://github.com/zhichaoxu-shufe/context-aware-decoding-qfs\n","authors":["Zhichao Xu"],"pdf_url":"https://arxiv.org/pdf/2312.14335v2.pdf","comment":"technical report"},{"id":"http://arxiv.org/abs/2401.00595v1","updated":"2023-12-31T22:21:36Z","published":"2023-12-31T22:21:36Z","title":"State of What Art? A Call for Multi-Prompt LLM Evaluation","summary":"  Recent advances in large language models (LLMs) have led to the development\nof various evaluation benchmarks. These benchmarks typically rely on a single\ninstruction template for evaluating all LLMs on a specific task. In this paper,\nwe comprehensively analyze the brittleness of results obtained via\nsingle-prompt evaluations across 6.5M instances, involving 20 different LLMs\nand 39 tasks from 3 benchmarks. To improve robustness of the analysis, we\npropose to evaluate LLMs with a set of diverse prompts instead. We discuss\ntailored evaluation metrics for specific use cases (e.g., LLM developers vs.\ndevelopers interested in a specific downstream task), ensuring a more reliable\nand meaningful assessment of LLM capabilities. We then implement these criteria\nand conduct evaluations of multiple models, providing insights into the true\nstrengths and limitations of current LLMs.\n","authors":["Moran Mizrahi","Guy Kaplan","Dan Malkin","Rotem Dror","Dafna Shahaf","Gabriel Stanovsky"],"pdf_url":"https://arxiv.org/pdf/2401.00595v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00907v1","updated":"2023-12-31T21:18:16Z","published":"2023-12-31T21:18:16Z","title":"LaFFi: Leveraging Hybrid Natural Language Feedback for Fine-tuning\n  Language Models","summary":"  Fine-tuning Large Language Models (LLMs) adapts a trained model to specific\ndownstream tasks, significantly improving task-specific performance. Supervised\nFine-Tuning (SFT) is a common approach, where an LLM is trained to produce\ndesired answers. However, LLMs trained with SFT sometimes make simple mistakes\nand result in hallucinations on reasoning tasks such as question-answering.\nWithout external feedback, it is difficult for SFT to learn a good mapping\nbetween the question and the desired answer, especially with a small dataset.\nThis paper introduces an alternative to SFT called Natural Language Feedback\nfor Finetuning LLMs (LaFFi). LaFFi has LLMs directly predict the feedback they\nwill receive from an annotator. We find that requiring such reflection can\nsignificantly improve the accuracy in in-domain question-answering tasks,\nproviding a promising direction for the application of natural language\nfeedback in the realm of SFT LLMs. Additional ablation studies show that the\nportion of human-annotated data in the annotated datasets affects the\nfine-tuning performance.\n","authors":["Qianxi Li","Yingyue Cao","Jikun Kang","Tianpei Yang","Xi Chen","Jun Jin","Matthew E. Taylor"],"pdf_url":"https://arxiv.org/pdf/2401.00907v1.pdf","comment":"Paper accepted in Human-Centric Representation Learning workshop at\n  AAAI 2024 (https://hcrl-workshop.github.io/2024/)"},{"id":"http://arxiv.org/abs/2401.00582v1","updated":"2023-12-31T20:21:58Z","published":"2023-12-31T20:21:58Z","title":"An Analysis of Embedding Layers and Similarity Scores using Siamese\n  Neural Networks","summary":"  Large Lanugage Models (LLMs) are gaining increasing popularity in a variety\nof use cases, from language understanding and writing to assistance in\napplication development. One of the most important aspects for optimal\nfuncionality of LLMs is embedding layers. Word embeddings are distributed\nrepresentations of words in a continuous vector space. In the context of LLMs,\nwords or tokens from the input text are transformed into high-dimensional\nvectors using unique algorithms specific to the model. Our research examines\nthe embedding algorithms from leading companies in the industry, such as\nOpenAI, Google's PaLM, and BERT. Using medical data, we have analyzed\nsimilarity scores of each embedding layer, observing differences in performance\namong each algorithm. To enhance each model and provide an additional encoding\nlayer, we also implemented Siamese Neural Networks. After observing changes in\nperformance with the addition of the model, we measured the carbon footage per\nepoch of training. The carbon footprint associated with large language models\n(LLMs) is a significant concern, and should be taken into consideration when\nselecting algorithms for a variety of use cases. Overall, our research compared\nthe accuracy different, leading embedding algorithms and their carbon footage,\nallowing for a holistic review of each embedding algorithm.\n","authors":["Yash Bingi","Yiqiao Yin"],"pdf_url":"https://arxiv.org/pdf/2401.00582v1.pdf","comment":"10 pages, 11 figures"},{"id":"http://arxiv.org/abs/2401.00579v1","updated":"2023-12-31T20:02:10Z","published":"2023-12-31T20:02:10Z","title":"Exploring the Effectiveness of Instruction Tuning in Biomedical Language\n  Processing","summary":"  Large Language Models (LLMs), particularly those similar to ChatGPT, have\nsignificantly influenced the field of Natural Language Processing (NLP). While\nthese models excel in general language tasks, their performance in\ndomain-specific downstream tasks such as biomedical and clinical Named Entity\nRecognition (NER), Relation Extraction (RE), and Medical Natural Language\nInference (NLI) is still evolving. In this context, our study investigates the\npotential of instruction tuning for biomedical language processing, applying\nthis technique to two general LLMs of substantial scale. We present a\ncomprehensive, instruction-based model trained on a dataset that consists of\napproximately $200,000$ instruction-focused samples. This dataset represents a\ncarefully curated compilation of existing data, meticulously adapted and\nreformatted to align with the specific requirements of our instruction-based\ntasks. This initiative represents an important step in utilising such models to\nachieve results on par with specialised encoder-only models like BioBERT and\nBioClinicalBERT for various classical biomedical NLP tasks. Our work includes\nan analysis of the dataset's composition and its impact on model performance,\nproviding insights into the intricacies of instruction tuning. By sharing our\ncodes, models, and the distinctively assembled instruction-based dataset, we\nseek to encourage ongoing research and development in this area.\n","authors":["Omid Rohanian","Mohammadmahdi Nouriborji","David A. Clifton"],"pdf_url":"https://arxiv.org/pdf/2401.00579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.00392v4","updated":"2023-12-31T19:32:39Z","published":"2022-10-01T23:18:30Z","title":"Physical Computing: A Category Theoretic Perspective on Physical\n  Computation and System Compositionality","summary":"  This paper introduces a category theory-based framework to redefine physical\ncomputing in light of advancements in quantum computing and non-standard\ncomputing systems. By integrating classical definitions within this broader\nperspective, the paper rigorously recontextualizes what constitutes physical\ncomputing devices and processes. It demonstrates how the compositional nature\nand relational structures of physical computing systems can be coherently\nformalized using category theory. This approach not only encapsulates recent\nformalisms in physical computing but also offers a structured method to explore\nthe dynamic interactions within these systems.\n","authors":["Nima Dehghani","Gianluca Caterina"],"pdf_url":"https://arxiv.org/pdf/2210.00392v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00575v1","updated":"2023-12-31T19:25:34Z","published":"2023-12-31T19:25:34Z","title":"Neural Networks Against (and For) Self-Training: Classification with\n  Small Labeled and Large Unlabeled Sets","summary":"  We propose a semi-supervised text classifier based on self-training using one\npositive and one negative property of neural networks. One of the weaknesses of\nself-training is the semantic drift problem, where noisy pseudo-labels\naccumulate over iterations and consequently the error rate soars. In order to\ntackle this challenge, we reshape the role of pseudo-labels and create a\nhierarchical order of information. In addition, a crucial step in self-training\nis to use the classifier confidence prediction to select the best candidate\npseudo-labels. This step cannot be efficiently done by neural networks, because\nit is known that their output is poorly calibrated. To overcome this challenge,\nwe propose a hybrid metric to replace the plain confidence measurement. Our\nmetric takes into account the prediction uncertainty via a subsampling\ntechnique. We evaluate our model in a set of five standard benchmarks, and show\nthat it significantly outperforms a set of ten diverse baseline models.\nFurthermore, we show that the improvement achieved by our model is additive to\nlanguage model pretraining, which is a widely used technique for using\nunlabeled documents. Our code is available at\nhttps://github.com/p-karisani/RST.\n","authors":["Payam Karisani"],"pdf_url":"https://arxiv.org/pdf/2401.00575v1.pdf","comment":"ACL Findings 2023"},{"id":"http://arxiv.org/abs/2401.00536v1","updated":"2023-12-31T16:48:03Z","published":"2023-12-31T16:48:03Z","title":"A Multi-Task, Multi-Modal Approach for Predicting Categorical and\n  Dimensional Emotions","summary":"  Speech emotion recognition (SER) has received a great deal of attention in\nrecent years in the context of spontaneous conversations. While there have been\nnotable results on datasets like the well known corpus of naturalistic dyadic\nconversations, IEMOCAP, for both the case of categorical and dimensional\nemotions, there are few papers which try to predict both paradigms at the same\ntime. Therefore, in this work, we aim to highlight the performance contribution\nof multi-task learning by proposing a multi-task, multi-modal system that\npredicts categorical and dimensional emotions. The results emphasise the\nimportance of cross-regularisation between the two types of emotions. Our\napproach consists of a multi-task, multi-modal architecture that uses parallel\nfeature refinement through self-attention for the feature of each modality. In\norder to fuse the features, our model introduces a set of learnable bridge\ntokens that merge the acoustic and linguistic features with the help of\ncross-attention. Our experiments for categorical emotions on 10-fold validation\nyield results comparable to the current state-of-the-art. In our configuration,\nour multi-task approach provides better results compared to learning each\nparadigm separately. On top of that, our best performing model achieves a high\nresult for valence compared to the previous multi-task experiments.\n","authors":["Alex-Răzvan Ispas","Théo Deschamps-Berger","Laurence Devillers"],"pdf_url":"https://arxiv.org/pdf/2401.00536v1.pdf","comment":"Companion Publication of the 25th International Conference on\n  Multimodal Interaction (pp. 311-317)"},{"id":"http://arxiv.org/abs/2306.05836v2","updated":"2023-12-31T15:22:18Z","published":"2023-06-09T12:09:15Z","title":"Can Large Language Models Infer Causation from Correlation?","summary":"  Causal inference is one of the hallmarks of human intelligence. While the\nfield of CausalNLP has attracted much interest in the recent years, existing\ncausal inference datasets in NLP primarily rely on discovering causality from\nempirical knowledge (e.g., commonsense knowledge). In this work, we propose the\nfirst benchmark dataset to test the pure causal inference skills of large\nlanguage models (LLMs). Specifically, we formulate a novel task Corr2Cause,\nwhich takes a set of correlational statements and determines the causal\nrelationship between the variables. We curate a large-scale dataset of more\nthan 200K samples, on which we evaluate seventeen existing LLMs. Through our\nexperiments, we identify a key shortcoming of LLMs in terms of their causal\ninference skills, and show that these models achieve almost close to random\nperformance on the task. This shortcoming is somewhat mitigated when we try to\nre-purpose LLMs for this skill via finetuning, but we find that these models\nstill fail to generalize -- they can only perform causal inference in\nin-distribution settings when variable names and textual expressions used in\nthe queries are similar to those in the training set, but fail in\nout-of-distribution settings generated by perturbing these queries. Corr2Cause\nis a challenging task for LLMs, and would be helpful in guiding future research\non improving LLMs' pure reasoning skills and generalizability. Our data is at\nhttps://huggingface.co/datasets/causalnlp/corr2cause. Our code is at\nhttps://github.com/causalNLP/corr2cause.\n","authors":["Zhijing Jin","Jiarui Liu","Zhiheng Lyu","Spencer Poff","Mrinmaya Sachan","Rada Mihalcea","Mona Diab","Bernhard Schölkopf"],"pdf_url":"https://arxiv.org/pdf/2306.05836v2.pdf","comment":"v2.0: added 5 fine-tuned model performance; de-duplicated data; and\n  provided more fine-grained error analysis"},{"id":"http://arxiv.org/abs/2401.00504v1","updated":"2023-12-31T13:56:15Z","published":"2023-12-31T13:56:15Z","title":"HSC-GPT: A Large Language Model for Human Settlements Construction","summary":"  The field of human settlement construction encompasses a range of spatial\ndesigns and management tasks, including urban planning and landscape\narchitecture design. These tasks involve a plethora of instructions and\ndescriptions presented in natural language, which are essential for\nunderstanding design requirements and producing effective design solutions.\nRecent research has sought to integrate natural language processing (NLP) and\ngenerative artificial intelligence (AI) into human settlement construction\ntasks. Due to the efficient processing and analysis capabilities of AI with\ndata, significant successes have been achieved in design within this domain.\nHowever, this task still faces several fundamental challenges. The semantic\ninformation involved includes complex spatial details, diverse data source\nformats, high sensitivity to regional culture, and demanding requirements for\ninnovation and rigor in work scenarios. These factors lead to limitations when\napplying general generative AI in this field, further exacerbated by a lack of\nhigh-quality data for model training. To address these challenges, this paper\nfirst proposes HSC-GPT, a large-scale language model framework specifically\ndesigned for tasks in human settlement construction, considering the unique\ncharacteristics of this domain.\n","authors":["Chen Ran","Yao Xueqi","Jiang Xuhui","Han Zhengqi","Guo Jingze","Zhang Xianyue","Lin Chunyu","Liu Chumin","Zhao Jing","Lian Zeke","Zhang Jingjing","Li Keke"],"pdf_url":"https://arxiv.org/pdf/2401.00504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11097v5","updated":"2023-12-31T12:09:52Z","published":"2023-10-17T09:27:43Z","title":"Experimenting AI Technologies for Disinformation Combat: the IDMO\n  Project","summary":"  The Italian Digital Media Observatory (IDMO) project, part of a European\ninitiative, focuses on countering disinformation and fake news. This report\noutlines contributions from Rai-CRITS to the project, including: (i) the\ncreation of novel datasets for testing technologies (ii) development of an\nautomatic model for categorizing Pagella Politica verdicts to facilitate\nbroader analysis (iii) creation of an automatic model for recognizing textual\nentailment with exceptional accuracy on the FEVER dataset (iv) assessment using\nGPT-4 to detecting content treatment style (v) a game to raise awareness about\nfake news at national events.\n","authors":["Lorenzo Canale","Alberto Messina"],"pdf_url":"https://arxiv.org/pdf/2310.11097v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00448v1","updated":"2023-12-31T10:53:58Z","published":"2023-12-31T10:53:58Z","title":"Beyond Chinchilla-Optimal: Accounting for Inference in Language Model\n  Scaling Laws","summary":"  Large language model (LLM) scaling laws are empirical formulas that estimate\nchanges in model quality as a result of increasing parameter count and training\ndata. However, these formulas, including the popular DeepMind Chinchilla\nscaling laws, neglect to include the cost of inference. We modify the\nChinchilla scaling laws to calculate the optimal LLM parameter count and\npre-training data size to train and deploy a model of a given quality and\ninference demand. We conduct our analysis both in terms of a compute budget and\nreal-world costs and find that LLM researchers expecting reasonably large\ninference demand (~1B requests) should train models smaller and longer than\nChinchilla-optimal.\n","authors":["Nikhil Sardana","Jonathan Frankle"],"pdf_url":"https://arxiv.org/pdf/2401.00448v1.pdf","comment":"8 pages, 2 figures, To appear in the 3rd NeurIPS Workshop on\n  Efficient Natural Language and Speech Processing (ENLSP), 2023"},{"id":"http://arxiv.org/abs/2401.00437v1","updated":"2023-12-31T09:34:51Z","published":"2023-12-31T09:34:51Z","title":"BatchEval: Towards Human-like Text Evaluation","summary":"  Significant progress has been made in automatic text evaluation with the\nintroduction of large language models (LLMs) as evaluators. However, current\nsample-wise evaluation paradigm suffers from the following issues: (1)\nSensitive to prompt design; (2) Poor resistance to noise; (3) Inferior ensemble\nperformance with static reference. Inspired by the fact that humans treat both\ncriterion definition and inter sample comparison as references for evaluation,\nwe propose BatchEval, a paradigm that conducts batch-wise evaluation\niteratively to alleviate the above problems. We explore variants under this\nparadigm and confirm the optimal settings are two stage procedure with\nheterogeneous batch composition strategy and decimal scoring format.\nComprehensive experiments across 3 LLMs on 4 text evaluation tasks demonstrate\nthat BatchEval outperforms state-of-the-art methods by 10.5% on Pearson\ncorrelations with only 64% API cost on average. Further analyses have been\nconducted to verify the robustness, generalization, and working mechanism of\nBatchEval.\n","authors":["Peiwen Yuan","Shaoxiong Feng","Yiwei Li","Xinglin Wang","Boyuan Pan","Heda Wang","Kan Li"],"pdf_url":"https://arxiv.org/pdf/2401.00437v1.pdf","comment":"19 pages, 9 figures"},{"id":"http://arxiv.org/abs/2401.00434v1","updated":"2023-12-31T09:22:54Z","published":"2023-12-31T09:22:54Z","title":"GeoGalactica: A Scientific Large Language Model in Geoscience","summary":"  Large language models (LLMs) have achieved huge success for their general\nknowledge and ability to solve a wide spectrum of tasks in natural language\nprocessing (NLP). Due to their impressive abilities, LLMs have shed light on\npotential inter-discipline applications to foster scientific discoveries of a\nspecific domain by using artificial intelligence (AI for science, AI4S). In the\nmeantime, utilizing NLP techniques in geoscience research and practice is wide\nand convoluted, contributing from knowledge extraction and document\nclassification to question answering and knowledge discovery. In this work, we\ntake the initial step to leverage LLM for science, through a rather\nstraightforward approach. We try to specialize an LLM into geoscience, by\nfurther pre-training the model with a vast amount of texts in geoscience, as\nwell as supervised fine-tuning (SFT) the resulting model with our custom\ncollected instruction tuning dataset. These efforts result in a model\nGeoGalactica consisting of 30 billion parameters. To our best knowledge, it is\nthe largest language model for the geoscience domain. More specifically,\nGeoGalactica is from further pre-training of Galactica. We train GeoGalactica\nover a geoscience-related text corpus containing 65 billion tokens curated from\nextensive data sources in the big science project Deep-time Digital Earth\n(DDE), preserving as the largest geoscience-specific text corpus. Then we\nfine-tune the model with 1 million pairs of instruction-tuning data consisting\nof questions that demand professional geoscience knowledge to answer. In this\ntechnical report, we will illustrate in detail all aspects of GeoGalactica,\nincluding data collection, data cleaning, base model selection, pre-training,\nSFT, and evaluation. We open-source our data curation tools and the checkpoints\nof GeoGalactica during the first 3/4 of pre-training.\n","authors":["Zhouhan Lin","Cheng Deng","Le Zhou","Tianhang Zhang","Yi Xu","Yutong Xu","Zhongmou He","Yuanyuan Shi","Beiya Dai","Yunchong Song","Boyi Zeng","Qiyuan Chen","Tao Shi","Tianyu Huang","Yiwei Xu","Shu Wang","Luoyi Fu","Weinan Zhang","Junxian He","Chao Ma","Yunqiang Zhu","Xinbing Wang","Chenghu Zhou"],"pdf_url":"https://arxiv.org/pdf/2401.00434v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00426v1","updated":"2023-12-31T08:39:04Z","published":"2023-12-31T08:39:04Z","title":"keqing: knowledge-based question answering is a nature chain-of-thought\n  mentor of LLM","summary":"  Large language models (LLMs) have exhibited remarkable performance on various\nnatural language processing (NLP) tasks, especially for question answering.\nHowever, in the face of problems beyond the scope of knowledge, these LLMs tend\nto talk nonsense with a straight face, where the potential solution could be\nincorporating an Information Retrieval (IR) module and generating response\nbased on these retrieved knowledge. In this paper, we present a novel framework\nto assist LLMs, such as ChatGPT, to retrieve question-related structured\ninformation on the knowledge graph, and demonstrate that Knowledge-based\nquestion answering (Keqing) could be a nature Chain-of-Thought (CoT) mentor to\nguide the LLM to sequentially find the answer entities of a complex question\nthrough interpretable logical chains. Specifically, the workflow of Keqing will\nexecute decomposing a complex question according to predefined templates,\nretrieving candidate entities on knowledge graph, reasoning answers of\nsub-questions, and finally generating response with reasoning paths, which\ngreatly improves the reliability of LLM's response. The experimental results on\nKBQA datasets show that Keqing can achieve competitive performance and\nillustrate the logic of answering each question.\n","authors":["Chaojie Wang","Yishi Xu","Zhong Peng","Chenxi Zhang","Bo Chen","Xinrun Wang","Lei Feng","Bo An"],"pdf_url":"https://arxiv.org/pdf/2401.00426v1.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2401.00424v1","updated":"2023-12-31T08:33:37Z","published":"2023-12-31T08:33:37Z","title":"SDIF-DA: A Shallow-to-Deep Interaction Framework with Data Augmentation\n  for Multi-modal Intent Detection","summary":"  Multi-modal intent detection aims to utilize various modalities to understand\nthe user's intentions, which is essential for the deployment of dialogue\nsystems in real-world scenarios. The two core challenges for multi-modal intent\ndetection are (1) how to effectively align and fuse different features of\nmodalities and (2) the limited labeled multi-modal intent training data. In\nthis work, we introduce a shallow-to-deep interaction framework with data\naugmentation (SDIF-DA) to address the above challenges. Firstly, SDIF-DA\nleverages a shallow-to-deep interaction module to progressively and effectively\nalign and fuse features across text, video, and audio modalities. Secondly, we\npropose a ChatGPT-based data augmentation approach to automatically augment\nsufficient training data. Experimental results demonstrate that SDIF-DA can\neffectively align and fuse multi-modal features by achieving state-of-the-art\nperformance. In addition, extensive analyses show that the introduced data\naugmentation approach can successfully distill knowledge from the large\nlanguage model.\n","authors":["Shijue Huang","Libo Qin","Bingbing Wang","Geng Tu","Ruifeng Xu"],"pdf_url":"https://arxiv.org/pdf/2401.00424v1.pdf","comment":"Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2312.08793v3","updated":"2023-12-31T05:30:30Z","published":"2023-12-14T10:27:15Z","title":"Forbidden Facts: An Investigation of Competing Objectives in Llama-2","summary":"  LLMs often face competing pressures (for example helpfulness vs.\nharmlessness). To understand how models resolve such conflicts, we study\nLlama-2-chat models on the forbidden fact task. Specifically, we instruct\nLlama-2 to truthfully complete a factual recall statement while forbidding it\nfrom saying the correct answer. This often makes the model give incorrect\nanswers. We decompose Llama-2 into 1000+ components, and rank each one with\nrespect to how useful it is for forbidding the correct answer. We find that in\naggregate, around 35 components are enough to reliably implement the full\nsuppression behavior. However, these components are fairly heterogeneous and\nmany operate using faulty heuristics. We discover that one of these heuristics\ncan be exploited via a manually designed adversarial attack which we call The\nCalifornia Attack. Our results highlight some roadblocks standing in the way of\nbeing able to successfully interpret advanced ML systems. Project website\navailable at https://forbiddenfacts.github.io .\n","authors":["Tony T. Wang","Miles Wang","Kaivalya Hariharan","Nir Shavit"],"pdf_url":"https://arxiv.org/pdf/2312.08793v3.pdf","comment":"Accepted to the ATTRIB and SoLaR workshops at NeurIPS 2023; (v3:\n  clarified experimental details)"},{"id":"http://arxiv.org/abs/2401.00396v1","updated":"2023-12-31T04:43:45Z","published":"2023-12-31T04:43:45Z","title":"RAGTruth: A Hallucination Corpus for Developing Trustworthy\n  Retrieval-Augmented Language Models","summary":"  Retrieval-augmented generation (RAG) has become a main technique for\nalleviating hallucinations in large language models (LLMs). Despite the\nintegration of RAG, LLMs may still present unsupported or contradictory claims\nto the retrieved contents. In order to develop effective hallucination\nprevention strategies under RAG, it is important to create benchmark datasets\nthat can measure the extent of hallucination. This paper presents RAGTruth, a\ncorpus tailored for analyzing word-level hallucinations in various domains and\ntasks within the standard RAG frameworks for LLM applications. RAGTruth\ncomprises nearly 18,000 naturally generated responses from diverse LLMs using\nRAG. These responses have undergone meticulous manual annotations at both the\nindividual cases and word levels, incorporating evaluations of hallucination\nintensity. We not only benchmark hallucination frequencies across different\nLLMs, but also critically assess the effectiveness of several existing\nhallucination detection methodologies. Furthermore, we show that using a\nhigh-quality dataset such as RAGTruth, it is possible to finetune a relatively\nsmall LLM and achieve a competitive level of performance in hallucination\ndetection when compared to the existing prompt-based approaches using\nstate-of-the-art large language models such as GPT-4.\n","authors":["Yuanhao Wu","Juno Zhu","Siliang Xu","Kashun Shum","Cheng Niu","Randy Zhong","Juntong Song","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.00396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05450v2","updated":"2023-12-31T04:20:00Z","published":"2023-10-09T06:54:02Z","title":"Empower Nested Boolean Logic via Self-Supervised Curriculum Learning","summary":"  Beyond the great cognitive powers showcased by language models, it is crucial\nto scrutinize whether their reasoning capabilities stem from strong\ngeneralization or merely exposure to relevant data. As opposed to constructing\nincreasingly complex logic, this paper probes into the boolean logic, the root\ncapability of a logical reasoner. We find that any pre-trained language models\neven including large language models only behave like a random selector in the\nface of multi-nested boolean logic, a task that humans can handle with ease. To\nempower language models with this fundamental capability, this paper proposes a\nnew self-supervised learning method \\textit{Curriculum Logical Reasoning}\n(\\textsc{Clr}), where we augment the training data with nested boolean logic\nchain step-by-step, and program the training from simpler logical patterns\ngradually to harder ones. This new training paradigm allows language models to\neffectively generalize to much harder and longer-hop logic, which can hardly be\nlearned through naive training. Furthermore, we show that boolean logic is a\ngreat foundation for improving the subsequent general logical tasks.\n","authors":["Hongqiu Wu","Linfeng Liu","Hai Zhao","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.05450v2.pdf","comment":"Accepted by EMNLP2023"},{"id":"http://arxiv.org/abs/2401.00388v1","updated":"2023-12-31T03:51:31Z","published":"2023-12-31T03:51:31Z","title":"FusionMind -- Improving question and answering with external context\n  fusion","summary":"  Answering questions using pre-trained language models (LMs) and knowledge\ngraphs (KGs) presents challenges in identifying relevant knowledge and\nperforming joint reasoning.We compared LMs (fine-tuned for the task) with the\npreviously published QAGNN method for the Question-answering (QA) objective and\nfurther measured the impact of additional factual context on the QAGNN\nperformance. The QAGNN method employs LMs to encode QA context and estimate KG\nnode importance, and effectively update the question choice entity\nrepresentations using Graph Neural Networks (GNNs). We further experimented\nwith enhancing the QA context encoding by incorporating relevant knowledge\nfacts for the question stem. The models are trained on the OpenbookQA dataset,\nwhich contains ~6000 4-way multiple choice questions and is widely used as a\nbenchmark for QA tasks. Through our experimentation, we found that\nincorporating knowledge facts context led to a significant improvement in\nperformance. In contrast, the addition of knowledge graphs to language models\nresulted in only a modest increase. This suggests that the integration of\ncontextual knowledge facts may be more impactful for enhancing question\nanswering performance compared to solely adding knowledge graphs.\n","authors":["Shreyas Verma","Manoj Parmar","Palash Choudhary","Sanchita Porwal"],"pdf_url":"https://arxiv.org/pdf/2401.00388v1.pdf","comment":"5 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2401.00383v1","updated":"2023-12-31T03:30:42Z","published":"2023-12-31T03:30:42Z","title":"Predicting Evoked Emotions in Conversations","summary":"  Understanding and predicting the emotional trajectory in multi-party\nmulti-turn conversations is of great significance. Such information can be\nused, for example, to generate empathetic response in human-machine interaction\nor to inform models of pre-emptive toxicity detection. In this work, we\nintroduce the novel problem of Predicting Emotions in Conversations (PEC) for\nthe next turn (n+1), given combinations of textual and/or emotion input up to\nturn n. We systematically approach the problem by modeling three dimensions\ninherently connected to evoked emotions in dialogues, including (i) sequence\nmodeling, (ii) self-dependency modeling, and (iii) recency modeling. These\nmodeling dimensions are then incorporated into two deep neural network\narchitectures, a sequence model and a graph convolutional network model. The\nformer is designed to capture the sequence of utterances in a dialogue, while\nthe latter captures the sequence of utterances and the network formation of\nmulti-party dialogues. We perform a comprehensive empirical evaluation of the\nvarious proposed models for addressing the PEC problem. The results indicate\n(i) the importance of the self-dependency and recency model dimensions for the\nprediction task, (ii) the quality of simpler sequence models in short\ndialogues, (iii) the importance of the graph neural models in improving the\npredictions in long dialogues.\n","authors":["Enas Altarawneh","Ameeta Agrawal","Michael Jenkin","Manos Papagelis"],"pdf_url":"https://arxiv.org/pdf/2401.00383v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00368v1","updated":"2023-12-31T02:13:18Z","published":"2023-12-31T02:13:18Z","title":"Improving Text Embeddings with Large Language Models","summary":"  In this paper, we introduce a novel and simple method for obtaining\nhigh-quality text embeddings using only synthetic data and less than 1k\ntraining steps. Unlike existing methods that often depend on multi-stage\nintermediate pre-training with billions of weakly-supervised text pairs,\nfollowed by fine-tuning with a few labeled datasets, our method does not\nrequire building complex training pipelines or relying on manually collected\ndatasets that are often constrained by task diversity and language coverage. We\nleverage proprietary LLMs to generate diverse synthetic data for hundreds of\nthousands of text embedding tasks across nearly 100 languages. We then\nfine-tune open-source decoder-only LLMs on the synthetic data using standard\ncontrastive loss. Experiments demonstrate that our method achieves strong\nperformance on highly competitive text embedding benchmarks without using any\nlabeled data. Furthermore, when fine-tuned with a mixture of synthetic and\nlabeled data, our model sets new state-of-the-art results on the BEIR and MTEB\nbenchmarks.\n","authors":["Liang Wang","Nan Yang","Xiaolong Huang","Linjun Yang","Rangan Majumder","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2401.00368v1.pdf","comment":"15 pages, 8 tables"},{"id":"http://arxiv.org/abs/2401.00366v1","updated":"2023-12-31T01:51:43Z","published":"2023-12-31T01:51:43Z","title":"Argumentation in Waltz's \"Emerging Structure of International Politics''","summary":"  We present an annotation scheme for argumentative and domain-specific aspects\nof scholarly articles on the theory of International Relations. At\nargumentation level we identify Claims and Support/Attack relations. At domain\nlevel we model discourse content in terms of Theory and Data-related\nstatements. We annotate Waltz's 1993 text on structural realism and show that\nour scheme can be reliably applied by domain experts enables insights on two\nresearch questions on justifications of claims.\n","authors":["Magdalena Wolska","Bernd Fröhlich","Katrin Girgensohn","Sassan Gholiagha","Dora Kiesel","Jürgen Neyer","Patrick Riehmann","Mitja Sienknecht","Benno Stein"],"pdf_url":"https://arxiv.org/pdf/2401.00366v1.pdf","comment":"9 pages"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2401.00910v1","updated":"2023-12-31T23:53:50Z","published":"2023-12-31T23:53:50Z","title":"WoodScape Motion Segmentation for Autonomous Driving -- CVPR 2023 OmniCV\n  Workshop Challenge","summary":"  Motion segmentation is a complex yet indispensable task in autonomous\ndriving. The challenges introduced by the ego-motion of the cameras, radial\ndistortion in fisheye lenses, and the need for temporal consistency make the\ntask more complicated, rendering traditional and standard Convolutional Neural\nNetwork (CNN) approaches less effective. The consequent laborious data\nlabeling, representation of diverse and uncommon scenarios, and extensive data\ncapture requirements underscore the imperative of synthetic data for improving\nmachine learning model performance. To this end, we employ the PD-WoodScape\nsynthetic dataset developed by Parallel Domain, alongside the WoodScape fisheye\ndataset. Thus, we present the WoodScape fisheye motion segmentation challenge\nfor autonomous driving, held as part of the CVPR 2023 Workshop on\nOmnidirectional Computer Vision (OmniCV). As one of the first competitions\nfocused on fisheye motion segmentation, we aim to explore and evaluate the\npotential and impact of utilizing synthetic data in this domain. In this paper,\nwe provide a detailed analysis on the competition which attracted the\nparticipation of 112 global teams and a total of 234 submissions. This study\ndelineates the complexities inherent in the task of motion segmentation,\nemphasizes the significance of fisheye datasets, articulate the necessity for\nsynthetic datasets and the resultant domain gap they engender, outlining the\nfoundational blueprint for devising successful solutions. Subsequently, we\ndelve into the details of the baseline experiments and winning methods\nevaluating their qualitative and quantitative results, providing with useful\ninsights.\n","authors":["Saravanabalagi Ramachandran","Nathaniel Cibik","Ganesh Sistu","John McDonald"],"pdf_url":"https://arxiv.org/pdf/2401.00910v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00608v1","updated":"2023-12-31T23:32:03Z","published":"2023-12-31T23:32:03Z","title":"Bringing Back the Context: Camera Trap Species Identification as Link\n  Prediction on Multimodal Knowledge Graphs","summary":"  Camera traps are valuable tools in animal ecology for biodiversity monitoring\nand conservation. However, challenges like poor generalization to deployment at\nnew unseen locations limit their practical application. Images are naturally\nassociated with heterogeneous forms of context possibly in different\nmodalities. In this work, we leverage the structured context associated with\nthe camera trap images to improve out-of-distribution generalization for the\ntask of species identification in camera traps. For example, a photo of a wild\nanimal may be associated with information about where and when it was taken, as\nwell as structured biology knowledge about the animal species. While typically\noverlooked by existing work, bringing back such context offers several\npotential benefits for better image understanding, such as addressing data\nscarcity and enhancing generalization. However, effectively integrating such\nheterogeneous context into the visual domain is a challenging problem. To\naddress this, we propose a novel framework that reformulates species\nclassification as link prediction in a multimodal knowledge graph (KG). This\nframework seamlessly integrates various forms of multimodal context for visual\nrecognition. We apply this framework for out-of-distribution species\nclassification on the iWildCam2020-WILDS and Snapshot Mountain Zebra datasets\nand achieve competitive performance with state-of-the-art approaches.\nFurthermore, our framework successfully incorporates biological taxonomy for\nimproved generalization and enhances sample efficiency for recognizing\nunder-represented species.\n","authors":["Vardaan Pahuja","Weidi Luo","Yu Gu","Cheng-Hao Tu","Hong-You Chen","Tanya Berger-Wolf","Charles Stewart","Song Gao","Wei-Lun Chao","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2401.00608v1.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2401.00604v1","updated":"2023-12-31T23:04:25Z","published":"2023-12-31T23:04:25Z","title":"SteinDreamer: Variance Reduction for Text-to-3D Score Distillation via\n  Stein Identity","summary":"  Score distillation has emerged as one of the most prevalent approaches for\ntext-to-3D asset synthesis. Essentially, score distillation updates 3D\nparameters by lifting and back-propagating scores averaged over different\nviews. In this paper, we reveal that the gradient estimation in score\ndistillation is inherent to high variance. Through the lens of variance\nreduction, the effectiveness of SDS and VSD can be interpreted as applications\nof various control variates to the Monte Carlo estimator of the distilled\nscore. Motivated by this rethinking and based on Stein's identity, we propose a\nmore general solution to reduce variance for score distillation, termed Stein\nScore Distillation (SSD). SSD incorporates control variates constructed by\nStein identity, allowing for arbitrary baseline functions. This enables us to\ninclude flexible guidance priors and network architectures to explicitly\noptimize for variance reduction. In our experiments, the overall pipeline,\ndubbed SteinDreamer, is implemented by instantiating the control variate with a\nmonocular depth estimator. The results suggest that SSD can effectively reduce\nthe distillation variance and consistently improve visual quality for both\nobject- and scene-level generation. Moreover, we demonstrate that SteinDreamer\nachieves faster convergence than existing methods due to more stable gradient\nupdates.\n","authors":["Peihao Wang","Zhiwen Fan","Dejia Xu","Dilin Wang","Sreyas Mohan","Forrest Iandola","Rakesh Ranjan","Yilei Li","Qiang Liu","Zhangyang Wang","Vikas Chandra"],"pdf_url":"https://arxiv.org/pdf/2401.00604v1.pdf","comment":"Project page: https://vita-group.github.io/SteinDreamer/"},{"id":"http://arxiv.org/abs/2401.00909v1","updated":"2023-12-31T22:47:06Z","published":"2023-12-31T22:47:06Z","title":"Taming Mode Collapse in Score Distillation for Text-to-3D Generation","summary":"  Despite the remarkable performance of score distillation in text-to-3D\ngeneration, such techniques notoriously suffer from view inconsistency issues,\nalso known as \"Janus\" artifact, where the generated objects fake each view with\nmultiple front faces. Although empirically effective methods have approached\nthis problem via score debiasing or prompt engineering, a more rigorous\nperspective to explain and tackle this problem remains elusive. In this paper,\nwe reveal that the existing score distillation-based text-to-3D generation\nframeworks degenerate to maximal likelihood seeking on each view independently\nand thus suffer from the mode collapse problem, manifesting as the Janus\nartifact in practice. To tame mode collapse, we improve score distillation by\nre-establishing in entropy term in the corresponding variational objective,\nwhich is applied to the distribution of rendered images. Maximizing the entropy\nencourages diversity among different views in generated 3D assets, thereby\nmitigating the Janus problem. Based on this new objective, we derive a new\nupdate rule for 3D score distillation, dubbed Entropic Score Distillation\n(ESD). We theoretically reveal that ESD can be simplified and implemented by\njust adopting the classifier-free guidance trick upon variational score\ndistillation. Although embarrassingly straightforward, our extensive\nexperiments successfully demonstrate that ESD can be an effective treatment for\nJanus artifacts in score distillation.\n","authors":["Peihao Wang","Dejia Xu","Zhiwen Fan","Dilin Wang","Sreyas Mohan","Forrest Iandola","Rakesh Ranjan","Yilei Li","Qiang Liu","Zhangyang Wang","Vikas Chandra"],"pdf_url":"https://arxiv.org/pdf/2401.00909v1.pdf","comment":"Project page: https://vita-group.github.io/3D-Mode-Collapse/"},{"id":"http://arxiv.org/abs/2311.13091v2","updated":"2023-12-31T20:04:25Z","published":"2023-11-22T01:43:57Z","title":"Stable Unlearnable Example: Enhancing the Robustness of Unlearnable\n  Examples via Stable Error-Minimizing Noise","summary":"  The open source of large amounts of image data promotes the development of\ndeep learning techniques. Along with this comes the privacy risk of these\nopen-source image datasets being exploited by unauthorized third parties to\ntrain deep learning models for commercial or illegal purposes. To avoid the\nabuse of public data, a poisoning-based technique, the unlearnable example, is\nproposed to significantly degrade the generalization performance of models by\nadding a kind of imperceptible noise to the data. To further enhance its\nrobustness against adversarial training, existing works leverage iterative\nadversarial training on both the defensive noise and the surrogate model.\nHowever, it still remains unknown whether the robustness of unlearnable\nexamples primarily comes from the effect of enhancement in the surrogate model\nor the defensive noise. Observing that simply removing the adversarial noise on\nthe training process of the defensive noise can improve the performance of\nrobust unlearnable examples, we identify that solely the surrogate model's\nrobustness contributes to the performance. Furthermore, we found a negative\ncorrelation exists between the robustness of defensive noise and the protection\nperformance, indicating defensive noise's instability issue. Motivated by this,\nto further boost the robust unlearnable example, we introduce stable\nerror-minimizing noise (SEM), which trains the defensive noise against random\nperturbation instead of the time-consuming adversarial perturbation to improve\nthe stability of defensive noise. Through extensive experiments, we demonstrate\nthat SEM achieves a new state-of-the-art performance on CIFAR-10, CIFAR-100,\nand ImageNet Subset in terms of both effectiveness and efficiency. The code is\navailable at https://github.com/liuyixin-louis/Stable-Unlearnable-Example.\n","authors":["Yixin Liu","Kaidi Xu","Xun Chen","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2311.13091v2.pdf","comment":"Accepted to AAAI 2024"},{"id":"http://arxiv.org/abs/2312.05634v2","updated":"2023-12-31T17:50:47Z","published":"2023-12-09T18:43:05Z","title":"PGS: Pose-Guided Supervision for Mitigating Clothes-Changing in Person\n  Re-Identification","summary":"  Person Re-Identification (Re-ID) task seeks to enhance the tracking of\nmultiple individuals by surveillance cameras. It provides additional support\nfor multimodal tasks, including text-based person retrieval and human matching.\nAmong the significant challenges faced in Re-ID, one of the most prominent is\ndealing with clothes-changing, where the same person may appear in different\noutfits. While previous methods have made notable progress in maintaining\nclothing data consistency and handling clothing change data, they still tend to\nrely excessively on clothing information, which can limit performance due to\nthe dynamic nature of human appearances. To mitigate this challenge, we propose\nthe Pose-Guided Supervision (PGS), an effective framework for learning pose\nguidance within the Re-ID task. Our PGS consists of three modules: a human\nencoder, a pose encoder, and a Pose-to-Human Projection module (PHP). The pose\nencoder module utilizes a frozen pre-trained model while we fine-tune a\npre-trained human-centric model for the human encoder module. Our PHP transfers\npose knowledge from the pose encoder module to the human encoder module through\nmultiple projectors. Our framework, following extensive experimentation on five\nbenchmark datasets, consistently surpasses the performance of current\nstate-of-the-art methods. Our code is available at\nhttps://github.com/huyquoctrinh/PGS.\n","authors":["Quoc-Huy Trinh","Nhat-Tan Bui","Dinh-Hieu Hoang","Phuoc-Thao Vo Thi","Hai-Dang Nguyen","Debesh Jha","Ulas Bagci","Ngan Le","Minh-Triet Tran"],"pdf_url":"https://arxiv.org/pdf/2312.05634v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00551v1","updated":"2023-12-31T17:41:48Z","published":"2023-12-31T17:41:48Z","title":"A Generalist FaceX via Learning Unified Facial Representation","summary":"  This work presents FaceX framework, a novel facial generalist model capable\nof handling diverse facial tasks simultaneously. To achieve this goal, we\ninitially formulate a unified facial representation for a broad spectrum of\nfacial editing tasks, which macroscopically decomposes a face into fundamental\nidentity, intra-personal variation, and environmental factors. Based on this,\nwe introduce Facial Omni-Representation Decomposing (FORD) for seamless\nmanipulation of various facial components, microscopically decomposing the core\naspects of most facial editing tasks. Furthermore, by leveraging the prior of a\npretrained StableDiffusion (SD) to enhance generation quality and accelerate\ntraining, we design Facial Omni-Representation Steering (FORS) to first\nassemble unified facial representations and then effectively steer the SD-aware\ngeneration process by the efficient Facial Representation Controller (FRC).\n%Without any additional features, Our versatile FaceX achieves competitive\nperformance compared to elaborate task-specific models on popular facial\nediting tasks. Full codes and models will be available at\nhttps://github.com/diffusion-facex/FaceX.\n","authors":["Yue Han","Jiangning Zhang","Junwei Zhu","Xiangtai Li","Yanhao Ge","Wei Li","Chengjie Wang","Yong Liu","Xiaoming Liu","Ying Tai"],"pdf_url":"https://arxiv.org/pdf/2401.00551v1.pdf","comment":"Project page: https://diffusion-facex.github.io/"},{"id":"http://arxiv.org/abs/2306.07520v4","updated":"2023-12-31T16:54:05Z","published":"2023-06-13T03:25:33Z","title":"Instruct-ReID: A Multi-purpose Person Re-identification Task with\n  Instructions","summary":"  Human intelligence can retrieve any person according to both visual and\nlanguage descriptions. However, the current computer vision community studies\nspecific person re-identification (ReID) tasks in different scenarios\nseparately, which limits the applications in the real world. This paper strives\nto resolve this problem by proposing a new instruct-ReID task that requires the\nmodel to retrieve images according to the given image or language instructions.\nOur instruct-ReID is a more general ReID setting, where existing 6 ReID tasks\ncan be viewed as special cases by designing different instructions. We propose\na large-scale OmniReID benchmark and an adaptive triplet loss as a baseline\nmethod to facilitate research in this new setting. Experimental results show\nthat the proposed multi-purpose ReID model, trained on our OmniReID benchmark\nwithout fine-tuning, can improve +0.5%, +0.6%, +7.7% mAP on Market1501, MSMT17,\nCUHK03 for traditional ReID, +6.4%, +7.1%, +11.2% mAP on PRCC, VC-Clothes, LTCC\nfor clothes-changing ReID, +11.7% mAP on COCAS+ real2 for clothes template\nbased clothes-changing ReID when using only RGB images, +24.9% mAP on COCAS+\nreal2 for our newly defined language-instructed ReID, +4.3% on LLCM for\nvisible-infrared ReID, +2.6% on CUHK-PEDES for text-to-image ReID. The\ndatasets, the model, and code will be available at\nhttps://github.com/hwz-zju/Instruct-ReID.\n","authors":["Weizhen He","Yiheng Deng","Shixiang Tang","Qihao Chen","Qingsong Xie","Yizhou Wang","Lei Bai","Feng Zhu","Rui Zhao","Wanli Ouyang","Donglian Qi","Yunfeng Yan"],"pdf_url":"https://arxiv.org/pdf/2306.07520v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.05673v3","updated":"2023-12-31T16:09:53Z","published":"2023-04-12T07:49:21Z","title":"Precise localization of corneal reflections in eye images using deep\n  learning trained on synthetic data","summary":"  We present a deep learning method for accurately localizing the center of a\nsingle corneal reflection (CR) in an eye image. Unlike previous approaches, we\nuse a convolutional neural network (CNN) that was trained solely using\nsimulated data. Using only simulated data has the benefit of completely\nsidestepping the time-consuming process of manual annotation that is required\nfor supervised training on real eye images. To systematically evaluate the\naccuracy of our method, we first tested it on images with simulated CRs placed\non different backgrounds and embedded in varying levels of noise. Second, we\ntested the method on high-quality videos captured from real eyes. Our method\noutperformed state-of-the-art algorithmic methods on real eye images with a 35%\nreduction in terms of spatial precision, and performed on par with\nstate-of-the-art on simulated images in terms of spatial accuracy.We conclude\nthat our method provides a precise method for CR center localization and\nprovides a solution to the data availability problem which is one of the\nimportant common roadblocks in the development of deep learning models for gaze\nestimation. Due to the superior CR center localization and ease of application,\nour method has the potential to improve the accuracy and precision of CR-based\neye trackers\n","authors":["Sean Anthony Byrne","Marcus Nyström","Virmarie Maquiling","Enkelejda Kasneci","Diederick C. Niehorster"],"pdf_url":"https://arxiv.org/pdf/2304.05673v3.pdf","comment":"Published in Behavioural Research Methods"},{"id":"http://arxiv.org/abs/2401.00523v1","updated":"2023-12-31T15:38:50Z","published":"2023-12-31T15:38:50Z","title":"Compressing Deep Image Super-resolution Models","summary":"  Deep learning techniques have been applied in the context of image\nsuper-resolution (SR), achieving remarkable advances in terms of reconstruction\nperformance. Existing techniques typically employ highly complex model\nstructures which result in large model sizes and slow inference speeds. This\noften leads to high energy consumption and restricts their adoption for\npractical applications. To address this issue, this work employs a three-stage\nworkflow for compressing deep SR models which significantly reduces their\nmemory requirement. Restoration performance has been maintained through\nteacher-student knowledge distillation using a newly designed distillation\nloss. We have applied this approach to two popular image super-resolution\nnetworks, SwinIR and EDSR, to demonstrate its effectiveness. The resulting\ncompact models, SwinIRmini and EDSRmini, attain an 89% and 96% reduction in\nboth model size and floating-point operations (FLOPs) respectively, compared to\ntheir original versions. They also retain competitive super-resolution\nperformance compared to their original models and other commonly used SR\napproaches. The source code and pre-trained models for these two lightweight SR\napproaches are released at https://pikapi22.github.io/CDISM/.\n","authors":["Yuxuan Jiang","Jakub Nawala","Fan Zhang","David Bull"],"pdf_url":"https://arxiv.org/pdf/2401.00523v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06542v3","updated":"2023-12-31T15:21:23Z","published":"2023-11-11T11:35:37Z","title":"Generation Of Colors using Bidirectional Long Short Term Memory Networks","summary":"  Human vision can distinguish between a vast spectrum of colours, estimated to\nbe between 2 to 7 million discernible shades. However, this impressive range\ndoes not inherently imply that all these colours have been precisely named and\ndescribed within our lexicon. We often associate colours with familiar objects\nand concepts in our daily lives. This research endeavors to bridge the gap\nbetween our visual perception of countless shades and our ability to articulate\nand name them accurately. A novel model has been developed to achieve this\ngoal, leveraging Bidirectional Long Short-Term Memory (BiLSTM) networks with\nActive learning. This model operates on a proprietary dataset meticulously\ncurated for this study. The primary objective of this research is to create a\nversatile tool for categorizing and naming previously unnamed colours or\nidentifying intermediate shades that elude traditional colour terminology. The\nfindings underscore the potential of this innovative approach in\nrevolutionizing our understanding of colour perception and language. Through\nrigorous experimentation and analysis, this study illuminates a promising\navenue for Natural Language Processing (NLP) applications in diverse\nindustries. By facilitating the exploration of the vast colour spectrum the\npotential applications of NLP are extended beyond conventional boundaries.\n","authors":["A. Sinha"],"pdf_url":"https://arxiv.org/pdf/2311.06542v3.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2308.10273v3","updated":"2023-12-31T14:33:40Z","published":"2023-08-20T14:06:34Z","title":"Turning Waste into Wealth: Leveraging Low-Quality Samples for Enhancing\n  Continuous Conditional Generative Adversarial Networks","summary":"  Continuous Conditional Generative Adversarial Networks (CcGANs) enable\ngenerative modeling conditional on continuous scalar variables (termed\nregression labels). However, they can produce subpar fake images due to limited\ntraining data. Although Negative Data Augmentation (NDA) effectively enhances\nunconditional and class-conditional GANs by introducing anomalies into real\ntraining images, guiding the GANs away from low-quality outputs, its impact on\nCcGANs is limited, as it fails to replicate negative samples that may occur\nduring the CcGAN sampling. We present a novel NDA approach called Dual-NDA\nspecifically tailored for CcGANs to address this problem. Dual-NDA employs two\ntypes of negative samples: visually unrealistic images generated from a\npre-trained CcGAN and label-inconsistent images created by manipulating real\nimages' labels. Leveraging these negative samples, we introduce a novel\ndiscriminator objective alongside a modified CcGAN training algorithm.\nEmpirical analysis on UTKFace and Steering Angle reveals that Dual-NDA\nconsistently enhances the visual fidelity and label consistency of fake images\ngenerated by CcGANs, exhibiting a substantial performance gain over the vanilla\nNDA. Moreover, by applying Dual-NDA, CcGANs demonstrate a remarkable\nadvancement beyond the capabilities of state-of-the-art conditional GANs and\ndiffusion models, establishing a new pinnacle of performance. Our codes can be\nfound at https://github.com/UBCDingXin/Dual-NDA.\n","authors":["Xin Ding","Yongwei Wang","Zuheng Xu"],"pdf_url":"https://arxiv.org/pdf/2308.10273v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00901v1","updated":"2023-12-31T13:53:37Z","published":"2023-12-31T13:53:37Z","title":"Video-GroundingDINO: Towards Open-Vocabulary Spatio-Temporal Video\n  Grounding","summary":"  Video grounding aims to localize a spatio-temporal section in a video\ncorresponding to an input text query. This paper addresses a critical\nlimitation in current video grounding methodologies by introducing an\nOpen-Vocabulary Spatio-Temporal Video Grounding task. Unlike prevalent\nclosed-set approaches that struggle with open-vocabulary scenarios due to\nlimited training data and predefined vocabularies, our model leverages\npre-trained representations from foundational spatial grounding models. This\nempowers it to effectively bridge the semantic gap between natural language and\ndiverse visual content, achieving strong performance in closed-set and\nopen-vocabulary settings. Our contributions include a novel spatio-temporal\nvideo grounding model, surpassing state-of-the-art results in closed-set\nevaluations on multiple datasets and demonstrating superior performance in\nopen-vocabulary scenarios. Notably, the proposed model outperforms\nstate-of-the-art methods in closed-set settings on VidSTG (Declarative and\nInterrogative) and HC-STVG (V1 and V2) datasets. Furthermore, in\nopen-vocabulary evaluations on HC-STVG V1 and YouCook-Interactions, our model\nsurpasses the recent best-performing models by $4.26$ m_vIoU and $1.83\\%$\naccuracy, demonstrating its efficacy in handling diverse linguistic and visual\nconcepts for improved video understanding. Our codes will be released at\nhttps://github.com/TalalWasim/Video-GroundingDINO.\n","authors":["Syed Talal Wasim","Muzammal Naseer","Salman Khan","Ming-Hsuan Yang","Fahad Shahbaz Khan"],"pdf_url":"https://arxiv.org/pdf/2401.00901v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00496v1","updated":"2023-12-31T13:32:18Z","published":"2023-12-31T13:32:18Z","title":"SAR-RARP50: Segmentation of surgical instrumentation and Action\n  Recognition on Robot-Assisted Radical Prostatectomy Challenge","summary":"  Surgical tool segmentation and action recognition are fundamental building\nblocks in many computer-assisted intervention applications, ranging from\nsurgical skills assessment to decision support systems. Nowadays,\nlearning-based action recognition and segmentation approaches outperform\nclassical methods, relying, however, on large, annotated datasets. Furthermore,\naction recognition and tool segmentation algorithms are often trained and make\npredictions in isolation from each other, without exploiting potential\ncross-task relationships. With the EndoVis 2022 SAR-RARP50 challenge, we\nrelease the first multimodal, publicly available, in-vivo, dataset for surgical\naction recognition and semantic instrumentation segmentation, containing 50\nsuturing video segments of Robotic Assisted Radical Prostatectomy (RARP). The\naim of the challenge is twofold. First, to enable researchers to leverage the\nscale of the provided dataset and develop robust and highly accurate\nsingle-task action recognition and tool segmentation approaches in the surgical\ndomain. Second, to further explore the potential of multitask-based learning\napproaches and determine their comparative advantage against their single-task\ncounterparts. A total of 12 teams participated in the challenge, contributing 7\naction recognition methods, 9 instrument segmentation techniques, and 4\nmultitask approaches that integrated both action recognition and instrument\nsegmentation.\n","authors":["Dimitrios Psychogyios","Emanuele Colleoni","Beatrice Van Amsterdam","Chih-Yang Li","Shu-Yu Huang","Yuchong Li","Fucang Jia","Baosheng Zou","Guotai Wang","Yang Liu","Maxence Boels","Jiayu Huo","Rachel Sparks","Prokar Dasgupta","Alejandro Granados","Sebastien Ourselin","Mengya Xu","An Wang","Yanan Wu","Long Bai","Hongliang Ren","Atsushi Yamada","Yuriko Harai","Yuto Ishikawa","Kazuyuki Hayashi","Jente Simoens","Pieter DeBacker","Francesco Cisternino","Gabriele Furnari","Alex Mottrie","Federica Ferraguti","Satoshi Kondo","Satoshi Kasai","Kousuke Hirasawa","Soohee Kim","Seung Hyun Lee","Kyu Eun Lee","Hyoun-Joong Kong","Kui Fu","Chao Li","Shan An","Stefanie Krell","Sebastian Bodenstedt","Nicolas Ayobi","Alejandra Perez","Santiago Rodriguez","Juanita Puentes","Pablo Arbelaez","Omid Mohareri","Danail Stoyanov"],"pdf_url":"https://arxiv.org/pdf/2401.00496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.15031v2","updated":"2023-12-31T12:29:58Z","published":"2023-09-26T16:01:15Z","title":"Nuclear Morphometry using a Deep Learning-based Algorithm has Prognostic\n  Relevance for Canine Cutaneous Mast Cell Tumors","summary":"  Variation in nuclear size and shape is an important criterion of malignancy\nfor many tumor types; however, categorical estimates by pathologists have poor\nreproducibility. Measurements of nuclear characteristics (morphometry) can\nimprove reproducibility, but manual methods are time consuming. In this study,\nwe evaluated fully automated morphometry using a deep learning-based algorithm\nin 96 canine cutaneous mast cell tumors with information on patient survival.\nAlgorithmic morphometry was compared with karyomegaly estimates by 11\npathologists, manual nuclear morphometry of 12 cells by 9 pathologists, and the\nmitotic count as a benchmark. The prognostic value of automated morphometry was\nhigh with an area under the ROC curve regarding the tumor-specific survival of\n0.943 (95% CI: 0.889 - 0.996) for the standard deviation (SD) of nuclear area,\nwhich was higher than manual morphometry of all pathologists combined (0.868,\n95% CI: 0.737 - 0.991) and the mitotic count (0.885, 95% CI: 0.765 - 1.00). At\nthe proposed thresholds, the hazard ratio for algorithmic morphometry (SD of\nnuclear area $\\geq 9.0 \\mu m^2$) was 18.3 (95% CI: 5.0 - 67.1), for manual\nmorphometry (SD of nuclear area $\\geq 10.9 \\mu m^2$) 9.0 (95% CI: 6.0 - 13.4),\nfor karyomegaly estimates 7.6 (95% CI: 5.7 - 10.1), and for the mitotic count\n30.5 (95% CI: 7.8 - 118.0). Inter-rater reproducibility for karyomegaly\nestimates was fair ($\\kappa$ = 0.226) with highly variable\nsensitivity/specificity values for the individual pathologists. Reproducibility\nfor manual morphometry (SD of nuclear area) was good (ICC = 0.654). This study\nsupports the use of algorithmic morphometry as a prognostic test to overcome\nthe limitations of estimates and manual measurements.\n","authors":["Andreas Haghofer","Eda Parlak","Alexander Bartel","Taryn A. Donovan","Charles-Antoine Assenmacher","Pompei Bolfa","Michael J. Dark","Andrea Fuchs-Baumgartinger","Andrea Klang","Kathrin Jäger","Robert Klopfleisch","Sophie Merz","Barbara Richter","F. Yvonne Schulman","Jonathan Ganz","Josef Scharinger","Marc Aubreville","Stephan M. Winkler","Matti Kiupel","Christof A. Bertram"],"pdf_url":"https://arxiv.org/pdf/2309.15031v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.10867v2","updated":"2023-12-31T12:25:45Z","published":"2022-11-20T04:39:57Z","title":"Rethinking the Paradigm of Content Constraints in GAN-based Unpaired\n  Image-to-Image Translation","summary":"  In an unpaired setting, lacking sufficient content constraints for\nimage-to-image translation (I2I) tasks, GAN-based approaches are usually prone\nto model collapse. Current solutions can be divided into two categories,\nreconstruction-based and Siamese network-based. The former requires that the\ntransformed or transforming image can be perfectly converted back to the\noriginal image, which is sometimes too strict and limits the generative\nperformance. The latter involves feeding the original and generated images into\na feature extractor and then matching their outputs. This is not efficient\nenough, and a universal feature extractor is not easily available. In this\npaper, we propose EnCo, a simple but efficient way to maintain the content by\nconstraining the representational similarity in the latent space of patch-level\nfeatures from the same stage of the \\textbf{En}coder and de\\textbf{Co}der of\nthe generator. For the similarity function, we use a simple MSE loss instead of\ncontrastive loss, which is currently widely used in I2I tasks. Benefits from\nthe design, EnCo training is extremely efficient, while the features from the\nencoder produce a more positive effect on the decoding, leading to more\nsatisfying generations. In addition, we rethink the role played by\ndiscriminators in sampling patches and propose a discriminative\nattention-guided (DAG) patch sampling strategy to replace random sampling. DAG\nis parameter-free and only requires negligible computational overhead, while\nsignificantly improving the performance of the model. Extensive experiments on\nmultiple datasets demonstrate the effectiveness and advantages of EnCo, and we\nachieve multiple state-of-the-art compared to previous methods. Our code is\navailable at https://github.com/XiudingCai/EnCo-pytorch.\n","authors":["Xiuding Cai","Yaoyao Zhu","Dong Miao","Linjie Fu","Yu Yao"],"pdf_url":"https://arxiv.org/pdf/2211.10867v2.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2401.00897v1","updated":"2023-12-31T12:03:21Z","published":"2023-12-31T12:03:21Z","title":"Masked Modeling for Self-supervised Representation Learning on Vision\n  and Beyond","summary":"  As the deep learning revolution marches on, self-supervised learning has\ngarnered increasing attention in recent years thanks to its remarkable\nrepresentation learning ability and the low dependence on labeled data. Among\nthese varied self-supervised techniques, masked modeling has emerged as a\ndistinctive approach that involves predicting parts of the original data that\nare proportionally masked during training. This paradigm enables deep models to\nlearn robust representations and has demonstrated exceptional performance in\nthe context of computer vision, natural language processing, and other\nmodalities. In this survey, we present a comprehensive review of the masked\nmodeling framework and its methodology. We elaborate on the details of\ntechniques within masked modeling, including diverse masking strategies,\nrecovering targets, network architectures, and more. Then, we systematically\ninvestigate its wide-ranging applications across domains. Furthermore, we also\nexplore the commonalities and differences between masked modeling methods in\ndifferent fields. Toward the end of this paper, we conclude by discussing the\nlimitations of current techniques and point out several potential avenues for\nadvancing masked modeling research. A paper list project with this survey is\navailable at \\url{https://github.com/Lupin1998/Awesome-MIM}.\n","authors":["Siyuan Li","Luyuan Zhang","Zedong Wang","Di Wu","Lirong Wu","Zicheng Liu","Jun Xia","Cheng Tan","Yang Liu","Baigui Sun","Stan Z. Li"],"pdf_url":"https://arxiv.org/pdf/2401.00897v1.pdf","comment":"Preprint v1, 19 pages of main texts. GitHub project at\n  https://github.com/Lupin1998/Awesome-MIM"},{"id":"http://arxiv.org/abs/2401.00463v1","updated":"2023-12-31T11:38:50Z","published":"2023-12-31T11:38:50Z","title":"Analyzing Local Representations of Self-supervised Vision Transformers","summary":"  In this paper, we present a comparative analysis of various self-supervised\nVision Transformers (ViTs), focusing on their local representative power.\nInspired by large language models, we examine the abilities of ViTs to perform\nvarious computer vision tasks with little to no fine-tuning. We design an\nevaluation framework to analyze the quality of local, i.e. patch-level,\nrepresentations in the context of few-shot semantic segmentation, instance\nidentification, object retrieval, and tracking. We discover that contrastive\nlearning based methods like DINO produce more universal patch representations\nthat can be immediately applied for downstream tasks with no parameter tuning,\ncompared to masked image modeling. The embeddings learned using the latter\napproach, e.g. in masked autoencoders, have high variance features that harm\ndistance-based algorithms, such as k-NN, and do not contain useful information\nfor most downstream tasks. Furthermore, we demonstrate that removing these\nhigh-variance features enhances k-NN by providing an analysis of the benchmarks\nfor this work and for Scale-MAE, a recent extension of masked autoencoders.\nFinally, we find an object instance retrieval setting where DINOv2, a model\npretrained on two orders of magnitude more data, performs worse than its less\ncompute-intensive counterpart DINO.\n","authors":["Ani Vanyan","Alvard Barseghyan","Hakob Tamazyan","Vahan Huroyan","Hrant Khachatrian","Martin Danelljan"],"pdf_url":"https://arxiv.org/pdf/2401.00463v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00460v1","updated":"2023-12-31T11:30:42Z","published":"2023-12-31T11:30:42Z","title":"RainSD: Rain Style Diversification Module for Image Synthesis\n  Enhancement using Feature-Level Style Distribution","summary":"  Autonomous driving technology nowadays targets to level 4 or beyond, but the\nresearchers are faced with some limitations for developing reliable driving\nalgorithms in diverse challenges. To promote the autonomous vehicles to spread\nwidely, it is important to address safety issues on this technology. Among\nvarious safety concerns, the sensor blockage problem by severe weather\nconditions can be one of the most frequent threats for multi-task learning\nbased perception algorithms during autonomous driving. To handle this problem,\nthe importance of the generation of proper datasets is becoming more\nsignificant. In this paper, a synthetic road dataset with sensor blockage\ngenerated from real road dataset BDD100K is suggested in the format of BDD100K\nannotation. Rain streaks for each frame were made by an experimentally\nestablished equation and translated utilizing the image-to-image translation\nnetwork based on style transfer. Using this dataset, the degradation of the\ndiverse multi-task networks for autonomous driving, such as lane detection,\ndriving area segmentation, and traffic object detection, has been thoroughly\nevaluated and analyzed. The tendency of the performance degradation of deep\nneural network-based perception systems for autonomous vehicle has been\nanalyzed in depth. Finally, we discuss the limitation and the future directions\nof the deep neural network-based perception algorithms and autonomous driving\ndataset generation based on image-to-image translation.\n","authors":["Hyeonjae Jeon","Junghyun Seo","Taesoo Kim","Sungho Son","Jungki Lee","Gyeungho Choi","Yongseob Lim"],"pdf_url":"https://arxiv.org/pdf/2401.00460v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2401.00456v1","updated":"2023-12-31T11:16:12Z","published":"2023-12-31T11:16:12Z","title":"Double-well Net for Image Segmentation","summary":"  In this study, our goal is to integrate classical mathematical models with\ndeep neural networks by introducing two novel deep neural network models for\nimage segmentation known as Double-well Nets. Drawing inspiration from the\nPotts model, our models leverage neural networks to represent a region force\nfunctional. We extend the well-know MBO (Merriman-Bence-Osher) scheme to solve\nthe Potts model. The widely recognized Potts model is approximated using a\ndouble-well potential and then solved by an operator-splitting method, which\nturns out to be an extension of the well-known MBO scheme. Subsequently, we\nreplace the region force functional in the Potts model with a UNet-type\nnetwork, which is data-driven, and also introduce control variables to enhance\neffectiveness. The resulting algorithm is a neural network activated by a\nfunction that minimizes the double-well potential. What sets our proposed\nDouble-well Nets apart from many existing deep learning methods for image\nsegmentation is their strong mathematical foundation. They are derived from the\nnetwork approximation theory and employ the MBO scheme to approximately solve\nthe Potts model. By incorporating mathematical principles, Double-well Nets\nbridge the MBO scheme and neural networks, and offer an alternative perspective\nfor designing networks with mathematical backgrounds. Through comprehensive\nexperiments, we demonstrate the performance of Double-well Nets, showcasing\ntheir superior accuracy and robustness compared to state-of-the-art neural\nnetworks. Overall, our work represents a valuable contribution to the field of\nimage segmentation by combining the strengths of classical variational models\nand deep neural networks. The Double-well Nets introduce an innovative approach\nthat leverages mathematical foundations to enhance segmentation performance.\n","authors":["Hao Liu","Jun Liu","Raymond Chan","Xue-Cheng Tai"],"pdf_url":"https://arxiv.org/pdf/2401.00456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00896v1","updated":"2023-12-31T10:51:52Z","published":"2023-12-31T10:51:52Z","title":"TrailBlazer: Trajectory Control for Diffusion-Based Video Generation","summary":"  Within recent approaches to text-to-video (T2V) generation, achieving\ncontrollability in the synthesized video is often a challenge. Typically, this\nissue is addressed by providing low-level per-frame guidance in the form of\nedge maps, depth maps, or an existing video to be altered. However, the process\nof obtaining such guidance can be labor-intensive. This paper focuses on\nenhancing controllability in video synthesis by employing straightforward\nbounding boxes to guide the subject in various ways, all without the need for\nneural network training, finetuning, optimization at inference time, or the use\nof pre-existing videos. Our algorithm, TrailBlazer, is constructed upon a\npre-trained (T2V) model, and easy to implement. The subject is directed by a\nbounding box through the proposed spatial and temporal attention map editing.\nMoreover, we introduce the concept of keyframing, allowing the subject\ntrajectory and overall appearance to be guided by both a moving bounding box\nand corresponding prompts, without the need to provide a detailed mask. The\nmethod is efficient, with negligible additional computation relative to the\nunderlying pre-trained model. Despite the simplicity of the bounding box\nguidance, the resulting motion is surprisingly natural, with emergent effects\nincluding perspective and movement toward the virtual camera as the box size\nincreases.\n","authors":["Wan-Duo Kurt Ma","J. P. Lewis","W. Bastiaan Kleijn"],"pdf_url":"https://arxiv.org/pdf/2401.00896v1.pdf","comment":"9 pages, 16 figures, Project Page:\n  https://hohonu-vicml.github.io/Trailblazer.Page/"},{"id":"http://arxiv.org/abs/2401.00442v1","updated":"2023-12-31T09:49:37Z","published":"2023-12-31T09:49:37Z","title":"A Comprehensive Overview of Fish-Eye Camera Distortion Correction\n  Methods","summary":"  The fisheye camera, with its unique wide field of view and other\ncharacteristics, has found extensive applications in various fields. However,\nthe fisheye camera suffers from significant distortion compared to pinhole\ncameras, resulting in distorted images of captured objects. Fish-eye camera\ndistortion is a common issue in digital image processing, requiring effective\ncorrection techniques to enhance image quality. This review provides a\ncomprehensive overview of various methods used for fish-eye camera distortion\ncorrection. The article explores the polynomial distortion model, which\nutilizes polynomial functions to model and correct radial distortions.\nAdditionally, alternative approaches such as panorama mapping, grid mapping,\ndirect methods, and deep learning-based methods are discussed. The review\nhighlights the advantages, limitations, and recent advancements of each method,\nenabling readers to make informed decisions based on their specific needs.\n","authors":["Jian Xu","De-Wei Han","Kang Li","Jun-Jie Li","Zhao-Yuan Ma"],"pdf_url":"https://arxiv.org/pdf/2401.00442v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10116v2","updated":"2023-12-31T09:40:10Z","published":"2023-11-16T06:53:03Z","title":"Wildfire Smoke Detection with Cross Contrast Patch Embedding","summary":"  The Transformer-based deep networks have increasingly shown significant\nadvantages over CNNs. Some existing work has applied it in the field of\nwildfire recognition or detection. However, we observed that the vanilla\nTransformer is not friendly for extracting smoke features. Because low-level\ninformation such as color, transparency and texture is very important for smoke\nrecognition, and transformer pays more attention to the semantic relevance\nbetween middle- or high-level features, and is not sensitive to the subtle\nchanges of low-level features along the space. To solve this problem, we\npropose the Cross Contrast Patch Embedding(CCPE) module based on the Swin\nTransformer, which uses the multi-scales spatial frequency contrast information\nin both vertical and horizontal directions to improve the discrimination of the\nnetwork on the underlying details. The fuzzy boundary of smoke makes the\npositive and negative label assignment for instances in a dilemma, which is\nanother challenge for wildfires detection. To solve this problem, a Separable\nNegative Sampling Mechanism(SNSM) is proposed. By using two different negative\ninstance sampling strategies on positive images and negative images\nrespectively, the problem of supervision signal confusion caused by label\ndiversity in the process of network training is alleviated. This paper also\nreleases the RealFire Test, the largest real wildfire test set so far, to\nevaluate the proposed method and promote future research. It contains 50,535\nimages from 3,649 video clips. The proposed method has been extensively tested\nand evaluated on RealFire Test dataset, and has a significant performance\nimprovement compared with the baseline detection models.\n","authors":["Chong Wang","Cheng Xu","Adeel Akram","Zhilin Shan","Qixing Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.10116v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00440v1","updated":"2023-12-31T09:38:53Z","published":"2023-12-31T09:38:53Z","title":"TSGAN: An Optical-to-SAR Dual Conditional GAN for Optical based SAR\n  Temporal Shifting","summary":"  In contrast to the well-investigated field of SAR-to-Optical translation,\nthis study explores the lesser-investigated domain of Optical-to-SAR\ntranslation, a challenging field due to the ill-posed nature of this\ntranslation. The complexity arises as a single optical data can have multiple\nSAR representations based on the SAR viewing geometry. We propose a novel\napproach, termed SAR Temporal Shifting, which inputs an optical data from the\ndesired timestamp along with a SAR data from a different temporal point but\nwith a consistent viewing geometry as the expected SAR data, both complemented\nwith a change map of optical data during the intervening period. This model\nmodifies the SAR data based on the changes observed in optical data to generate\nthe SAR data for the desired timestamp. Our model, a dual conditional\nGenerative Adversarial Network (GAN), named Temporal Shifting GAN (TSGAN),\nincorporates a siamese encoder in both the Generator and the Discriminator. To\nprevent the model from overfitting on the input SAR data, we employed a change\nweighted loss function. Our approach surpasses traditional translation methods\nby eliminating the GAN's fiction phenomenon, particularly in unchanged regions,\nresulting in higher SSIM and PSNR in these areas. Additionally, modifications\nto the Pix2Pix architecture and the inclusion of attention mechanisms have\nenhanced the model's performance on all regions of the data. This research\npaves the way for leveraging legacy optical datasets, the most abundant and\nlongstanding source of Earth datary data, extending their use to SAR domains\nand temporal analyses. To foster further research, we provide the code,\ndatasets used in our study, and a framework for generating paired SAR-Optical\ndatasets for new regions of interest. These resources are available on\ngithub.com/moienr/TemporalGAN\n","authors":["Moien Rangzan","Sara Attarchi","Richard Gloaguen","Seyed Kazem Alavipanah"],"pdf_url":"https://arxiv.org/pdf/2401.00440v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2312.14335v2","updated":"2023-12-31T22:31:48Z","published":"2023-12-21T23:42:13Z","title":"Context-aware Decoding Reduces Hallucination in Query-focused\n  Summarization","summary":"  Query-focused summarization (QFS) aims to provide a summary of a single\ndocument/multi documents that can satisfy the information needs of a given\nquery. It is useful for various real-world applications, such as abstractive\nsnippet generation or more recent retrieval augmented generation (RAG). A\nprototypical QFS pipeline consists of a retriever (sparse or dense retrieval)\nand a generator (usually a large language model). However, applying large\nlanguage models (LLM) potentially leads to hallucinations, especially when the\nevidence contradicts the prior belief of LLMs. There has been growing interest\nin developing new decoding methods to improve generation quality and reduce\nhallucination. In this work, we conduct a large-scale reproducibility study on\none recently proposed decoding method -- Context-aware Decoding (CAD). In\naddition to replicating CAD's experiments on news summarization datasets, we\ninclude experiments on QFS datasets, and conduct more rigorous analysis on\ncomputational complexity and hyperparameter sensitivity. Experiments with eight\ndifferent language models show that performance-wise, CAD improves QFS quality\nby (1) reducing factuality errors/hallucinations while (2) mostly retaining the\nmatch of lexical patterns, measured by ROUGE scores, while also at a cost of\nincreased inference-time FLOPs and reduced decoding speed. The code\nimplementation based on Huggingface Library is made available\nhttps://github.com/zhichaoxu-shufe/context-aware-decoding-qfs\n","authors":["Zhichao Xu"],"pdf_url":"https://arxiv.org/pdf/2312.14335v2.pdf","comment":"technical report"},{"id":"http://arxiv.org/abs/2308.08434v2","updated":"2023-12-31T14:59:21Z","published":"2023-08-16T15:28:22Z","title":"A Bi-Step Grounding Paradigm for Large Language Models in Recommendation\n  Systems","summary":"  As the focus on Large Language Models (LLMs) in the field of recommendation\nintensifies, the optimization of LLMs for recommendation purposes (referred to\nas LLM4Rec) assumes a crucial role in augmenting their effectiveness in\nproviding recommendations. However, existing approaches for LLM4Rec often\nassess performance using restricted sets of candidates, which may not\naccurately reflect the models' overall ranking capabilities. In this paper, our\nobjective is to investigate the comprehensive ranking capacity of LLMs and\npropose a two-step grounding framework known as BIGRec (Bi-step Grounding\nParadigm for Recommendation). It initially grounds LLMs to the recommendation\nspace by fine-tuning them to generate meaningful tokens for items and\nsubsequently identifies appropriate actual items that correspond to the\ngenerated tokens. By conducting extensive experiments on two datasets, we\nsubstantiate the superior performance, capacity for handling few-shot\nscenarios, and versatility across multiple domains exhibited by BIGRec.\nFurthermore, we observe that the marginal benefits derived from increasing the\nquantity of training samples are modest for BIGRec, implying that LLMs possess\nthe limited capability to assimilate statistical information, such as\npopularity and collaborative filtering, due to their robust semantic priors.\nThese findings also underline the efficacy of integrating diverse statistical\ninformation into the LLM4Rec framework, thereby pointing towards a potential\navenue for future research. Our code and data are available at\nhttps://github.com/SAI990323/Grounding4Rec.\n","authors":["Keqin Bao","Jizhi Zhang","Wenjie Wang","Yang Zhang","Zhengyi Yang","Yancheng Luo","Chong Chen","Fuli Feng","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2308.08434v2.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2401.00465v1","updated":"2023-12-31T11:39:18Z","published":"2023-12-31T11:39:18Z","title":"V2X communication coverage analysis for connected vehicles in\n  intelligent transportation networks: A case study for the city of Xanthi,\n  Greece","summary":"  Intelligent transportation systems (ITS) have been developed to improve\ntraffic flow, efficiency, and safety in transportation. Technological\nadvancements in communication such as the Vehicle-to-Everything (V2X),\nVehicle-to-Vehicle (V2V) and Vehicle-to Infrastructure (V2I) enable the\nreal-time exchange of information between vehicles and other entities on the\nroad network, and thus play a significant role in their safety and efficiency.\nThis paper presents a simulation study that models V2V and V2I communication to\nidentify the most suitable range of data transmission between vehicles and\ninfrastructure. The provincial city of Xanthi, Greece is used as a cases study,\nand the goal is to evaluate whether the proposed placement of Road Side Unit\n(RSU) provided adequate communication coverage on the city's road network. An\nanalysis through different scenarios identified improvements in traffic\nmanagement, driving behavior and environmental conditions under different RSU\ncoverage. The results highlight that the communication range of 400 meters is\nthe most adequate option for optimum traffic management in the city of Xanthi.\n","authors":["Evangelos Bazinas","Andreas Gregoriades","Marios Raspopoulos","Michael Georgiades"],"pdf_url":"https://arxiv.org/pdf/2401.00465v1.pdf","comment":"Wireless World Research Forum, Meeting 49, March 28th-30th 2023,\n  Pozna\\'n, Poland, Towards sustainable and automated communications"},{"id":"http://arxiv.org/abs/2312.02445v2","updated":"2023-12-31T05:09:01Z","published":"2023-12-05T02:53:46Z","title":"LLaRA: Aligning Large Language Models with Sequential Recommenders","summary":"  Sequential recommendation aims to predict the subsequent items matching user\npreference based on her/his historical interactions. With the development of\nLarge Language Models (LLMs), there is growing interest in exploring the\npotential of LLMs for sequential recommendation by framing it as a language\nmodeling task. Prior works represent items in the textual prompts using either\nID indexing or text indexing and feed the prompts into LLMs, but falling short\nof either encapsulating comprehensive world knowledge or exhibiting sufficient\nsequential understanding. To harness the complementary strengths of traditional\nrecommenders (which encode user behavioral knowledge) and LLMs (which possess\nworld knowledge about items), we propose LLaRA -- a Large Language and\nRecommendation Assistant framework. Specifically, LLaRA represents items in\nLLM's input prompts using a novel hybrid approach that integrates ID-based item\nembeddings from traditional recommenders with textual item features. Viewing\nthe ``sequential behavior of the user'' as a new modality in recommendation, we\nemploy an adapter to bridge the modality gap between ID embeddings of the\ntraditional recommenders and the input space of LLMs. Furthermore, instead of\ndirectly exposing the hybrid prompt to LLMs, we apply a curriculum learning\napproach to gradually ramp up training complexity. We first warm up the LLM\nwith text-only prompting, which aligns more naturally with the LLM's language\nmodeling capabilities. Thereafter, we progressively transition to hybrid\nprompting, training the adapter to incorporate behavioral knowledge from the\ntraditional sequential recommender into the LLM. Extensive experiments\ndemonstrate the efficacy of LLaRA framework. Our code and data are available at\nhttps://github.com/ljy0ustc/LLaRA .\n","authors":["Jiayi Liao","Sihang Li","Zhengyi Yang","Jiancan Wu","Yancheng Yuan","Xiang Wang"],"pdf_url":"https://arxiv.org/pdf/2312.02445v2.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2401.00368v1","updated":"2023-12-31T02:13:18Z","published":"2023-12-31T02:13:18Z","title":"Improving Text Embeddings with Large Language Models","summary":"  In this paper, we introduce a novel and simple method for obtaining\nhigh-quality text embeddings using only synthetic data and less than 1k\ntraining steps. Unlike existing methods that often depend on multi-stage\nintermediate pre-training with billions of weakly-supervised text pairs,\nfollowed by fine-tuning with a few labeled datasets, our method does not\nrequire building complex training pipelines or relying on manually collected\ndatasets that are often constrained by task diversity and language coverage. We\nleverage proprietary LLMs to generate diverse synthetic data for hundreds of\nthousands of text embedding tasks across nearly 100 languages. We then\nfine-tune open-source decoder-only LLMs on the synthetic data using standard\ncontrastive loss. Experiments demonstrate that our method achieves strong\nperformance on highly competitive text embedding benchmarks without using any\nlabeled data. Furthermore, when fine-tuned with a mixture of synthetic and\nlabeled data, our model sets new state-of-the-art results on the BEIR and MTEB\nbenchmarks.\n","authors":["Liang Wang","Nan Yang","Xiaolong Huang","Linjun Yang","Rangan Majumder","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2401.00368v1.pdf","comment":"15 pages, 8 tables"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2401.00611v1","updated":"2023-12-31T23:57:05Z","published":"2023-12-31T23:57:05Z","title":"A Compact Representation for Bayesian Neural Networks By Removing\n  Permutation Symmetry","summary":"  Bayesian neural networks (BNNs) are a principled approach to modeling\npredictive uncertainties in deep learning, which are important in\nsafety-critical applications. Since exact Bayesian inference over the weights\nin a BNN is intractable, various approximate inference methods exist, among\nwhich sampling methods such as Hamiltonian Monte Carlo (HMC) are often\nconsidered the gold standard. While HMC provides high-quality samples, it lacks\ninterpretable summary statistics because its sample mean and variance is\nmeaningless in neural networks due to permutation symmetry. In this paper, we\nfirst show that the role of permutations can be meaningfully quantified by a\nnumber of transpositions metric. We then show that the recently proposed\nrebasin method allows us to summarize HMC samples into a compact representation\nthat provides a meaningful explicit uncertainty estimate for each weight in a\nneural network, thus unifying sampling methods with variational inference. We\nshow that this compact representation allows us to compare trained BNNs\ndirectly in weight space across sampling methods and variational inference, and\nto efficiently prune neural networks trained without explicit Bayesian\nframeworks by exploiting uncertainty estimates from HMC.\n","authors":["Tim Z. Xiao","Weiyang Liu","Robert Bamler"],"pdf_url":"https://arxiv.org/pdf/2401.00611v1.pdf","comment":"Accepted at NeurIPS 2023 Workshop on Unifying Representations in\n  Neural Models; 4 pages + appendix"},{"id":"http://arxiv.org/abs/2401.00910v1","updated":"2023-12-31T23:53:50Z","published":"2023-12-31T23:53:50Z","title":"WoodScape Motion Segmentation for Autonomous Driving -- CVPR 2023 OmniCV\n  Workshop Challenge","summary":"  Motion segmentation is a complex yet indispensable task in autonomous\ndriving. The challenges introduced by the ego-motion of the cameras, radial\ndistortion in fisheye lenses, and the need for temporal consistency make the\ntask more complicated, rendering traditional and standard Convolutional Neural\nNetwork (CNN) approaches less effective. The consequent laborious data\nlabeling, representation of diverse and uncommon scenarios, and extensive data\ncapture requirements underscore the imperative of synthetic data for improving\nmachine learning model performance. To this end, we employ the PD-WoodScape\nsynthetic dataset developed by Parallel Domain, alongside the WoodScape fisheye\ndataset. Thus, we present the WoodScape fisheye motion segmentation challenge\nfor autonomous driving, held as part of the CVPR 2023 Workshop on\nOmnidirectional Computer Vision (OmniCV). As one of the first competitions\nfocused on fisheye motion segmentation, we aim to explore and evaluate the\npotential and impact of utilizing synthetic data in this domain. In this paper,\nwe provide a detailed analysis on the competition which attracted the\nparticipation of 112 global teams and a total of 234 submissions. This study\ndelineates the complexities inherent in the task of motion segmentation,\nemphasizes the significance of fisheye datasets, articulate the necessity for\nsynthetic datasets and the resultant domain gap they engender, outlining the\nfoundational blueprint for devising successful solutions. Subsequently, we\ndelve into the details of the baseline experiments and winning methods\nevaluating their qualitative and quantitative results, providing with useful\ninsights.\n","authors":["Saravanabalagi Ramachandran","Nathaniel Cibik","Ganesh Sistu","John McDonald"],"pdf_url":"https://arxiv.org/pdf/2401.00910v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2401.00416v1","updated":"2023-12-31T07:44:05Z","published":"2023-12-31T07:44:05Z","title":"SVFAP: Self-supervised Video Facial Affect Perceiver","summary":"  Video-based facial affect analysis has recently attracted increasing\nattention owing to its critical role in human-computer interaction. Previous\nstudies mainly focus on developing various deep learning architectures and\ntraining them in a fully supervised manner. Although significant progress has\nbeen achieved by these supervised methods, the longstanding lack of large-scale\nhigh-quality labeled data severely hinders their further improvements.\nMotivated by the recent success of self-supervised learning in computer vision,\nthis paper introduces a self-supervised approach, termed Self-supervised Video\nFacial Affect Perceiver (SVFAP), to address the dilemma faced by supervised\nmethods. Specifically, SVFAP leverages masked facial video autoencoding to\nperform self-supervised pre-training on massive unlabeled facial videos.\nConsidering that large spatiotemporal redundancy exists in facial videos, we\npropose a novel temporal pyramid and spatial bottleneck Transformer as the\nencoder of SVFAP, which not only enjoys low computational cost but also\nachieves excellent performance. To verify the effectiveness of our method, we\nconduct experiments on nine datasets spanning three downstream tasks, including\ndynamic facial expression recognition, dimensional emotion recognition, and\npersonality recognition. Comprehensive results demonstrate that SVFAP can learn\npowerful affect-related representations via large-scale self-supervised\npre-training and it significantly outperforms previous state-of-the-art methods\non all datasets. Codes will be available at https://github.com/sunlicai/SVFAP.\n","authors":["Licai Sun","Zheng Lian","Kexin Wang","Yu He","Mingyu Xu","Haiyang Sun","Bin Liu","Jianhua Tao"],"pdf_url":"https://arxiv.org/pdf/2401.00416v1.pdf","comment":"Submitted to IEEE Trans. on Affective Computing (February 8, 2023)"},{"id":"http://arxiv.org/abs/2401.00894v1","updated":"2023-12-31T05:50:15Z","published":"2023-12-31T05:50:15Z","title":"Balanced Multi-modal Federated Learning via Cross-Modal Infiltration","summary":"  Federated learning (FL) underpins advancements in privacy-preserving\ndistributed computing by collaboratively training neural networks without\nexposing clients' raw data. Current FL paradigms primarily focus on uni-modal\ndata, while exploiting the knowledge from distributed multimodal data remains\nlargely unexplored. Existing multimodal FL (MFL) solutions are mainly designed\nfor statistical or modality heterogeneity from the input side, however, have\nyet to solve the fundamental issue,\"modality imbalance\", in distributed\nconditions, which can lead to inadequate information exploitation and\nheterogeneous knowledge aggregation on different modalities.In this paper, we\npropose a novel Cross-Modal Infiltration Federated Learning (FedCMI) framework\nthat effectively alleviates modality imbalance and knowledge heterogeneity via\nknowledge transfer from the global dominant modality. To avoid the loss of\ninformation in the weak modality due to merely imitating the behavior of\ndominant modality, we design the two-projector module to integrate the\nknowledge from dominant modality while still promoting the local feature\nexploitation of weak modality. In addition, we introduce a class-wise\ntemperature adaptation scheme to achieve fair performance across different\nclasses. Extensive experiments over popular datasets are conducted and give us\na gratifying confirmation of the proposed framework for fully exploring the\ninformation of each modality in MFL.\n","authors":["Yunfeng Fan","Wenchao Xu","Haozhao Wang","Jiaqi Zhu","Song Guo"],"pdf_url":"https://arxiv.org/pdf/2401.00894v1.pdf","comment":"10 pages, 5 figures 4 tables"},{"id":"http://arxiv.org/abs/2401.00403v1","updated":"2023-12-31T05:37:27Z","published":"2023-12-31T05:37:27Z","title":"Client-wise Modality Selection for Balanced Multi-modal Federated\n  Learning","summary":"  Selecting proper clients to participate in the iterative federated learning\n(FL) rounds is critical to effectively harness a broad range of distributed\ndatasets. Existing client selection methods simply consider the variability\namong FL clients with uni-modal data, however, have yet to consider clients\nwith multi-modalities. We reveal that traditional client selection scheme in\nMFL may suffer from a severe modality-level bias, which impedes the\ncollaborative exploitation of multi-modal data, leading to insufficient local\ndata exploration and global aggregation. To tackle this challenge, we propose a\nClient-wise Modality Selection scheme for MFL (CMSFed) that can comprehensively\nutilize information from each modality via avoiding such client selection bias\ncaused by modality imbalance. Specifically, in each MFL round, the local data\nfrom different modalities are selectively employed to participate in local\ntraining and aggregation to mitigate potential modality imbalance of the global\nmodel. To approximate the fully aggregated model update in a balanced way, we\nintroduce a novel local training loss function to enhance the weak modality and\nalign the divergent feature spaces caused by inconsistent modality adoption\nstrategies for different clients simultaneously. Then, a modality-level\ngradient decoupling method is designed to derive respective submodular\nfunctions to maintain the gradient diversity during the selection progress and\nbalance MFL according to local modality imbalance in each iteration. Our\nextensive experiments showcase the superiority of CMSFed over baselines and its\neffectiveness in multi-modal data exploitation.\n","authors":["Yunfeng Fan","Wenchao Xu","Haozhao Wang","Penghui Ruan","Song Guo"],"pdf_url":"https://arxiv.org/pdf/2401.00403v1.pdf","comment":"10 pages,6 figures,2 tables"},{"id":"http://arxiv.org/abs/2401.00393v1","updated":"2023-12-31T04:34:58Z","published":"2023-12-31T04:34:58Z","title":"Generative Model-Driven Synthetic Training Image Generation: An Approach\n  to Cognition in Rail Defect Detection","summary":"  Recent advancements in cognitive computing, with the integration of deep\nlearning techniques, have facilitated the development of intelligent cognitive\nsystems (ICS). This is particularly beneficial in the context of rail defect\ndetection, where the ICS would emulate human-like analysis of image data for\ndefect patterns. Despite the success of Convolutional Neural Networks (CNN) in\nvisual defect classification, the scarcity of large datasets for rail defect\ndetection remains a challenge due to infrequent accident events that would\nresult in defective parts and images. Contemporary researchers have addressed\nthis data scarcity challenge by exploring rule-based and generative data\naugmentation models. Among these, Variational Autoencoder (VAE) models can\ngenerate realistic data without extensive baseline datasets for noise modeling.\nThis study proposes a VAE-based synthetic image generation technique for rail\ndefects, incorporating weight decay regularization and image reconstruction\nloss to prevent overfitting. The proposed method is applied to create a\nsynthetic dataset for the Canadian Pacific Railway (CPR) with just 50 real\nsamples across five classes. Remarkably, 500 synthetic samples are generated\nwith a minimal reconstruction loss of 0.021. A Visual Transformer (ViT) model\nunderwent fine-tuning using this synthetic CPR dataset, achieving high accuracy\nrates (98%-99%) in classifying the five defect classes. This research offers a\npromising solution to the data scarcity challenge in rail defect detection,\nshowcasing the potential for robust ICS development in this domain.\n","authors":["Rahatara Ferdousi","Chunsheng Yang","M. Anwar Hossain","Fedwa Laamarti","M. Shamim Hossain","Abdulmotaleb El Saddik"],"pdf_url":"https://arxiv.org/pdf/2401.00393v1.pdf","comment":"26 pages, 13 figures, Springer Journal"}]},"2024-01-03T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2401.01879v1","updated":"2024-01-03T18:39:13Z","published":"2024-01-03T18:39:13Z","title":"Theoretical guarantees on the best-of-n alignment policy","summary":"  A simple and effective method for the alignment of generative models is the\nbest-of-$n$ policy, where $n$ samples are drawn from a base policy, and ranked\nbased on a reward function, and the highest ranking one is selected. A commonly\nused analytical expression in the literature claims that the KL divergence\nbetween the best-of-$n$ policy and the base policy is equal to $\\log (n) -\n(n-1)/n.$ We disprove the validity of this claim, and show that it is an upper\nbound on the actual KL divergence. We also explore the tightness of this upper\nbound in different regimes. Finally, we propose a new estimator for the KL\ndivergence and empirically show that it provides a tight approximation through\na few examples.\n","authors":["Ahmad Beirami","Alekh Agarwal","Jonathan Berant","Alexander D'Amour","Jacob Eisenstein","Chirag Nagpal","Ananda Theertha Suresh"],"pdf_url":"https://arxiv.org/pdf/2401.01879v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01862v1","updated":"2024-01-03T18:09:33Z","published":"2024-01-03T18:09:33Z","title":"A Vision Check-up for Language Models","summary":"  What does learning to model relationships between strings teach large\nlanguage models (LLMs) about the visual world? We systematically evaluate LLMs'\nabilities to generate and recognize an assortment of visual concepts of\nincreasing complexity and then demonstrate how a preliminary visual\nrepresentation learning system can be trained using models of text. As language\nmodels lack the ability to consume or output visual information as pixels, we\nuse code to represent images in our study. Although LLM-generated images do not\nlook like natural images, results on image generation and the ability of models\nto correct these generated images indicate that precise modeling of strings can\nteach language models about numerous aspects of the visual world. Furthermore,\nexperiments on self-supervised visual representation learning, utilizing images\ngenerated with text models, highlight the potential to train vision models\ncapable of making semantic assessments of natural images using just LLMs.\n","authors":["Pratyusha Sharma","Tamar Rott Shaham","Manel Baradad","Stephanie Fu","Adrian Rodriguez-Munoz","Shivam Duggal","Phillip Isola","Antonio Torralba"],"pdf_url":"https://arxiv.org/pdf/2401.01862v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01854v1","updated":"2024-01-03T17:48:10Z","published":"2024-01-03T17:48:10Z","title":"Multilingual Instruction Tuning With Just a Pinch of Multilinguality","summary":"  As instruction-tuned large language models (LLMs) gain global adoption, their\nability to follow instructions in multiple languages becomes increasingly\ncrucial. One promising approach is cross-lingual transfer, where a model\nacquires specific functionality on some language by finetuning on another\nlanguage. In this work, we investigate how multilinguality during instruction\ntuning of a multilingual LLM affects instruction-following across languages. We\nfirst show that many languages transfer some instruction-following capabilities\nto other languages from even monolingual tuning. Furthermore, we find that only\n40 multilingual examples in an English tuning set substantially improve\nmultilingual instruction-following, both in seen and unseen languages during\ntuning. In general, we observe that models tuned on multilingual mixtures\nexhibit comparable or superior performance in several languages compared to\nmonolingually tuned models, despite training on 10x fewer examples in those\nlanguages. Finally, we find that increasing the number of languages in the\ninstruction tuning set from 1 to only 2, 3, or 4 increases cross-lingual\ngeneralization. Our results suggest that building massively multilingual\ninstruction-tuned models can be done with only a very small set of multilingual\ninstruction-responses.\n","authors":["Uri Shaham","Jonathan Herzig","Roee Aharoni","Idan Szpektor","Reut Tsarfaty","Matan Eyal"],"pdf_url":"https://arxiv.org/pdf/2401.01854v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01843v1","updated":"2024-01-03T17:22:48Z","published":"2024-01-03T17:22:48Z","title":"Investigating Semi-Supervised Learning Algorithms in Text Datasets","summary":"  Using large training datasets enhances the generalization capabilities of\nneural networks. Semi-supervised learning (SSL) is useful when there are few\nlabeled data and a lot of unlabeled data. SSL methods that use data\naugmentation are most successful for image datasets. In contrast, texts do not\nhave consistent augmentation methods as images. Consequently, methods that use\naugmentation are not as effective in text data as they are in image data. In\nthis study, we compared SSL algorithms that do not require augmentation; these\nare self-training, co-training, tri-training, and tri-training with\ndisagreement. In the experiments, we used 4 different text datasets for\ndifferent tasks. We examined the algorithms from a variety of perspectives by\nasking experiment questions and suggested several improvements. Among the\nalgorithms, tri-training with disagreement showed the closest performance to\nthe Oracle; however, performance gap shows that new semi-supervised algorithms\nor improvements in existing methods are needed.\n","authors":["Himmet Toprak Kesgin","Mehmet Fatih Amasyali"],"pdf_url":"https://arxiv.org/pdf/2401.01843v1.pdf","comment":"Innovations in Intelligent Systems and Applications Conference (ASYU)"},{"id":"http://arxiv.org/abs/2401.01313v2","updated":"2024-01-03T17:13:00Z","published":"2024-01-02T17:56:30Z","title":"A Comprehensive Survey of Hallucination Mitigation Techniques in Large\n  Language Models","summary":"  As Large Language Models (LLMs) continue to advance in their ability to write\nhuman-like text, a key challenge remains around their tendency to hallucinate\ngenerating content that appears factual but is ungrounded. This issue of\nhallucination is arguably the biggest hindrance to safely deploying these\npowerful LLMs into real-world production systems that impact people's lives.\nThe journey toward widespread adoption of LLMs in practical settings heavily\nrelies on addressing and mitigating hallucinations. Unlike traditional AI\nsystems focused on limited tasks, LLMs have been exposed to vast amounts of\nonline text data during training. While this allows them to display impressive\nlanguage fluency, it also means they are capable of extrapolating information\nfrom the biases in training data, misinterpreting ambiguous prompts, or\nmodifying the information to align superficially with the input. This becomes\nhugely alarming when we rely on language generation capabilities for sensitive\napplications, such as summarizing medical records, financial analysis reports,\netc. This paper presents a comprehensive survey of over 32 techniques developed\nto mitigate hallucination in LLMs. Notable among these are Retrieval Augmented\nGeneration (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023),\nCoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023). Furthermore, we\nintroduce a detailed taxonomy categorizing these methods based on various\nparameters, such as dataset utilization, common tasks, feedback mechanisms, and\nretriever types. This classification helps distinguish the diverse approaches\nspecifically designed to tackle hallucination issues in LLMs. Additionally, we\nanalyze the challenges and limitations inherent in these techniques, providing\na solid foundation for future research in addressing hallucinations and related\nphenomena within the realm of LLMs.\n","authors":["S. M Towhidul Islam Tonmoy","S M Mehedi Zaman","Vinija Jain","Anku Rani","Vipula Rawte","Aman Chadha","Amitava Das"],"pdf_url":"https://arxiv.org/pdf/2401.01313v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10997v3","updated":"2024-01-03T17:04:40Z","published":"2023-12-18T07:47:33Z","title":"Retrieval-Augmented Generation for Large Language Models: A Survey","summary":"  Large Language Models (LLMs) demonstrate significant capabilities but face\nchallenges such as hallucination, outdated knowledge, and non-transparent,\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\nemerged as a promising solution by incorporating knowledge from external\ndatabases. This enhances the accuracy and credibility of the models,\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\nupdates and integration of domain-specific information. RAG synergistically\nmerges LLMs' intrinsic knowledge with the vast, dynamic repositories of\nexternal databases. This comprehensive review paper offers a detailed\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\ntripartite foundation of RAG frameworks, which includes the retrieval , the\ngeneration and the augmentation techniques. The paper highlights the\nstate-of-the-art technologies embedded in each of these critical components,\nproviding a profound understanding of the advancements in RAG systems.\nFurthermore, this paper introduces the metrics and benchmarks for assessing RAG\nmodels, along with the most up-to-date evaluation framework. In conclusion, the\npaper delineates prospective avenues for research, including the identification\nof challenges, the expansion of multi-modalities, and the progression of the\nRAG infrastructure and its ecosystem.\n","authors":["Yunfan Gao","Yun Xiong","Xinyu Gao","Kangxiang Jia","Jinliu Pan","Yuxi Bi","Yi Dai","Jiawei Sun","Qianyu Guo","Meng Wang","Haofen Wang"],"pdf_url":"https://arxiv.org/pdf/2312.10997v3.pdf","comment":"Ongoing Work"},{"id":"http://arxiv.org/abs/2401.01830v1","updated":"2024-01-03T16:47:13Z","published":"2024-01-03T16:47:13Z","title":"Iterative Mask Filling: An Effective Text Augmentation Method Using\n  Masked Language Modeling","summary":"  Data augmentation is an effective technique for improving the performance of\nmachine learning models. However, it has not been explored as extensively in\nnatural language processing (NLP) as it has in computer vision. In this paper,\nwe propose a novel text augmentation method that leverages the Fill-Mask\nfeature of the transformer-based BERT model. Our method involves iteratively\nmasking words in a sentence and replacing them with language model predictions.\nWe have tested our proposed method on various NLP tasks and found it to be\neffective in many cases. Our results are presented along with a comparison to\nexisting augmentation methods. Experimental results show that our proposed\nmethod significantly improves performance, especially on topic classification\ndatasets.\n","authors":["Himmet Toprak Kesgin","Mehmet Fatih Amasyali"],"pdf_url":"https://arxiv.org/pdf/2401.01830v1.pdf","comment":"Published in International Conference on Advanced Engineering,\n  Technology and Applications (ICAETA 2023). The final version is available\n  online at https://link.springer.com/chapter/10.1007/978-3-031-50920-9_35"},{"id":"http://arxiv.org/abs/2401.01825v1","updated":"2024-01-03T16:42:13Z","published":"2024-01-03T16:42:13Z","title":"Physio: An LLM-Based Physiotherapy Advisor","summary":"  The capabilities of the most recent language models have increased the\ninterest in integrating them into real-world applications. However, the fact\nthat these models generate plausible, yet incorrect text poses a constraint\nwhen considering their use in several domains. Healthcare is a prime example of\na domain where text-generative trustworthiness is a hard requirement to\nsafeguard patient well-being. In this paper, we present Physio, a chat-based\napplication for physical rehabilitation. Physio is capable of making an initial\ndiagnosis while citing reliable health sources to support the information\nprovided. Furthermore, drawing upon external knowledge databases, Physio can\nrecommend rehabilitation exercises and over-the-counter medication for symptom\nrelief. By combining these features, Physio can leverage the power of\ngenerative models for language processing while also conditioning its response\non dependable and verifiable sources. A live demo of Physio is available at\nhttps://physio.inesctec.pt.\n","authors":["Rúben Almeida","Hugo Sousa","Luís F. Cunha","Nuno Guimarães","Ricardo Campos","Alípio Jorge"],"pdf_url":"https://arxiv.org/pdf/2401.01825v1.pdf","comment":"Demo, ECIR 2024, 3rd Sword AI challenge 2023"},{"id":"http://arxiv.org/abs/2401.01780v1","updated":"2024-01-03T15:12:42Z","published":"2024-01-03T15:12:42Z","title":"Navigating Uncertainty: Optimizing API Dependency for Hallucination\n  Reduction in Closed-Book Question Answering","summary":"  While Large Language Models (LLM) are able to accumulate and restore\nknowledge, they are still prone to hallucination. Especially when faced with\nfactual questions, LLM cannot only rely on knowledge stored in parameters to\nguarantee truthful and correct answers. Augmenting these models with the\nability to search on external information sources, such as the web, is a\npromising approach to ground knowledge to retrieve information. However,\nsearching in a large collection of documents introduces additional\ncomputational/time costs. An optimal behavior would be to query external\nresources only when the LLM is not confident about answers. In this paper, we\npropose a new LLM able to self-estimate if it is able to answer directly or\nneeds to request an external tool. We investigate a supervised approach by\nintroducing a hallucination masking mechanism in which labels are generated\nusing a close book question-answering task. In addition, we propose to leverage\nparameter-efficient fine-tuning techniques to train our model on a small amount\nof data. Our model directly provides answers for $78.2\\%$ of the known queries\nand opts to search for $77.2\\%$ of the unknown ones. This results in the API\nbeing utilized only $62\\%$ of the time.\n","authors":["Pierre Erbacher","Louis Falissar","Vincent Guigue","Laure Soulier"],"pdf_url":"https://arxiv.org/pdf/2401.01780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01761v1","updated":"2024-01-03T14:28:55Z","published":"2024-01-03T14:28:55Z","title":"Cross-target Stance Detection by Exploiting Target Analytical\n  Perspectives","summary":"  Cross-target stance detection (CTSD) is an important task, which infers the\nattitude of the destination target by utilizing annotated data derived from the\nsource target. One important approach in CTSD is to extract domain-invariant\nfeatures to bridge the knowledge gap between multiple targets. However, the\nanalysis of informal and short text structure, and implicit expressions,\ncomplicate the extraction of domain-invariant knowledge. In this paper, we\npropose a Multi-Perspective Prompt-Tuning (MPPT) model for CTSD that uses the\nanalysis perspective as a bridge to transfer knowledge. First, we develop a\ntwo-stage instruct-based chain-of-thought method (TsCoT) to elicit target\nanalysis perspectives and provide natural language explanations (NLEs) from\nmultiple viewpoints by formulating instructions based on large language model\n(LLM). Second, we propose a multi-perspective prompt-tuning framework\n(MultiPLN) to fuse the NLEs into the stance predictor. Extensive experiments\nresults demonstrate the superiority of MPPT against the state-of-the-art\nbaseline methods.\n","authors":["Daijun Ding","Rong Chen","Bowen Zhang","Xu Huang","Li Dong","Xiaowen Zhao","Ge Song","Liwen Jing"],"pdf_url":"https://arxiv.org/pdf/2401.01761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01759v1","updated":"2024-01-03T14:24:02Z","published":"2024-01-03T14:24:02Z","title":"VGA: Vision and Graph Fused Attention Network for Rumor Detection","summary":"  With the development of social media, rumors have been spread broadly on\nsocial media platforms, causing great harm to society. Beside textual\ninformation, many rumors also use manipulated images or conceal textual\ninformation within images to deceive people and avoid being detected, making\nmultimodal rumor detection be a critical problem. The majority of multimodal\nrumor detection methods mainly concentrate on extracting features of source\nclaims and their corresponding images, while ignoring the comments of rumors\nand their propagation structures. These comments and structures imply the\nwisdom of crowds and are proved to be crucial to debunk rumors. Moreover, these\nmethods usually only extract visual features in a basic manner, seldom consider\ntampering or textual information in images. Therefore, in this study, we\npropose a novel Vision and Graph Fused Attention Network (VGA) for rumor\ndetection to utilize propagation structures among posts so as to obtain the\ncrowd opinions and further explore visual tampering features, as well as the\ntextual information hidden in images. We conduct extensive experiments on three\ndatasets, demonstrating that VGA can effectively detect multimodal rumors and\noutperform state-of-the-art methods significantly.\n","authors":["Lin Bai","Caiyan Jia","Ziying Song","Chaoqun Cui"],"pdf_url":"https://arxiv.org/pdf/2401.01759v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01283v2","updated":"2024-01-03T14:01:49Z","published":"2024-01-02T16:51:17Z","title":"Quality and Quantity of Machine Translation References for Automated\n  Metrics","summary":"  Automatic machine translation metrics often use human translations to\ndetermine the quality system translations. Common wisdom in the field dictates\nthat the human references should be of very high quality. However, there are no\ncost-benefit analyses that could be used to guide practitioners who plan to\ncollect references for machine translation evaluation. We find that\nhigher-quality references lead to better metric correlations with humans at the\nsegment-level. Having up to 7 references per segment and taking their average\nhelps all metrics. Interestingly, the references from vendors of different\nqualities can be mixed together and improve metric success. Higher quality\nreferences, however, cost more to create and we frame this as an optimization\nproblem: given a specific budget, what references should be collected to\nmaximize metric success. These findings can be used by evaluators of shared\ntasks when references need to be created under a certain budget.\n","authors":["Vilém Zouhar","Ondřej Bojar"],"pdf_url":"https://arxiv.org/pdf/2401.01283v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07913v3","updated":"2024-01-03T13:29:43Z","published":"2023-12-13T06:11:42Z","title":"A Survey of Text Watermarking in the Era of Large Language Models","summary":"  Text watermarking algorithms play a crucial role in the copyright protection\nof textual content, yet their capabilities and application scenarios have been\nlimited historically. The recent developments in large language models (LLMs)\nhave opened new opportunities for the advancement of text watermarking\ntechniques. LLMs not only enhance the capabilities of text watermarking\nalgorithms through their text understanding and generation abilities but also\nnecessitate the use of text watermarking algorithms for their own copyright\nprotection. This paper conducts a comprehensive survey of the current state of\ntext watermarking technology, covering four main aspects: (1) an overview and\ncomparison of different text watermarking techniques; (2) evaluation methods\nfor text watermarking algorithms, including their success rates, impact on text\nquality, robustness, and unforgeability; (3) potential application scenarios\nfor text watermarking technology; (4) current challenges and future directions\nfor development. This survey aims to provide researchers with a thorough\nunderstanding of text watermarking technology, thereby promoting its further\nadvancement.\n","authors":["Aiwei Liu","Leyi Pan","Yijian Lu","Jingjing Li","Xuming Hu","Lijie Wen","Irwin King","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2312.07913v3.pdf","comment":"39 pages, 7 figures"},{"id":"http://arxiv.org/abs/2310.19923v3","updated":"2024-01-03T13:26:41Z","published":"2023-10-30T18:35:30Z","title":"Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long\n  Documents","summary":"  Text embedding models have emerged as powerful tools for transforming\nsentences into fixed-sized feature vectors that encapsulate semantic\ninformation. While these models are essential for tasks like information\nretrieval, semantic clustering, and text re-ranking, most existing open-source\nmodels, especially those built on architectures like BERT, struggle to\nrepresent lengthy documents and often resort to truncation. One common approach\nto mitigate this challenge involves splitting documents into smaller paragraphs\nfor embedding. However, this strategy results in a much larger set of vectors,\nconsequently leading to increased memory consumption and computationally\nintensive vector searches with elevated latency.\n  To address these challenges, we introduce Jina Embeddings 2, an open-source\ntext embedding model capable of accommodating up to 8192 tokens. This model is\ndesigned to transcend the conventional 512-token limit and adeptly process long\ndocuments. Jina Embeddings 2 not only achieves state-of-the-art performance on\na range of embedding-related tasks in the MTEB benchmark but also matches the\nperformance of OpenAI's proprietary ada-002 model. Additionally, our\nexperiments indicate that an extended context can enhance performance in tasks\nsuch as NarrativeQA.\n","authors":["Michael Günther","Jackmin Ong","Isabelle Mohr","Alaeddine Abdessalem","Tanguy Abel","Mohammad Kalim Akram","Susana Guzman","Georgios Mastrapas","Saba Sturua","Bo Wang","Maximilian Werk","Nan Wang","Han Xiao"],"pdf_url":"https://arxiv.org/pdf/2310.19923v3.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2401.01711v1","updated":"2024-01-03T12:28:33Z","published":"2024-01-03T12:28:33Z","title":"Evaluating Large Language Models in Semantic Parsing for Conversational\n  Question Answering over Knowledge Graphs","summary":"  Conversational question answering systems often rely on semantic parsing to\nenable interactive information retrieval, which involves the generation of\nstructured database queries from a natural language input. For\ninformation-seeking conversations about facts stored within a knowledge graph,\ndialogue utterances are transformed into graph queries in a process that is\ncalled knowledge-based conversational question answering. This paper evaluates\nthe performance of large language models that have not been explicitly\npre-trained on this task. Through a series of experiments on an extensive\nbenchmark dataset, we compare models of varying sizes with different prompting\ntechniques and identify common issue types in the generated output. Our results\ndemonstrate that large language models are capable of generating graph queries\nfrom dialogues, with significant improvements achievable through few-shot\nprompting and fine-tuning techniques, especially for smaller models that\nexhibit lower zero-shot performance.\n","authors":["Phillip Schneider","Manuel Klettner","Kristiina Jokinen","Elena Simperl","Florian Matthes"],"pdf_url":"https://arxiv.org/pdf/2401.01711v1.pdf","comment":"Accepted to ICAART 2024"},{"id":"http://arxiv.org/abs/2312.06281v2","updated":"2024-01-03T12:20:35Z","published":"2023-12-11T10:35:32Z","title":"EQ-Bench: An Emotional Intelligence Benchmark for Large Language Models","summary":"  We introduce EQ-Bench, a novel benchmark designed to evaluate aspects of\nemotional intelligence in Large Language Models (LLMs). We assess the ability\nof LLMs to understand complex emotions and social interactions by asking them\nto predict the intensity of emotional states of characters in a dialogue. The\nbenchmark is able to discriminate effectively between a wide range of models.\nWe find that EQ-Bench correlates strongly with comprehensive multi-domain\nbenchmarks like MMLU (Hendrycks et al., 2020) (r=0.97), indicating that we may\nbe capturing similar aspects of broad intelligence. Our benchmark produces\nhighly repeatable results using a set of 60 English-language questions. We also\nprovide open-source code for an automated benchmarking pipeline at\nhttps://github.com/EQ-bench/EQ-Bench and a leaderboard at https://eqbench.com\n","authors":["Samuel J. Paech"],"pdf_url":"https://arxiv.org/pdf/2312.06281v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01699v1","updated":"2024-01-03T12:06:02Z","published":"2024-01-03T12:06:02Z","title":"WordArt Designer API: User-Driven Artistic Typography Synthesis with\n  Large Language Models on ModelScope","summary":"  This paper introduces the WordArt Designer API, a novel framework for\nuser-driven artistic typography synthesis utilizing Large Language Models\n(LLMs) on ModelScope. We address the challenge of simplifying artistic\ntypography for non-professionals by offering a dynamic, adaptive, and\ncomputationally efficient alternative to traditional rigid templates. Our\napproach leverages the power of LLMs to understand and interpret user input,\nfacilitating a more intuitive design process. We demonstrate through various\ncase studies how users can articulate their aesthetic preferences and\nfunctional requirements, which the system then translates into unique and\ncreative typographic designs. Our evaluations indicate significant improvements\nin user satisfaction, design flexibility, and creative expression over existing\nsystems. The WordArt Designer API not only democratizes the art of typography\nbut also opens up new possibilities for personalized digital communication and\ndesign.\n","authors":["Jun-Yan He","Zhi-Qi Cheng","Chenyang Li","Jingdong Sun","Wangmeng Xiang","Yusen Hu","Xianhui Lin","Xiaoyang Kang","Zengke Jin","Bin Luo","Yifeng Geng","Xuansong Xie","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2401.01699v1.pdf","comment":"Spotlight Paper from the Workshop on Machine Learning for Creativity\n  and Design at the 37th Conference on Neural Information Processing Systems\n  (NeurIPS 2023). 5 pages, 5 figures"},{"id":"http://arxiv.org/abs/2401.01698v1","updated":"2024-01-03T12:05:38Z","published":"2024-01-03T12:05:38Z","title":"Patterns of Persistence and Diffusibility across World's Languages","summary":"  Language similarities can be caused by genetic relatedness, areal contact,\nuniversality, or chance. Colexification, i.e.~a type of similarity where a\nsingle lexical form is used to convey multiple meanings, is underexplored. In\nour work, we shed light on the linguistic causes of cross-lingual similarity in\ncolexification and phonology, by exploring genealogical stability (persistence)\nand contact-induced change (diffusibility). We construct large-scale graphs\nincorporating semantic, genealogical, phonological and geographical data for\n1,966 languages. We then show the potential of this resource, by investigating\nseveral established hypotheses from previous work in linguistics, while\nproposing new ones. Our results strongly support a previously established\nhypothesis in the linguistic literature, while offering contradicting evidence\nto another. Our large scale resource opens for further research across\ndisciplines, e.g.~in multilingual NLP and comparative linguistics.\n","authors":["Yiyi Chen","Johannes Bjerva"],"pdf_url":"https://arxiv.org/pdf/2401.01698v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2310.06452v2","updated":"2024-01-03T11:58:42Z","published":"2023-10-10T09:25:44Z","title":"Understanding the Effects of RLHF on LLM Generalisation and Diversity","summary":"  Large language models (LLMs) fine-tuned with reinforcement learning from\nhuman feedback (RLHF) have been used in some of the most widely deployed AI\nmodels to date, such as OpenAI's ChatGPT or Anthropic's Claude. % , or Meta's\nLLaMA-2. While there has been significant work developing these methods, our\nunderstanding of the benefits and downsides of each stage in RLHF is still\nlimited. To fill this gap, we present an extensive analysis of how each stage\nof the process (i.e.~supervised fine-tuning (SFT), reward modelling, and RLHF)\naffects two key properties: out-of-distribution (OOD) generalisation and output\ndiversity. OOD generalisation is crucial given the wide range of real-world\nscenarios in which these models are being used, while output diversity refers\nto the model's ability to generate varied outputs and is important for a\nvariety of use cases. We perform our analysis across two base models on both\nsummarisation and instruction following tasks, the latter being highly relevant\nfor current LLM use cases. We find that RLHF generalises better than SFT to new\ninputs, particularly as the distribution shift between train and test becomes\nlarger. However, RLHF significantly reduces output diversity compared to SFT\nacross a variety of measures, implying a tradeoff in current LLM fine-tuning\nmethods between generalisation and diversity. Our results provide guidance on\nwhich fine-tuning method should be used depending on the application, and show\nthat more research is needed to improve the tradeoff between generalisation and\ndiversity.\n","authors":["Robert Kirk","Ishita Mediratta","Christoforos Nalmpantis","Jelena Luketina","Eric Hambro","Edward Grefenstette","Roberta Raileanu"],"pdf_url":"https://arxiv.org/pdf/2310.06452v2.pdf","comment":"Code available here: https://github.com/facebookresearch/rlfh-gen-div"},{"id":"http://arxiv.org/abs/2401.01692v1","updated":"2024-01-03T11:54:30Z","published":"2024-01-03T11:54:30Z","title":"Predicting challenge moments from students' discourse: A comparison of\n  GPT-4 to two traditional natural language processing approaches","summary":"  Effective collaboration requires groups to strategically regulate themselves\nto overcome challenges. Research has shown that groups may fail to regulate due\nto differences in members' perceptions of challenges which may benefit from\nexternal support. In this study, we investigated the potential of leveraging\nthree distinct natural language processing models: an expert knowledge\nrule-based model, a supervised machine learning (ML) model and a Large Language\nmodel (LLM), in challenge detection and challenge dimension identification\n(cognitive, metacognitive, emotional and technical/other challenges) from\nstudent discourse, was investigated. The results show that the supervised ML\nand the LLM approaches performed considerably well in both tasks, in contrast\nto the rule-based approach, whose efficacy heavily relies on the engineered\nfeatures by experts. The paper provides an extensive discussion of the three\napproaches' performance for automated detection and support of students'\nchallenge moments in collaborative learning activities. It argues that,\nalthough LLMs provide many advantages, they are unlikely to be the panacea to\nissues of the detection and feedback provision of socially shared regulation of\nlearning due to their lack of reliability, as well as issues of validity\nevaluation, privacy and confabulation. We conclude the paper with a discussion\non additional considerations, including model transparency to explore feasible\nand meaningful analytical feedback for students and educators using LLMs.\n","authors":["Wannapon Suraworachet","Jennifer Seon","Mutlu Cukurova"],"pdf_url":"https://arxiv.org/pdf/2401.01692v1.pdf","comment":"13 pages, 1 figure"},{"id":"http://arxiv.org/abs/2401.01078v2","updated":"2024-01-03T11:54:14Z","published":"2024-01-02T07:46:34Z","title":"Vietnamese Poem Generation & The Prospect Of Cross-Language Poem-To-Poem\n  Translation","summary":"  Poetry generation has been a challenging task in the field of Natural\nLanguage Processing, as it requires the model to understand the nuances of\nlanguage, sentiment, and style. In this paper, we propose using Large Language\nModels to generate Vietnamese poems from natural language prompts, thereby\nfacilitating an intuitive process with enhanced content control. Our most\nefficacious model, the GPT-3 Babbage variant, achieves a custom evaluation\nscore of 0.8, specifically tailored to the \"luc bat\" genre of Vietnamese\npoetry. Furthermore, we also explore the idea of paraphrasing poems into normal\ntext prompts and yield a relatively high score of 0.718 in the \"luc bat\" genre.\nThis experiment presents the potential for cross-Language poem-to-poem\ntranslation with translated poems as the inputs while concurrently maintaining\ncomplete control over the generated content.\n","authors":["Triet Minh Huynh","Quan Le Bao"],"pdf_url":"https://arxiv.org/pdf/2401.01078v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01667v1","updated":"2024-01-03T11:06:01Z","published":"2024-01-03T11:06:01Z","title":"MLPs Compass: What is learned when MLPs are combined with PLMs?","summary":"  While Transformer-based pre-trained language models and their variants\nexhibit strong semantic representation capabilities, the question of\ncomprehending the information gain derived from the additional components of\nPLMs remains an open question in this field. Motivated by recent efforts that\nprove Multilayer-Perceptrons (MLPs) modules achieving robust structural capture\ncapabilities, even outperforming Graph Neural Networks (GNNs), this paper aims\nto quantify whether simple MLPs can further enhance the already potent ability\nof PLMs to capture linguistic information. Specifically, we design a simple yet\neffective probing framework containing MLPs components based on BERT structure\nand conduct extensive experiments encompassing 10 probing tasks spanning three\ndistinct linguistic levels. The experimental results demonstrate that MLPs can\nindeed enhance the comprehension of linguistic structure by PLMs. Our research\nprovides interpretable and valuable insights into crafting variations of PLMs\nutilizing MLPs for tasks that emphasize diverse linguistic structures.\n","authors":["Li Zhou","Wenyu Chen","Yong Cao","Dingyi Zeng","Wanlong Liu","Hong Qu"],"pdf_url":"https://arxiv.org/pdf/2401.01667v1.pdf","comment":"Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2401.01637v1","updated":"2024-01-03T09:27:01Z","published":"2024-01-03T09:27:01Z","title":"Social Media Ready Caption Generation for Brands","summary":"  Social media advertisements are key for brand marketing, aiming to attract\nconsumers with captivating captions and pictures or logos. While previous\nresearch has focused on generating captions for general images, incorporating\nbrand personalities into social media captioning remains unexplored. Brand\npersonalities are shown to be affecting consumers' behaviours and social\ninteractions and thus are proven to be a key aspect of marketing strategies.\nCurrent open-source multimodal LLMs are not directly suited for this task.\nHence, we propose a pipeline solution to assist brands in creating engaging\nsocial media captions that align with the image and the brand personalities.\nOur architecture is based on two parts: a the first part contains an image\ncaptioning model that takes in an image that the brand wants to post online and\ngives a plain English caption; b the second part takes in the generated caption\nalong with the target brand personality and outputs a catchy\npersonality-aligned social media caption. Along with brand personality, our\nsystem also gives users the flexibility to provide hashtags, Instagram handles,\nURLs, and named entities they want the caption to contain, making the captions\nmore semantically related to the social media handles. Comparative evaluations\nagainst various baselines demonstrate the effectiveness of our approach, both\nqualitatively and quantitatively.\n","authors":["Himanshu Maheshwari","Koustava Goswami","Apoorv Saxena","Balaji Vasan Srinivasan"],"pdf_url":"https://arxiv.org/pdf/2401.01637v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01623v1","updated":"2024-01-03T08:49:12Z","published":"2024-01-03T08:49:12Z","title":"Can AI Be as Creative as Humans?","summary":"  Creativity serves as a cornerstone for societal progress and innovation, but\nits assessment remains a complex and often subjective endeavor. With the rise\nof advanced generative AI models capable of tasks once reserved for human\ncreativity, the study of AI's creative potential becomes imperative for its\nresponsible development and application. This paper addresses the complexities\nin defining and evaluating creativity by introducing a new concept called\nRelative Creativity. Instead of trying to define creativity universally, we\nshift the focus to whether AI can match the creative abilities of a\nhypothetical human. This perspective draws inspiration from the Turing Test,\nexpanding upon it to address the challenges and subjectivities inherent in\nevaluating creativity. This methodological shift facilitates a statistically\nquantifiable evaluation of AI's creativity, which we term Statistical\nCreativity. This approach allows for direct comparisons of AI's creative\nabilities with those of specific human groups. Building on this foundation, we\ndiscuss the application of statistical creativity in contemporary\nprompt-conditioned autoregressive models. In addition to defining and analyzing\na measure of creativity, we introduce an actionable training guideline,\neffectively bridging the gap between theoretical quantification of creativity\nand practical model training. Through these multifaceted contributions, the\npaper establishes a cohesive, continuously evolving, and transformative\nframework for assessing and fostering statistical creativity in AI models.\n","authors":["Haonan Wang","James Zou","Michael Mozer","Linjun Zhang","Anirudh Goyal","Alex Lamb","Zhun Deng","Michael Qizhe Xie","Hannah Brown","Kenji Kawaguchi"],"pdf_url":"https://arxiv.org/pdf/2401.01623v1.pdf","comment":"The paper presents \"Relative Creativity,\" comparing AI creativity to\n  human creativity, inspired by the Turing Test. It introduces \"Statistical\n  Creativity\" for measurable assessment and provides AI training guidelines to\n  foster AI's creative capabilities"},{"id":"http://arxiv.org/abs/2401.01620v1","updated":"2024-01-03T08:41:27Z","published":"2024-01-03T08:41:27Z","title":"Large Language Model Capabilities in Perioperative Risk Prediction and\n  Prognostication","summary":"  We investigate whether general-domain large language models such as GPT-4\nTurbo can perform risk stratification and predict post-operative outcome\nmeasures using a description of the procedure and a patient's clinical notes\nderived from the electronic health record. We examine predictive performance on\n8 different tasks: prediction of ASA Physical Status Classification, hospital\nadmission, ICU admission, unplanned admission, hospital mortality, PACU Phase 1\nduration, hospital duration, and ICU duration. Few-shot and chain-of-thought\nprompting improves predictive performance for several of the tasks. We achieve\nF1 scores of 0.50 for ASA Physical Status Classification, 0.81 for ICU\nadmission, and 0.86 for hospital mortality. Performance on duration prediction\ntasks were universally poor across all prompt strategies. Current generation\nlarge language models can assist clinicians in perioperative risk\nstratification on classification tasks and produce high-quality natural\nlanguage summaries and explanations.\n","authors":["Philip Chung","Christine T Fong","Andrew M Walters","Nima Aghaeepour","Meliha Yetisgen","Vikas N O'Reilly-Shah"],"pdf_url":"https://arxiv.org/pdf/2401.01620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01614v1","updated":"2024-01-03T08:33:09Z","published":"2024-01-03T08:33:09Z","title":"GPT-4V(ision) is a Generalist Web Agent, if Grounded","summary":"  The recent development on large multimodal models (LMMs), especially\nGPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries\nof multimodal models beyond traditional tasks like image captioning and visual\nquestion answering. In this work, we explore the potential of LMMs like GPT-4V\nas a generalist web agent that can follow natural language instructions to\ncomplete tasks on any given website. We propose SEEACT, a generalist web agent\nthat harnesses the power of LMMs for integrated visual understanding and acting\non the web. We evaluate on the recent MIND2WEB benchmark. In addition to\nstandard offline evaluation on cached websites, we enable a new online\nevaluation setting by developing a tool that allows running web agents on live\nwebsites. We show that GPT-4V presents a great potential for web agents - it\ncan successfully complete 50% of the tasks on live websites if we manually\nground its textual plans into actions on the websites. This substantially\noutperforms text-only LLMs like GPT-4 or smaller models (FLAN-T5 and BLIP-2)\nspecifically fine-tuned for web agents. However, grounding still remains a\nmajor challenge. Existing LMM grounding strategies like set-of-mark prompting\nturns out not effective for web agents, and the best grounding strategy we\ndevelop in this paper leverages both the HTML text and visuals. Yet, there is\nstill a substantial gap with oracle grounding, leaving ample room for further\nimprovement.\n","authors":["Boyuan Zheng","Boyu Gou","Jihyung Kil","Huan Sun","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2401.01614v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01262v2","updated":"2024-01-03T08:17:53Z","published":"2024-01-02T16:09:36Z","title":"Fairness Certification for Natural Language Processing and Large\n  Language Models","summary":"  Natural Language Processing (NLP) plays an important role in our daily lives,\nparticularly due to the enormous progress of Large Language Models (LLM).\nHowever, NLP has many fairness-critical use cases, e.g., as an expert system in\nrecruitment or as an LLM-based tutor in education. Since NLP is based on human\nlanguage, potentially harmful biases can diffuse into NLP systems and produce\nunfair results, discriminate against minorities or generate legal issues.\nHence, it is important to develop a fairness certification for NLP approaches.\nWe follow a qualitative research approach towards a fairness certification for\nNLP. In particular, we have reviewed a large body of literature on algorithmic\nfairness, and we have conducted semi-structured expert interviews with a wide\nrange of experts from that area. We have systematically devised six fairness\ncriteria for NLP, which can be further refined into 18 sub-categories. Our\ncriteria offer a foundation for operationalizing and testing processes to\ncertify fairness, both from the perspective of the auditor and the audited\norganization.\n","authors":["Vincent Freiberger","Erik Buchmann"],"pdf_url":"https://arxiv.org/pdf/2401.01262v2.pdf","comment":"In depth discussion of our results can be found in the Appendix B"},{"id":"http://arxiv.org/abs/2401.01600v1","updated":"2024-01-03T08:06:26Z","published":"2024-01-03T08:06:26Z","title":"PLLaMa: An Open-source Large Language Model for Plant Science","summary":"  Large Language Models (LLMs) have exhibited remarkable capabilities in\nunderstanding and interacting with natural language across various sectors.\nHowever, their effectiveness is limited in specialized areas requiring high\naccuracy, such as plant science, due to a lack of specific expertise in these\nfields. This paper introduces PLLaMa, an open-source language model that\nevolved from LLaMa-2. It's enhanced with a comprehensive database, comprising\nmore than 1.5 million scholarly articles in plant science. This development\nsignificantly enriches PLLaMa with extensive knowledge and proficiency in plant\nand agricultural sciences. Our initial tests, involving specific datasets\nrelated to plants and agriculture, show that PLLaMa substantially improves its\nunderstanding of plant science-related topics. Moreover, we have formed an\ninternational panel of professionals, including plant scientists, agricultural\nengineers, and plant breeders. This team plays a crucial role in verifying the\naccuracy of PLLaMa's responses to various academic inquiries, ensuring its\neffective and reliable application in the field. To support further research\nand development, we have made the model's checkpoints and source codes\naccessible to the scientific community. These resources are available for\ndownload at \\url{https://github.com/Xianjun-Yang/PLLaMa}.\n","authors":["Xianjun Yang","Junfeng Gao","Wenxin Xue","Erik Alexandersson"],"pdf_url":"https://arxiv.org/pdf/2401.01600v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2401.01596v1","updated":"2024-01-03T07:58:25Z","published":"2024-01-03T07:58:25Z","title":"MedSumm: A Multimodal Approach to Summarizing Code-Mixed Hindi-English\n  Clinical Queries","summary":"  In the healthcare domain, summarizing medical questions posed by patients is\ncritical for improving doctor-patient interactions and medical decision-making.\nAlthough medical data has grown in complexity and quantity, the current body of\nresearch in this domain has primarily concentrated on text-based methods,\noverlooking the integration of visual cues. Also prior works in the area of\nmedical question summarisation have been limited to the English language. This\nwork introduces the task of multimodal medical question summarization for\ncodemixed input in a low-resource setting. To address this gap, we introduce\nthe Multimodal Medical Codemixed Question Summarization MMCQS dataset, which\ncombines Hindi-English codemixed medical queries with visual aids. This\nintegration enriches the representation of a patient's medical condition,\nproviding a more comprehensive perspective. We also propose a framework named\nMedSumm that leverages the power of LLMs and VLMs for this task. By utilizing\nour MMCQS dataset, we demonstrate the value of integrating visual information\nfrom images to improve the creation of medically detailed summaries. This\nmultimodal strategy not only improves healthcare decision-making but also\npromotes a deeper comprehension of patient queries, paving the way for future\nexploration in personalized and responsive medical care. Our dataset, code, and\npre-trained models will be made publicly available.\n","authors":["Akash Ghosh","Arkadeep Acharya","Prince Jha","Aniket Gaudgaul","Rajdeep Majumdar","Sriparna Saha","Aman Chadha","Raghav Jain","Setu Sinha","Shivani Agarwal"],"pdf_url":"https://arxiv.org/pdf/2401.01596v1.pdf","comment":"ECIR 2024"},{"id":"http://arxiv.org/abs/2401.01572v1","updated":"2024-01-03T06:56:56Z","published":"2024-01-03T06:56:56Z","title":"Hallucinations in Neural Automatic Speech Recognition: Identifying\n  Errors and Hallucinatory Models","summary":"  Hallucinations are a type of output error produced by deep neural networks.\nWhile this has been studied in natural language processing, they have not been\nresearched previously in automatic speech recognition. Here, we define\nhallucinations in ASR as transcriptions generated by a model that are\nsemantically unrelated to the source utterance, yet still fluent and coherent.\nThe similarity of hallucinations to probable natural language outputs of the\nmodel creates a danger of deception and impacts the credibility of the system.\nWe show that commonly used metrics, such as word error rates, cannot\ndifferentiate between hallucinatory and non-hallucinatory models. To address\nthis, we propose a perturbation-based method for assessing the susceptibility\nof an automatic speech recognition (ASR) model to hallucination at test time,\nwhich does not require access to the training dataset. We demonstrate that this\nmethod helps to distinguish between hallucinatory and non-hallucinatory models\nthat have similar baseline word error rates. We further explore the\nrelationship between the types of ASR errors and the types of dataset noise to\ndetermine what types of noise are most likely to create hallucinatory outputs.\nWe devise a framework for identifying hallucinations by analysing their\nsemantic connection with the ground truth and their fluency. Finally, we\ndiscover how to induce hallucinations with a random noise injection to the\nutterance.\n","authors":["Rita Frieske","Bertram E. Shi"],"pdf_url":"https://arxiv.org/pdf/2401.01572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04444v3","updated":"2024-01-03T06:38:36Z","published":"2023-10-02T22:35:40Z","title":"What's the Magic Word? A Control Theory of LLM Prompting","summary":"  Prompt engineering is crucial for deploying LLMs but is poorly understood\nmathematically. We formalize LLM systems as a class of discrete stochastic\ndynamical systems to explore prompt engineering through the lens of control\ntheory. We investigate the reachable set of output token sequences $R_y(\\mathbf\nx_0)$ for which there exists a control input sequence $\\mathbf u$ for each\n$\\mathbf y \\in R_y(\\mathbf x_0)$ that steers the LLM to output $\\mathbf y$ from\ninitial state sequence $\\mathbf x_0$. We offer analytic analysis on the\nlimitations on the controllability of self-attention in terms of reachable set,\nwhere we prove an upper bound on the reachable set of outputs $R_y(\\mathbf\nx_0)$ as a function of the singular values of the parameter matrices. We\npresent complementary empirical analysis on the controllability of a panel of\nLLMs, including Falcon-7b, Llama-7b, and Falcon-40b. Our results demonstrate a\nlower bound on the reachable set of outputs $R_y(\\mathbf x_0)$ w.r.t. initial\nstate sequences $\\mathbf x_0$ sampled from the Wikitext dataset. We find that\nthe correct next Wikitext token following sequence $\\mathbf x_0$ is reachable\nover 97% of the time with prompts of $k\\leq 10$ tokens. We also establish that\nthe top 75 most likely next tokens, as estimated by the LLM itself, are\nreachable at least 85% of the time with prompts of $k\\leq 10$ tokens.\nIntriguingly, short prompt sequences can dramatically alter the likelihood of\nspecific outputs, even making the least likely tokens become the most likely\nones. This control-centric analysis of LLMs demonstrates the significant and\npoorly understood role of input sequences in steering output probabilities,\noffering a foundational perspective for enhancing language model system\ncapabilities.\n","authors":["Aman Bhargava","Cameron Witkowski","Manav Shah","Matt Thomson"],"pdf_url":"https://arxiv.org/pdf/2310.04444v3.pdf","comment":"23 pages, 8 figures. Under review for ICLR 2024"},{"id":"http://arxiv.org/abs/2306.17408v3","updated":"2024-01-03T05:00:00Z","published":"2023-06-30T05:50:26Z","title":"LMBot: Distilling Graph Knowledge into Language Model for Graph-less\n  Deployment in Twitter Bot Detection","summary":"  As malicious actors employ increasingly advanced and widespread bots to\ndisseminate misinformation and manipulate public opinion, the detection of\nTwitter bots has become a crucial task. Though graph-based Twitter bot\ndetection methods achieve state-of-the-art performance, we find that their\ninference depends on the neighbor users multi-hop away from the targets, and\nfetching neighbors is time-consuming and may introduce bias. At the same time,\nwe find that after finetuning on Twitter bot detection, pretrained language\nmodels achieve competitive performance and do not require a graph structure\nduring deployment. Inspired by this finding, we propose a novel bot detection\nframework LMBot that distills the knowledge of graph neural networks (GNNs)\ninto language models (LMs) for graph-less deployment in Twitter bot detection\nto combat the challenge of data dependency. Moreover, LMBot is compatible with\ngraph-based and graph-less datasets. Specifically, we first represent each user\nas a textual sequence and feed them into the LM for domain adaptation. For\ngraph-based datasets, the output of LMs provides input features for the GNN,\nenabling it to optimize for bot detection and distill knowledge back to the LM\nin an iterative, mutually enhancing process. Armed with the LM, we can perform\ngraph-less inference, which resolves the graph data dependency and sampling\nbias issues. For datasets without graph structure, we simply replace the GNN\nwith an MLP, which has also shown strong performance. Our experiments\ndemonstrate that LMBot achieves state-of-the-art performance on four Twitter\nbot detection benchmarks. Extensive studies also show that LMBot is more\nrobust, versatile, and efficient compared to graph-based Twitter bot detection\nmethods.\n","authors":["Zijian Cai","Zhaoxuan Tan","Zhenyu Lei","Zifeng Zhu","Hongrui Wang","Qinghua Zheng","Minnan Luo"],"pdf_url":"https://arxiv.org/pdf/2306.17408v3.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2305.11348v2","updated":"2024-01-03T04:00:15Z","published":"2023-05-18T23:47:00Z","title":"In the Name of Fairness: Assessing the Bias in Clinical Record\n  De-identification","summary":"  Data sharing is crucial for open science and reproducible research, but the\nlegal sharing of clinical data requires the removal of protected health\ninformation from electronic health records. This process, known as\nde-identification, is often achieved through the use of machine learning\nalgorithms by many commercial and open-source systems. While these systems have\nshown compelling results on average, the variation in their performance across\ndifferent demographic groups has not been thoroughly examined. In this work, we\ninvestigate the bias of de-identification systems on names in clinical notes\nvia a large-scale empirical analysis. To achieve this, we create 16 name sets\nthat vary along four demographic dimensions: gender, race, name popularity, and\nthe decade of popularity. We insert these names into 100 manually curated\nclinical templates and evaluate the performance of nine public and private\nde-identification methods. Our findings reveal that there are statistically\nsignificant performance gaps along a majority of the demographic dimensions in\nmost methods. We further illustrate that de-identification quality is affected\nby polysemy in names, gender context, and clinical note characteristics. To\nmitigate the identified gaps, we propose a simple and method-agnostic solution\nby fine-tuning de-identification methods with clinical context and diverse\nnames. Overall, it is imperative to address the bias in existing methods\nimmediately so that downstream stakeholders can build high-quality systems to\nserve all demographic parties fairly.\n","authors":["Yuxin Xiao","Shulammite Lim","Tom Joseph Pollard","Marzyeh Ghassemi"],"pdf_url":"https://arxiv.org/pdf/2305.11348v2.pdf","comment":"Accepted by FAccT 2023; updated appendix with the de-identification\n  performance of GPT-4"},{"id":"http://arxiv.org/abs/2401.01523v1","updated":"2024-01-03T03:28:55Z","published":"2024-01-03T03:28:55Z","title":"GOAT-Bench: Safety Insights to Large Multimodal Models through\n  Meme-Based Social Abuse","summary":"  The exponential growth of social media has profoundly transformed how\ninformation is created, disseminated, and absorbed, exceeding any precedent in\nthe digital age. Regrettably, this explosion has also spawned a significant\nincrease in the online abuse of memes. Evaluating the negative impact of memes\nis notably challenging, owing to their often subtle and implicit meanings,\nwhich are not directly conveyed through the overt text and imagery. In light of\nthis, large multimodal models (LMMs) have emerged as a focal point of interest\ndue to their remarkable capabilities in handling diverse multimodal tasks. In\nresponse to this development, our paper aims to thoroughly examine the capacity\nof various LMMs (e.g. GPT-4V) to discern and respond to the nuanced aspects of\nsocial abuse manifested in memes. We introduce the comprehensive meme\nbenchmark, GOAT-Bench, comprising over 6K varied memes encapsulating themes\nsuch as implicit hate speech, sexism, and cyberbullying, etc. Utilizing\nGOAT-Bench, we delve into the ability of LMMs to accurately assess hatefulness,\nmisogyny, offensiveness, sarcasm, and harmful content. Our extensive\nexperiments across a range of LMMs reveal that current models still exhibit a\ndeficiency in safety awareness, showing insensitivity to various forms of\nimplicit abuse. We posit that this shortfall represents a critical impediment\nto the realization of safe artificial intelligence. The GOAT-Bench and\naccompanying resources are publicly accessible at https://goatlmm.github.io/,\ncontributing to ongoing research in this vital field.\n","authors":["Hongzhan Lin","Ziyang Luo","Bo Wang","Ruichao Yang","Jing Ma"],"pdf_url":"https://arxiv.org/pdf/2401.01523v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2401.01076v2","updated":"2024-01-03T02:13:29Z","published":"2024-01-02T07:40:12Z","title":"DialCLIP: Empowering CLIP as Multi-Modal Dialog Retriever","summary":"  Recently, substantial advancements in pre-trained vision-language models have\ngreatly enhanced the capabilities of multi-modal dialog systems. These models\nhave demonstrated significant improvements by fine-tuning on downstream tasks.\nHowever, the existing pre-trained models primarily focus on effectively\ncapturing the alignment between vision and language modalities, often ignoring\nthe intricate nature of dialog context. In this paper, we propose a\nparameter-efficient prompt-tuning method named DialCLIP for multi-modal dialog\nretrieval. Specifically, our approach introduces a multi-modal context prompt\ngenerator to learn context features which are subsequently distilled into\nprompts within the pre-trained vision-language model CLIP. Besides, we\nintroduce domain prompt to mitigate the disc repancy from the downstream dialog\ndata. To facilitate various types of retrieval, we also design multiple experts\nto learn mappings from CLIP outputs to multi-modal representation space, with\neach expert being responsible to one specific retrieval type. Extensive\nexperiments show that DialCLIP achieves state-of-the-art performance on two\nwidely recognized benchmark datasets (i.e., PhotoChat and MMDialog) by tuning a\nmere 0.04% of the total parameters. These results highlight the efficacy and\nefficiency of our proposed approach, underscoring its potential to advance the\nfield of multi-modal dialog retrieval.\n","authors":["Zhichao Yin","Binyuan Hui","Min Yang","Fei Huang","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2401.01076v2.pdf","comment":"ICASSP 2024"},{"id":"http://arxiv.org/abs/2401.01498v1","updated":"2024-01-03T02:03:36Z","published":"2024-01-03T02:03:36Z","title":"Utilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic\n  Token Prediction","summary":"  We propose a novel text-to-speech (TTS) framework centered around a neural\ntransducer. Our approach divides the whole TTS pipeline into semantic-level\nsequence-to-sequence (seq2seq) modeling and fine-grained acoustic modeling\nstages, utilizing discrete semantic tokens obtained from wav2vec2.0 embeddings.\nFor a robust and efficient alignment modeling, we employ a neural transducer\nnamed token transducer for the semantic token prediction, benefiting from its\nhard monotonic alignment constraints. Subsequently, a non-autoregressive (NAR)\nspeech generator efficiently synthesizes waveforms from these semantic tokens.\nAdditionally, a reference speech controls temporal dynamics and acoustic\nconditions at each stage. This decoupled framework reduces the training\ncomplexity of TTS while allowing each stage to focus on semantic and acoustic\nmodeling. Our experimental results on zero-shot adaptive TTS demonstrate that\nour model surpasses the baseline in terms of speech quality and speaker\nsimilarity, both objectively and subjectively. We also delve into the inference\nspeed and prosody control capabilities of our approach, highlighting the\npotential of neural transducers in TTS frameworks.\n","authors":["Minchan Kim","Myeonghun Jeong","Byoung Jin Choi","Semin Kim","Joun Yeop Lee","Nam Soo Kim"],"pdf_url":"https://arxiv.org/pdf/2401.01498v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2401.01495v1","updated":"2024-01-03T01:58:31Z","published":"2024-01-03T01:58:31Z","title":"A Two-Stage Multimodal Emotion Recognition Model Based on Graph\n  Contrastive Learning","summary":"  In terms of human-computer interaction, it is becoming more and more\nimportant to correctly understand the user's emotional state in a conversation,\nso the task of multimodal emotion recognition (MER) started to receive more\nattention. However, existing emotion classification methods usually perform\nclassification only once. Sentences are likely to be misclassified in a single\nround of classification. Previous work usually ignores the similarities and\ndifferences between different morphological features in the fusion process. To\naddress the above issues, we propose a two-stage emotion recognition model\nbased on graph contrastive learning (TS-GCL). First, we encode the original\ndataset with different preprocessing modalities. Second, a graph contrastive\nlearning (GCL) strategy is introduced for these three modal data with other\nstructures to learn similarities and differences within and between modalities.\nFinally, we use MLP twice to achieve the final emotion classification. This\nstaged classification method can help the model to better focus on different\nlevels of emotional information, thereby improving the performance of the\nmodel. Extensive experiments show that TS-GCL has superior performance on\nIEMOCAP and MELD datasets compared with previous methods.\n","authors":["Wei Ai","FuChen Zhang","Tao Meng","YunTao Shou","HongEn Shao","Keqin Li"],"pdf_url":"https://arxiv.org/pdf/2401.01495v1.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2401.01487v1","updated":"2024-01-03T01:21:30Z","published":"2024-01-03T01:21:30Z","title":"Natural Language Processing and Multimodal Stock Price Prediction","summary":"  In the realm of financial decision-making, predicting stock prices is\npivotal. Artificial intelligence techniques such as long short-term memory\nnetworks (LSTMs), support-vector machines (SVMs), and natural language\nprocessing (NLP) models are commonly employed to predict said prices. This\npaper utilizes stock percentage change as training data, in contrast to the\ntraditional use of raw currency values, with a focus on analyzing publicly\nreleased news articles. The choice of percentage change aims to provide models\nwith context regarding the significance of price fluctuations and overall price\nchange impact on a given stock. The study employs specialized BERT natural\nlanguage processing models to predict stock price trends, with a particular\nemphasis on various data modalities. The results showcase the capabilities of\nsuch strategies with a small natural language processing model to accurately\npredict overall stock trends, and highlight the effectiveness of certain data\nfeatures and sector-specific data.\n","authors":["Kevin Taylor","Jerry Ng"],"pdf_url":"https://arxiv.org/pdf/2401.01487v1.pdf","comment":"13 pages, 13 figures"},{"id":"http://arxiv.org/abs/2306.02052v2","updated":"2024-01-03T00:56:20Z","published":"2023-06-03T08:50:13Z","title":"Conflicts, Villains, Resolutions: Towards models of Narrative Media\n  Framing","summary":"  Despite increasing interest in the automatic detection of media frames in\nNLP, the problem is typically simplified as single-label classification and\nadopts a topic-like view on frames, evading modelling the broader\ndocument-level narrative. In this work, we revisit a widely used\nconceptualization of framing from the communication sciences which explicitly\ncaptures elements of narratives, including conflict and its resolution, and\nintegrate it with the narrative framing of key entities in the story as heroes,\nvictims or villains. We adapt an effective annotation paradigm that breaks a\ncomplex annotation task into a series of simpler binary questions, and present\nan annotated data set of English news articles, and a case study on the framing\nof climate change in articles from news outlets across the political spectrum.\nFinally, we explore automatic multi-label prediction of our frames with\nsupervised and semi-supervised approaches, and present a novel retrieval-based\nmethod which is both effective and transparent in its predictions. We conclude\nwith a discussion of opportunities and challenges for future work on\ndocument-level models of narrative framing.\n","authors":["Lea Frermann","Jiatong Li","Shima Khanehzar","Gosia Mikolajczak"],"pdf_url":"https://arxiv.org/pdf/2306.02052v2.pdf","comment":"Published in ACL 2023"},{"id":"http://arxiv.org/abs/2310.12072v2","updated":"2024-01-03T00:32:43Z","published":"2023-10-18T16:07:01Z","title":"SPEED: Speculative Pipelined Execution for Efficient Decoding","summary":"  Generative Large Language Models (LLMs) based on the Transformer architecture\nhave recently emerged as a dominant foundation model for a wide range of\nNatural Language Processing tasks. Nevertheless, their application in real-time\nscenarios has been highly restricted due to the significant inference latency\nassociated with these models. This is particularly pronounced due to the\nautoregressive nature of generative LLM inference, where tokens are generated\nsequentially since each token depends on all previous output tokens. It is\ntherefore challenging to achieve any token-level parallelism, making inference\nextremely memory-bound. In this work, we propose SPEED, which improves\ninference efficiency by speculatively executing multiple future tokens in\nparallel with the current token using predicted values based on early-layer\nhidden states. For Transformer decoders that employ parameter sharing, the\nmemory operations for the tokens executing in parallel can be amortized, which\nallows us to accelerate generative LLM inference. We demonstrate the efficiency\nof our method in terms of latency reduction relative to model accuracy and\ndemonstrate how speculation allows for training deeper decoders with parameter\nsharing with minimal runtime overhead.\n","authors":["Coleman Hooper","Sehoon Kim","Hiva Mohammadzadeh","Hasan Genc","Kurt Keutzer","Amir Gholami","Sophia Shao"],"pdf_url":"https://arxiv.org/pdf/2310.12072v2.pdf","comment":"NeurIPS Workshop on Efficient Natural Language and Speech Processing\n  (2023)"},{"id":"http://arxiv.org/abs/2401.01472v1","updated":"2024-01-03T00:13:52Z","published":"2024-01-03T00:13:52Z","title":"A First Look at Information Highlighting in Stack Overflow Answers","summary":"  Context: Navigating the knowledge of Stack Overflow (SO) remains challenging.\nTo make the posts vivid to users, SO allows users to write and edit posts with\nMarkdown or HTML so that users can leverage various formatting styles (e.g.,\nbold, italic, and code) to highlight the important information. Nonetheless,\nthere have been limited studies on the highlighted information. Objective: We\ncarried out the first large-scale exploratory study on the information\nhighlighted in SO answers in our recent study. To extend our previous study, we\ndevelop approaches to automatically recommend highlighted content with\nformatting styles using neural network architectures initially designed for the\nNamed Entity Recognition task. Method: In this paper, we studied 31,169,429\nanswers of Stack Overflow. For training recommendation models, we choose CNN\nand BERT models for each type of formatting (i.e., Bold, Italic, Code, and\nHeading) using the information highlighting dataset we collected from SO\nanswers. Results: Our models based on CNN architecture achieve precision\nranging from 0.71 to 0.82. The trained model for automatic code content\nhighlighting achieves a recall of 0.73 and an F1 score of 0.71, outperforming\nthe trained models for other formatting styles. The BERT models have even lower\nrecalls and F1 scores than the CNN models. Our analysis of failure cases\nindicates that the majority of the failure cases are missing identification\n(i.e., the model misses the content that is supposed to be highlighted) due to\nthe models tend to learn the frequently highlighted words while struggling to\nlearn less frequent words. Conclusion: Our findings suggest that it is possible\nto develop recommendation models for highlighting information for answers with\ndifferent formatting styles on Stack Overflow.\n","authors":["Shahla Shaan Ahmed","Shaowei Wang","Yuan Tian"," Tse-Hsun"," Chen","Haoxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.01472v1.pdf","comment":"This work is submitted to Information and Software Technology Journal"},{"id":"http://arxiv.org/abs/2401.01469v1","updated":"2024-01-03T00:09:34Z","published":"2024-01-03T00:09:34Z","title":"Question-Answering Based Summarization of Electronic Health Records\n  using Retrieval Augmented Generation","summary":"  Summarization of electronic health records (EHRs) can substantially minimize\n'screen time' for both patients as well as medical personnel. In recent years\nsummarization of EHRs have employed machine learning pipelines using state of\nthe art neural models. However, these models have produced less than adequate\nresults that are attributed to the difficulty of obtaining sufficient annotated\ndata for training. Moreover, the requirement to consider the entire content of\nan EHR in summarization has resulted in poor performance due to the fact that\nattention mechanisms in modern large language models (LLMs) adds a quadratic\ncomplexity in terms of the size of the input. We propose here a method that\nmitigates these shortcomings by combining semantic search, retrieval augmented\ngeneration (RAG) and question-answering using the latest LLMs. In our approach\nsummarization is the extraction of answers to specific questions that are\ndeemed important by subject-matter experts (SMEs). Our approach is quite\nefficient; requires minimal to no training; does not suffer from the\n'hallucination' problem of LLMs; and it ensures diversity, since the summary\nwill not have repeated content but diverse answers to specific questions.\n","authors":["Walid Saba","Suzanne Wendelken","James. Shanahan"],"pdf_url":"https://arxiv.org/pdf/2401.01469v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01989v1","updated":"2024-01-03T21:38:40Z","published":"2024-01-03T21:38:40Z","title":"Revisiting Zero-Shot Abstractive Summarization in the Era of Large\n  Language Models from the Perspective of Position Bias","summary":"  We characterize and study zero-shot abstractive summarization in Large\nLanguage Models (LLMs) by measuring position bias, which we propose as a\ngeneral formulation of the more restrictive lead bias phenomenon studied\npreviously in the literature. Position bias captures the tendency of a model\nunfairly prioritizing information from certain parts of the input text over\nothers, leading to undesirable behavior. Through numerous experiments on four\ndiverse real-world datasets, we study position bias in multiple LLM models such\nas GPT 3.5-Turbo, Llama-2, and Dolly-v2, as well as state-of-the-art pretrained\nencoder-decoder abstractive summarization models such as Pegasus and BART. Our\nfindings lead to novel insights and discussion on performance and position bias\nof models for zero-shot summarization tasks.\n","authors":["Anshuman Chhabra","Hadi Askari","Prasant Mohapatra"],"pdf_url":"https://arxiv.org/pdf/2401.01989v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01967v1","updated":"2024-01-03T20:26:15Z","published":"2024-01-03T20:26:15Z","title":"A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO\n  and Toxicity","summary":"  While alignment algorithms are now commonly used to tune pre-trained language\nmodels towards a user's preferences, we lack explanations for the underlying\nmechanisms in which models become ``aligned'', thus making it difficult to\nexplain phenomena like jailbreaks. In this work we study a popular algorithm,\ndirect preference optimization (DPO), and the mechanisms by which it reduces\ntoxicity. Namely, we first study how toxicity is represented and elicited in a\npre-trained language model, GPT2-medium. We then apply DPO with a carefully\ncrafted pairwise dataset to reduce toxicity. We examine how the resulting model\naverts toxic outputs, and find that capabilities learned from pre-training are\nnot removed, but rather bypassed. We use this insight to demonstrate a simple\nmethod to un-align the model, reverting it back to its toxic behavior.\n","authors":["Andrew Lee","Xiaoyan Bai","Itamar Pres","Martin Wattenberg","Jonathan K. Kummerfeld","Rada Mihalcea"],"pdf_url":"https://arxiv.org/pdf/2401.01967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.02468v4","updated":"2024-01-03T19:33:57Z","published":"2023-03-04T17:59:43Z","title":"Lon-ea at SemEval-2023 Task 11: A Comparison of Activation Functions for\n  Soft and Hard Label Prediction","summary":"  We study the influence of different activation functions in the output layer\nof deep neural network models for soft and hard label prediction in the\nlearning with disagreement task. In this task, the goal is to quantify the\namount of disagreement via predicting soft labels. To predict the soft labels,\nwe use BERT-based preprocessors and encoders and vary the activation function\nused in the output layer, while keeping other parameters constant. The soft\nlabels are then used for the hard label prediction. The activation functions\nconsidered are sigmoid as well as a step-function that is added to the model\npost-training and a sinusoidal activation function, which is introduced for the\nfirst time in this paper.\n","authors":["Peyman Hosseini","Mehran Hosseini","Sana Sabah Al-Azzawi","Marcus Liwicki","Ignacio Castro","Matthew Purver"],"pdf_url":"https://arxiv.org/pdf/2303.02468v4.pdf","comment":"Accepted in ACL 2023 SemEval Workshop as selected task paper"},{"id":"http://arxiv.org/abs/2401.01952v1","updated":"2024-01-03T19:31:58Z","published":"2024-01-03T19:31:58Z","title":"Instruct-Imagen: Image Generation with Multi-modal Instruction","summary":"  This paper presents instruct-imagen, a model that tackles heterogeneous image\ngeneration tasks and generalizes across unseen tasks. We introduce *multi-modal\ninstruction* for image generation, a task representation articulating a range\nof generation intents with precision. It uses natural language to amalgamate\ndisparate modalities (e.g., text, edge, style, subject, etc.), such that\nabundant generation intents can be standardized in a uniform format.\n  We then build instruct-imagen by fine-tuning a pre-trained text-to-image\ndiffusion model with a two-stage framework. First, we adapt the model using the\nretrieval-augmented training, to enhance model's capabilities to ground its\ngeneration on external multimodal context. Subsequently, we fine-tune the\nadapted model on diverse image generation tasks that requires vision-language\nunderstanding (e.g., subject-driven generation, etc.), each paired with a\nmulti-modal instruction encapsulating the task's essence. Human evaluation on\nvarious image generation datasets reveals that instruct-imagen matches or\nsurpasses prior task-specific models in-domain and demonstrates promising\ngeneralization to unseen and more complex tasks.\n","authors":["Hexiang Hu","Kelvin C. K. Chan","Yu-Chuan Su","Wenhu Chen","Yandong Li","Kihyuk Sohn","Yang Zhao","Xue Ben","Boqing Gong","William Cohen","Ming-Wei Chang","Xuhui Jia"],"pdf_url":"https://arxiv.org/pdf/2401.01952v1.pdf","comment":"20 pages, 18 figures"},{"id":"http://arxiv.org/abs/2401.01943v1","updated":"2024-01-03T19:03:32Z","published":"2024-01-03T19:03:32Z","title":"Generalist embedding models are better at short-context clinical\n  semantic search than specialized embedding models","summary":"  The increasing use of tools and solutions based on Large Language Models\n(LLMs) for various tasks in the medical domain has become a prominent trend.\nTheir use in this highly critical and sensitive domain has thus raised\nimportant questions about their robustness, especially in response to\nvariations in input, and the reliability of the generated outputs. This study\naddresses these questions by constructing a textual dataset based on the\nICD-10-CM code descriptions, widely used in US hospitals and containing many\nclinical terms, and their easily reproducible rephrasing. We then benchmarked\nexisting embedding models, either generalist or specialized in the clinical\ndomain, in a semantic search task where the goal was to correctly match the\nrephrased text to the original description. Our results showed that generalist\nmodels performed better than clinical models, suggesting that existing clinical\nspecialized models are more sensitive to small changes in input that confuse\nthem. The highlighted problem of specialized models may be due to the fact that\nthey have not been trained on sufficient data, and in particular on datasets\nthat are not diverse enough to have a reliable global language understanding,\nwhich is still necessary for accurate handling of medical documents.\n","authors":["Jean-Baptiste Excoffier","Tom Roehr","Alexei Figueroa","Michalis Papaaioannou","Keno Bressem","Matthieu Ortala"],"pdf_url":"https://arxiv.org/pdf/2401.01943v1.pdf","comment":"11 pages, 1 figure, 5 tables"},{"id":"http://arxiv.org/abs/2401.01916v1","updated":"2024-01-03T04:47:02Z","published":"2024-01-03T04:47:02Z","title":"AstroLLaMA-Chat: Scaling AstroLLaMA with Conversational and Diverse\n  Datasets","summary":"  We explore the potential of enhancing LLM performance in astronomy-focused\nquestion-answering through targeted, continual pre-training. By employing a\ncompact 7B-parameter LLaMA-2 model and focusing exclusively on a curated set of\nastronomy corpus -- comprising abstracts, introductions, and conclusions -- we\nachieve notable improvements in specialized topic comprehension. While general\nLLMs like GPT-4 outperform in broader question-answering scenarios due to\nsuperior reasoning capabilities, our findings suggest that continual\npre-training with limited resources can still enhance model performance on\nspecialized topics. Additionally, we present an extension of AstroLLaMA: the\nfine-tuning of the 7B LLaMA model on a domain-specific conversational dataset,\nculminating in the release of the chat-enabled AstroLLaMA for community use.\nComprehensive quantitative benchmarking is currently in progress and will be\ndetailed in an upcoming full paper. The model, AstroLLaMA-Chat, is now\navailable at https://huggingface.co/universeTBD, providing the first\nopen-source conversational AI tool tailored for the astronomy community.\n","authors":["Ernest Perkowski","Rui Pan","Tuan Dung Nguyen","Yuan-Sen Ting","Sandor Kruk","Tong Zhang","Charlie O'Neill","Maja Jablonska","Michael J. Smith","Kevin Schawinski","Kartheik Iyer","Ioana Ciucă for UniverseTBD"],"pdf_url":"https://arxiv.org/pdf/2401.01916v1.pdf","comment":"4 pages, 1 figure, model is available at\n  https://huggingface.co/universeTBD, submitted to RNAAS"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2301.02310v3","updated":"2024-01-03T18:59:57Z","published":"2023-01-05T21:48:33Z","title":"PressureVision++: Estimating Fingertip Pressure from Diverse RGB Images","summary":"  Touch plays a fundamental role in manipulation for humans; however, machine\nperception of contact and pressure typically requires invasive sensors. Recent\nresearch has shown that deep models can estimate hand pressure based on a\nsingle RGB image. However, evaluations have been limited to controlled settings\nsince collecting diverse data with ground-truth pressure measurements is\ndifficult. We present a novel approach that enables diverse data to be captured\nwith only an RGB camera and a cooperative participant. Our key insight is that\npeople can be prompted to apply pressure in a certain way, and this prompt can\nserve as a weak label to supervise models to perform well under varied\nconditions. We collect a novel dataset with 51 participants making fingertip\ncontact with diverse objects. Our network, PressureVision++, outperforms human\nannotators and prior work. We also demonstrate an application of\nPressureVision++ to mixed reality where pressure estimation allows everyday\nsurfaces to be used as arbitrary touch-sensitive interfaces. Code, data, and\nmodels are available online.\n","authors":["Patrick Grady","Jeremy A. Collins","Chengcheng Tang","Christopher D. Twigg","Kunal Aneja","James Hays","Charles C. Kemp"],"pdf_url":"https://arxiv.org/pdf/2301.02310v3.pdf","comment":"WACV 2024"},{"id":"http://arxiv.org/abs/2401.01887v1","updated":"2024-01-03T18:57:27Z","published":"2024-01-03T18:57:27Z","title":"LEAP-VO: Long-term Effective Any Point Tracking for Visual Odometry","summary":"  Visual odometry estimates the motion of a moving camera based on visual\ninput. Existing methods, mostly focusing on two-view point tracking, often\nignore the rich temporal context in the image sequence, thereby overlooking the\nglobal motion patterns and providing no assessment of the full trajectory\nreliability. These shortcomings hinder performance in scenarios with occlusion,\ndynamic objects, and low-texture areas. To address these challenges, we present\nthe Long-term Effective Any Point Tracking (LEAP) module. LEAP innovatively\ncombines visual, inter-track, and temporal cues with mindfully selected anchors\nfor dynamic track estimation. Moreover, LEAP's temporal probabilistic\nformulation integrates distribution updates into a learnable iterative\nrefinement module to reason about point-wise uncertainty. Based on these\ntraits, we develop LEAP-VO, a robust visual odometry system adept at handling\nocclusions and dynamic scenes. Our mindful integration showcases a novel\npractice by employing long-term point tracking as the front-end. Extensive\nexperiments demonstrate that the proposed pipeline significantly outperforms\nexisting baselines across various visual odometry benchmarks.\n","authors":["Weirong Chen","Le Chen","Rui Wang","Marc Pollefeys"],"pdf_url":"https://arxiv.org/pdf/2401.01887v1.pdf","comment":"15 pages, 11 figures"},{"id":"http://arxiv.org/abs/2401.01885v1","updated":"2024-01-03T18:55:16Z","published":"2024-01-03T18:55:16Z","title":"From Audio to Photoreal Embodiment: Synthesizing Humans in Conversations","summary":"  We present a framework for generating full-bodied photorealistic avatars that\ngesture according to the conversational dynamics of a dyadic interaction. Given\nspeech audio, we output multiple possibilities of gestural motion for an\nindividual, including face, body, and hands. The key behind our method is in\ncombining the benefits of sample diversity from vector quantization with the\nhigh-frequency details obtained through diffusion to generate more dynamic,\nexpressive motion. We visualize the generated motion using highly\nphotorealistic avatars that can express crucial nuances in gestures (e.g.\nsneers and smirks). To facilitate this line of research, we introduce a\nfirst-of-its-kind multi-view conversational dataset that allows for\nphotorealistic reconstruction. Experiments show our model generates appropriate\nand diverse gestures, outperforming both diffusion- and VQ-only methods.\nFurthermore, our perceptual evaluation highlights the importance of\nphotorealism (vs. meshes) in accurately assessing subtle motion details in\nconversational gestures. Code and dataset available online.\n","authors":["Evonne Ng","Javier Romero","Timur Bagautdinov","Shaojie Bai","Trevor Darrell","Angjoo Kanazawa","Alexander Richard"],"pdf_url":"https://arxiv.org/pdf/2401.01885v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.17033v3","updated":"2024-01-03T18:41:04Z","published":"2023-05-26T15:40:11Z","title":"The Brain Tumor Segmentation (BraTS) Challenge 2023: Focus on Pediatrics\n  (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs)","summary":"  Pediatric tumors of the central nervous system are the most common cause of\ncancer-related death in children. The five-year survival rate for high-grade\ngliomas in children is less than 20\\%. Due to their rarity, the diagnosis of\nthese entities is often delayed, their treatment is mainly based on historic\ntreatment concepts, and clinical trials require multi-institutional\ncollaborations. The MICCAI Brain Tumor Segmentation (BraTS) Challenge is a\nlandmark community benchmark event with a successful history of 12 years of\nresource creation for the segmentation and analysis of adult glioma. Here we\npresent the CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023 challenge, which\nrepresents the first BraTS challenge focused on pediatric brain tumors with\ndata acquired across multiple international consortia dedicated to pediatric\nneuro-oncology and clinical trials. The BraTS-PEDs 2023 challenge focuses on\nbenchmarking the development of volumentric segmentation algorithms for\npediatric brain glioma through standardized quantitative performance evaluation\nmetrics utilized across the BraTS 2023 cluster of challenges. Models gaining\nknowledge from the BraTS-PEDs multi-parametric structural MRI (mpMRI) training\ndata will be evaluated on separate validation and unseen test mpMRI dataof\nhigh-grade pediatric glioma. The CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023\nchallenge brings together clinicians and AI/imaging scientists to lead to\nfaster development of automated segmentation techniques that could benefit\nclinical trials, and ultimately the care of children with brain tumors.\n","authors":["Anahita Fathi Kazerooni","Nastaran Khalili","Xinyang Liu","Debanjan Haldar","Zhifan Jiang","Syed Muhammed Anwar","Jake Albrecht","Maruf Adewole","Udunna Anazodo","Hannah Anderson","Sina Bagheri","Ujjwal Baid","Timothy Bergquist","Austin J. Borja","Evan Calabrese","Verena Chung","Gian-Marco Conte","Farouk Dako","James Eddy","Ivan Ezhov","Ariana Familiar","Keyvan Farahani","Shuvanjan Haldar","Juan Eugenio Iglesias","Anastasia Janas","Elaine Johansen","Blaise V Jones","Florian Kofler","Dominic LaBella","Hollie Anne Lai","Koen Van Leemput","Hongwei Bran Li","Nazanin Maleki","Aaron S McAllister","Zeke Meier","Bjoern Menze","Ahmed W Moawad","Khanak K Nandolia","Julija Pavaine","Marie Piraud","Tina Poussaint","Sanjay P Prabhu","Zachary Reitman","Andres Rodriguez","Jeffrey D Rudie","Ibraheem Salman Shaikh","Lubdha M. Shah","Nakul Sheth","Russel Taki Shinohara","Wenxin Tu","Karthik Viswanathan","Chunhao Wang","Jeffrey B Ware","Benedikt Wiestler","Walter Wiggins","Anna Zapaishchykova","Mariam Aboian","Miriam Bornhorst","Peter de Blank","Michelle Deutsch","Maryam Fouladi","Lindsey Hoffman","Benjamin Kann","Margot Lazow","Leonie Mikael","Ali Nabavizadeh","Roger Packer","Adam Resnick","Brian Rood","Arastoo Vossough","Spyridon Bakas","Marius George Linguraru"],"pdf_url":"https://arxiv.org/pdf/2305.17033v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01868v1","updated":"2024-01-03T18:23:30Z","published":"2024-01-03T18:23:30Z","title":"Step length measurement in the wild using FMCW radar","summary":"  With an aging population, numerous assistive and monitoring technologies are\nunder development to enable older adults to age in place. To facilitate aging\nin place predicting risk factors such as falls, and hospitalization and\nproviding early interventions are important. Much of the work on ambient\nmonitoring for risk prediction has centered on gait speed analysis, utilizing\nprivacy-preserving sensors like radar. Despite compelling evidence that\nmonitoring step length, in addition to gait speed, is crucial for predicting\nrisk, radar-based methods have not explored step length measurement in the\nhome. Furthermore, laboratory experiments on step length measurement using\nradars are limited to proof of concept studies with few healthy subjects. To\naddress this gap, a radar-based step length measurement system for the home is\nproposed based on detection and tracking using radar point cloud, followed by\nDoppler speed profiling of the torso to obtain step lengths in the home. The\nproposed method was evaluated in a clinical environment, involving 35 frail\nolder adults, to establish its validity. Additionally, the method was assessed\nin people's homes, with 21 frail older adults who had participated in the\nclinical assessment. The proposed radar-based step length measurement method\nwas compared to the gold standard Zeno Walkway Gait Analysis System, revealing\na 4.5cm/8.3% error in a clinical setting. Furthermore, it exhibited excellent\nreliability (ICC(2,k)=0.91, 95% CI 0.82 to 0.96) in uncontrolled home settings.\nThe method also proved accurate in uncontrolled home settings, as indicated by\na strong agreement (ICC(3,k)=0.81 (95% CI 0.53 to 0.92)) between home\nmeasurements and in-clinic assessments.\n","authors":["Parthipan Siva","Alexander Wong","Patricia Hewston","George Ioannidis","Dr. Jonathan Adachi","Dr. Alexander Rabinovich","Andrea Lee","Alexandra Papaioannou"],"pdf_url":"https://arxiv.org/pdf/2401.01868v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01862v1","updated":"2024-01-03T18:09:33Z","published":"2024-01-03T18:09:33Z","title":"A Vision Check-up for Language Models","summary":"  What does learning to model relationships between strings teach large\nlanguage models (LLMs) about the visual world? We systematically evaluate LLMs'\nabilities to generate and recognize an assortment of visual concepts of\nincreasing complexity and then demonstrate how a preliminary visual\nrepresentation learning system can be trained using models of text. As language\nmodels lack the ability to consume or output visual information as pixels, we\nuse code to represent images in our study. Although LLM-generated images do not\nlook like natural images, results on image generation and the ability of models\nto correct these generated images indicate that precise modeling of strings can\nteach language models about numerous aspects of the visual world. Furthermore,\nexperiments on self-supervised visual representation learning, utilizing images\ngenerated with text models, highlight the potential to train vision models\ncapable of making semantic assessments of natural images using just LLMs.\n","authors":["Pratyusha Sharma","Tamar Rott Shaham","Manel Baradad","Stephanie Fu","Adrian Rodriguez-Munoz","Shivam Duggal","Phillip Isola","Antonio Torralba"],"pdf_url":"https://arxiv.org/pdf/2401.01862v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01858v1","updated":"2024-01-03T18:06:28Z","published":"2024-01-03T18:06:28Z","title":"Synthetic dataset of ID and Travel Document","summary":"  This paper presents a new synthetic dataset of ID and travel documents,\ncalled SIDTD. The SIDTD dataset is created to help training and evaluating\nforged ID documents detection systems. Such a dataset has become a necessity as\nID documents contain personal information and a public dataset of real\ndocuments can not be released. Moreover, forged documents are scarce, compared\nto legit ones, and the way they are generated varies from one fraudster to\nanother resulting in a class of high intra-variability. In this paper we\ntrained state-of-the-art models on this dataset and we compare them to the\nperformance achieved in larger, but private, datasets. The creation of this\ndataset will help to document image analysis community to progress in the task\nof ID document verification.\n","authors":["Carlos Boned","Maxime Talarmain","Nabil Ghanmi","Guillaume Chiron","Sanket Biswas","Ahmad Montaser Awal","Oriol Ramos Terrades"],"pdf_url":"https://arxiv.org/pdf/2401.01858v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01839v1","updated":"2024-01-03T17:11:27Z","published":"2024-01-03T17:11:27Z","title":"Frequency Domain Modality-invariant Feature Learning for\n  Visible-infrared Person Re-Identification","summary":"  Visible-infrared person re-identification (VI-ReID) is challenging due to the\nsignificant cross-modality discrepancies between visible and infrared images.\nWhile existing methods have focused on designing complex network architectures\nor using metric learning constraints to learn modality-invariant features, they\noften overlook which specific component of the image causes the modality\ndiscrepancy problem. In this paper, we first reveal that the difference in the\namplitude component of visible and infrared images is the primary factor that\ncauses the modality discrepancy and further propose a novel Frequency Domain\nmodality-invariant feature learning framework (FDMNet) to reduce modality\ndiscrepancy from the frequency domain perspective. Our framework introduces two\nnovel modules, namely the Instance-Adaptive Amplitude Filter (IAF) module and\nthe Phrase-Preserving Normalization (PPNorm) module, to enhance the\nmodality-invariant amplitude component and suppress the modality-specific\ncomponent at both the image- and feature-levels. Extensive experimental results\non two standard benchmarks, SYSU-MM01 and RegDB, demonstrate the superior\nperformance of our FDMNet against state-of-the-art methods.\n","authors":["Yulin Li","Tianzhu Zhang","Yongdong Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.01839v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01827v1","updated":"2024-01-03T16:43:47Z","published":"2024-01-03T16:43:47Z","title":"Moonshot: Towards Controllable Video Generation and Editing with\n  Multimodal Conditions","summary":"  Most existing video diffusion models (VDMs) are limited to mere text\nconditions. Thereby, they are usually lacking in control over visual appearance\nand geometry structure of the generated videos. This work presents Moonshot, a\nnew video generation model that conditions simultaneously on multimodal inputs\nof image and text. The model builts upon a core module, called multimodal video\nblock (MVB), which consists of conventional spatialtemporal layers for\nrepresenting video features, and a decoupled cross-attention layer to address\nimage and text inputs for appearance conditioning. In addition, we carefully\ndesign the model architecture such that it can optionally integrate with\npre-trained image ControlNet modules for geometry visual conditions, without\nneeding of extra training overhead as opposed to prior methods. Experiments\nshow that with versatile multimodal conditioning mechanisms, Moonshot\ndemonstrates significant improvement on visual quality and temporal consistency\ncompared to existing models. In addition, the model can be easily repurposed\nfor a variety of generative applications, such as personalized video\ngeneration, image animation and video editing, unveiling its potential to serve\nas a fundamental architecture for controllable video generation. Models will be\nmade public on https://github.com/salesforce/LAVIS.\n","authors":["David Junhao Zhang","Dongxu Li","Hung Le","Mike Zheng Shou","Caiming Xiong","Doyen Sahoo"],"pdf_url":"https://arxiv.org/pdf/2401.01827v1.pdf","comment":"project page: https://showlab.github.io/Moonshot/"},{"id":"http://arxiv.org/abs/2312.15927v2","updated":"2024-01-03T16:43:33Z","published":"2023-12-26T07:45:32Z","title":"M3D: Dataset Condensation by Minimizing Maximum Mean Discrepancy","summary":"  Training state-of-the-art (SOTA) deep models often requires extensive data,\nresulting in substantial training and storage costs. To address these\nchallenges, dataset condensation has been developed to learn a small synthetic\nset that preserves essential information from the original large-scale dataset.\nNowadays, optimization-oriented methods have been the primary method in the\nfield of dataset condensation for achieving SOTA results. However, the bi-level\noptimization process hinders the practical application of such methods to\nrealistic and larger datasets. To enhance condensation efficiency, previous\nworks proposed Distribution-Matching (DM) as an alternative, which\nsignificantly reduces the condensation cost. Nonetheless, current DM-based\nmethods have yielded less comparable results to optimization-oriented methods\ndue to their focus on aligning only the first moment of the distributions. In\nthis paper, we present a novel DM-based method named M3D for dataset\ncondensation by Minimizing the Maximum Mean Discrepancy between feature\nrepresentations of the synthetic and real images. By embedding their\ndistributions in a reproducing kernel Hilbert space, we align all orders of\nmoments of the distributions of real and synthetic images, resulting in a more\ngeneralized condensed set. Notably, our method even surpasses the SOTA\noptimization-oriented method IDC on the high-resolution ImageNet dataset.\nExtensive analysis is conducted to verify the effectiveness of the proposed\nmethod.\n","authors":["Hansong Zhang","Shikun Li","Pengju Wang","Dan Zeng","Shiming Ge"],"pdf_url":"https://arxiv.org/pdf/2312.15927v2.pdf","comment":"This work has been accepted in AAAI-24"},{"id":"http://arxiv.org/abs/2401.01822v1","updated":"2024-01-03T16:38:56Z","published":"2024-01-03T16:38:56Z","title":"HawkRover: An Autonomous mmWave Vehicular Communication Testbed with\n  Multi-sensor Fusion and Deep Learning","summary":"  Connected and automated vehicles (CAVs) have become a transformative\ntechnology that can change our daily life. Currently, millimeter-wave (mmWave)\nbands are identified as the promising CAV connectivity solution. While it can\nprovide high data rate, their realization faces many challenges such as high\nattenuation during mmWave signal propagation and mobility management. Existing\nsolution has to initiate pilot signal to measure channel information, then\napply signal processing to calculate the best narrow beam towards the receiver\nend to guarantee sufficient signal power. This process takes significant\noverhead and time, hence not suitable for vehicles. In this study, we propose\nan autonomous and low-cost testbed to collect extensive co-located mmWave\nsignal and other sensors data such as LiDAR (Light Detection and Ranging),\ncameras, ultrasonic, etc, traditionally for ``automated'', to facilitate mmWave\nvehicular communications. Intuitively, these sensors can build a 3D map around\nthe vehicle and signal propagation path can be estimated, eliminating iterative\nthe process via pilot signals. This multimodal data fusion, together with AI,\nis expected to bring significant advances in ``connected'' research.\n","authors":["Ethan Zhu","Haijian Sun"],"pdf_url":"https://arxiv.org/pdf/2401.01822v1.pdf","comment":"submitted to IEEE conferences for future publications"},{"id":"http://arxiv.org/abs/2401.01823v1","updated":"2024-01-03T16:38:56Z","published":"2024-01-03T16:38:56Z","title":"Detours for Navigating Instructional Videos","summary":"  We introduce the video detours problem for navigating instructional videos.\nGiven a source video and a natural language query asking to alter the how-to\nvideo's current path of execution in a certain way, the goal is to find a\nrelated ''detour video'' that satisfies the requested alteration. To address\nthis challenge, we propose VidDetours, a novel video-language approach that\nlearns to retrieve the targeted temporal segments from a large repository of\nhow-to's using video-and-text conditioned queries. Furthermore, we devise a\nlanguage-based pipeline that exploits how-to video narration text to create\nweakly supervised training data. We demonstrate our idea applied to the domain\nof how-to cooking videos, where a user can detour from their current recipe to\nfind steps with alternate ingredients, tools, and techniques. Validating on a\nground truth annotated dataset of 16K samples, we show our model's significant\nimprovements over best available methods for video retrieval and question\nanswering, with recall rates exceeding the state of the art by 35%.\n","authors":["Kumar Ashutosh","Zihui Xue","Tushar Nagarajan","Kristen Grauman"],"pdf_url":"https://arxiv.org/pdf/2401.01823v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2401.01808v1","updated":"2024-01-03T16:10:07Z","published":"2024-01-03T16:10:07Z","title":"aMUSEd: An Open MUSE Reproduction","summary":"  We present aMUSEd, an open-source, lightweight masked image model (MIM) for\ntext-to-image generation based on MUSE. With 10 percent of MUSE's parameters,\naMUSEd is focused on fast image generation. We believe MIM is under-explored\ncompared to latent diffusion, the prevailing approach for text-to-image\ngeneration. Compared to latent diffusion, MIM requires fewer inference steps\nand is more interpretable. Additionally, MIM can be fine-tuned to learn\nadditional styles with only a single image. We hope to encourage further\nexploration of MIM by demonstrating its effectiveness on large-scale\ntext-to-image generation and releasing reproducible training code. We also\nrelease checkpoints for two models which directly produce images at 256x256 and\n512x512 resolutions.\n","authors":["Suraj Patil","William Berman","Robin Rombach","Patrick von Platen"],"pdf_url":"https://arxiv.org/pdf/2401.01808v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.14650v3","updated":"2024-01-03T15:18:44Z","published":"2022-07-29T12:50:32Z","title":"SYNTA: A novel approach for deep learning-based image analysis in muscle\n  histopathology using photo-realistic synthetic data","summary":"  Artificial intelligence (AI), machine learning, and deep learning (DL)\nmethods are becoming increasingly important in the field of biomedical image\nanalysis. However, to exploit the full potential of such methods, a\nrepresentative number of experimentally acquired images containing a\nsignificant number of manually annotated objects is needed as training data.\nHere we introduce SYNTA (synthetic data) as a novel approach for the generation\nof synthetic, photo-realistic, and highly complex biomedical images as training\ndata for DL systems. We show the versatility of our approach in the context of\nmuscle fiber and connective tissue analysis in histological sections. We\ndemonstrate that it is possible to perform robust and expert-level segmentation\ntasks on previously unseen real-world data, without the need for manual\nannotations using synthetic training data alone. Being a fully parametric\ntechnique, our approach poses an interpretable and controllable alternative to\nGenerative Adversarial Networks (GANs) and has the potential to significantly\naccelerate quantitative image analysis in a variety of biomedical applications\nin microscopy and beyond.\n","authors":["Leonid Mill","Oliver Aust","Jochen A. Ackermann","Philipp Burger","Monica Pascual","Katrin Palumbo-Zerr","Gerhard Krönke","Stefan Uderhardt","Georg Schett","Christoph S. Clemen","Rolf Schröder","Christian Holtzhausen","Samir Jabari","Andreas Maier","Anika Grüneboom"],"pdf_url":"https://arxiv.org/pdf/2207.14650v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01219v2","updated":"2024-01-03T15:00:34Z","published":"2024-01-02T14:18:11Z","title":"Distribution Matching for Multi-Task Learning of Classification Tasks: a\n  Large-Scale Study on Faces & Beyond","summary":"  Multi-Task Learning (MTL) is a framework, where multiple related tasks are\nlearned jointly and benefit from a shared representation space, or parameter\ntransfer. To provide sufficient learning support, modern MTL uses annotated\ndata with full, or sufficiently large overlap across tasks, i.e., each input\nsample is annotated for all, or most of the tasks. However, collecting such\nannotations is prohibitive in many real applications, and cannot benefit from\ndatasets available for individual tasks. In this work, we challenge this setup\nand show that MTL can be successful with classification tasks with little, or\nnon-overlapping annotations, or when there is big discrepancy in the size of\nlabeled data per task. We explore task-relatedness for co-annotation and\nco-training, and propose a novel approach, where knowledge exchange is enabled\nbetween the tasks via distribution matching. To demonstrate the general\napplicability of our method, we conducted diverse case studies in the domains\nof affective computing, face recognition, species recognition, and shopping\nitem classification using nine datasets. Our large-scale study of affective\ntasks for basic expression recognition and facial action unit detection\nillustrates that our approach is network agnostic and brings large performance\nimprovements compared to the state-of-the-art in both tasks and across all\nstudied databases. In all case studies, we show that co-training via\ntask-relatedness is advantageous and prevents negative transfer (which occurs\nwhen MT model's performance is worse than that of at least one single-task\nmodel).\n","authors":["Dimitrios Kollias","Viktoriia Sharmanska","Stefanos Zafeiriou"],"pdf_url":"https://arxiv.org/pdf/2401.01219v2.pdf","comment":"accepted at AAAI 2024. arXiv admin note: text overlap with\n  arXiv:2105.03790"},{"id":"http://arxiv.org/abs/2312.16476v2","updated":"2024-01-03T14:40:49Z","published":"2023-12-27T08:50:01Z","title":"SVGDreamer: Text Guided SVG Generation with Diffusion Model","summary":"  Recently, text-guided scalable vector graphics (SVGs) synthesis has shown\npromise in domains such as iconography and sketch. However, existing\ntext-to-SVG generation methods lack editability and struggle with visual\nquality and result diversity. To address these limitations, we propose a novel\ntext-guided vector graphics synthesis method called SVGDreamer. SVGDreamer\nincorporates a semantic-driven image vectorization (SIVE) process that enables\nthe decomposition of synthesis into foreground objects and background, thereby\nenhancing editability. Specifically, the SIVE process introduce attention-based\nprimitive control and an attention-mask loss function for effective control and\nmanipulation of individual elements. Additionally, we propose a Vectorized\nParticle-based Score Distillation (VPSD) approach to tackle the challenges of\ncolor over-saturation, vector primitives over-smoothing, and limited result\ndiversity in existing text-to-SVG generation methods. Furthermore, on the basis\nof VPSD, we introduce Reward Feedback Learning (ReFL) to accelerate VPSD\nconvergence and improve aesthetic appeal. Extensive experiments have been\nconducted to validate the effectiveness of SVGDreamer, demonstrating its\nsuperiority over baseline methods in terms of editability, visual quality, and\ndiversity. The code and demo of SVGDreamer can be found at\n\\href{https://ximinng.github.io/SVGDreamer-project/}{https://ximinng.github.io/SVGDreamer-project/}.\n","authors":["Ximing Xing","Haitao Zhou","Chuang Wang","Jing Zhang","Dong Xu","Qian Yu"],"pdf_url":"https://arxiv.org/pdf/2312.16476v2.pdf","comment":"19 pages, 15 figures, project link:\n  https://ximinng.github.io/SVGDreamer-project/"},{"id":"http://arxiv.org/abs/2307.14823v2","updated":"2024-01-03T14:38:02Z","published":"2023-07-27T13:00:21Z","title":"Fading memory as inductive bias in residual recurrent networks","summary":"  Residual connections have been proposed as an architecture-based inductive\nbias to mitigate the problem of exploding and vanishing gradients and increased\ntask performance in both feed-forward and recurrent networks (RNNs) when\ntrained with the backpropagation algorithm. Yet, little is known about how\nresidual connections in RNNs influence their dynamics and fading memory\nproperties. Here, we introduce weakly coupled residual recurrent networks\n(WCRNNs) in which residual connections result in well-defined Lyapunov\nexponents and allow for studying properties of fading memory. We investigate\nhow the residual connections of WCRNNs influence their performance, network\ndynamics, and memory properties on a set of benchmark tasks. We show that\nseveral distinct forms of residual connections yield effective inductive biases\nthat result in increased network expressivity. In particular, those are\nresidual connections that (i) result in network dynamics at the proximity of\nthe edge of chaos, (ii) allow networks to capitalize on characteristic spectral\nproperties of the data, and (iii) result in heterogeneous memory properties. In\naddition, we demonstrate how our results can be extended to non-linear\nresiduals and introduce a weakly coupled residual initialization scheme that\ncan be used for Elman RNNs.\n","authors":["Igor Dubinin","Felix Effenberger"],"pdf_url":"https://arxiv.org/pdf/2307.14823v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.07665v2","updated":"2024-01-03T14:36:11Z","published":"2023-08-15T09:27:57Z","title":"Inversion-by-Inversion: Exemplar-based Sketch-to-Photo Synthesis via\n  Stochastic Differential Equations without Training","summary":"  Exemplar-based sketch-to-photo synthesis allows users to generate\nphoto-realistic images based on sketches. Recently, diffusion-based methods\nhave achieved impressive performance on image generation tasks, enabling\nhighly-flexible control through text-driven generation or energy functions.\nHowever, generating photo-realistic images with color and texture from sketch\nimages remains challenging for diffusion models. Sketches typically consist of\nonly a few strokes, with most regions left blank, making it difficult for\ndiffusion-based methods to produce photo-realistic images. In this work, we\npropose a two-stage method named ``Inversion-by-Inversion\" for exemplar-based\nsketch-to-photo synthesis. This approach includes shape-enhancing inversion and\nfull-control inversion. During the shape-enhancing inversion process, an\nuncolored photo is generated with the guidance of a shape-energy function. This\nstep is essential to ensure control over the shape of the generated photo. In\nthe full-control inversion process, we propose an appearance-energy function to\ncontrol the color and texture of the final generated photo.Importantly, our\nInversion-by-Inversion pipeline is training-free and can accept different types\nof exemplars for color and texture control. We conducted extensive experiments\nto evaluate our proposed method, and the results demonstrate its effectiveness.\nThe code and project can be found at\nhttps://ximinng.github.io/inversion-by-inversion-project/.\n","authors":["Ximing Xing","Chuang Wang","Haitao Zhou","Zhihao Hu","Chongxuan Li","Dong Xu","Qian Yu"],"pdf_url":"https://arxiv.org/pdf/2308.07665v2.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2203.01577v4","updated":"2024-01-03T14:31:13Z","published":"2022-03-03T09:02:52Z","title":"HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object\n  Interaction","summary":"  We present HOI4D, a large-scale 4D egocentric dataset with rich annotations,\nto catalyze the research of category-level human-object interaction. HOI4D\nconsists of 2.4M RGB-D egocentric video frames over 4000 sequences collected by\n4 participants interacting with 800 different object instances from 16\ncategories over 610 different indoor rooms. Frame-wise annotations for panoptic\nsegmentation, motion segmentation, 3D hand pose, category-level object pose and\nhand action have also been provided, together with reconstructed object meshes\nand scene point clouds. With HOI4D, we establish three benchmarking tasks to\npromote category-level HOI from 4D visual signals including semantic\nsegmentation of 4D dynamic point cloud sequences, category-level object pose\ntracking, and egocentric action segmentation with diverse interaction targets.\nIn-depth analysis shows HOI4D poses great challenges to existing methods and\nproduces great research opportunities.\n","authors":["Yunze Liu","Yun Liu","Che Jiang","Kangbo Lyu","Weikang Wan","Hao Shen","Boqiang Liang","Zhoujie Fu","He Wang","Li Yi"],"pdf_url":"https://arxiv.org/pdf/2203.01577v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01759v1","updated":"2024-01-03T14:24:02Z","published":"2024-01-03T14:24:02Z","title":"VGA: Vision and Graph Fused Attention Network for Rumor Detection","summary":"  With the development of social media, rumors have been spread broadly on\nsocial media platforms, causing great harm to society. Beside textual\ninformation, many rumors also use manipulated images or conceal textual\ninformation within images to deceive people and avoid being detected, making\nmultimodal rumor detection be a critical problem. The majority of multimodal\nrumor detection methods mainly concentrate on extracting features of source\nclaims and their corresponding images, while ignoring the comments of rumors\nand their propagation structures. These comments and structures imply the\nwisdom of crowds and are proved to be crucial to debunk rumors. Moreover, these\nmethods usually only extract visual features in a basic manner, seldom consider\ntampering or textual information in images. Therefore, in this study, we\npropose a novel Vision and Graph Fused Attention Network (VGA) for rumor\ndetection to utilize propagation structures among posts so as to obtain the\ncrowd opinions and further explore visual tampering features, as well as the\ntextual information hidden in images. We conduct extensive experiments on three\ndatasets, demonstrating that VGA can effectively detect multimodal rumors and\noutperform state-of-the-art methods significantly.\n","authors":["Lin Bai","Caiyan Jia","Ziying Song","Chaoqun Cui"],"pdf_url":"https://arxiv.org/pdf/2401.01759v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01752v1","updated":"2024-01-03T14:08:39Z","published":"2024-01-03T14:08:39Z","title":"FullLoRA-AT: Efficiently Boosting the Robustness of Pretrained Vision\n  Transformers","summary":"  In recent years, the Vision Transformer (ViT) model has gradually become\nmainstream in various computer vision tasks, and the robustness of the model\nhas received increasing attention. However, existing large models tend to\nprioritize performance during training, potentially neglecting the robustness,\nwhich may lead to serious security concerns. In this paper, we establish a new\nchallenge: exploring how to use a small number of additional parameters for\nadversarial finetuning to quickly and effectively enhance the adversarial\nrobustness of a standardly trained model. To address this challenge, we develop\nthe novel LNLoRA module, incorporating a learnable layer normalization before\nthe conventional LoRA module, which helps mitigate magnitude differences in\nparameters between the adversarial and standard training paradigms.\n  Furthermore, we propose the FullLoRA-AT framework by integrating the\nlearnable LNLoRA modules into all key components of ViT-based models while\nkeeping the pretrained model frozen, which can significantly improve the model\nrobustness via adversarial finetuning in a parameter-efficient manner.\n  Extensive experiments on CIFAR-10, CIFAR-100, and Imagenette demonstrate the\nsuperiority of our proposed FullLoRA-AT framework. It achieves comparable\nrobustness with full finetuning while only requiring about 5% of the learnable\nparameters. This also effectively addresses concerns regarding extra model\nstorage space and enormous training time caused by adversarial finetuning.\n","authors":["Zheng Yuan","Jie Zhang","Shiguang Shan"],"pdf_url":"https://arxiv.org/pdf/2401.01752v1.pdf","comment":"10 pages, 2 figures, 6 tables"},{"id":"http://arxiv.org/abs/2312.16250v2","updated":"2024-01-03T13:59:14Z","published":"2023-12-25T17:20:57Z","title":"A Comprehensive Study of Object Tracking in Low-Light Environments","summary":"  Accurate object tracking in low-light environments is crucial, particularly\nin surveillance and ethology applications. However, achieving this is\nsignificantly challenging due to the poor quality of captured sequences.\nFactors such as noise, color imbalance, and low contrast contribute to these\nchallenges. This paper presents a comprehensive study examining the impact of\nthese distortions on automatic object trackers. Additionally, we propose a\nsolution to enhance tracking performance by integrating denoising and low-light\nenhancement methods into the transformer-based object tracking system.\nExperimental results show that the proposed tracker, trained with low-light\nsynthetic datasets, outperforms both the vanilla MixFormer and Siam R-CNN.\n","authors":["Anqi Yi","Nantheera Anantrasirichai"],"pdf_url":"https://arxiv.org/pdf/2312.16250v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01750v1","updated":"2024-01-03T13:58:35Z","published":"2024-01-03T13:58:35Z","title":"Towards Robust Semantic Segmentation against Patch-based Attack via\n  Attention Refinement","summary":"  The attention mechanism has been proven effective on various visual tasks in\nrecent years. In the semantic segmentation task, the attention mechanism is\napplied in various methods, including the case of both Convolution Neural\nNetworks (CNN) and Vision Transformer (ViT) as backbones. However, we observe\nthat the attention mechanism is vulnerable to patch-based adversarial attacks.\nThrough the analysis of the effective receptive field, we attribute it to the\nfact that the wide receptive field brought by global attention may lead to the\nspread of the adversarial patch. To address this issue, in this paper, we\npropose a Robust Attention Mechanism (RAM) to improve the robustness of the\nsemantic segmentation model, which can notably relieve the vulnerability\nagainst patch-based attacks. Compared to the vallina attention mechanism, RAM\nintroduces two novel modules called Max Attention Suppression and Random\nAttention Dropout, both of which aim to refine the attention matrix and limit\nthe influence of a single adversarial patch on the semantic segmentation\nresults of other positions. Extensive experiments demonstrate the effectiveness\nof our RAM to improve the robustness of semantic segmentation models against\nvarious patch-based attack methods under different attack settings.\n","authors":["Zheng Yuan","Jie Zhang","Yude Wang","Shiguang Shan","Xilin Chen"],"pdf_url":"https://arxiv.org/pdf/2401.01750v1.pdf","comment":"30 pages, 3 figures, 12 tables"},{"id":"http://arxiv.org/abs/2401.01749v1","updated":"2024-01-03T13:57:09Z","published":"2024-01-03T13:57:09Z","title":"Few-shot Image Generation via Information Transfer from the Built\n  Geodesic Surface","summary":"  Images generated by most of generative models trained with limited data often\nexhibit deficiencies in either fidelity, diversity, or both. One effective\nsolution to address the limitation is few-shot generative model adaption.\nHowever, the type of approaches typically rely on a large-scale pre-trained\nmodel, serving as a source domain, to facilitate information transfer to the\ntarget domain. In this paper, we propose a method called Information Transfer\nfrom the Built Geodesic Surface (ITBGS), which contains two module: Feature\nAugmentation on Geodesic Surface (FAGS); Interpolation and Regularization\n(I\\&R). With the FAGS module, a pseudo-source domain is created by projecting\nimage features from the training dataset into the Pre-Shape Space, subsequently\ngenerating new features on the Geodesic surface. Thus, no pre-trained models is\nneeded for the adaption process during the training of generative models with\nFAGS. I\\&R module are introduced for supervising the interpolated images and\nregularizing their relative distances, respectively, to further enhance the\nquality of generated images. Through qualitative and quantitative experiments,\nwe demonstrate that the proposed method consistently achieves optimal or\ncomparable results across a diverse range of semantically distinct datasets,\neven in extremely few-shot scenarios.\n","authors":["Yuexing Han","Liheng Ruan","Bing Wang"],"pdf_url":"https://arxiv.org/pdf/2401.01749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01736v1","updated":"2024-01-03T13:19:14Z","published":"2024-01-03T13:19:14Z","title":"Few-shot Adaptation of Multi-modal Foundation Models: A Survey","summary":"  Multi-modal (vision-language) models, such as CLIP, are replacing traditional\nsupervised pre-training models (e.g., ImageNet-based pre-training) as the new\ngeneration of visual foundation models. These models with robust and aligned\nsemantic representations learned from billions of internet image-text pairs and\ncan be applied to various downstream tasks in a zero-shot manner. However, in\nsome fine-grained domains like medical imaging and remote sensing, the\nperformance of multi-modal foundation models often leaves much to be desired.\nConsequently, many researchers have begun to explore few-shot adaptation\nmethods for these models, gradually deriving three main technical approaches:\n1) prompt-based methods, 2) adapter-based methods, and 3) external\nknowledge-based methods. Nevertheless, this rapidly developing field has\nproduced numerous results without a comprehensive survey to systematically\norganize the research progress. Therefore, in this survey, we introduce and\nanalyze the research advancements in few-shot adaptation methods for\nmulti-modal models, summarizing commonly used datasets and experimental setups,\nand comparing the results of different methods. In addition, due to the lack of\nreliable theoretical support for existing methods, we derive the few-shot\nadaptation generalization error bound for multi-modal models. The theorem\nreveals that the generalization error of multi-modal foundation models is\nconstrained by three factors: domain gap, model capacity, and sample size.\nBased on this, we propose three possible solutions from the following aspects:\n1) adaptive domain generalization, 2) adaptive model selection, and 3) adaptive\nknowledge utilization.\n","authors":["Fan Liu","Tianshu Zhang","Wenwen Dai","Wenwen Cai Xiaocong Zhou","Delong Chen"],"pdf_url":"https://arxiv.org/pdf/2401.01736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01734v1","updated":"2024-01-03T13:16:38Z","published":"2024-01-03T13:16:38Z","title":"Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data","summary":"  Assistive robots should be able to wash, fold or iron clothes. However, due\nto the variety, deformability and self-occlusions of clothes, creating\ngeneral-purpose robot systems for cloth manipulation is challenging. Synthetic\ndata is a promising direction to improve generalization, though its usability\nis often limited by the sim-to-real gap. To advance the use of synthetic data\nfor cloth manipulation and to enable tasks such as robotic folding, we present\na synthetic data pipeline to train keypoint detectors for almost flattened\ncloth items. To test its performance, we have also collected a real-world\ndataset. We train detectors for both T-shirts, towels and shorts and obtain an\naverage precision of 64.3%. Fine-tuning on real-world data improves performance\nto 74.2%. Additional insight is provided by discussing various failure modes of\nthe keypoint detectors and by comparing different approaches to obtain cloth\nmeshes and materials. We also quantify the remaining sim-to-real gap and argue\nthat further improvements to the fidelity of cloth assets will be required to\nfurther reduce this gap. The code, dataset and trained models are available\nonline.\n","authors":["Thomas Lips","Victor-Louis De Gusseme","Francis wyffels"],"pdf_url":"https://arxiv.org/pdf/2401.01734v1.pdf","comment":"submitted to journal on 20/12"},{"id":"http://arxiv.org/abs/2401.01730v1","updated":"2024-01-03T13:07:14Z","published":"2024-01-03T13:07:14Z","title":"STAF: 3D Human Mesh Recovery from Video with Spatio-Temporal Alignment\n  Fusion","summary":"  The recovery of 3D human mesh from monocular images has significantly been\ndeveloped in recent years. However, existing models usually ignore spatial and\ntemporal information, which might lead to mesh and image misalignment and\ntemporal discontinuity. For this reason, we propose a novel Spatio-Temporal\nAlignment Fusion (STAF) model. As a video-based model, it leverages coherence\nclues from human motion by an attention-based Temporal Coherence Fusion Module\n(TCFM). As for spatial mesh-alignment evidence, we extract fine-grained local\ninformation through predicted mesh projection on the feature maps. Based on the\nspatial features, we further introduce a multi-stage adjacent Spatial Alignment\nFusion Module (SAFM) to enhance the feature representation of the target frame.\nIn addition to the above, we propose an Average Pooling Module (APM) to allow\nthe model to focus on the entire input sequence rather than just the target\nframe. This method can remarkably improve the smoothness of recovery results\nfrom video. Extensive experiments on 3DPW, MPII3D, and H36M demonstrate the\nsuperiority of STAF. We achieve a state-of-the-art trade-off between precision\nand smoothness. Our code and more video results are on the project page\nhttps://yw0208.github.io/staf/\n","authors":["Wei Yao","Hongwen Zhang","Yunlian Sun","Jinhui Tang"],"pdf_url":"https://arxiv.org/pdf/2401.01730v1.pdf","comment":"Project Page: https://yw0208.github.io/staf/"},{"id":"http://arxiv.org/abs/2401.01724v1","updated":"2024-01-03T13:03:44Z","published":"2024-01-03T13:03:44Z","title":"Lightweight Adaptive Feature De-drifting for Compressed Image\n  Classification","summary":"  JPEG is a widely used compression scheme to efficiently reduce the volume of\ntransmitted images. The artifacts appear among blocks due to the information\nloss, which not only affects the quality of images but also harms the\nsubsequent high-level tasks in terms of feature drifting. High-level vision\nmodels trained on high-quality images will suffer performance degradation when\ndealing with compressed images, especially on mobile devices. Numerous\nlearning-based JPEG artifact removal methods have been proposed to handle\nvisual artifacts. However, it is not an ideal choice to use these JPEG artifact\nremoval methods as a pre-processing for compressed image classification for the\nfollowing reasons: 1. These methods are designed for human vision rather than\nhigh-level vision models; 2. These methods are not efficient enough to serve as\npre-processing on resource-constrained devices. To address these issues, this\npaper proposes a novel lightweight AFD module to boost the performance of\npre-trained image classification models when facing compressed images. First, a\nFDE-Net is devised to generate the spatial-wise FDM in the DCT domain. Next,\nthe estimated FDM is transmitted to the FE-Net to generate the mapping\nrelationship between degraded features and corresponding high-quality features.\nA simple but effective RepConv block equipped with structural\nre-parameterization is utilized in FE-Net, which enriches feature\nrepresentation in the training phase while maintaining efficiency in the\ndeployment phase. After training on limited compressed images, the AFD-Module\ncan serve as a \"plug-and-play\" model for pre-trained classification models to\nimprove their performance on compressed images. Experiments demonstrate that\nour proposed AFD module can comprehensively improve the accuracy of the\npre-trained classification models and significantly outperform the existing\nmethods.\n","authors":["Long Peng","Yang Cao","Yuejin Sun","Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2401.01724v1.pdf","comment":"Accepted by IEEE Transactions on Multimedia 2024"},{"id":"http://arxiv.org/abs/2401.01720v1","updated":"2024-01-03T12:54:31Z","published":"2024-01-03T12:54:31Z","title":"Local Adaptive Clustering Based Image Matching for Automatic Visual\n  Identification","summary":"  Monitoring cameras are extensively utilized in industrial production to\nmonitor equipment running. With advancements in computer vision, device\nrecognition using image features is viable. This paper presents a\nvision-assisted identification system that implements real-time automatic\nequipment labeling through image matching in surveillance videos. The system\ndeploys the ORB algorithm to extract image features and the GMS algorithm to\nremove incorrect matching points. According to the principles of clustering and\ntemplate locality, a method known as Local Adaptive Clustering (LAC) has been\nestablished to enhance label positioning. This method segments matching\ntemplates using the cluster center, which improves the efficiency and stability\nof labels. The experimental results demonstrate that LAC effectively curtails\nthe label drift.\n","authors":["Zhizhen Wang"],"pdf_url":"https://arxiv.org/pdf/2401.01720v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01717v1","updated":"2024-01-03T12:47:02Z","published":"2024-01-03T12:47:02Z","title":"Fact-checking based fake news detection: a review","summary":"  This paper reviews and summarizes the research results on fact-based fake\nnews from the perspectives of tasks and problems, algorithm strategies, and\ndatasets. First, the paper systematically explains the task definition and core\nproblems of fact-based fake news detection. Second, the paper summarizes the\nexisting detection methods based on the algorithm principles. Third, the paper\nanalyzes the classic and newly proposed datasets in the field, and summarizes\nthe experimental results on each dataset. Finally, the paper summarizes the\nadvantages and disadvantages of existing methods, proposes several challenges\nthat methods in this field may face, and looks forward to the next stage of\nresearch. It is hoped that this paper will provide reference for subsequent\nwork in the field.\n","authors":["Yuzhou Yang","Yangming Zhou","Qichao Ying","Zhenxing Qian","Dan Zeng","Liang Liu"],"pdf_url":"https://arxiv.org/pdf/2401.01717v1.pdf","comment":"Invited short review paper (in Chinese)"},{"id":"http://arxiv.org/abs/2212.08123v3","updated":"2024-01-03T12:22:46Z","published":"2022-12-15T20:23:09Z","title":"Bayesian posterior approximation with stochastic ensembles","summary":"  We introduce ensembles of stochastic neural networks to approximate the\nBayesian posterior, combining stochastic methods such as dropout with deep\nensembles. The stochastic ensembles are formulated as families of distributions\nand trained to approximate the Bayesian posterior with variational inference.\nWe implement stochastic ensembles based on Monte Carlo dropout, DropConnect and\na novel non-parametric version of dropout and evaluate them on a toy problem\nand CIFAR image classification. For both tasks, we test the quality of the\nposteriors directly against Hamiltonian Monte Carlo simulations. Our results\nshow that stochastic ensembles provide more accurate posterior estimates than\nother popular baselines for Bayesian inference.\n","authors":["Oleksandr Balabanov","Bernhard Mehlig","Hampus Linander"],"pdf_url":"https://arxiv.org/pdf/2212.08123v3.pdf","comment":"19 pages, CVPR 2023"},{"id":"http://arxiv.org/abs/2311.11467v2","updated":"2024-01-03T12:09:50Z","published":"2023-08-02T13:37:08Z","title":"SkateboardAI: The Coolest Video Action Recognition for Skateboarding","summary":"  Impressed by the coolest skateboarding sports program from 2021 Tokyo Olympic\nGames, we are the first to curate the original real-world video datasets\n\"SkateboardAI\" in the wild, even self-design and implement diverse uni-modal\nand multi-modal video action recognition approaches to recognize different\ntricks accurately. For uni-modal methods, we separately apply (1) CNN and LSTM;\n(2) CNN and BiLSTM; (3) CNN and BiLSTM with effective attention mechanisms; (4)\nTransformer-based action recognition pipeline. Transferred to the multi-modal\nconditions, we investigated the two-stream Inflated-3D architecture on\n\"SkateboardAI\" datasets to compare its performance with uni-modal cases. In\nsum, our objective is developing an excellent AI sport referee for the coolest\nskateboarding competitions.\n","authors":["Hanxiao Chen"],"pdf_url":"https://arxiv.org/pdf/2311.11467v2.pdf","comment":"The original first-author work has been accepted and presented by\n  CVPR 2022 WiCV Workshop (This is the long-version paper)"},{"id":"http://arxiv.org/abs/2401.01699v1","updated":"2024-01-03T12:06:02Z","published":"2024-01-03T12:06:02Z","title":"WordArt Designer API: User-Driven Artistic Typography Synthesis with\n  Large Language Models on ModelScope","summary":"  This paper introduces the WordArt Designer API, a novel framework for\nuser-driven artistic typography synthesis utilizing Large Language Models\n(LLMs) on ModelScope. We address the challenge of simplifying artistic\ntypography for non-professionals by offering a dynamic, adaptive, and\ncomputationally efficient alternative to traditional rigid templates. Our\napproach leverages the power of LLMs to understand and interpret user input,\nfacilitating a more intuitive design process. We demonstrate through various\ncase studies how users can articulate their aesthetic preferences and\nfunctional requirements, which the system then translates into unique and\ncreative typographic designs. Our evaluations indicate significant improvements\nin user satisfaction, design flexibility, and creative expression over existing\nsystems. The WordArt Designer API not only democratizes the art of typography\nbut also opens up new possibilities for personalized digital communication and\ndesign.\n","authors":["Jun-Yan He","Zhi-Qi Cheng","Chenyang Li","Jingdong Sun","Wangmeng Xiang","Yusen Hu","Xianhui Lin","Xiaoyang Kang","Zengke Jin","Bin Luo","Yifeng Geng","Xuansong Xie","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2401.01699v1.pdf","comment":"Spotlight Paper from the Workshop on Machine Learning for Creativity\n  and Design at the 37th Conference on Neural Information Processing Systems\n  (NeurIPS 2023). 5 pages, 5 figures"},{"id":"http://arxiv.org/abs/2401.01693v1","updated":"2024-01-03T11:54:48Z","published":"2024-01-03T11:54:48Z","title":"AID-DTI: Accelerating High-fidelity Diffusion Tensor Imaging with\n  Detail-Preserving Model-based Deep Learning","summary":"  Deep learning has shown great potential in accelerating diffusion tensor\nimaging (DTI). Nevertheless, existing methods tend to suffer from Rician noise\nand detail loss in reconstructing the DTI-derived parametric maps especially\nwhen sparsely sampled q-space data are used. This paper proposes a novel\nmethod, AID-DTI (Accelerating hIgh fiDelity Diffusion Tensor Imaging), to\nfacilitate fast and accurate DTI with only six measurements. AID-DTI is\nequipped with a newly designed Singular Value Decomposition (SVD)-based\nregularizer, which can effectively capture fine details while suppressing noise\nduring network training. Experimental results on Human Connectome Project (HCP)\ndata consistently demonstrate that the proposed method estimates DTI parameter\nmaps with fine-grained details and outperforms three state-of-the-art methods\nboth quantitatively and qualitatively.\n","authors":["Wenxin Fan","Jian Cheng","Cheng Li","Xinrui Ma","Jing Yang","Juan Zou","Ruoyou Wu","Qiegen Liu","Shanshan Wang"],"pdf_url":"https://arxiv.org/pdf/2401.01693v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.17190v2","updated":"2024-01-03T11:44:30Z","published":"2023-10-26T07:05:38Z","title":"Lookup Table meets Local Laplacian Filter: Pyramid Reconstruction\n  Network for Tone Mapping","summary":"  Tone mapping aims to convert high dynamic range (HDR) images to low dynamic\nrange (LDR) representations, a critical task in the camera imaging pipeline. In\nrecent years, 3-Dimensional LookUp Table (3D LUT) based methods have gained\nattention due to their ability to strike a favorable balance between\nenhancement performance and computational efficiency. However, these methods\noften fail to deliver satisfactory results in local areas since the look-up\ntable is a global operator for tone mapping, which works based on pixel values\nand fails to incorporate crucial local information. To this end, this paper\naims to address this issue by exploring a novel strategy that integrates global\nand local operators by utilizing closed-form Laplacian pyramid decomposition\nand reconstruction. Specifically, we employ image-adaptive 3D LUTs to\nmanipulate the tone in the low-frequency image by leveraging the specific\ncharacteristics of the frequency information. Furthermore, we utilize local\nLaplacian filters to refine the edge details in the high-frequency components\nin an adaptive manner. Local Laplacian filters are widely used to preserve edge\ndetails in photographs, but their conventional usage involves manual tuning and\nfixed implementation within camera imaging pipelines or photo editing tools. We\npropose to learn parameter value maps progressively for local Laplacian filters\nfrom annotated data using a lightweight network. Our model achieves\nsimultaneous global tone manipulation and local edge detail preservation in an\nend-to-end manner. Extensive experimental results on two benchmark datasets\ndemonstrate that the proposed method performs favorably against\nstate-of-the-art methods.\n","authors":["Feng Zhang","Ming Tian","Zhiqiang Li","Bin Xu","Qingbo Lu","Changxin Gao","Nong Sang"],"pdf_url":"https://arxiv.org/pdf/2310.17190v2.pdf","comment":"12 pages, 6 figures, accepted by NeurlPS 2023"},{"id":"http://arxiv.org/abs/2401.01686v1","updated":"2024-01-03T11:44:09Z","published":"2024-01-03T11:44:09Z","title":"ODTrack: Online Dense Temporal Token Learning for Visual Tracking","summary":"  Online contextual reasoning and association across consecutive video frames\nare critical to perceive instances in visual tracking. However, most current\ntop-performing trackers persistently lean on sparse temporal relationships\nbetween reference and search frames via an offline mode. Consequently, they can\nonly interact independently within each image-pair and establish limited\ntemporal correlations. To alleviate the above problem, we propose a simple,\nflexible and effective video-level tracking pipeline, named \\textbf{ODTrack},\nwhich densely associates the contextual relationships of video frames in an\nonline token propagation manner. ODTrack receives video frames of arbitrary\nlength to capture the spatio-temporal trajectory relationships of an instance,\nand compresses the discrimination features (localization information) of a\ntarget into a token sequence to achieve frame-to-frame association. This new\nsolution brings the following benefits: 1) the purified token sequences can\nserve as prompts for the inference in the next video frame, whereby past\ninformation is leveraged to guide future inference; 2) the complex online\nupdate strategies are effectively avoided by the iterative propagation of token\nsequences, and thus we can achieve more efficient model representation and\ncomputation. ODTrack achieves a new \\textit{SOTA} performance on seven\nbenchmarks, while running at real-time speed. Code and models are available at\n\\url{https://github.com/GXNU-ZhongLab/ODTrack}.\n","authors":["Yaozong Zheng","Bineng Zhong","Qihua Liang","Zhiyi Mo","Shengping Zhang","Xianxian Li"],"pdf_url":"https://arxiv.org/pdf/2401.01686v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01685v1","updated":"2024-01-03T11:41:57Z","published":"2024-01-03T11:41:57Z","title":"Modality Exchange Network for Retinogeniculate Visual Pathway\n  Segmentation","summary":"  Accurate segmentation of the retinogeniculate visual pathway (RGVP) aids in\nthe diagnosis and treatment of visual disorders by identifying disruptions or\nabnormalities within the pathway. However, the complex anatomical structure and\nconnectivity of RGVP make it challenging to achieve accurate segmentation. In\nthis study, we propose a novel Modality Exchange Network (ME-Net) that\neffectively utilizes multi-modal magnetic resonance (MR) imaging information to\nenhance RGVP segmentation. Our ME-Net has two main contributions. Firstly, we\nintroduce an effective multi-modal soft-exchange technique. Specifically, we\ndesign a channel and spatially mixed attention module to exchange modality\ninformation between T1-weighted and fractional anisotropy MR images. Secondly,\nwe propose a cross-fusion module that further enhances the fusion of\ninformation between the two modalities. Experimental results demonstrate that\nour method outperforms existing state-of-the-art approaches in terms of RGVP\nsegmentation performance.\n","authors":["Hua Han","Cheng Li","Lei Xie","Yuanjing Feng","Alou Diakite","Shanshan Wang"],"pdf_url":"https://arxiv.org/pdf/2401.01685v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01676v1","updated":"2024-01-03T11:25:11Z","published":"2024-01-03T11:25:11Z","title":"Performance Evaluation of GPS Trajectory Rasterization Methods","summary":"  The availability of the Global Positioning System (GPS) trajectory data is\nincreasing along with the availability of different GPS receivers and with the\nincreasing use of various mobility services. GPS trajectory is an important\ndata source which is used in traffic density detection, transport mode\ndetection, mapping data inferences with the use of different methods such as\nimage processing and machine learning methods. While the data size increases,\nefficient representation of this type of data is becoming difficult to be used\nin these methods. A common approach is the representation of GPS trajectory\ninformation such as average speed, bearing, etc. in raster image form and\napplying analysis methods. In this study, we evaluate GPS trajectory data\nrasterization using the spatial join functions of QGIS, PostGIS+QGIS, and our\niterative spatial structured grid aggregation implementation coded in the\nPython programming language. Our implementation is also parallelizable, and\nthis parallelization is also included as the fourth method. According to the\nresults of experiment carried out with an example GPS trajectory dataset, QGIS\nmethod and PostGIS+QGIS method showed relatively low performance with respect\nto our method using the metric of total processing time. PostGIS+QGIS method\nachieved the best results for spatial join though its total performance\ndecreased quickly while test area size increases. On the other hand, both of\nour methods' performances decrease directly proportional to GPS point. And our\nmethods' performance can be increased proportional to the increase with the\nnumber of processor cores and/or with multiple computing clusters.\n","authors":["Necip Enes Gengec","Ergin Tari"],"pdf_url":"https://arxiv.org/pdf/2401.01676v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01674v1","updated":"2024-01-03T11:16:38Z","published":"2024-01-03T11:16:38Z","title":"Transformer RGBT Tracking with Spatio-Temporal Multimodal Tokens","summary":"  Many RGBT tracking researches primarily focus on modal fusion design, while\noverlooking the effective handling of target appearance changes. While some\napproaches have introduced historical frames or fuse and replace initial\ntemplates to incorporate temporal information, they have the risk of disrupting\nthe original target appearance and accumulating errors over time. To alleviate\nthese limitations, we propose a novel Transformer RGBT tracking approach, which\nmixes spatio-temporal multimodal tokens from the static multimodal templates\nand multimodal search regions in Transformer to handle target appearance\nchanges, for robust RGBT tracking. We introduce independent dynamic template\ntokens to interact with the search region, embedding temporal information to\naddress appearance changes, while also retaining the involvement of the initial\nstatic template tokens in the joint feature extraction process to ensure the\npreservation of the original reliable target appearance information that\nprevent deviations from the target appearance caused by traditional temporal\nupdates. We also use attention mechanisms to enhance the target features of\nmultimodal template tokens by incorporating supplementary modal cues, and make\nthe multimodal search region tokens interact with multimodal dynamic template\ntokens via attention mechanisms, which facilitates the conveyance of\nmultimodal-enhanced target change information. Our module is inserted into the\ntransformer backbone network and inherits joint feature extraction,\nsearch-template matching, and cross-modal interaction. Extensive experiments on\nthree RGBT benchmark datasets show that the proposed approach maintains\ncompetitive performance compared to other state-of-the-art tracking algorithms\nwhile running at 39.1 FPS.\n","authors":["Dengdi Sun","Yajie Pan","Andong Lu","Chenglong Li","Bin Luo"],"pdf_url":"https://arxiv.org/pdf/2401.01674v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05308v2","updated":"2024-01-03T11:12:33Z","published":"2023-03-09T14:58:01Z","title":"SpyroPose: SE(3) Pyramids for Object Pose Distribution Estimation","summary":"  Object pose estimation is a core computer vision problem and often an\nessential component in robotics. Pose estimation is usually approached by\nseeking the single best estimate of an object's pose, but this approach is\nill-suited for tasks involving visual ambiguity. In such cases it is desirable\nto estimate the uncertainty as a pose distribution to allow downstream tasks to\nmake informed decisions. Pose distributions can have arbitrary complexity which\nmotivates estimating unparameterized distributions, however, until now they\nhave only been used for orientation estimation on SO(3) due to the difficulty\nin training on and normalizing over SE(3). We propose a novel method for pose\ndistribution estimation on SE(3). We use a hierarchical grid, a pyramid, which\nenables efficient importance sampling during training and sparse evaluation of\nthe pyramid at inference, allowing real time 6D pose distribution estimation.\nOur method outperforms state-of-the-art methods on SO(3), and to the best of\nour knowledge, we provide the first quantitative results on pose distribution\nestimation on SE(3). Code will be available at spyropose.github.io\n","authors":["Rasmus Laurvig Haugaard","Frederik Hagelskjær","Thorbjørn Mosekjær Iversen"],"pdf_url":"https://arxiv.org/pdf/2303.05308v2.pdf","comment":"ICCVW 2023 (R6D)"},{"id":"http://arxiv.org/abs/2401.01662v1","updated":"2024-01-03T10:47:20Z","published":"2024-01-03T10:47:20Z","title":"Simultaneous q-Space Sampling Optimization and Reconstruction for Fast\n  and High-fidelity Diffusion Magnetic Resonance Imaging","summary":"  Diffusion Magnetic Resonance Imaging (dMRI) plays a crucial role in the\nnoninvasive investigation of tissue microstructural properties and structural\nconnectivity in the \\textit{in vivo} human brain. However, to effectively\ncapture the intricate characteristics of water diffusion at various directions\nand scales, it is important to employ comprehensive q-space sampling.\nUnfortunately, this requirement leads to long scan times, limiting the clinical\napplicability of dMRI. To address this challenge, we propose SSOR, a\nSimultaneous q-Space sampling Optimization and Reconstruction framework. We\njointly optimize a subset of q-space samples using a continuous representation\nof spherical harmonic functions and a reconstruction network. Additionally, we\nintegrate the unique properties of diffusion magnetic resonance imaging (dMRI)\nin both the q-space and image domains by applying $l1$-norm and total-variation\nregularization. The experiments conducted on HCP data demonstrate that SSOR has\npromising strengths both quantitatively and qualitatively and exhibits\nrobustness to noise.\n","authors":["Jing Yang","Jian Cheng","Cheng Li","Wenxin Fan","Juan Zou","Ruoyou Wu","Shanshan Wang"],"pdf_url":"https://arxiv.org/pdf/2401.01662v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01659v1","updated":"2024-01-03T10:35:35Z","published":"2024-01-03T10:35:35Z","title":"DiffYOLO: Object Detection for Anti-Noise via YOLO and Diffusion Models","summary":"  Object detection models represented by YOLO series have been widely used and\nhave achieved great results on the high quality datasets, but not all the\nworking conditions are ideal. To settle down the problem of locating targets on\nlow quality datasets, the existing methods either train a new object detection\nnetwork, or need a large collection of low-quality datasets to train. However,\nwe propose a framework in this paper and apply it on the YOLO models called\nDiffYOLO. Specifically, we extract feature maps from the denoising diffusion\nprobabilistic models to enhance the well-trained models, which allows us\nfine-tune YOLO on high-quality datasets and test on low-quality datasets. The\nresults proved this framework can not only prove the performance on noisy\ndatasets, but also prove the detection results on high-quality test datasets.\nWe will supplement more experiments later (with various datasets and network\narchitectures).\n","authors":["Yichen Liu","Huajian Zhang","Daqing Gao"],"pdf_url":"https://arxiv.org/pdf/2401.01659v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00110v2","updated":"2024-01-03T10:12:30Z","published":"2023-12-30T01:24:25Z","title":"Diffusion Model with Perceptual Loss","summary":"  Diffusion models trained with mean squared error loss tend to generate\nunrealistic samples. Current state-of-the-art models rely on classifier-free\nguidance to improve sample quality, yet its surprising effectiveness is not\nfully understood. In this paper, We show that the effectiveness of\nclassifier-free guidance partly originates from it being a form of implicit\nperceptual guidance. As a result, we can directly incorporate perceptual loss\nin diffusion training to improve sample quality. Since the score matching\nobjective used in diffusion training strongly resembles the denoising\nautoencoder objective used in unsupervised training of perceptual networks, the\ndiffusion model itself is a perceptual network and can be used to generate\nmeaningful perceptual loss. We propose a novel self-perceptual objective that\nresults in diffusion models capable of generating more realistic samples. For\nconditional generation, our method only improves sample quality without\nentanglement with the conditional input and therefore does not sacrifice sample\ndiversity. Our method can also improve sample quality for unconditional\ngeneration, which was not possible with classifier-free guidance before.\n","authors":["Shanchuan Lin","Xiao Yang"],"pdf_url":"https://arxiv.org/pdf/2401.00110v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01651v1","updated":"2024-01-03T10:08:40Z","published":"2024-01-03T10:08:40Z","title":"AIGCBench: Comprehensive Evaluation of Image-to-Video Content Generated\n  by AI","summary":"  The burgeoning field of Artificial Intelligence Generated Content (AIGC) is\nwitnessing rapid advancements, particularly in video generation. This paper\nintroduces AIGCBench, a pioneering comprehensive and scalable benchmark\ndesigned to evaluate a variety of video generation tasks, with a primary focus\non Image-to-Video (I2V) generation. AIGCBench tackles the limitations of\nexisting benchmarks, which suffer from a lack of diverse datasets, by including\na varied and open-domain image-text dataset that evaluates different\nstate-of-the-art algorithms under equivalent conditions. We employ a novel text\ncombiner and GPT-4 to create rich text prompts, which are then used to generate\nimages via advanced Text-to-Image models. To establish a unified evaluation\nframework for video generation tasks, our benchmark includes 11 metrics\nspanning four dimensions to assess algorithm performance. These dimensions are\ncontrol-video alignment, motion effects, temporal consistency, and video\nquality. These metrics are both reference video-dependent and video-free,\nensuring a comprehensive evaluation strategy. The evaluation standard proposed\ncorrelates well with human judgment, providing insights into the strengths and\nweaknesses of current I2V algorithms. The findings from our extensive\nexperiments aim to stimulate further research and development in the I2V field.\nAIGCBench represents a significant step toward creating standardized benchmarks\nfor the broader AIGC landscape, proposing an adaptable and equitable framework\nfor future assessments of video generation tasks.\n","authors":["Fanda Fan","Chunjie Luo","Jianfeng Zhan","Wanling Gao"],"pdf_url":"https://arxiv.org/pdf/2401.01651v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01650v1","updated":"2024-01-03T10:07:11Z","published":"2024-01-03T10:07:11Z","title":"De-Confusing Pseudo-Labels in Source-Free Domain Adaptation","summary":"  Source-free domain adaptation (SFDA) aims to transfer knowledge learned from\na source domain to an unlabeled target domain, where the source data is\nunavailable during adaptation. Existing approaches for SFDA focus on\nself-training usually including well-established entropy minimization and\npseudo-labeling techniques. Recent work suggested a co-learning strategy to\nimprove the quality of the generated target pseudo-labels using robust\npretrained networks such as Swin-B. However, since the generated pseudo-labels\ndepend on the source model, they may be noisy due to domain shift. In this\npaper, we view SFDA from the perspective of label noise learning and learn to\nde-confuse the pseudo-labels. More specifically, we learn a noise transition\nmatrix of the pseudo-labels to capture the label corruption of each class and\nlearn the underlying true label distribution. Estimating the noise transition\nmatrix enables a better true class-posterior estimation results with better\nprediction accuracy. We demonstrate the effectiveness of our approach applied\nwith several SFDA methods: SHOT, SHOT++, and AaD. We obtain state-of-the-art\nresults on three domain adaptation datasets: VisDA, DomainNet, and OfficeHome.\n","authors":["Idit Diamant","Idan Achituve","Arnon Netzer"],"pdf_url":"https://arxiv.org/pdf/2401.01650v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2212.03795"},{"id":"http://arxiv.org/abs/2401.01647v1","updated":"2024-01-03T09:46:43Z","published":"2024-01-03T09:46:43Z","title":"SIGNeRF: Scene Integrated Generation for Neural Radiance Fields","summary":"  Advances in image diffusion models have recently led to notable improvements\nin the generation of high-quality images. In combination with Neural Radiance\nFields (NeRFs), they enabled new opportunities in 3D generation. However, most\ngenerative 3D approaches are object-centric and applying them to editing\nexisting photorealistic scenes is not trivial. We propose SIGNeRF, a novel\napproach for fast and controllable NeRF scene editing and scene-integrated\nobject generation. A new generative update strategy ensures 3D consistency\nacross the edited images, without requiring iterative optimization. We find\nthat depth-conditioned diffusion models inherently possess the capability to\ngenerate 3D consistent views by requesting a grid of images instead of single\nviews. Based on these insights, we introduce a multi-view reference sheet of\nmodified images. Our method updates an image collection consistently based on\nthe reference sheet and refines the original NeRF with the newly generated\nimage set in one go. By exploiting the depth conditioning mechanism of the\nimage diffusion model, we gain fine control over the spatial location of the\nedit and enforce shape guidance by a selected region or an external mesh.\n","authors":["Jan-Niklas Dihlmann","Andreas Engelhardt","Hendrik Lensch"],"pdf_url":"https://arxiv.org/pdf/2401.01647v1.pdf","comment":"Project Page: https://signerf.jdihlmann.com"},{"id":"http://arxiv.org/abs/2312.13763v2","updated":"2024-01-03T09:40:56Z","published":"2023-12-21T11:41:02Z","title":"Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed\n  Diffusion Models","summary":"  Text-guided diffusion models have revolutionized image and video generation\nand have also been successfully used for optimization-based 3D object\nsynthesis. Here, we instead focus on the underexplored text-to-4D setting and\nsynthesize dynamic, animated 3D objects using score distillation methods with\nan additional temporal dimension. Compared to previous work, we pursue a novel\ncompositional generation-based approach, and combine text-to-image,\ntext-to-video, and 3D-aware multiview diffusion models to provide feedback\nduring 4D object optimization, thereby simultaneously enforcing temporal\nconsistency, high-quality visual appearance and realistic geometry. Our method,\ncalled Align Your Gaussians (AYG), leverages dynamic 3D Gaussian Splatting with\ndeformation fields as 4D representation. Crucial to AYG is a novel method to\nregularize the distribution of the moving 3D Gaussians and thereby stabilize\nthe optimization and induce motion. We also propose a motion amplification\nmechanism as well as a new autoregressive synthesis scheme to generate and\ncombine multiple 4D sequences for longer generation. These techniques allow us\nto synthesize vivid dynamic scenes, outperform previous work qualitatively and\nquantitatively and achieve state-of-the-art text-to-4D performance. Due to the\nGaussian 4D representation, different 4D animations can be seamlessly combined,\nas we demonstrate. AYG opens up promising avenues for animation, simulation and\ndigital content creation as well as synthetic data generation.\n","authors":["Huan Ling","Seung Wook Kim","Antonio Torralba","Sanja Fidler","Karsten Kreis"],"pdf_url":"https://arxiv.org/pdf/2312.13763v2.pdf","comment":"Project page:\n  https://research.nvidia.com/labs/toronto-ai/AlignYourGaussians/"},{"id":"http://arxiv.org/abs/2401.01646v1","updated":"2024-01-03T09:39:36Z","published":"2024-01-03T09:39:36Z","title":"Prototypical Information Bottlenecking and Disentangling for Multimodal\n  Cancer Survival Prediction","summary":"  Multimodal learning significantly benefits cancer survival prediction,\nespecially the integration of pathological images and genomic data. Despite\nadvantages of multimodal learning for cancer survival prediction, massive\nredundancy in multimodal data prevents it from extracting discriminative and\ncompact information: (1) An extensive amount of intra-modal task-unrelated\ninformation blurs discriminability, especially for gigapixel whole slide images\n(WSIs) with many patches in pathology and thousands of pathways in genomic\ndata, leading to an ``intra-modal redundancy\" issue. (2) Duplicated information\namong modalities dominates the representation of multimodal data, which makes\nmodality-specific information prone to being ignored, resulting in an\n``inter-modal redundancy\" issue. To address these, we propose a new framework,\nPrototypical Information Bottlenecking and Disentangling (PIBD), consisting of\nPrototypical Information Bottleneck (PIB) module for intra-modal redundancy and\nPrototypical Information Disentanglement (PID) module for inter-modal\nredundancy. Specifically, a variant of information bottleneck, PIB, is proposed\nto model prototypes approximating a bunch of instances for different risk\nlevels, which can be used for selection of discriminative instances within\nmodality. PID module decouples entangled multimodal data into compact distinct\ncomponents: modality-common and modality-specific knowledge, under the guidance\nof the joint prototypical distribution. Extensive experiments on five cancer\nbenchmark datasets demonstrated our superiority over other methods.\n","authors":["Yilan Zhang","Yingxue Xu","Jianqi Chen","Fengying Xie","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2401.01646v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01643v1","updated":"2024-01-03T09:37:33Z","published":"2024-01-03T09:37:33Z","title":"S3Net: Innovating Stereo Matching and Semantic Segmentation with a\n  Single-Branch Semantic Stereo Network in Satellite Epipolar Imagery","summary":"  Stereo matching and semantic segmentation are significant tasks in binocular\nsatellite 3D reconstruction. However, previous studies primarily view these as\nindependent parallel tasks, lacking an integrated multitask learning framework.\nThis work introduces a solution, the Single-branch Semantic Stereo Network\n(S3Net), which innovatively combines semantic segmentation and stereo matching\nusing Self-Fuse and Mutual-Fuse modules. Unlike preceding methods that utilize\nsemantic or disparity information independently, our method dentifies and\nleverages the intrinsic link between these two tasks, leading to a more\naccurate understanding of semantic information and disparity estimation.\nComparative testing on the US3D dataset proves the effectiveness of our S3Net.\nOur model improves the mIoU in semantic segmentation from 61.38 to 67.39, and\nreduces the D1-Error and average endpoint error (EPE) in disparity estimation\nfrom 10.051 to 9.579 and 1.439 to 1.403 respectively, surpassing existing\ncompetitive methods. Our codes are available at:https://github.com/CVEO/S3Net.\n","authors":["Qingyuan Yang","Guanzhou Chen","Xiaoliang Tan","Tong Wang","Jiaqi Wang","Xiaodong Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.01643v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01642v1","updated":"2024-01-03T09:37:03Z","published":"2024-01-03T09:37:03Z","title":"BLADE: Box-Level Supervised Amodal Segmentation through Directed\n  Expansion","summary":"  Perceiving the complete shape of occluded objects is essential for human and\nmachine intelligence. While the amodal segmentation task is to predict the\ncomplete mask of partially occluded objects, it is time-consuming and\nlabor-intensive to annotate the pixel-level ground truth amodal masks.\nBox-level supervised amodal segmentation addresses this challenge by relying\nsolely on ground truth bounding boxes and instance classes as supervision,\nthereby alleviating the need for exhaustive pixel-level annotations.\nNevertheless, current box-level methodologies encounter limitations in\ngenerating low-resolution masks and imprecise boundaries, failing to meet the\ndemands of practical real-world applications. We present a novel solution to\ntackle this problem by introducing a directed expansion approach from visible\nmasks to corresponding amodal masks. Our approach involves a hybrid end-to-end\nnetwork based on the overlapping region - the area where different instances\nintersect. Diverse segmentation strategies are applied for overlapping regions\nand non-overlapping regions according to distinct characteristics. To guide the\nexpansion of visible masks, we introduce an elaborately-designed connectivity\nloss for overlapping regions, which leverages correlations with visible masks\nand facilitates accurate amodal segmentation. Experiments are conducted on\nseveral challenging datasets and the results show that our proposed method can\noutperform existing state-of-the-art methods with large margins.\n","authors":["Zhaochen Liu","Zhixuan Li","Tingting Jiang"],"pdf_url":"https://arxiv.org/pdf/2401.01642v1.pdf","comment":"Accepted to AAAI 2024;"},{"id":"http://arxiv.org/abs/2312.01711v3","updated":"2024-01-03T09:35:21Z","published":"2023-12-04T07:53:59Z","title":"Regressor-Segmenter Mutual Prompt Learning for Crowd Counting","summary":"  Crowd counting has achieved significant progress by training regressors to\npredict instance positions. In heavily crowded scenarios, however, regressors\nare challenged by uncontrollable annotation variance, which causes density map\nbias and context information inaccuracy. In this study, we propose mutual\nprompt learning (mPrompt), which leverages a regressor and a segmenter as\nguidance for each other, solving bias and inaccuracy caused by annotation\nvariance while distinguishing foreground from background. In specific, mPrompt\nleverages point annotations to tune the segmenter and predict pseudo head masks\nin a way of point prompt learning. It then uses the predicted segmentation\nmasks, which serve as spatial constraint, to rectify biased point annotations\nas context prompt learning. mPrompt defines a way of mutual information\nmaximization from prompt learning, mitigating the impact of annotation variance\nwhile improving model accuracy. Experiments show that mPrompt significantly\nreduces the Mean Average Error (MAE), demonstrating the potential to be general\nframework for down-stream vision tasks.\n","authors":["Mingyue Guo","Li Yuan","Zhaoyi Yan","Binghui Chen","Yaowei Wang","Qixiang Ye"],"pdf_url":"https://arxiv.org/pdf/2312.01711v3.pdf","comment":"mPrompt defines a way of mutual information maximization from prompt\n  learning"},{"id":"http://arxiv.org/abs/2304.01186v2","updated":"2024-01-03T09:10:12Z","published":"2023-04-03T17:55:14Z","title":"Follow Your Pose: Pose-Guided Text-to-Video Generation using Pose-Free\n  Videos","summary":"  Generating text-editable and pose-controllable character videos have an\nimperious demand in creating various digital human. Nevertheless, this task has\nbeen restricted by the absence of a comprehensive dataset featuring paired\nvideo-pose captions and the generative prior models for videos. In this work,\nwe design a novel two-stage training scheme that can utilize easily obtained\ndatasets (i.e.,image pose pair and pose-free video) and the pre-trained\ntext-to-image (T2I) model to obtain the pose-controllable character videos.\nSpecifically, in the first stage, only the keypoint-image pairs are used only\nfor a controllable text-to-image generation. We learn a zero-initialized\nconvolutional encoder to encode the pose information. In the second stage, we\nfinetune the motion of the above network via a pose-free video dataset by\nadding the learnable temporal self-attention and reformed cross-frame\nself-attention blocks. Powered by our new designs, our method successfully\ngenerates continuously pose-controllable character videos while keeps the\nediting and concept composition ability of the pre-trained T2I model. The code\nand models will be made publicly available.\n","authors":["Yue Ma","Yingqing He","Xiaodong Cun","Xintao Wang","Siran Chen","Ying Shan","Xiu Li","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2304.01186v2.pdf","comment":"Project page: https://follow-your-pose.github.io/; Github repository:\n  https://github.com/mayuelala/FollowYourPose"},{"id":"http://arxiv.org/abs/2401.01624v1","updated":"2024-01-03T08:49:29Z","published":"2024-01-03T08:49:29Z","title":"Context-Aware Interaction Network for RGB-T Semantic Segmentation","summary":"  RGB-T semantic segmentation is a key technique for autonomous driving scenes\nunderstanding. For the existing RGB-T semantic segmentation methods, however,\nthe effective exploration of the complementary relationship between different\nmodalities is not implemented in the information interaction between multiple\nlevels. To address such an issue, the Context-Aware Interaction Network\n(CAINet) is proposed for RGB-T semantic segmentation, which constructs\ninteraction space to exploit auxiliary tasks and global context for explicitly\nguided learning. Specifically, we propose a Context-Aware Complementary\nReasoning (CACR) module aimed at establishing the complementary relationship\nbetween multimodal features with the long-term context in both spatial and\nchannel dimensions. Further, considering the importance of global contextual\nand detailed information, we propose the Global Context Modeling (GCM) module\nand Detail Aggregation (DA) module, and we introduce specific auxiliary\nsupervision to explicitly guide the context interaction and refine the\nsegmentation map. Extensive experiments on two benchmark datasets of MFNet and\nPST900 demonstrate that the proposed CAINet achieves state-of-the-art\nperformance. The code is available at https://github.com/YingLv1106/CAINet.\n","authors":["Ying Lv","Zhi Liu","Gongyang Li"],"pdf_url":"https://arxiv.org/pdf/2401.01624v1.pdf","comment":"13 pages, 7 figures, Accepted by IEEE Transactions on Multimedia 2024"},{"id":"http://arxiv.org/abs/2401.01614v1","updated":"2024-01-03T08:33:09Z","published":"2024-01-03T08:33:09Z","title":"GPT-4V(ision) is a Generalist Web Agent, if Grounded","summary":"  The recent development on large multimodal models (LMMs), especially\nGPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries\nof multimodal models beyond traditional tasks like image captioning and visual\nquestion answering. In this work, we explore the potential of LMMs like GPT-4V\nas a generalist web agent that can follow natural language instructions to\ncomplete tasks on any given website. We propose SEEACT, a generalist web agent\nthat harnesses the power of LMMs for integrated visual understanding and acting\non the web. We evaluate on the recent MIND2WEB benchmark. In addition to\nstandard offline evaluation on cached websites, we enable a new online\nevaluation setting by developing a tool that allows running web agents on live\nwebsites. We show that GPT-4V presents a great potential for web agents - it\ncan successfully complete 50% of the tasks on live websites if we manually\nground its textual plans into actions on the websites. This substantially\noutperforms text-only LLMs like GPT-4 or smaller models (FLAN-T5 and BLIP-2)\nspecifically fine-tuned for web agents. However, grounding still remains a\nmajor challenge. Existing LMM grounding strategies like set-of-mark prompting\nturns out not effective for web agents, and the best grounding strategy we\ndevelop in this paper leverages both the HTML text and visuals. Yet, there is\nstill a substantial gap with oracle grounding, leaving ample room for further\nimprovement.\n","authors":["Boyuan Zheng","Boyu Gou","Jihyung Kil","Huan Sun","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2401.01614v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.12867v2","updated":"2024-01-03T08:29:03Z","published":"2023-09-22T13:43:22Z","title":"Accurate and Fast Compressed Video Captioning","summary":"  Existing video captioning approaches typically require to first sample video\nframes from a decoded video and then conduct a subsequent process (e.g.,\nfeature extraction and/or captioning model learning). In this pipeline, manual\nframe sampling may ignore key information in videos and thus degrade\nperformance. Additionally, redundant information in the sampled frames may\nresult in low efficiency in the inference of video captioning. Addressing this,\nwe study video captioning from a different perspective in compressed domain,\nwhich brings multi-fold advantages over the existing pipeline: 1) Compared to\nraw images from the decoded video, the compressed video, consisting of\nI-frames, motion vectors and residuals, is highly distinguishable, which allows\nus to leverage the entire video for learning without manual sampling through a\nspecialized model design; 2) The captioning model is more efficient in\ninference as smaller and less redundant information is processed. We propose a\nsimple yet effective end-to-end transformer in the compressed domain for video\ncaptioning that enables learning from the compressed video for captioning. We\nshow that even with a simple design, our method can achieve state-of-the-art\nperformance on different benchmarks while running almost 2x faster than\nexisting approaches. Code is available at https://github.com/acherstyx/CoCap.\n","authors":["Yaojie Shen","Xin Gu","Kai Xu","Heng Fan","Longyin Wen","Libo Zhang"],"pdf_url":"https://arxiv.org/pdf/2309.12867v2.pdf","comment":"ICCV 2023"},{"id":"http://arxiv.org/abs/2312.06892v2","updated":"2024-01-03T08:21:03Z","published":"2023-12-11T23:30:37Z","title":"VitalLens: Take A Vital Selfie","summary":"  This report introduces VitalLens, an app that estimates vital signs such as\nheart rate and respiration rate from selfie video in real time. VitalLens uses\na computer vision model trained on a diverse dataset of video and physiological\nsensor data. We benchmark performance on several diverse datasets, including\nVV-Medium, which consists of 289 unique participants. VitalLens outperforms\nseveral existing methods including POS and MTTS-CAN on all datasets while\nmaintaining a fast inference speed. On VV-Medium, VitalLens achieves mean\nabsolute errors of 0.71 bpm for heart rate estimation, and 0.76 bpm for\nrespiratory rate estimation.\n","authors":["Philipp V. Rouast"],"pdf_url":"https://arxiv.org/pdf/2312.06892v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2401.01598v1","updated":"2024-01-03T07:59:17Z","published":"2024-01-03T07:59:17Z","title":"Learning Prompt with Distribution-Based Feature Replay for Few-Shot\n  Class-Incremental Learning","summary":"  Few-shot Class-Incremental Learning (FSCIL) aims to continuously learn new\nclasses based on very limited training data without forgetting the old ones\nencountered. Existing studies solely relied on pure visual networks, while in\nthis paper we solved FSCIL by leveraging the Vision-Language model (e.g., CLIP)\nand propose a simple yet effective framework, named Learning Prompt with\nDistribution-based Feature Replay (LP-DiF). We observe that simply using CLIP\nfor zero-shot evaluation can substantially outperform the most influential\nmethods. Then, prompt tuning technique is involved to further improve its\nadaptation ability, allowing the model to continually capture specific\nknowledge from each session. To prevent the learnable prompt from forgetting\nold knowledge in the new session, we propose a pseudo-feature replay approach.\nSpecifically, we preserve the old knowledge of each class by maintaining a\nfeature-level Gaussian distribution with a diagonal covariance matrix, which is\nestimated by the image features of training images and synthesized features\ngenerated from a VAE. When progressing to a new session, pseudo-features are\nsampled from old-class distributions combined with training images of the\ncurrent session to optimize the prompt, thus enabling the model to learn new\nknowledge while retaining old knowledge. Experiments on three prevalent\nbenchmarks, i.e., CIFAR100, mini-ImageNet, CUB-200, and two more challenging\nbenchmarks, i.e., SUN-397 and CUB-200$^*$ proposed in this paper showcase the\nsuperiority of LP-DiF, achieving new state-of-the-art (SOTA) in FSCIL. Code is\npublicly available at https://github.com/1170300714/LP-DiF.\n","authors":["Zitong Huang","Ze Chen","Zhixing Chen","Erjin Zhou","Xinxing Xu","Rick Siow Mong Goh","Yong Liu","Chunmei Feng","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2401.01598v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01591v1","updated":"2024-01-03T07:54:13Z","published":"2024-01-03T07:54:13Z","title":"MLIP: Medical Language-Image Pre-training with Masked Local\n  Representation Learning","summary":"  Existing contrastive language-image pre-training aims to learn a joint\nrepresentation by matching abundant image-text pairs. However, the number of\nimage-text pairs in medical datasets is usually orders of magnitude smaller\nthan that in natural datasets. Besides, medical image-text pairs often involve\nnumerous complex fine-grained correspondences. This paper aims to enhance the\ndata efficiency by introducing multiple-to-multiple local relationship modeling\nto capture denser supervisions. More specifically, we propose a Medical\nLanguage-Image Pre-training (MLIP) framework, which exploits the limited\nimage-text medical data more efficiently through patch-sentence matching.\nFurthermore, we introduce a masked contrastive learning strategy with semantic\nintegrity estimation to reduce redundancy in images while preserving the\nunderlying semantics. Our evaluation results show that MLIP outperforms\nprevious work in zero/few-shot classification and few-shot segmentation tasks\nby a large margin.\n","authors":["Jiarun Liu","Hong-Yu Zhou","Cheng Li","Weijian Huang","Hao Yang","Yong Liang","Shanshan Wang"],"pdf_url":"https://arxiv.org/pdf/2401.01591v1.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2310.05195v2","updated":"2024-01-03T07:40:15Z","published":"2023-10-08T15:04:50Z","title":"GMMFormer: Gaussian-Mixture-Model Based Transformer for Efficient\n  Partially Relevant Video Retrieval","summary":"  Given a text query, partially relevant video retrieval (PRVR) seeks to find\nuntrimmed videos containing pertinent moments in a database. For PRVR, clip\nmodeling is essential to capture the partial relationship between texts and\nvideos. Current PRVR methods adopt scanning-based clip construction to achieve\nexplicit clip modeling, which is information-redundant and requires a large\nstorage overhead. To solve the efficiency problem of PRVR methods, this paper\nproposes GMMFormer, a Gaussian-Mixture-Model based Transformer which models\nclip representations implicitly. During frame interactions, we incorporate\nGaussian-Mixture-Model constraints to focus each frame on its adjacent frames\ninstead of the whole video. Then generated representations will contain\nmulti-scale clip information, achieving implicit clip modeling. In addition,\nPRVR methods ignore semantic differences between text queries relevant to the\nsame video, leading to a sparse embedding space. We propose a query diverse\nloss to distinguish these text queries, making the embedding space more\nintensive and contain more semantic information. Extensive experiments on three\nlarge-scale video datasets (i.e., TVR, ActivityNet Captions, and Charades-STA)\ndemonstrate the superiority and efficiency of GMMFormer. Code is available at\n\\url{https://github.com/huangmozhi9527/GMMFormer}.\n","authors":["Yuting Wang","Jinpeng Wang","Bin Chen","Ziyun Zeng","Shu-Tao Xia"],"pdf_url":"https://arxiv.org/pdf/2310.05195v2.pdf","comment":"Accepted by AAAI 2024. Code is released at\n  https://github.com/huangmozhi9527/GMMFormer"},{"id":"http://arxiv.org/abs/2401.01587v1","updated":"2024-01-03T07:39:58Z","published":"2024-01-03T07:39:58Z","title":"Real-Time Human Fall Detection using a Lightweight Pose Estimation\n  Technique","summary":"  The elderly population is increasing rapidly around the world. There are no\nenough caretakers for them. Use of AI-based in-home medical care systems is\ngaining momentum due to this. Human fall detection is one of the most important\ntasks of medical care system for the aged people. Human fall is a common\nproblem among elderly people. Detection of a fall and providing medical help as\nearly as possible is very important to reduce any further complexity. The\nchances of death and other medical complications can be reduced by detecting\nand providing medical help as early as possible after the fall. There are many\nstate-of-the-art fall detection techniques available these days, but the\nmajority of them need very high computing power. In this paper, we proposed a\nlightweight and fast human fall detection system using pose estimation. We used\n`Movenet' for human joins key-points extraction. Our proposed method can work\nin real-time on any low-computing device with any basic camera. All computation\ncan be processed locally, so there is no problem of privacy of the subject. We\nused two datasets `GMDCSA' and `URFD' for the experiment. We got the\nsensitivity value of 0.9375 and 0.9167 for the dataset `GMDCSA' and `URFD'\nrespectively. The source code and the dataset GMDCSA of our work are available\nonline to access.\n","authors":["Ekram Alam","Abu Sufian","Paramartha Dutta","Marco Leo"],"pdf_url":"https://arxiv.org/pdf/2401.01587v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01583v1","updated":"2024-01-03T07:22:54Z","published":"2024-01-03T07:22:54Z","title":"Enhancing the medical foundation model with multi-scale and\n  cross-modality feature learning","summary":"  The development of multi-modal medical foundation models has attracted\nsignificant attention in the field of medicine and healthcare due to their\npromising prospects in various clinical applications. One area of focus in this\nresearch direction is the extractions of features at different scales. While\nprevious studies have explored feature learning at individual scales,\ninvestigation on integrating the diverse scales and modalities of information\nis lacking, which may hinder the potential for mutual reinforcement among these\nfeatures. This paper aims to bridge this gap by proposing a method that\neffectively exploits multi-scale and cross-modality information to enhance the\nperformance of medical foundation models. The proposed method simultaneously\nexploit features at the local, instance, modality and global aspects,\nfacilitating comprehensive representation learning within the models. We\nevaluate the effectiveness of the proposed method on six open-source datasets\nacross different clinical tasks, demonstrating its ability to enhance the\nperformance of medical foundation models.\n","authors":["Weijian Huang","Cheng Li","Hong-Yu Zhou","Jiarun Liu","Hao Yang","Yong Liang","Shanshan Wang"],"pdf_url":"https://arxiv.org/pdf/2401.01583v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.17240v2","updated":"2024-01-03T07:08:12Z","published":"2023-12-28T18:58:33Z","title":"An Improved Baseline for Reasoning Segmentation with Large Language\n  Model","summary":"  While LISA effectively bridges the gap between segmentation and large\nlanguage models to enable reasoning segmentation, it poses certain limitations:\nunable to distinguish different instances of the target region, and constrained\nby the pre-defined textual response formats. In this work, we introduce LISA++,\nan update to the existing LISA model, focusing on improving core\nfunctionalities while keeping the base architecture intact. The main\nenhancements in LISA++ include: \\textbf{1) Enhanced Segmentation}: The instance\nsegmentation ability has been added, providing a more detailed scene analysis\nalong with the existing multi-region semantic segmentation. \\textbf{2) More\nNatural Conversation}: Improved capability for multi-turn dialogue, with the\nability to incorporate segmentation results directly into text responses, i.e.,\nSegmentation in Dialogue (SiD). These improvements are achieved by curating the\nexisting samples of generic segmentation datasets, aimed specifically at\nenhancing the segmentation and conversational skills without structural change\nand additional data sources. Comparative analysis with the original LISA model\nshows significant advancements in these areas, positioning LISA++ as a notable\nupgrade in visual understanding and interaction. LISA++'s adaptability and\nimproved features highlight the versatility of the mask-as-embedding paradigm\nproposed by LISA, and the potential as a foundational model for diverse\napplications.\n","authors":["Senqiao Yang","Tianyuan Qu","Xin Lai","Zhuotao Tian","Bohao Peng","Shu Liu","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2312.17240v2.pdf","comment":"Tech report. The LaTex compilation crash was fixed"},{"id":"http://arxiv.org/abs/2401.01578v1","updated":"2024-01-03T07:05:49Z","published":"2024-01-03T07:05:49Z","title":"Context-Guided Spatio-Temporal Video Grounding","summary":"  Spatio-temporal video grounding (or STVG) task aims at locating a\nspatio-temporal tube for a specific instance given a text query. Despite\nadvancements, current methods easily suffer the distractors or heavy object\nappearance variations in videos due to insufficient object information from the\ntext, leading to degradation. Addressing this, we propose a novel framework,\ncontext-guided STVG (CG-STVG), which mines discriminative instance context for\nobject in videos and applies it as a supplementary guidance for target\nlocalization. The key of CG-STVG lies in two specially designed modules,\nincluding instance context generation (ICG), which focuses on discovering\nvisual context information (in both appearance and motion) of the instance, and\ninstance context refinement (ICR), which aims to improve the instance context\nfrom ICG by eliminating irrelevant or even harmful information from the\ncontext. During grounding, ICG, together with ICR, are deployed at each\ndecoding stage of a Transformer architecture for instance context learning.\nParticularly, instance context learned from one decoding stage is fed to the\nnext stage, and leveraged as a guidance containing rich and discriminative\nobject feature to enhance the target-awareness in decoding feature, which\nconversely benefits generating better new instance context for improving\nlocalization finally. Compared to existing methods, CG-STVG enjoys object\ninformation in text query and guidance from mined instance visual context for\nmore accurate target localization. In our experiments on three benchmarks,\nincluding HCSTVG-v1/-v2 and VidSTG, CG-STVG sets new state-of-the-arts in\nm_tIoU and m_vIoU on all of them, showing its efficacy. The code will be\nreleased at https://github.com/HengLan/CGSTVG.\n","authors":["Xin Gu","Heng Fan","Yan Huang","Tiejian Luo","Libo Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.01578v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01577v1","updated":"2024-01-03T07:02:35Z","published":"2024-01-03T07:02:35Z","title":"Test-Time Personalization with Meta Prompt for Gaze Estimation","summary":"  Despite the recent remarkable achievement in gaze estimation, efficient and\naccurate personalization of gaze estimation without labels is a practical\nproblem but rarely touched on in the literature. To achieve efficient\npersonalization, we take inspiration from the recent advances in Natural\nLanguage Processing (NLP) by updating a negligible number of parameters,\n\"prompts\", at the test time. Specifically, the prompt is additionally attached\nwithout perturbing original network and can contain less than 1% of a\nResNet-18's parameters. Our experiments show high efficiency of the prompt\ntuning approach. The proposed one can be 10 times faster in terms of adaptation\nspeed than the methods compared. However, it is non-trivial to update the\nprompt for personalized gaze estimation without labels. At the test time, it is\nessential to ensure that the minimizing of particular unsupervised loss leads\nto the goals of minimizing gaze estimation error. To address this difficulty,\nwe propose to meta-learn the prompt to ensure that its updates align with the\ngoal. Our experiments show that the meta-learned prompt can be effectively\nadapted even with a simple symmetry loss. In addition, we experiment on four\ncross-dataset validations to show the remarkable advantages of the proposed\nmethod.\n","authors":["Huan Liu","Julia Qi","Zhenhao Li","Mohammad Hassanpour","Yang Wang","Konstantinos Plataniotis","Yuanhao Yu"],"pdf_url":"https://arxiv.org/pdf/2401.01577v1.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2401.01575v1","updated":"2024-01-03T07:00:32Z","published":"2024-01-03T07:00:32Z","title":"Enhancing Generalization of Invisible Facial Privacy Cloak via Gradient\n  Accumulation","summary":"  The blooming of social media and face recognition (FR) systems has increased\npeople's concern about privacy and security. A new type of adversarial privacy\ncloak (class-universal) can be applied to all the images of regular users, to\nprevent malicious FR systems from acquiring their identity information. In this\nwork, we discover the optimization dilemma in the existing methods -- the local\noptima problem in large-batch optimization and the gradient information\nelimination problem in small-batch optimization. To solve these problems, we\npropose Gradient Accumulation (GA) to aggregate multiple small-batch gradients\ninto a one-step iterative gradient to enhance the gradient stability and reduce\nthe usage of quantization operations. Experiments show that our proposed method\nachieves high performance on the Privacy-Commons dataset against black-box face\nrecognition models.\n","authors":["Xuannan Liu","Yaoyao Zhong","Weihong Deng","Hongzhi Shi","Xingchen Cui","Yunfeng Yin","Dongchao Wen"],"pdf_url":"https://arxiv.org/pdf/2401.01575v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01574v1","updated":"2024-01-03T06:58:52Z","published":"2024-01-03T06:58:52Z","title":"A Transformer-Based Adaptive Semantic Aggregation Method for UAV Visual\n  Geo-Localization","summary":"  This paper addresses the task of Unmanned Aerial Vehicles (UAV) visual\ngeo-localization, which aims to match images of the same geographic target\ntaken by different platforms, i.e., UAVs and satellites. In general, the key to\nachieving accurate UAV-satellite image matching lies in extracting visual\nfeatures that are robust against viewpoint changes, scale variations, and\nrotations. Current works have shown that part matching is crucial for UAV\nvisual geo-localization since part-level representations can capture image\ndetails and help to understand the semantic information of scenes. However, the\nimportance of preserving semantic characteristics in part-level representations\nis not well discussed. In this paper, we introduce a transformer-based adaptive\nsemantic aggregation method that regards parts as the most representative\nsemantics in an image. Correlations of image patches to different parts are\nlearned in terms of the transformer's feature map. Then our method decomposes\npart-level features into an adaptive sum of all patch features. By doing this,\nthe learned parts are encouraged to focus on patches with typical semantics.\nExtensive experiments on the University-1652 dataset have shown the superiority\nof our method over the current works.\n","authors":["Shishen Li","Cuiwei Liu","Huaijun Qiu","Zhaokui Li"],"pdf_url":"https://arxiv.org/pdf/2401.01574v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2401.01573v1","updated":"2024-01-03T06:58:09Z","published":"2024-01-03T06:58:09Z","title":"View Distribution Alignment with Progressive Adversarial Learning for\n  UAV Visual Geo-Localization","summary":"  Unmanned Aerial Vehicle (UAV) visual geo-localization aims to match images of\nthe same geographic target captured from different views, i.e., the UAV view\nand the satellite view. It is very challenging due to the large appearance\ndifferences in UAV-satellite image pairs. Previous works map images captured by\nUAVs and satellites to a shared feature space and employ a classification\nframework to learn location-dependent features while neglecting the overall\ndistribution shift between the UAV view and the satellite view. In this paper,\nwe address these limitations by introducing distribution alignment of the two\nviews to shorten their distance in a common space. Specifically, we propose an\nend-to-end network, called PVDA (Progressive View Distribution Alignment).\nDuring training, feature encoder, location classifier, and view discriminator\nare jointly optimized by a novel progressive adversarial learning strategy.\nCompetition between feature encoder and view discriminator prompts both of them\nto be stronger. It turns out that the adversarial learning is progressively\nemphasized until UAV-view images are indistinguishable from satellite-view\nimages. As a result, the proposed PVDA becomes powerful in learning\nlocation-dependent yet view-invariant features with good scalability towards\nunseen images of new locations. Compared to the state-of-the-art methods, the\nproposed PVDA requires less inference time but has achieved superior\nperformance on the University-1652 dataset.\n","authors":["Cuiwei Liu","Jiahao Liu","Huaijun Qiu","Zhaokui Li","Xiangbin Shi"],"pdf_url":"https://arxiv.org/pdf/2401.01573v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2305.18891v2","updated":"2024-01-03T06:55:36Z","published":"2023-05-30T09:47:29Z","title":"EmotionGesture: Audio-Driven Diverse Emotional Co-Speech 3D Gesture\n  Generation","summary":"  Generating vivid and diverse 3D co-speech gestures is crucial for various\napplications in animating virtual avatars. While most existing methods can\ngenerate gestures from audio directly, they usually overlook that emotion is\none of the key factors of authentic co-speech gesture generation. In this work,\nwe propose EmotionGesture, a novel framework for synthesizing vivid and diverse\nemotional co-speech 3D gestures from audio. Considering emotion is often\nentangled with the rhythmic beat in speech audio, we first develop an\nEmotion-Beat Mining module (EBM) to extract the emotion and audio beat features\nas well as model their correlation via a transcript-based visual-rhythm\nalignment. Then, we propose an initial pose based Spatial-Temporal Prompter\n(STP) to generate future gestures from the given initial poses. STP effectively\nmodels the spatial-temporal correlations between the initial poses and the\nfuture gestures, thus producing the spatial-temporal coherent pose prompt. Once\nwe obtain pose prompts, emotion, and audio beat features, we will generate 3D\nco-speech gestures through a transformer architecture. However, considering the\nposes of existing datasets often contain jittering effects, this would lead to\ngenerating unstable gestures. To address this issue, we propose an effective\nobjective function, dubbed Motion-Smooth Loss. Specifically, we model motion\noffset to compensate for jittering ground-truth by forcing gestures to be\nsmooth. Last, we present an emotion-conditioned VAE to sample emotion features,\nenabling us to generate diverse emotional results. Extensive experiments\ndemonstrate that our framework outperforms the state-of-the-art, achieving\nvivid and diverse emotional co-speech 3D gestures. Our code and dataset will be\nreleased at the project page:\nhttps://xingqunqi-lab.github.io/Emotion-Gesture-Web/\n","authors":["Xingqun Qi","Chen Liu","Lincheng Li","Jie Hou","Haoran Xin","Xin Yu"],"pdf_url":"https://arxiv.org/pdf/2305.18891v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2401.01569v1","updated":"2024-01-03T06:55:06Z","published":"2024-01-03T06:55:06Z","title":"AttentionLut: Attention Fusion-based Canonical Polyadic LUT for\n  Real-time Image Enhancement","summary":"  Recently, many algorithms have employed image-adaptive lookup tables (LUTs)\nto achieve real-time image enhancement. Nonetheless, a prevailing trend among\nexisting methods has been the employment of linear combinations of basic LUTs\nto formulate image-adaptive LUTs, which limits the generalization ability of\nthese methods. To address this limitation, we propose a novel framework named\nAttentionLut for real-time image enhancement, which utilizes the attention\nmechanism to generate image-adaptive LUTs. Our proposed framework consists of\nthree lightweight modules. We begin by employing the global image context\nfeature module to extract image-adaptive features. Subsequently, the attention\nfusion module integrates the image feature with the priori attention feature\nobtained during training to generate image-adaptive canonical polyadic tensors.\nFinally, the canonical polyadic reconstruction module is deployed to\nreconstruct image-adaptive residual 3DLUT, which is subsequently utilized for\nenhancing input images. Experiments on the benchmark MIT-Adobe FiveK dataset\ndemonstrate that the proposed method achieves better enhancement performance\nquantitatively and qualitatively than the state-of-the-art methods.\n","authors":["Kang Fu","Yicong Peng","Zicheng Zhang","Qihang Xu","Xiaohong Liu","Jia Wang","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2401.01569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.14484v3","updated":"2024-01-03T06:34:30Z","published":"2023-04-27T19:52:47Z","title":"OriCon3D: Effective 3D Object Detection using Orientation and Confidence","summary":"  In this paper, we propose an advanced methodology for the detection of 3D\nobjects and precise estimation of their spatial positions from a single image.\nUnlike conventional frameworks that rely solely on center-point and dimension\npredictions, our research leverages a deep convolutional neural network-based\n3D object weighted orientation regression paradigm. These estimates are then\nseamlessly integrated with geometric constraints obtained from a 2D bounding\nbox, resulting in derivation of a comprehensive 3D bounding box. Our novel\nnetwork design encompasses two key outputs. The first output involves the\nestimation of 3D object orientation through the utilization of a\ndiscrete-continuous loss function. Simultaneously, the second output predicts\nobjectivity-based confidence scores with minimal variance. Additionally, we\nalso introduce enhancements to our methodology through the incorporation of\nlightweight residual feature extractors. By combining the derived estimates\nwith the geometric constraints inherent in the 2D bounding box, our approach\nsignificantly improves the accuracy of 3D object pose determination, surpassing\nbaseline methodologies. Our method is rigorously evaluated on the KITTI 3D\nobject detection benchmark, demonstrating superior performance.\n","authors":["Dhyey Manish Rajani","Surya Pratap Singh","Rahul Kashyap Swayampakula"],"pdf_url":"https://arxiv.org/pdf/2304.14484v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01558v1","updated":"2024-01-03T06:18:30Z","published":"2024-01-03T06:18:30Z","title":"One-Step Late Fusion Multi-view Clustering with Compressed Subspace","summary":"  Late fusion multi-view clustering (LFMVC) has become a rapidly growing class\nof methods in the multi-view clustering (MVC) field, owing to its excellent\ncomputational speed and clustering performance. One bottleneck faced by\nexisting late fusion methods is that they are usually aligned to the average\nkernel function, which makes the clustering performance highly dependent on the\nquality of datasets. Another problem is that they require subsequent k-means\nclustering after obtaining the consensus partition matrix to get the final\ndiscrete labels, and the resulting separation of the label learning and cluster\nstructure optimization processes limits the integrity of these models. To\naddress the above issues, we propose an integrated framework named One-Step\nLate Fusion Multi-view Clustering with Compressed Subspace (OS-LFMVC-CS).\nSpecifically, we use the consensus subspace to align the partition matrix while\noptimizing the partition fusion, and utilize the fused partition matrix to\nguide the learning of discrete labels. A six-step iterative optimization\napproach with verified convergence is proposed. Sufficient experiments on\nmultiple datasets validate the effectiveness and efficiency of our proposed\nmethod.\n","authors":["Qiyuan Ou","Pei Zhang","Sihang Zhou","En Zhu"],"pdf_url":"https://arxiv.org/pdf/2401.01558v1.pdf","comment":"Accepted by ICASSP2024"},{"id":"http://arxiv.org/abs/2401.01553v1","updated":"2024-01-03T05:59:48Z","published":"2024-01-03T05:59:48Z","title":"Multi-modal Learning with Missing Modality in Predicting Axillary Lymph\n  Node Metastasis","summary":"  Multi-modal Learning has attracted widespread attention in medical image\nanalysis. Using multi-modal data, whole slide images (WSIs) and clinical\ninformation, can improve the performance of deep learning models in the\ndiagnosis of axillary lymph node metastasis. However, clinical information is\nnot easy to collect in clinical practice due to privacy concerns, limited\nresources, lack of interoperability, etc. Although patient selection can ensure\nthe training set to have multi-modal data for model development, missing\nmodality of clinical information can appear during test. This normally leads to\nperformance degradation, which limits the use of multi-modal models in the\nclinic. To alleviate this problem, we propose a bidirectional distillation\nframework consisting of a multi-modal branch and a single-modal branch. The\nsingle-modal branch acquires the complete multi-modal knowledge from the\nmulti-modal branch, while the multi-modal learns the robust features of WSI\nfrom the single-modal. We conduct experiments on a public dataset of Lymph Node\nMetastasis in Early Breast Cancer to validate the method. Our approach not only\nachieves state-of-the-art performance with an AUC of 0.861 on the test set\nwithout missing data, but also yields an AUC of 0.842 when the rate of missing\nmodality is 80\\%. This shows the effectiveness of the approach in dealing with\nmulti-modal data and missing modality. Such a model has the potential to\nimprove treatment decision-making for early breast cancer patients who have\naxillary lymph node metastatic status.\n","authors":["Shichuan Zhang","Sunyi Zheng","Zhongyi Shui","Honglin Li","Lin Yang"],"pdf_url":"https://arxiv.org/pdf/2401.01553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01552v1","updated":"2024-01-03T05:57:39Z","published":"2024-01-03T05:57:39Z","title":"CRA-PCN: Point Cloud Completion with Intra- and Inter-level\n  Cross-Resolution Transformers","summary":"  Point cloud completion is an indispensable task for recovering complete point\nclouds due to incompleteness caused by occlusion, limited sensor resolution,\netc. The family of coarse-to-fine generation architectures has recently\nexhibited great success in point cloud completion and gradually became\nmainstream. In this work, we unveil one of the key ingredients behind these\nmethods: meticulously devised feature extraction operations with explicit\ncross-resolution aggregation. We present Cross-Resolution Transformer that\nefficiently performs cross-resolution aggregation with local attention\nmechanisms. With the help of our recursive designs, the proposed operation can\ncapture more scales of features than common aggregation operations, which is\nbeneficial for capturing fine geometric characteristics. While prior\nmethodologies have ventured into various manifestations of inter-level\ncross-resolution aggregation, the effectiveness of intra-level one and their\ncombination has not been analyzed. With unified designs, Cross-Resolution\nTransformer can perform intra- or inter-level cross-resolution aggregation by\nswitching inputs. We integrate two forms of Cross-Resolution Transformers into\none up-sampling block for point generation, and following the coarse-to-fine\nmanner, we construct CRA-PCN to incrementally predict complete shapes with\nstacked up-sampling blocks. Extensive experiments demonstrate that our method\noutperforms state-of-the-art methods by a large margin on several widely used\nbenchmarks. Codes are available at https://github.com/EasyRy/CRA-PCN.\n","authors":["Yi Rong","Haoran Zhou","Lixin Yuan","Cheng Mei","Jiahao Wang","Tong Lu"],"pdf_url":"https://arxiv.org/pdf/2401.01552v1.pdf","comment":"Accepted to AAAI 2024"},{"id":"http://arxiv.org/abs/2401.01548v1","updated":"2024-01-03T05:51:25Z","published":"2024-01-03T05:51:25Z","title":"Boosting of Implicit Neural Representation-based Image Denoiser","summary":"  Implicit Neural Representation (INR) has emerged as an effective method for\nunsupervised image denoising. However, INR models are typically\noverparameterized; consequently, these models are prone to overfitting during\nlearning, resulting in suboptimal results, even noisy ones. To tackle this\nproblem, we propose a general recipe for regularizing INR models in image\ndenoising. In detail, we propose to iteratively substitute the supervision\nsignal with the mean value derived from both the prediction and supervision\nsignal during the learning process. We theoretically prove that such a simple\niterative substitute can gradually enhance the signal-to-noise ratio of the\nsupervision signal, thereby benefiting INR models during the learning process.\nOur experimental results demonstrate that INR models can be effectively\nregularized by the proposed approach, relieving overfitting and boosting image\ndenoising performance.\n","authors":["Zipei Yan","Zhengji Liu","Jizhou Li"],"pdf_url":"https://arxiv.org/pdf/2401.01548v1.pdf","comment":"Accepted by ICASSP 2024, code: https://github.com/TIDS-Lab/ITS"},{"id":"http://arxiv.org/abs/2401.01545v1","updated":"2024-01-03T05:42:17Z","published":"2024-01-03T05:42:17Z","title":"DDN-SLAM: Real-time Dense Dynamic Neural Implicit SLAM with Joint\n  Semantic Encoding","summary":"  We propose DDN-SLAM, a real-time dense neural implicit semantic SLAM system\ndesigned for dynamic scenes. While existing neural implicit SLAM systems\nperform well in static scenes, they often encounter challenges in real-world\nenvironments with dynamic interferences, leading to ineffective tracking and\nmapping. DDN-SLAM utilizes the priors provided by the deep semantic system,\ncombined with conditional probability fields, for segmentation.By constructing\ndepth-guided static masks and employing joint multi-resolution hashing\nencoding, we ensure fast hole filling and high-quality mapping while mitigating\nthe effects of dynamic information interference. To enhance tracking\nrobustness, we utilize sparse feature points validated with optical flow and\nkeyframes, enabling loop closure detection and global bundle optimization.\nFurthermore, DDN-SLAM supports monocular, stereo, and RGB-D inputs, operating\nrobustly at a frequency of 20-30Hz. Extensive experiments on 6 virtual/real\ndatasets demonstrate that our method outperforms state-of-the-art approaches in\nboth dynamic and static scenes.\n","authors":["Mingrui Li","Jiaming He","Guangan Jiang","Hongyu Wang"],"pdf_url":"https://arxiv.org/pdf/2401.01545v1.pdf","comment":"11pages, 4figures"},{"id":"http://arxiv.org/abs/2401.01544v1","updated":"2024-01-03T05:33:14Z","published":"2024-01-03T05:33:14Z","title":"Collaborative Perception for Connected and Autonomous Driving:\n  Challenges, Possible Solutions and Opportunities","summary":"  Autonomous driving has attracted significant attention from both academia and\nindustries, which is expected to offer a safer and more efficient driving\nsystem. However, current autonomous driving systems are mostly based on a\nsingle vehicle, which has significant limitations which still poses threats to\ndriving safety. Collaborative perception with connected and autonomous vehicles\n(CAVs) shows a promising solution to overcoming these limitations. In this\narticle, we first identify the challenges of collaborative perception, such as\ndata sharing asynchrony, data volume, and pose errors. Then, we discuss the\npossible solutions to address these challenges with various technologies, where\nthe research opportunities are also elaborated. Furthermore, we propose a\nscheme to deal with communication efficiency and latency problems, which is a\nchannel-aware collaborative perception framework to dynamically adjust the\ncommunication graph and minimize latency, thereby improving perception\nperformance while increasing communication efficiency. Finally, we conduct\nexperiments to demonstrate the effectiveness of our proposed scheme.\n","authors":["Senkang Hu","Zhengru Fang","Yiqin Deng","Xianhao Chen","Yuguang Fang"],"pdf_url":"https://arxiv.org/pdf/2401.01544v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.03359v5","updated":"2024-01-03T05:30:54Z","published":"2022-04-07T10:57:12Z","title":"ECCV Caption: Correcting False Negatives by Collecting\n  Machine-and-Human-verified Image-Caption Associations for MS-COCO","summary":"  Image-Text matching (ITM) is a common task for evaluating the quality of\nVision and Language (VL) models. However, existing ITM benchmarks have a\nsignificant limitation. They have many missing correspondences, originating\nfrom the data construction process itself. For example, a caption is only\nmatched with one image although the caption can be matched with other similar\nimages and vice versa. To correct the massive false negatives, we construct the\nExtended COCO Validation (ECCV) Caption dataset by supplying the missing\nassociations with machine and human annotators. We employ five state-of-the-art\nITM models with diverse properties for our annotation process. Our dataset\nprovides x3.6 positive image-to-caption associations and x8.5 caption-to-image\nassociations compared to the original MS-COCO. We also propose to use an\ninformative ranking-based metric mAP@R, rather than the popular Recall@K (R@K).\nWe re-evaluate the existing 25 VL models on existing and proposed benchmarks.\nOur findings are that the existing benchmarks, such as COCO 1K R@K, COCO 5K\nR@K, CxC R@1 are highly correlated with each other, while the rankings change\nwhen we shift to the ECCV mAP@R. Lastly, we delve into the effect of the bias\nintroduced by the choice of machine annotator. Source code and dataset are\navailable at https://github.com/naver-ai/eccv-caption\n","authors":["Sanghyuk Chun","Wonjae Kim","Song Park","Minsuk Chang","Seong Joon Oh"],"pdf_url":"https://arxiv.org/pdf/2204.03359v5.pdf","comment":"Published in ECCV 2022; 32 pages (2.3MB); Code and dataset:\n  https://github.com/naver-ai/eccv-caption; v5 fixes errors in Table 4: the\n  COCO 1K R@1 numbers were incorrect. All other tables and figures are correct.\n  v5 also adds RSUM scores in Tab 4 and 5: RSUM has a high correlation with\n  COCO 1K recalls; v4 fixes errors in v3 -- see the v4 comment for details"},{"id":"http://arxiv.org/abs/2401.01543v1","updated":"2024-01-03T05:26:57Z","published":"2024-01-03T05:26:57Z","title":"Retraining-free Model Quantization via One-Shot Weight-Coupling Learning","summary":"  Quantization is of significance for compressing the over-parameterized deep\nneural models and deploying them on resource-limited devices. Fixed-precision\nquantization suffers from performance drop due to the limited numerical\nrepresentation ability. Conversely, mixed-precision quantization (MPQ) is\nadvocated to compress the model effectively by allocating heterogeneous\nbit-width for layers. MPQ is typically organized into a searching-retraining\ntwo-stage process. Previous works only focus on determining the optimal\nbit-width configuration in the first stage efficiently, while ignoring the\nconsiderable time costs in the second stage. However, retraining always\nconsumes hundreds of GPU-hours on the cutting-edge GPUs, thus hindering\ndeployment efficiency significantly. In this paper, we devise a one-shot\ntraining-searching paradigm for mixed-precision model compression.\nSpecifically, in the first stage, all potential bit-width configurations are\ncoupled and thus optimized simultaneously within a set of shared weights.\nHowever, our observations reveal a previously unseen and severe bit-width\ninterference phenomenon among highly coupled weights during optimization,\nleading to considerable performance degradation under a high compression ratio.\nTo tackle this problem, we first design a bit-width scheduler to dynamically\nfreeze the most turbulent bit-width of layers during training, to ensure the\nrest bit-widths converged properly. Then, taking inspiration from information\ntheory, we present an information distortion mitigation technique to align the\nbehaviour of the bad-performing bit-widths to the well-performing ones.\n","authors":["Chen Tang","Yuan Meng","Jiacheng Jiang","Shuzhao Xie","Rongwei Lu","Xinzhu Ma","Zhi Wang","Wenwu Zhu"],"pdf_url":"https://arxiv.org/pdf/2401.01543v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00789v2","updated":"2024-01-03T05:08:23Z","published":"2024-01-01T15:31:06Z","title":"Retrieval-Augmented Egocentric Video Captioning","summary":"  Understanding human actions from videos of first-person view poses\nsignificant challenges. Most prior approaches explore representation learning\non egocentric videos only, while overlooking the potential benefit of\nexploiting existing large-scale third-person videos. In this paper, (1) we\ndevelop EgoInstructor, a retrieval-augmented multimodal captioning model that\nautomatically retrieves semantically relevant third-person instructional videos\nto enhance the video captioning of egocentric videos. (2) For training the\ncross-view retrieval module, we devise an automatic pipeline to discover\nego-exo video pairs from distinct large-scale egocentric and exocentric\ndatasets. (3) We train the cross-view retrieval module with a novel EgoExoNCE\nloss that pulls egocentric and exocentric video features closer by aligning\nthem to shared text features that describe similar actions. (4) Through\nextensive experiments, our cross-view retrieval module demonstrates superior\nperformance across seven benchmarks. Regarding egocentric video captioning,\nEgoInstructor exhibits significant improvements by leveraging third-person\nvideos as references.\n","authors":["Jilan Xu","Yifei Huang","Junlin Hou","Guo Chen","Yuejie Zhang","Rui Feng","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2401.00789v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01539v1","updated":"2024-01-03T04:35:58Z","published":"2024-01-03T04:35:58Z","title":"DDPM based X-ray Image Synthesizer","summary":"  Access to high-quality datasets in the medical industry limits machine\nlearning model performance. To address this issue, we propose a Denoising\nDiffusion Probabilistic Model (DDPM) combined with a UNet architecture for\nX-ray image synthesis. Focused on pneumonia medical condition, our methodology\nemploys over 3000 pneumonia X-ray images obtained from Kaggle for training.\nResults demonstrate the effectiveness of our approach, as the model\nsuccessfully generated realistic images with low Mean Squared Error (MSE). The\nsynthesized images showed distinct differences from non-pneumonia images,\nhighlighting the model's ability to capture key features of positive cases.\nBeyond pneumonia, the applications of this synthesizer extend to various\nmedical conditions, provided an ample dataset is available. The capability to\nproduce high-quality images can potentially enhance machine learning models'\nperformance, aiding in more accurate and efficient medical diagnoses. This\ninnovative DDPM-based X-ray photo synthesizer presents a promising avenue for\naddressing the scarcity of positive medical image datasets, paving the way for\nimproved medical image analysis and diagnosis in the healthcare industry.\n","authors":["Praveen Mahaulpatha","Thulana Abeywardane","Tomson George"],"pdf_url":"https://arxiv.org/pdf/2401.01539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14705v2","updated":"2024-01-03T04:14:07Z","published":"2023-12-22T14:06:03Z","title":"SCUNet++: Swin-UNet and CNN Bottleneck Hybrid Architecture with\n  Multi-Fusion Dense Skip Connection for Pulmonary Embolism CT Image\n  Segmentation","summary":"  Pulmonary embolism (PE) is a prevalent lung disease that can lead to right\nventricular hypertrophy and failure in severe cases, ranking second in severity\nonly to myocardial infarction and sudden death. Pulmonary artery CT angiography\n(CTPA) is a widely used diagnostic method for PE. However, PE detection\npresents challenges in clinical practice due to limitations in imaging\ntechnology. CTPA can produce noises similar to PE, making confirmation of its\npresence time-consuming and prone to overdiagnosis. Nevertheless, the\ntraditional segmentation method of PE can not fully consider the hierarchical\nstructure of features, local and global spatial features of PE CT images. In\nthis paper, we propose an automatic PE segmentation method called SCUNet++\n(Swin Conv UNet++). This method incorporates multiple fusion dense skip\nconnections between the encoder and decoder, utilizing the Swin Transformer as\nthe encoder. And fuses features of different scales in the decoder subnetwork\nto compensate for spatial information loss caused by the inevitable\ndownsampling in Swin-UNet or other state-of-the-art methods, effectively\nsolving the above problem. We provide a theoretical analysis of this method in\ndetail and validate it on publicly available PE CT image datasets FUMPE and\nCAD-PE. The experimental results indicate that our proposed method achieved a\nDice similarity coefficient (DSC) of 83.47% and a Hausdorff distance 95th\npercentile (HD95) of 3.83 on the FUMPE dataset, as well as a DSC of 83.42% and\nan HD95 of 5.10 on the CAD-PE dataset. These findings demonstrate that our\nmethod exhibits strong performance in PE segmentation tasks, potentially\nenhancing the accuracy of automatic segmentation of PE and providing a powerful\ndiagnostic tool for clinical physicians. Our source code and new FUMPE dataset\nare available at https://github.com/JustlfC03/SCUNet-plusplus.\n","authors":["Yifei Chen","Binfeng Zou","Zhaoxin Guo","Yiyu Huang","Yifan Huang","Feiwei Qin","Qinhai Li","Changmiao Wang"],"pdf_url":"https://arxiv.org/pdf/2312.14705v2.pdf","comment":"10 pages, 7 figures, accept WACV2024"},{"id":"http://arxiv.org/abs/2401.01529v1","updated":"2024-01-03T03:51:16Z","published":"2024-01-03T03:51:16Z","title":"Glance and Focus: Memory Prompting for Multi-Event Video Question\n  Answering","summary":"  Video Question Answering (VideoQA) has emerged as a vital tool to evaluate\nagents' ability to understand human daily behaviors. Despite the recent success\nof large vision language models in many multi-modal tasks, complex situation\nreasoning over videos involving multiple human-object interaction events still\nremains challenging. In contrast, humans can easily tackle it by using a series\nof episode memories as anchors to quickly locate question-related key moments\nfor reasoning. To mimic this effective reasoning strategy, we propose the\nGlance-Focus model. One simple way is to apply an action detection model to\npredict a set of actions as key memories. However, these actions within a\nclosed set vocabulary are hard to generalize to various video domains. Instead\nof that, we train an Encoder-Decoder to generate a set of dynamic event\nmemories at the glancing stage. Apart from using supervised bipartite matching\nto obtain the event memories, we further design an unsupervised memory\ngeneration method to get rid of dependence on event annotations. Next, at the\nfocusing stage, these event memories act as a bridge to establish the\ncorrelation between the questions with high-level event concepts and low-level\nlengthy video content. Given the question, the model first focuses on the\ngenerated key event memory, then focuses on the most relevant moment for\nreasoning through our designed multi-level cross-attention mechanism. We\nconduct extensive experiments on four Multi-Event VideoQA benchmarks including\nSTAR, EgoTaskQA, AGQA, and NExT-QA. Our proposed model achieves\nstate-of-the-art results, surpassing current large models in various\nchallenging reasoning tasks. The code and models are available at\nhttps://github.com/ByZ0e/Glance-Focus.\n","authors":["Ziyi Bai","Ruiping Wang","Xilin Chen"],"pdf_url":"https://arxiv.org/pdf/2401.01529v1.pdf","comment":"Accepted in NeurIPS 2023"},{"id":"http://arxiv.org/abs/2401.01524v1","updated":"2024-01-03T03:33:48Z","published":"2024-01-03T03:33:48Z","title":"Multimodal self-supervised learning for lesion localization","summary":"  Multimodal deep learning utilizing imaging and diagnostic reports has made\nimpressive progress in the field of medical imaging diagnostics, demonstrating\na particularly strong capability for auxiliary diagnosis in cases where\nsufficient annotation information is lacking. Nonetheless, localizing diseases\naccurately without detailed positional annotations remains a challenge.\nAlthough existing methods have attempted to utilize local information to\nachieve fine-grained semantic alignment, their capability in extracting the\nfine-grained semantics of the comprehensive contextual within reports is\nlimited. To solve this problem, we introduce a new method that takes full\nsentences from textual reports as the basic units for local semantic alignment.\nOur approach combines chest X-ray images with their corresponding textual\nreports, performing contrastive learning at both global and local levels. The\nleading results obtained by our method on multiple datasets confirm its\nefficacy in the task of lesion localization.\n","authors":["Hao Yang","Hong-Yu Zhou","Cheng Li","Weijian Huang","Jiarun Liu","Yong Liang","Shanshan Wang"],"pdf_url":"https://arxiv.org/pdf/2401.01524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01522v1","updated":"2024-01-03T03:14:55Z","published":"2024-01-03T03:14:55Z","title":"LORE++: Logical Location Regression Network for Table Structure\n  Recognition with Pre-training","summary":"  Table structure recognition (TSR) aims at extracting tables in images into\nmachine-understandable formats. Recent methods solve this problem by predicting\nthe adjacency relations of detected cell boxes or learning to directly generate\nthe corresponding markup sequences from the table images. However, existing\napproaches either count on additional heuristic rules to recover the table\nstructures, or face challenges in capturing long-range dependencies within\ntables, resulting in increased complexity. In this paper, we propose an\nalternative paradigm. We model TSR as a logical location regression problem and\npropose a new TSR framework called LORE, standing for LOgical location\nREgression network, which for the first time regresses logical location as well\nas spatial location of table cells in a unified network. Our proposed LORE is\nconceptually simpler, easier to train, and more accurate than other paradigms\nof TSR. Moreover, inspired by the persuasive success of pre-trained models on a\nnumber of computer vision and natural language processing tasks, we propose two\npre-training tasks to enrich the spatial and logical representations at the\nfeature level of LORE, resulting in an upgraded version called LORE++. The\nincorporation of pre-training in LORE++ has proven to enjoy significant\nadvantages, leading to a substantial enhancement in terms of accuracy,\ngeneralization, and few-shot capability compared to its predecessor.\nExperiments on standard benchmarks against methods of previous paradigms\ndemonstrate the superiority of LORE++, which highlights the potential and\npromising prospect of the logical location regression paradigm for TSR.\n","authors":["Rujiao Long","Hangdi Xing","Zhibo Yang","Qi Zheng","Zhi Yu","Cong Yao","Fei Huang"],"pdf_url":"https://arxiv.org/pdf/2401.01522v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2303.03730"},{"id":"http://arxiv.org/abs/2401.01520v1","updated":"2024-01-03T03:08:32Z","published":"2024-01-03T03:08:32Z","title":"S$^{2}$-DMs:Skip-Step Diffusion Models","summary":"  Diffusion models have emerged as powerful generative tools, rivaling GANs in\nsample quality and mirroring the likelihood scores of autoregressive models. A\nsubset of these models, exemplified by DDIMs, exhibit an inherent asymmetry:\nthey are trained over $T$ steps but only sample from a subset of $T$ during\ngeneration. This selective sampling approach, though optimized for speed,\ninadvertently misses out on vital information from the unsampled steps, leading\nto potential compromises in sample quality. To address this issue, we present\nthe S$^{2}$-DMs, which is a new training method by using an innovative\n$L_{skip}$, meticulously designed to reintegrate the information omitted during\nthe selective sampling phase. The benefits of this approach are manifold: it\nnotably enhances sample quality, is exceptionally simple to implement, requires\nminimal code modifications, and is flexible enough to be compatible with\nvarious sampling algorithms. On the CIFAR10 dataset, models trained using our\nalgorithm showed an improvement of 3.27% to 14.06% over models trained with\ntraditional methods across various sampling algorithms (DDIMs, PNDMs, DEIS) and\ndifferent numbers of sampling steps (10, 20, ..., 1000). On the CELEBA dataset,\nthe improvement ranged from 8.97% to 27.08%. Access to the code and additional\nresources is provided in the github.\n","authors":["Yixuan Wang","Shuangyin Li"],"pdf_url":"https://arxiv.org/pdf/2401.01520v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2401.00695v2","updated":"2024-01-03T02:33:49Z","published":"2024-01-01T08:19:21Z","title":"Credible Teacher for Semi-Supervised Object Detection in Open Scene","summary":"  Semi-Supervised Object Detection (SSOD) has achieved resounding success by\nleveraging unlabeled data to improve detection performance. However, in Open\nScene Semi-Supervised Object Detection (O-SSOD), unlabeled data may contains\nunknown objects not observed in the labeled data, which will increase\nuncertainty in the model's predictions for known objects. It is detrimental to\nthe current methods that mainly rely on self-training, as more uncertainty\nleads to the lower localization and classification precision of pseudo labels.\nTo this end, we propose Credible Teacher, an end-to-end framework. Credible\nTeacher adopts an interactive teaching mechanism using flexible labels to\nprevent uncertain pseudo labels from misleading the model and gradually reduces\nits uncertainty through the guidance of other credible pseudo labels. Empirical\nresults have demonstrated our method effectively restrains the adverse effect\ncaused by O-SSOD and significantly outperforms existing counterparts.\n","authors":["Jingyu Zhuang","Kuo Wang","Liang Lin","Guanbin Li"],"pdf_url":"https://arxiv.org/pdf/2401.00695v2.pdf","comment":"Accpet by ICASSP 2024"},{"id":"http://arxiv.org/abs/2401.01510v1","updated":"2024-01-03T02:29:34Z","published":"2024-01-03T02:29:34Z","title":"Answering from Sure to Uncertain: Uncertainty-Aware Curriculum Learning\n  for Video Question Answering","summary":"  While significant advancements have been made in video question answering\n(VideoQA), the potential benefits of enhancing model generalization through\ntailored difficulty scheduling have been largely overlooked in existing\nresearch. This paper seeks to bridge that gap by incorporating VideoQA into a\ncurriculum learning (CL) framework that progressively trains models from\nsimpler to more complex data. Recognizing that conventional self-paced CL\nmethods rely on training loss for difficulty measurement, which might not\naccurately reflect the intricacies of video-question pairs, we introduce the\nconcept of uncertainty-aware CL. Here, uncertainty serves as the guiding\nprinciple for dynamically adjusting the difficulty. Furthermore, we address the\nchallenge posed by uncertainty by presenting a probabilistic modeling approach\nfor VideoQA. Specifically, we conceptualize VideoQA as a stochastic computation\ngraph, where the hidden representations are treated as stochastic variables.\nThis yields two distinct types of uncertainty: one related to the inherent\nuncertainty in the data and another pertaining to the model's confidence. In\npractice, we seamlessly integrate the VideoQA model into our framework and\nconduct comprehensive experiments. The findings affirm that our approach not\nonly achieves enhanced performance but also effectively quantifies uncertainty\nin the context of VideoQA.\n","authors":["Haopeng Li","Qiuhong Ke","Mingming Gong","Tom Drummond"],"pdf_url":"https://arxiv.org/pdf/2401.01510v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01505v1","updated":"2024-01-03T02:22:34Z","published":"2024-01-03T02:22:34Z","title":"Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex\n  and Professional Sports","summary":"  Reasoning over sports videos for question answering is an important task with\nnumerous applications, such as player training and information retrieval.\nHowever, this task has not been explored due to the lack of relevant datasets\nand the challenging nature it presents. Most datasets for video question\nanswering (VideoQA) focus mainly on general and coarse-grained understanding of\ndaily-life videos, which is not applicable to sports scenarios requiring\nprofessional action understanding and fine-grained motion analysis. In this\npaper, we introduce the first dataset, named Sports-QA, specifically designed\nfor the sports VideoQA task. The Sports-QA dataset includes various types of\nquestions, such as descriptions, chronologies, causalities, and counterfactual\nconditions, covering multiple sports. Furthermore, to address the\ncharacteristics of the sports VideoQA task, we propose a new Auto-Focus\nTransformer (AFT) capable of automatically focusing on particular scales of\ntemporal information for question answering. We conduct extensive experiments\non Sports-QA, including baseline studies and the evaluation of different\nmethods. The results demonstrate that our AFT achieves state-of-the-art\nperformance.\n","authors":["Haopeng Li","Andong Deng","Qiuhong Ke","Jun Liu","Hossein Rahmani","Yulan Guo","Bernt Schiele","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2401.01505v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01107v2","updated":"2024-01-03T02:13:31Z","published":"2024-01-02T08:57:09Z","title":"CityPulse: Fine-Grained Assessment of Urban Change with Street View Time\n  Series","summary":"  Urban transformations have profound societal impact on both individuals and\ncommunities at large. Accurately assessing these shifts is essential for\nunderstanding their underlying causes and ensuring sustainable urban planning.\nTraditional measurements often encounter constraints in spatial and temporal\ngranularity, failing to capture real-time physical changes. While street view\nimagery, capturing the heartbeat of urban spaces from a pedestrian point of\nview, can add as a high-definition, up-to-date, and on-the-ground visual proxy\nof urban change. We curate the largest street view time series dataset to date,\nand propose an end-to-end change detection model to effectively capture\nphysical alterations in the built environment at scale. We demonstrate the\neffectiveness of our proposed method by benchmark comparisons with previous\nliterature and implementing it at the city-wide level. Our approach has the\npotential to supplement existing dataset and serve as a fine-grained and\naccurate assessment of urban change.\n","authors":["Tianyuan Huang","Zejia Wu","Jiajun Wu","Jackelyn Hwang","Ram Rajagopal"],"pdf_url":"https://arxiv.org/pdf/2401.01107v2.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2309.11715v3","updated":"2024-01-03T02:01:09Z","published":"2023-09-21T01:35:13Z","title":"Deshadow-Anything: When Segment Anything Model Meets Zero-shot shadow\n  removal","summary":"  Segment Anything (SAM), an advanced universal image segmentation model\ntrained on an expansive visual dataset, has set a new benchmark in image\nsegmentation and computer vision. However, it faced challenges when it came to\ndistinguishing between shadows and their backgrounds. To address this, we\ndeveloped Deshadow-Anything, considering the generalization of large-scale\ndatasets, and we performed Fine-tuning on large-scale datasets to achieve image\nshadow removal. The diffusion model can diffuse along the edges and textures of\nan image, helping to remove shadows while preserving the details of the image.\nFurthermore, we design Multi-Self-Attention Guidance (MSAG) and adaptive input\nperturbation (DDPM-AIP) to accelerate the iterative training speed of\ndiffusion. Experiments on shadow removal tasks demonstrate that these methods\ncan effectively improve image restoration performance.\n","authors":["Xiao Feng Zhang","Tian Yi Song","Jia Wei Yao"],"pdf_url":"https://arxiv.org/pdf/2309.11715v3.pdf","comment":"it needs revised"},{"id":"http://arxiv.org/abs/2401.01496v1","updated":"2024-01-03T02:01:09Z","published":"2024-01-03T02:01:09Z","title":"From Pixel to Slide image: Polarization Modality-based Pathological\n  Diagnosis Using Representation Learning","summary":"  Thyroid cancer is the most common endocrine malignancy, and accurately\ndistinguishing between benign and malignant thyroid tumors is crucial for\ndeveloping effective treatment plans in clinical practice. Pathologically,\nthyroid tumors pose diagnostic challenges due to improper specimen sampling. In\nthis study, we have designed a three-stage model using representation learning\nto integrate pixel-level and slice-level annotations for distinguishing thyroid\ntumors. This structure includes a pathology structure recognition method to\npredict structures related to thyroid tumors, an encoder-decoder network to\nextract pixel-level annotation information by learning the feature\nrepresentations of image blocks, and an attention-based learning mechanism for\nthe final classification task. This mechanism learns the importance of\ndifferent image blocks in a pathological region, globally considering the\ninformation from each block. In the third stage, all information from the image\nblocks in a region is aggregated using attention mechanisms, followed by\nclassification to determine the category of the region. Experimental results\ndemonstrate that our proposed method can predict microscopic structures more\naccurately. After color-coding, the method achieves results on unstained\npathology slides that approximate the quality of Hematoxylin and eosin\nstaining, reducing the need for stained pathology slides. Furthermore, by\nleveraging the concept of indirect measurement and extracting polarized\nfeatures from structures correlated with lesions, the proposed method can also\nclassify samples where membrane structures cannot be obtained through sampling,\nproviding a potential objective and highly accurate indirect diagnostic\ntechnique for thyroid tumors.\n","authors":["Jia Dong","Yao Yao","Yang Dong","Hui Ma"],"pdf_url":"https://arxiv.org/pdf/2401.01496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00436v2","updated":"2024-01-03T01:42:22Z","published":"2023-12-31T09:24:28Z","title":"Diff-PCR: Diffusion-Based Correspondence Searching in Doubly Stochastic\n  Matrix Space for Point Cloud Registration","summary":"  Efficiently finding optimal correspondences between point clouds is crucial\nfor solving both rigid and non-rigid point cloud registration problems.\nExisting methods often rely on geometric or semantic feature embedding to\nestablish correspondences and estimate transformations or flow fields.\nRecently, state-of-the-art methods have employed RAFT-like iterative updates to\nrefine the solution. However, these methods have certain limitations. Firstly,\ntheir iterative refinement design lacks transparency, and their iterative\nupdates follow a fixed path during the refinement process, which can lead to\nsuboptimal results. Secondly, these methods overlook the importance of refining\nor optimizing correspondences (or matching matrices) as a precursor to solving\ntransformations or flow fields. They typically compute candidate\ncorrespondences based on distances in the point feature space. However, they\nonly project the candidate matching matrix into some matrix space once with\nSinkhorn or dual softmax operations to obtain final correspondences. This\none-shot projected matching matrix may be far from the globally optimal one,\nand these approaches do not consider the distribution of the target matching\nmatrix. In this paper, we propose a novel approach that exploits the Denoising\nDiffusion Model to predict a searching gradient for the optimal matching matrix\nwithin the Doubly Stochastic Matrix Space. During the reverse denoising\nprocess, our method iteratively searches for better solutions along this\ndenoising gradient, which points towards the maximum likelihood direction of\nthe target matching matrix. Our method offers flexibility by allowing the\nsearch to start from any initial matching matrix provided by the online\nbackbone or white noise. Experimental evaluations on the 3DMatch/3DLoMatch and\n4DMatch/4DLoMatch datasets demonstrate the effectiveness of our newly designed\nframework.\n","authors":["Qianliang Wu","Haobo Jiang","Yaqing Ding","Lei Luo","Jin Xie","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2401.00436v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.00731v2","updated":"2024-01-03T01:26:10Z","published":"2022-11-30T02:47:18Z","title":"FuRPE: Learning Full-body Reconstruction from Part Experts","summary":"  In the field of full-body reconstruction, the scarcity of annotated data\noften impedes the efficacy of prevailing methods. To address this issue, we\nintroduce FuRPE, a novel framework that employs part-experts and an ingenious\npseudo ground-truth selection scheme to derive high-quality pseudo labels.\nThese labels, central to our approach, equip our network with the capability to\nefficiently learn from the available data. Integral to FuRPE is a unique\nexponential moving average training strategy and expert-derived feature\ndistillation strategy. These novel elements of FuRPE not only serve to further\nrefine the model but also to reduce potential biases that may arise from\ninaccuracies in pseudo labels, thereby optimizing the network's training\nprocess and enhancing the robustness of the model. We apply FuRPE to train both\ntwo-stage and fully convolutional single-stage full-body reconstruction\nnetworks. Our exhaustive experiments on numerous benchmark datasets illustrate\na substantial performance boost over existing methods, underscoring FuRPE's\npotential to reshape the state-of-the-art in full-body reconstruction.\n","authors":["Zhaoxin Fan","Yuqing Pan","Hao Xu","Zhenbo Song","Zhicheng Wang","Kejian Wu","Hongyan Liu","Jun He"],"pdf_url":"https://arxiv.org/pdf/2212.00731v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01597v3","updated":"2024-01-03T01:15:50Z","published":"2023-12-04T03:18:46Z","title":"SCLIP: Rethinking Self-Attention for Dense Vision-Language Inference","summary":"  Recent advances in contrastive language-image pretraining (CLIP) have\ndemonstrated strong capabilities in zero-shot classification by aligning visual\nrepresentations with target text embeddings in an image level. However, in\ndense prediction tasks, CLIP often struggles to localize visual features within\nan image and fails to give accurate pixel-level predictions, which prevents it\nfrom functioning as a generalized visual foundation model. In this work, we aim\nto enhance CLIP's potential for semantic segmentation with minimal\nmodifications to its pretrained models. By rethinking self-attention, we\nsurprisingly find that CLIP can adapt to dense prediction tasks by simply\nintroducing a novel Correlative Self-Attention (CSA) mechanism. Specifically,\nwe replace the traditional self-attention block of CLIP vision encoder's last\nlayer by our CSA module and reuse its pretrained projection matrices of query,\nkey, and value, leading to a training-free adaptation approach for CLIP's\nzero-shot semantic segmentation. Extensive experiments show the advantage of\nCSA: we obtain a 38.2% average zero-shot mIoU across eight semantic\nsegmentation benchmarks highlighted in this paper, significantly outperforming\nthe existing SoTA's 33.9% and the vanilla CLIP's 14.1%.\n","authors":["Feng Wang","Jieru Mei","Alan Yuille"],"pdf_url":"https://arxiv.org/pdf/2312.01597v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01482v1","updated":"2024-01-03T01:11:16Z","published":"2024-01-03T01:11:16Z","title":"Incorporating Geo-Diverse Knowledge into Prompting for Increased\n  Geographical Robustness in Object Recognition","summary":"  Existing object recognition models have been shown to lack robustness in\ndiverse geographical scenarios due to significant domain shifts in design and\ncontext. Class representations need to be adapted to more accurately reflect an\nobject concept under these shifts. In the absence of training data from target\ngeographies, we hypothesize that geography-specific descriptive knowledge of\nobject categories can be leveraged to enhance robustness. For this purpose, we\nexplore the feasibility of probing a large-language model for\ngeography-specific object knowledge, and we investigate integrating knowledge\nin zero-shot and learnable soft prompting with the CLIP vision-language model.\nIn particular, we propose a geography knowledge regularization method to ensure\nthat soft prompts trained on a source set of geographies generalize to an\nunseen target set of geographies. Our gains on DollarStreet when generalizing\nfrom a model trained only on data from Europe are as large as +2.8 on countries\nfrom Africa, and +4.6 on the hardest classes. We further show competitive\nperformance vs. few-shot target training, and provide insights into how\ndescriptive knowledge captures geographical differences.\n","authors":["Kyle Buettner","Sina Malakouti","Xiang Lorraine Li","Adriana Kovashka"],"pdf_url":"https://arxiv.org/pdf/2401.01482v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.17097v3","updated":"2024-01-03T01:03:58Z","published":"2023-10-26T01:40:28Z","title":"Navigating Data Heterogeneity in Federated Learning A Semi-Supervised\n  Federated Object Detection","summary":"  Federated Learning (FL) has emerged as a potent framework for training models\nacross distributed data sources while maintaining data privacy. Nevertheless,\nit faces challenges with limited high-quality labels and non-IID client data,\nparticularly in applications like autonomous driving. To address these hurdles,\nwe navigate the uncharted waters of Semi-Supervised Federated Object Detection\n(SSFOD). We present a pioneering SSFOD framework, designed for scenarios where\nlabeled data reside only at the server while clients possess unlabeled data.\nNotably, our method represents the inaugural implementation of SSFOD for\nclients with 0% labeled non-IID data, a stark contrast to previous studies that\nmaintain some subset of labels at each client. We propose FedSTO, a two-stage\nstrategy encompassing Selective Training followed by Orthogonally enhanced\nfull-parameter training, to effectively address data shift (e.g. weather\nconditions) between server and clients. Our contributions include selectively\nrefining the backbone of the detector to avert overfitting, orthogonality\nregularization to boost representation divergence, and local EMA-driven pseudo\nlabel assignment to yield high-quality pseudo labels. Extensive validation on\nprominent autonomous driving datasets (BDD100K, Cityscapes, and SODA10M)\nattests to the efficacy of our approach, demonstrating state-of-the-art\nresults. Remarkably, FedSTO, using just 20-30% of labels, performs nearly as\nwell as fully-supervised centralized training methods.\n","authors":["Taehyeon Kim","Eric Lin","Junu Lee","Christian Lau","Vaikkunth Mugunthan"],"pdf_url":"https://arxiv.org/pdf/2310.17097v3.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2401.01470v1","updated":"2024-01-03T00:10:33Z","published":"2024-01-03T00:10:33Z","title":"Token Propagation Controller for Efficient Vision Transformer","summary":"  Vision transformers (ViTs) have achieved promising results on a variety of\nComputer Vision tasks, however their quadratic complexity in the number of\ninput tokens has limited their application specially in resource-constrained\nsettings. Previous approaches that employ gradual token reduction to address\nthis challenge assume that token redundancy in one layer implies redundancy in\nall the following layers. We empirically demonstrate that this assumption is\noften not correct, i.e., tokens that are redundant in one layer can be useful\nin later layers. We employ this key insight to propose a novel token\npropagation controller (TPC) that incorporates two different\ntoken-distributions, i.e., pause probability and restart probability to control\nthe reduction and reuse of tokens respectively, which results in more efficient\ntoken utilization. To improve the estimates of token distributions, we propose\na smoothing mechanism that acts as a regularizer and helps remove noisy\noutliers. Furthermore, to improve the training-stability of our proposed TPC,\nwe introduce a model stabilizer that is able to implicitly encode local image\nstructures and minimize accuracy fluctuations during model training. We present\nextensive experimental results on the ImageNet-1K dataset using DeiT, LV-ViT\nand Swin models to demonstrate the effectiveness of our proposed method. For\nexample, compared to baseline models, our proposed method improves the\ninference speed of the DeiT-S by 250% while increasing the classification\naccuracy by 1.0%.\n","authors":["Wentao Zhu"],"pdf_url":"https://arxiv.org/pdf/2401.01470v1.pdf","comment":"Accepted by WACV 2024"},{"id":"http://arxiv.org/abs/2401.01990v1","updated":"2024-01-03T21:39:06Z","published":"2024-01-03T21:39:06Z","title":"GPS-SSL: Guided Positive Sampling to Inject Prior Into Self-Supervised\n  Learning","summary":"  We propose Guided Positive Sampling Self-Supervised Learning (GPS-SSL), a\ngeneral method to inject a priori knowledge into Self-Supervised Learning (SSL)\npositive samples selection. Current SSL methods leverage Data-Augmentations\n(DA) for generating positive samples and incorporate prior knowledge - an\nincorrect, or too weak DA will drastically reduce the quality of the learned\nrepresentation. GPS-SSL proposes instead to design a metric space where\nEuclidean distances become a meaningful proxy for semantic relationship. In\nthat space, it is now possible to generate positive samples from nearest\nneighbor sampling. Any prior knowledge can now be embedded into that metric\nspace independently from the employed DA. From its simplicity, GPS-SSL is\napplicable to any SSL method, e.g. SimCLR or BYOL. A key benefit of GPS-SSL is\nin reducing the pressure in tailoring strong DAs. For example GPS-SSL reaches\n85.58% on Cifar10 with weak DA while the baseline only reaches 37.51%. We\ntherefore move a step forward towards the goal of making SSL less reliant on\nDA. We also show that even when using strong DAs, GPS-SSL outperforms the\nbaselines on under-studied domains. We evaluate GPS-SSL along with multiple\nbaseline SSL methods on numerous downstream datasets from different domains\nwhen the models use strong or minimal data augmentations. We hope that GPS-SSL\nwill open new avenues in studying how to inject a priori knowledge into SSL in\na principled manner.\n","authors":["Aarash Feizi","Randall Balestriero","Adriana Romero-Soriano","Reihaneh Rabbany"],"pdf_url":"https://arxiv.org/pdf/2401.01990v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01984v1","updated":"2024-01-03T21:24:44Z","published":"2024-01-03T21:24:44Z","title":"AUPIMO: Redefining Visual Anomaly Detection Benchmarks with High Speed\n  and Low Tolerance","summary":"  Recent advances in visual anomaly detection research have seen AUROC and\nAUPRO scores on public benchmark datasets such as MVTec and VisA converge\ntowards perfect recall, giving the impression that these benchmarks are\nnear-solved. However, high AUROC and AUPRO scores do not always reflect\nqualitative performance, which limits the validity of these metrics in\nreal-world applications. We argue that the artificial ceiling imposed by the\nlack of an adequate evaluation metric restrains progression of the field, and\nit is crucial that we revisit the evaluation metrics used to rate our\nalgorithms. In response, we introduce Per-IMage Overlap (PIMO), a novel metric\nthat addresses the shortcomings of AUROC and AUPRO. PIMO retains the\nrecall-based nature of the existing metrics but introduces two distinctions:\nthe assignment of curves (and respective area under the curve) is per-image,\nand its X-axis relies solely on normal images. Measuring recall per image\nsimplifies instance score indexing and is more robust to noisy annotations. As\nwe show, it also accelerates computation and enables the usage of statistical\ntests to compare models. By imposing low tolerance for false positives on\nnormal images, PIMO provides an enhanced model validation procedure and\nhighlights performance variations across datasets. Our experiments demonstrate\nthat PIMO offers practical advantages and nuanced performance insights that\nredefine anomaly detection benchmarks -- notably challenging the perception\nthat MVTec AD and VisA datasets have been solved by contemporary models.\nAvailable on GitHub: https://github.com/jpcbertoldo/aupimo.\n","authors":["Joao P. C. Bertoldo","Dick Ameln","Ashwin Vaidya","Samet Akçay"],"pdf_url":"https://arxiv.org/pdf/2401.01984v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01974v1","updated":"2024-01-03T20:48:47Z","published":"2024-01-03T20:48:47Z","title":"Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as\n  Programmers","summary":"  Visual reasoning is dominated by end-to-end neural networks scaled to\nbillions of model parameters and training examples. However, even the largest\nmodels struggle with compositional reasoning, generalization, fine-grained\nspatial and temporal reasoning, and counting. Visual reasoning with large\nlanguage models (LLMs) as controllers can, in principle, address these\nlimitations by decomposing the task and solving subtasks by orchestrating a set\nof (visual) tools. Recently, these models achieved great performance on tasks\nsuch as compositional visual question answering, visual grounding, and video\ntemporal reasoning. Nevertheless, in their current form, these models heavily\nrely on human engineering of in-context examples in the prompt, which are often\ndataset- and task-specific and require significant labor by highly skilled\nprogrammers. In this work, we present a framework that mitigates these issues\nby introducing spatially and temporally abstract routines and by leveraging a\nsmall number of labeled examples to automatically generate in-context examples,\nthereby avoiding human-created in-context examples. On a number of visual\nreasoning tasks, we show that our framework leads to consistent gains in\nperformance, makes LLMs as controllers setup more robust, and removes the need\nfor human engineering of in-context examples.\n","authors":["Aleksandar Stanić","Sergi Caelles","Michael Tschannen"],"pdf_url":"https://arxiv.org/pdf/2401.01974v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01970v1","updated":"2024-01-03T20:39:02Z","published":"2024-01-03T20:39:02Z","title":"FMGS: Foundation Model Embedded 3D Gaussian Splatting for Holistic 3D\n  Scene Understanding","summary":"  Precisely perceiving the geometric and semantic properties of real-world 3D\nobjects is crucial for the continued evolution of augmented reality and robotic\napplications. To this end, we present \\algfull{} (\\algname{}), which\nincorporates vision-language embeddings of foundation models into 3D Gaussian\nSplatting (GS). The key contribution of this work is an efficient method to\nreconstruct and represent 3D vision-language models. This is achieved by\ndistilling feature maps generated from image-based foundation models into those\nrendered from our 3D model. To ensure high-quality rendering and fast training,\nwe introduce a novel scene representation by integrating strengths from both GS\nand multi-resolution hash encodings (MHE). Our effective training procedure\nalso introduces a pixel alignment loss that makes the rendered feature distance\nof same semantic entities close, following the pixel-level semantic boundaries.\nOur results demonstrate remarkable multi-view semantic consistency,\nfacilitating diverse downstream tasks, beating state-of-the-art methods by\n$\\mathbf{10.2}$ percent on open-vocabulary language-based object detection,\ndespite that we are $\\mathbf{851\\times}$ faster for inference. This research\nexplores the intersection of vision, language, and 3D scene representation,\npaving the way for enhanced scene understanding in uncontrolled real-world\nenvironments. We plan to release the code upon paper acceptance.\n","authors":["Xingxing Zuo","Pouya Samangouei","Yunwen Zhou","Yan Di","Mingyang Li"],"pdf_url":"https://arxiv.org/pdf/2401.01970v1.pdf","comment":"19 pages, Project page coming soon"},{"id":"http://arxiv.org/abs/2310.20065v2","updated":"2024-01-03T19:57:42Z","published":"2023-10-30T22:29:50Z","title":"LinFlo-Net: A two-stage deep learning method to generate simulation\n  ready meshes of the heart","summary":"  We present a deep learning model to automatically generate computer models of\nthe human heart from patient imaging data with an emphasis on its capability to\ngenerate thin-walled cardiac structures. Our method works by deforming a\ntemplate mesh to fit the cardiac structures to the given image. Compared with\nprior deep learning methods that adopted this approach, our framework is\ndesigned to minimize mesh self-penetration, which typically arises when\ndeforming surface meshes separated by small distances. We achieve this by using\na two-stage diffeomorphic deformation process along with a novel loss function\nderived from the kinematics of motion that penalizes surface contact and\ninterpenetration. Our model demonstrates comparable accuracy with\nstate-of-the-art methods while additionally producing meshes free of\nself-intersections. The resultant meshes are readily usable in physics based\nsimulation, minimizing the need for post-processing and cleanup.\n","authors":["Arjun Narayanan","Fanwei Kong","Shawn Shadden"],"pdf_url":"https://arxiv.org/pdf/2310.20065v2.pdf","comment":"Accepted manuscript in the Journal of Biomechanical Engineering"},{"id":"http://arxiv.org/abs/2401.01952v1","updated":"2024-01-03T19:31:58Z","published":"2024-01-03T19:31:58Z","title":"Instruct-Imagen: Image Generation with Multi-modal Instruction","summary":"  This paper presents instruct-imagen, a model that tackles heterogeneous image\ngeneration tasks and generalizes across unseen tasks. We introduce *multi-modal\ninstruction* for image generation, a task representation articulating a range\nof generation intents with precision. It uses natural language to amalgamate\ndisparate modalities (e.g., text, edge, style, subject, etc.), such that\nabundant generation intents can be standardized in a uniform format.\n  We then build instruct-imagen by fine-tuning a pre-trained text-to-image\ndiffusion model with a two-stage framework. First, we adapt the model using the\nretrieval-augmented training, to enhance model's capabilities to ground its\ngeneration on external multimodal context. Subsequently, we fine-tune the\nadapted model on diverse image generation tasks that requires vision-language\nunderstanding (e.g., subject-driven generation, etc.), each paired with a\nmulti-modal instruction encapsulating the task's essence. Human evaluation on\nvarious image generation datasets reveals that instruct-imagen matches or\nsurpasses prior task-specific models in-domain and demonstrates promising\ngeneralization to unseen and more complex tasks.\n","authors":["Hexiang Hu","Kelvin C. K. Chan","Yu-Chuan Su","Wenhu Chen","Yandong Li","Kihyuk Sohn","Yang Zhao","Xue Ben","Boqing Gong","William Cohen","Ming-Wei Chang","Xuhui Jia"],"pdf_url":"https://arxiv.org/pdf/2401.01952v1.pdf","comment":"20 pages, 18 figures"},{"id":"http://arxiv.org/abs/2401.01951v1","updated":"2024-01-03T19:27:20Z","published":"2024-01-03T19:27:20Z","title":"Can We Generate Realistic Hands Only Using Convolution?","summary":"  The enduring inability of image generative models to recreate intricate\ngeometric features, such as those present in human hands and fingers has been\nan ongoing problem in image generation for nearly a decade. While strides have\nbeen made by increasing model sizes and diversifying training datasets, this\nissue remains prevalent across all models, from denoising diffusion models to\nGenerative Adversarial Networks (GAN), pointing to a fundamental shortcoming in\nthe underlying architectures. In this paper, we demonstrate how this problem\ncan be mitigated by augmenting convolution layers geometric capabilities\nthrough providing them with a single input channel incorporating the relative\n$n$-dimensional Cartesian coordinate system. We show that this drastically\nimproves quality of hand and face images generated by GANs and Variational\nAutoEncoders (VAE).\n","authors":["Mehran Hosseini","Peyman Hosseini"],"pdf_url":"https://arxiv.org/pdf/2401.01951v1.pdf","comment":"Contains 17 pages, 14 figures, and 6 tables"},{"id":"http://arxiv.org/abs/2401.01922v1","updated":"2024-01-03T15:09:25Z","published":"2024-01-03T15:09:25Z","title":"Unsupervised Object-Centric Learning from Multiple Unspecified\n  Viewpoints","summary":"  Visual scenes are extremely diverse, not only because there are infinite\npossible combinations of objects and backgrounds but also because the\nobservations of the same scene may vary greatly with the change of viewpoints.\nWhen observing a multi-object visual scene from multiple viewpoints, humans can\nperceive the scene compositionally from each viewpoint while achieving the\nso-called ``object constancy'' across different viewpoints, even though the\nexact viewpoints are untold. This ability is essential for humans to identify\nthe same object while moving and to learn from vision efficiently. It is\nintriguing to design models that have a similar ability. In this paper, we\nconsider a novel problem of learning compositional scene representations from\nmultiple unspecified (i.e., unknown and unrelated) viewpoints without using any\nsupervision and propose a deep generative model which separates latent\nrepresentations into a viewpoint-independent part and a viewpoint-dependent\npart to solve this problem. During the inference, latent representations are\nrandomly initialized and iteratively updated by integrating the information in\ndifferent viewpoints with neural networks. Experiments on several specifically\ndesigned synthetic datasets have shown that the proposed method can effectively\nlearn from multiple unspecified viewpoints.\n","authors":["Jinyang Yuan","Tonglin Chen","Zhimeng Shen","Bin Li","Xiangyang Xue"],"pdf_url":"https://arxiv.org/pdf/2401.01922v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2112.03568"},{"id":"http://arxiv.org/abs/2309.15216v2","updated":"2024-01-03T12:52:42Z","published":"2023-09-26T19:21:09Z","title":"Diabetic Retinopathy Using Gaussian Filter","summary":"  The retina is an essential component of the visual system, and maintaining\neyesight depends on the timely and correct detection of disorders. This\nresearch specifically addresses the early-stage detection and severity\nclassification of diabetic retinopathy (DR), a serious public health hazard. We\ncompare the results of different deep learning models such as InceptionV3,\nDenseNet121 and other CNN based models by using different image filters, such\nas Gaussian, grayscale and Gabor. These models could detect subtle pathological\nalterations and use that information to estimate the risk of retinal illnesses.\nThe objective is to improve the diagnostic processes for diabetic retinopathy,\nthe primary cause of diabetes-related blindness, by utilizing deep learning\nmodels. A comparative analysis between Greyscale, Gaussian and Gabor filters\nhas been provided after applying these filters on the retinal images. The\nGaussian filter resulted to be the most promising filter giving the best\naccuracies for all the models. The best performing model was InceptionV3 which\ngave an accuracy of 96% on Gaussian images, therefore Gaussian filter emerged\nas our most promising filter.\n","authors":["Roshan Vasu Muddaluru","Sharvaani Ravikumar Thoguluva","Shruti Prabha","Tanuja Konda Reddy","Dr. Suja P"],"pdf_url":"https://arxiv.org/pdf/2309.15216v2.pdf","comment":"6 pages, 5 figures, conference, 2 tables"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2401.01883v1","updated":"2024-01-03T18:53:22Z","published":"2024-01-03T18:53:22Z","title":"Mining Temporal Attack Patterns from Cyberthreat Intelligence Reports","summary":"  Defending from cyberattacks requires practitioners to operate on high-level\nadversary behavior. Cyberthreat intelligence (CTI) reports on past cyberattack\nincidents describe the chain of malicious actions with respect to time. To\navoid repeating cyberattack incidents, practitioners must proactively identify\nand defend against recurring chain of actions - which we refer to as temporal\nattack patterns. Automatically mining the patterns among actions provides\nstructured and actionable information on the adversary behavior of past\ncyberattacks. The goal of this paper is to aid security practitioners in\nprioritizing and proactive defense against cyberattacks by mining temporal\nattack patterns from cyberthreat intelligence reports. To this end, we propose\nChronoCTI, an automated pipeline for mining temporal attack patterns from\ncyberthreat intelligence (CTI) reports of past cyberattacks. To construct\nChronoCTI, we build the ground truth dataset of temporal attack patterns and\napply state-of-the-art large language models, natural language processing, and\nmachine learning techniques. We apply ChronoCTI on a set of 713 CTI reports,\nwhere we identify 124 temporal attack patterns - which we categorize into nine\npattern categories. We identify that the most prevalent pattern category is to\ntrick victim users into executing malicious code to initiate the attack,\nfollowed by bypassing the anti-malware system in the victim network. Based on\nthe observed patterns, we advocate organizations to train users about\ncybersecurity best practices, introduce immutable operating systems with\nlimited functionalities, and enforce multi-user authentications. Moreover, we\nadvocate practitioners to leverage the automated mining capability of ChronoCTI\nand design countermeasures against the recurring attack patterns.\n","authors":["Md Rayhanur Rahman","Brandon Wroblewski","Quinn Matthews","Brantley Morgan","Tim Menzies","Laurie Williams"],"pdf_url":"https://arxiv.org/pdf/2401.01883v1.pdf","comment":"A modified version of this pre-print is submitted to IEEE\n  Transactions on Software Engineering, and is under review"},{"id":"http://arxiv.org/abs/2401.01835v1","updated":"2024-01-03T17:01:44Z","published":"2024-01-03T17:01:44Z","title":"Concurrent Brainstorming & Hypothesis Satisfying: An Iterative Framework\n  for Enhanced Retrieval-Augmented Generation (R2CBR3H-SR)","summary":"  Addressing the complexity of comprehensive information retrieval, this study\nintroduces an innovative, iterative retrieval-augmented generation system. Our\napproach uniquely integrates a vector-space driven re-ranking mechanism with\nconcurrent brainstorming to expedite the retrieval of highly relevant\ndocuments, thereby streamlining the generation of potential queries. This sets\nthe stage for our novel hybrid process, which synergistically combines\nhypothesis formulation with satisfying decision-making strategy to determine\ncontent adequacy, leveraging a chain of thought-based prompting technique. This\nunified hypothesize-satisfied phase intelligently distills information to\nascertain whether user queries have been satisfactorily addressed. Upon\nreaching this criterion, the system refines its output into a concise\nrepresentation, maximizing conceptual density with minimal verbosity. The\niterative nature of the workflow enhances process efficiency and accuracy.\nCrucially, the concurrency within the brainstorming phase significantly\naccelerates recursive operations, facilitating rapid convergence to solution\nsatisfaction. Compared to conventional methods, our system demonstrates a\nmarked improvement in computational time and cost-effectiveness. This research\nadvances the state-of-the-art in intelligent retrieval systems, setting a new\nbenchmark for resource-efficient information extraction and abstraction in\nknowledge-intensive applications.\n","authors":["Arash Shahmansoori"],"pdf_url":"https://arxiv.org/pdf/2401.01835v1.pdf","comment":"3 pages, 1 table, double column IEEE journal format paper"},{"id":"http://arxiv.org/abs/2401.01825v1","updated":"2024-01-03T16:42:13Z","published":"2024-01-03T16:42:13Z","title":"Physio: An LLM-Based Physiotherapy Advisor","summary":"  The capabilities of the most recent language models have increased the\ninterest in integrating them into real-world applications. However, the fact\nthat these models generate plausible, yet incorrect text poses a constraint\nwhen considering their use in several domains. Healthcare is a prime example of\na domain where text-generative trustworthiness is a hard requirement to\nsafeguard patient well-being. In this paper, we present Physio, a chat-based\napplication for physical rehabilitation. Physio is capable of making an initial\ndiagnosis while citing reliable health sources to support the information\nprovided. Furthermore, drawing upon external knowledge databases, Physio can\nrecommend rehabilitation exercises and over-the-counter medication for symptom\nrelief. By combining these features, Physio can leverage the power of\ngenerative models for language processing while also conditioning its response\non dependable and verifiable sources. A live demo of Physio is available at\nhttps://physio.inesctec.pt.\n","authors":["Rúben Almeida","Hugo Sousa","Luís F. Cunha","Nuno Guimarães","Ricardo Campos","Alípio Jorge"],"pdf_url":"https://arxiv.org/pdf/2401.01825v1.pdf","comment":"Demo, ECIR 2024, 3rd Sword AI challenge 2023"},{"id":"http://arxiv.org/abs/2401.01781v1","updated":"2024-01-03T15:14:11Z","published":"2024-01-03T15:14:11Z","title":"Evaluating Trustworthiness of Online News Publishers via Article\n  Classification","summary":"  The proliferation of low-quality online information in today's era has\nunderscored the need for robust and automatic mechanisms to evaluate the\ntrustworthiness of online news publishers. In this paper, we analyse the\ntrustworthiness of online news media outlets by leveraging a dataset of 4033\nnews stories from 40 different sources. We aim to infer the trustworthiness\nlevel of the source based on the classification of individual articles'\ncontent. The trust labels are obtained from NewsGuard, a journalistic\norganization that evaluates news sources using well-established editorial and\npublishing criteria. The results indicate that the classification model is\nhighly effective in classifying the trustworthiness levels of the news\narticles. This research has practical applications in alerting readers to\npotentially untrustworthy news sources, assisting journalistic organizations in\nevaluating new or unfamiliar media outlets and supporting the selection of\narticles for their trustworthiness assessment.\n","authors":["John Bianchi","Manuel Pratelli","Marinella Petrocchi","Fabio Pinelli"],"pdf_url":"https://arxiv.org/pdf/2401.01781v1.pdf","comment":"This paper will appear in the proceedings of the 2024 ACM/SIGAPP\n  Symposium on Applied Computing, Avila, Spain, April 8-12, 2024. The version\n  here submitted is the accepted version before publisher typesetting"},{"id":"http://arxiv.org/abs/2401.01780v1","updated":"2024-01-03T15:12:42Z","published":"2024-01-03T15:12:42Z","title":"Navigating Uncertainty: Optimizing API Dependency for Hallucination\n  Reduction in Closed-Book Question Answering","summary":"  While Large Language Models (LLM) are able to accumulate and restore\nknowledge, they are still prone to hallucination. Especially when faced with\nfactual questions, LLM cannot only rely on knowledge stored in parameters to\nguarantee truthful and correct answers. Augmenting these models with the\nability to search on external information sources, such as the web, is a\npromising approach to ground knowledge to retrieve information. However,\nsearching in a large collection of documents introduces additional\ncomputational/time costs. An optimal behavior would be to query external\nresources only when the LLM is not confident about answers. In this paper, we\npropose a new LLM able to self-estimate if it is able to answer directly or\nneeds to request an external tool. We investigate a supervised approach by\nintroducing a hallucination masking mechanism in which labels are generated\nusing a close book question-answering task. In addition, we propose to leverage\nparameter-efficient fine-tuning techniques to train our model on a small amount\nof data. Our model directly provides answers for $78.2\\%$ of the known queries\nand opts to search for $77.2\\%$ of the unknown ones. This results in the API\nbeing utilized only $62\\%$ of the time.\n","authors":["Pierre Erbacher","Louis Falissar","Vincent Guigue","Laure Soulier"],"pdf_url":"https://arxiv.org/pdf/2401.01780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01751v1","updated":"2024-01-03T14:06:06Z","published":"2024-01-03T14:06:06Z","title":"Text mining arXiv: a look through quantitative finance papers","summary":"  This paper explores articles hosted on the arXiv preprint server with the aim\nto uncover valuable insights hidden in this vast collection of research.\nEmploying text mining techniques and through the application of natural\nlanguage processing methods, we examine the contents of quantitative finance\npapers posted in arXiv from 1997 to 2022. We extract and analyze crucial\ninformation from the entire documents, including the references, to understand\nthe topics trends over time and to find out the most cited researchers and\njournals on this domain. Additionally, we compare numerous algorithms to\nperform topic modeling, including state-of-the-art approaches.\n","authors":["Michele Leonardo Bianchi"],"pdf_url":"https://arxiv.org/pdf/2401.01751v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01711v1","updated":"2024-01-03T12:28:33Z","published":"2024-01-03T12:28:33Z","title":"Evaluating Large Language Models in Semantic Parsing for Conversational\n  Question Answering over Knowledge Graphs","summary":"  Conversational question answering systems often rely on semantic parsing to\nenable interactive information retrieval, which involves the generation of\nstructured database queries from a natural language input. For\ninformation-seeking conversations about facts stored within a knowledge graph,\ndialogue utterances are transformed into graph queries in a process that is\ncalled knowledge-based conversational question answering. This paper evaluates\nthe performance of large language models that have not been explicitly\npre-trained on this task. Through a series of experiments on an extensive\nbenchmark dataset, we compare models of varying sizes with different prompting\ntechniques and identify common issue types in the generated output. Our results\ndemonstrate that large language models are capable of generating graph queries\nfrom dialogues, with significant improvements achievable through few-shot\nprompting and fine-tuning techniques, especially for smaller models that\nexhibit lower zero-shot performance.\n","authors":["Phillip Schneider","Manuel Klettner","Kristiina Jokinen","Elena Simperl","Florian Matthes"],"pdf_url":"https://arxiv.org/pdf/2401.01711v1.pdf","comment":"Accepted to ICAART 2024"},{"id":"http://arxiv.org/abs/2311.07878v4","updated":"2024-01-03T11:21:39Z","published":"2023-11-14T03:15:48Z","title":"Evaluating LLMs on Document-Based QA: Exact Answer Selection and\n  Numerical Extraction using Cogtale dataset","summary":"  Document-based Question-Answering (QA) tasks are crucial for precise\ninformation retrieval. While some existing work focus on evaluating large\nlanguage models performance on retrieving and answering questions from\ndocuments, assessing the LLMs performance on QA types that require exact answer\nselection from predefined options and numerical extraction is yet to be fully\nassessed. In this paper, we specifically focus on this underexplored context\nand conduct empirical analysis of LLMs (GPT-4 and GPT-3.5) on question types,\nincluding single-choice, yes-no, multiple-choice, and number extraction\nquestions from documents in zero-shot setting. We use the CogTale dataset for\nevaluation, which provide human expert-tagged responses, offering a robust\nbenchmark for precision and factual grounding. We found that LLMs, particularly\nGPT-4, can precisely answer many single-choice and yes-no questions given\nrelevant context, demonstrating their efficacy in information retrieval tasks.\nHowever, their performance diminishes when confronted with multiple-choice and\nnumber extraction formats, lowering the overall performance of the model on\nthis task, indicating that these models may not yet be sufficiently reliable\nfor the task. This limits the applications of LLMs on applications demanding\nprecise information extraction from documents, such as meta-analysis tasks.\nThese findings hinge on the assumption that the retrievers furnish pertinent\ncontext necessary for accurate responses, emphasizing the need for further\nresearch. Our work offers a framework for ongoing dataset evaluation, ensuring\nthat LLM applications for information retrieval and document analysis continue\nto meet evolving standards.\n","authors":["Zafaryab Rasool","Stefanus Kurniawan","Sherwin Balugo","Scott Barnett","Rajesh Vasa","Courtney Chesser","Benjamin M. Hampstead","Sylvie Belleville","Kon Mouzakis","Alex Bahar-Fuchs"],"pdf_url":"https://arxiv.org/pdf/2311.07878v4.pdf","comment":"10 pages, 1 figure"},{"id":"http://arxiv.org/abs/2401.01614v1","updated":"2024-01-03T08:33:09Z","published":"2024-01-03T08:33:09Z","title":"GPT-4V(ision) is a Generalist Web Agent, if Grounded","summary":"  The recent development on large multimodal models (LMMs), especially\nGPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries\nof multimodal models beyond traditional tasks like image captioning and visual\nquestion answering. In this work, we explore the potential of LMMs like GPT-4V\nas a generalist web agent that can follow natural language instructions to\ncomplete tasks on any given website. We propose SEEACT, a generalist web agent\nthat harnesses the power of LMMs for integrated visual understanding and acting\non the web. We evaluate on the recent MIND2WEB benchmark. In addition to\nstandard offline evaluation on cached websites, we enable a new online\nevaluation setting by developing a tool that allows running web agents on live\nwebsites. We show that GPT-4V presents a great potential for web agents - it\ncan successfully complete 50% of the tasks on live websites if we manually\nground its textual plans into actions on the websites. This substantially\noutperforms text-only LLMs like GPT-4 or smaller models (FLAN-T5 and BLIP-2)\nspecifically fine-tuned for web agents. However, grounding still remains a\nmajor challenge. Existing LMM grounding strategies like set-of-mark prompting\nturns out not effective for web agents, and the best grounding strategy we\ndevelop in this paper leverages both the HTML text and visuals. Yet, there is\nstill a substantial gap with oracle grounding, leaving ample room for further\nimprovement.\n","authors":["Boyuan Zheng","Boyu Gou","Jihyung Kil","Huan Sun","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2401.01614v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.15661v3","updated":"2024-01-03T08:06:51Z","published":"2023-12-25T09:09:54Z","title":"Unlocking the Potential of Large Language Models for Explainable\n  Recommendations","summary":"  Generating user-friendly explanations regarding why an item is recommended\nhas become increasingly common, largely due to advances in language generation\ntechnology, which can enhance user trust and facilitate more informed\ndecision-making when using online services. However, existing explainable\nrecommendation systems focus on using small-size language models. It remains\nuncertain what impact replacing the explanation generator with the recently\nemerging large language models (LLMs) would have. Can we expect unprecedented\nresults?\n  In this study, we propose LLMXRec, a simple yet effective two-stage\nexplainable recommendation framework aimed at further boosting the explanation\nquality by employing LLMs. Unlike most existing LLM-based recommendation works,\na key characteristic of LLMXRec is its emphasis on the close collaboration\nbetween previous recommender models and LLM-based explanation generators.\nSpecifically, by adopting several key fine-tuning techniques, including\nparameter-efficient instructing tuning and personalized prompt techniques,\ncontrollable and fluent explanations can be well generated to achieve the goal\nof explanation recommendation. Most notably, we provide three different\nperspectives to evaluate the effectiveness of the explanations. Finally, we\nconduct extensive experiments over several benchmark recommender models and\npublicly available datasets. The experimental results not only yield positive\nresults in terms of effectiveness and efficiency but also uncover some\npreviously unknown outcomes. To facilitate further explorations in this area,\nthe full code and detailed original results are open-sourced at\nhttps://github.com/GodFire66666/LLM_rec_explanation/.\n","authors":["Yucong Luo","Mingyue Cheng","Hao Zhang","Junyu Lu","Qi Liu","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2312.15661v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05195v2","updated":"2024-01-03T07:40:15Z","published":"2023-10-08T15:04:50Z","title":"GMMFormer: Gaussian-Mixture-Model Based Transformer for Efficient\n  Partially Relevant Video Retrieval","summary":"  Given a text query, partially relevant video retrieval (PRVR) seeks to find\nuntrimmed videos containing pertinent moments in a database. For PRVR, clip\nmodeling is essential to capture the partial relationship between texts and\nvideos. Current PRVR methods adopt scanning-based clip construction to achieve\nexplicit clip modeling, which is information-redundant and requires a large\nstorage overhead. To solve the efficiency problem of PRVR methods, this paper\nproposes GMMFormer, a Gaussian-Mixture-Model based Transformer which models\nclip representations implicitly. During frame interactions, we incorporate\nGaussian-Mixture-Model constraints to focus each frame on its adjacent frames\ninstead of the whole video. Then generated representations will contain\nmulti-scale clip information, achieving implicit clip modeling. In addition,\nPRVR methods ignore semantic differences between text queries relevant to the\nsame video, leading to a sparse embedding space. We propose a query diverse\nloss to distinguish these text queries, making the embedding space more\nintensive and contain more semantic information. Extensive experiments on three\nlarge-scale video datasets (i.e., TVR, ActivityNet Captions, and Charades-STA)\ndemonstrate the superiority and efficiency of GMMFormer. Code is available at\n\\url{https://github.com/huangmozhi9527/GMMFormer}.\n","authors":["Yuting Wang","Jinpeng Wang","Bin Chen","Ziyun Zeng","Shu-Tao Xia"],"pdf_url":"https://arxiv.org/pdf/2310.05195v2.pdf","comment":"Accepted by AAAI 2024. Code is released at\n  https://github.com/huangmozhi9527/GMMFormer"},{"id":"http://arxiv.org/abs/2401.01566v1","updated":"2024-01-03T06:44:05Z","published":"2024-01-03T06:44:05Z","title":"Team IELAB at TREC Clinical Trial Track 2023: Enhancing Clinical Trial\n  Retrieval with Neural Rankers and Large Language Models","summary":"  We describe team ielab from CSIRO and The University of Queensland's approach\nto the 2023 TREC Clinical Trials Track. Our approach was to use neural rankers\nbut to utilise Large Language Models to overcome the issue of lack of training\ndata for such rankers. Specifically, we employ ChatGPT to generate relevant\npatient descriptions for randomly selected clinical trials from the corpus.\nThis synthetic dataset, combined with human-annotated training data from\nprevious years, is used to train both dense and sparse retrievers based on\nPubmedBERT. Additionally, a cross-encoder re-ranker is integrated into the\nsystem. To further enhance the effectiveness of our approach, we prompting\nGPT-4 as a TREC annotator to provide judgments on our run files. These\njudgments are subsequently employed to re-rank the results. This architecture\ntightly integrates strong PubmedBERT-based rankers with the aid of SOTA Large\nLanguage Models, demonstrating a new approach to clinical trial retrieval.\n","authors":["Shengyao Zhuang","Bevan Koopman","Guido Zuccon"],"pdf_url":"https://arxiv.org/pdf/2401.01566v1.pdf","comment":"TREC Notebook"},{"id":"http://arxiv.org/abs/2401.01527v1","updated":"2024-01-03T03:40:53Z","published":"2024-01-03T03:40:53Z","title":"Poisoning Attacks against Recommender Systems: A Survey","summary":"  Modern recommender systems have seen substantial success, yet they remain\nvulnerable to malicious activities, notably poisoning attacks. These attacks\ninvolve injecting malicious data into the training datasets of RS, thereby\ncompromising their integrity and manipulating recommendation outcomes for\ngaining illicit profits. This survey paper provides a systematic and up-to-date\nreview of the research landscape on Poisoning Attacks against Recommendation\n(PAR). A novel and comprehensive taxonomy is proposed, categorizing existing\nPAR methodologies into three distinct categories: Component-Specific,\nGoal-Driven, and Capability Probing. For each category, we discuss its\nmechanism in detail, along with associated methods. Furthermore, this paper\nhighlights potential future research avenues in this domain. Additionally, to\nfacilitate and benchmark the empirical comparison of PAR, we introduce an\nopen-source library, ARLib, which encompasses a comprehensive collection of PAR\nmodels and common datasets. The library is released at\n\\url{https://github.com/CoderWZW/ARLib}.\n","authors":["Zongwei Wang","Min Gao","Junliang Yu","Hao Ma","Hongzhi Yin","Shazia Sadiq"],"pdf_url":"https://arxiv.org/pdf/2401.01527v1.pdf","comment":"9 pages,5 figures"},{"id":"http://arxiv.org/abs/2401.01525v1","updated":"2024-01-03T03:34:58Z","published":"2024-01-03T03:34:58Z","title":"Expected Transaction Value Optimization for Precise Marketing in FinTech\n  Platforms","summary":"  FinTech platforms facilitated by digital payments are watching growth\nrapidly, which enable the distribution of mutual funds personalized to\nindividual investors via mobile Apps. As the important intermediation of\nfinancial products investment, these platforms distribute thousands of mutual\nfunds obtaining impressions under guaranteed delivery (GD) strategy required by\nfund companies. Driven by the profit from fund purchases of users, the platform\naims to maximize each transaction amount of customers by promoting mutual funds\nto these investors who will be interested in. Different from the conversions in\ntraditional advertising or e-commerce recommendations, the investment amount in\neach purchase varies greatly even for the same financial product, which\nprovides a significant challenge for the promotion recommendation of mutual\nfunds. In addition to predicting the click-through rate (CTR) or the conversion\nrate (CVR) as in traditional recommendations, it is essential for FinTech\nplatforms to estimate the customers' purchase amount for each delivered fund\nand achieve an effective allocation of impressions based on the predicted\nresults to optimize the total expected transaction value (ETV). In this paper,\nwe propose an ETV optimized customer allocation framework (EOCA) that aims to\nmaximize the total ETV of recommended funds, under the constraints of GD dealt\nwith fund companies. To the best of our knowledge, it's the first attempt to\nsolve the GD problem for financial product promotions based on customer\npurchase amount prediction. We conduct extensive experiments on large scale\nreal-world datasets and online tests based on LiCaiTong, Tencent wealth\nmanagement platform, to demonstrate the effectiveness of our proposed EOCA\nframework.\n","authors":["Yunpeng Weng","Xing Tang","Liang Chen","Dugang Liu","Xiuqiang He"],"pdf_url":"https://arxiv.org/pdf/2401.01525v1.pdf","comment":"Accepted by Workshop on Deep Learning Practice for High-Dimensional\n  Sparse Data in RecSys'23 (DLP@RecSys), Singapore, 2023"},{"id":"http://arxiv.org/abs/2401.01511v1","updated":"2024-01-03T02:32:55Z","published":"2024-01-03T02:32:55Z","title":"Enhancing Multilingual Information Retrieval in Mixed Human Resources\n  Environments: A RAG Model Implementation for Multicultural Enterprise","summary":"  The advent of Large Language Models has revolutionized information retrieval,\nushering in a new era of expansive knowledge accessibility. While these models\nexcel in providing open-world knowledge, effectively extracting answers in\ndiverse linguistic environments with varying levels of literacy remains a\nformidable challenge. Retrieval Augmented Generation (RAG) emerges as a\npromising solution, bridging the gap between information availability and\nmultilingual comprehension. However, deploying RAG models in real-world\nscenarios demands careful consideration of various factors. This paper\naddresses the critical challenges associated with implementing RAG models in\nmulticultural environments. We delve into essential considerations, including\ndata feeding strategies, timely updates, mitigation of hallucinations,\nprevention of erroneous responses, and optimization of delivery speed. Our work\ninvolves the integration of a diverse array of tools, meticulously combined to\nfacilitate the seamless adoption of RAG models across languages and literacy\nlevels within a multicultural organizational context. Through strategic tweaks\nin our approaches, we achieve not only effectiveness but also efficiency,\nensuring the accelerated and accurate delivery of information in a manner that\nis tailored to the unique requirements of multilingual and multicultural\nsettings.\n","authors":["Syed Rameel Ahmad"],"pdf_url":"https://arxiv.org/pdf/2401.01511v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01497v1","updated":"2024-01-03T02:02:58Z","published":"2024-01-03T02:02:58Z","title":"A Pre-trained Sequential Recommendation Framework: Popularity Dynamics\n  for Zero-shot Transfer","summary":"  Sequential recommenders are crucial to the success of online applications,\n\\eg e-commerce, video streaming, and social media. While model architectures\ncontinue to improve, for every new application domain, we still have to train a\nnew model from scratch for high quality recommendations. On the other hand,\npre-trained language and vision models have shown great success in zero-shot or\nfew-shot adaptation to new application domains. Inspired by the success of\npre-trained models in peer AI fields, we propose a novel pre-trained sequential\nrecommendation framework: PrepRec. We learn universal item representations by\nmodeling item popularity dynamics. Through extensive experiments on five\nreal-world datasets, we show that PrepRec, without any auxiliary information,\ncan not only zero-shot transfer to a new domain, but achieve competitive\nperformance compared to state-of-the-art sequential recommender models with\nonly a fraction of the model size. In addition, with a simple post-hoc\ninterpolation, PrepRec can improve the performance of existing sequential\nrecommenders on average by 13.8\\% in Recall@10 and 29.5% in NDCG@10. We provide\nan anonymized implementation of PrepRec at\nhttps://anonymous.4open.science/r/PrepRec--2F60/\n","authors":["Junting Wang","Praneet Rathi","Hari Sundaram"],"pdf_url":"https://arxiv.org/pdf/2401.01497v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01472v1","updated":"2024-01-03T00:13:52Z","published":"2024-01-03T00:13:52Z","title":"A First Look at Information Highlighting in Stack Overflow Answers","summary":"  Context: Navigating the knowledge of Stack Overflow (SO) remains challenging.\nTo make the posts vivid to users, SO allows users to write and edit posts with\nMarkdown or HTML so that users can leverage various formatting styles (e.g.,\nbold, italic, and code) to highlight the important information. Nonetheless,\nthere have been limited studies on the highlighted information. Objective: We\ncarried out the first large-scale exploratory study on the information\nhighlighted in SO answers in our recent study. To extend our previous study, we\ndevelop approaches to automatically recommend highlighted content with\nformatting styles using neural network architectures initially designed for the\nNamed Entity Recognition task. Method: In this paper, we studied 31,169,429\nanswers of Stack Overflow. For training recommendation models, we choose CNN\nand BERT models for each type of formatting (i.e., Bold, Italic, Code, and\nHeading) using the information highlighting dataset we collected from SO\nanswers. Results: Our models based on CNN architecture achieve precision\nranging from 0.71 to 0.82. The trained model for automatic code content\nhighlighting achieves a recall of 0.73 and an F1 score of 0.71, outperforming\nthe trained models for other formatting styles. The BERT models have even lower\nrecalls and F1 scores than the CNN models. Our analysis of failure cases\nindicates that the majority of the failure cases are missing identification\n(i.e., the model misses the content that is supposed to be highlighted) due to\nthe models tend to learn the frequently highlighted words while struggling to\nlearn less frequent words. Conclusion: Our findings suggest that it is possible\nto develop recommendation models for highlighting information for answers with\ndifferent formatting styles on Stack Overflow.\n","authors":["Shahla Shaan Ahmed","Shaowei Wang","Yuan Tian"," Tse-Hsun"," Chen","Haoxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.01472v1.pdf","comment":"This work is submitted to Information and Software Technology Journal"},{"id":"http://arxiv.org/abs/2310.19251v3","updated":"2024-01-03T22:33:05Z","published":"2023-10-30T03:37:32Z","title":"Pre-trained Recommender Systems: A Causal Debiasing Perspective","summary":"  Recent studies on pre-trained vision/language models have demonstrated the\npractical benefit of a new, promising solution-building paradigm in AI where\nmodels can be pre-trained on broad data describing a generic task space and\nthen adapted successfully to solve a wide range of downstream tasks, even when\ntraining data is severely limited (e.g., in zero- or few-shot learning\nscenarios). Inspired by such progress, we investigate in this paper the\npossibilities and challenges of adapting such a paradigm to the context of\nrecommender systems, which is less investigated from the perspective of\npre-trained model. In particular, we propose to develop a generic recommender\nthat captures universal interaction patterns by training on generic user-item\ninteraction data extracted from different domains, which can then be fast\nadapted to improve few-shot learning performance in unseen new domains (with\nlimited data).\n  However, unlike vision/language data which share strong conformity in the\nsemantic space, universal patterns underlying recommendation data collected\nacross different domains (e.g., different countries or different E-commerce\nplatforms) are often occluded by both in-domain and cross-domain biases\nimplicitly imposed by the cultural differences in their user and item bases, as\nwell as their uses of different e-commerce platforms. As shown in our\nexperiments, such heterogeneous biases in the data tend to hinder the\neffectiveness of the pre-trained model. To address this challenge, we further\nintroduce and formalize a causal debiasing perspective, which is substantiated\nvia a hierarchical Bayesian deep learning model, named PreRec. Our empirical\nstudies on real-world data show that the proposed model could significantly\nimprove the recommendation performance in zero- and few-shot learning settings\nunder both cross-market and cross-platform scenarios.\n","authors":["Ziqian Lin","Hao Ding","Nghia Hoang","Branislav Kveton","Anoop Deoras","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2310.19251v3.pdf","comment":"8 pages, WSDM 24"},{"id":"http://arxiv.org/abs/2401.01978v1","updated":"2024-01-03T20:58:03Z","published":"2024-01-03T20:58:03Z","title":"Tailor: Size Recommendations for High-End Fashion Marketplaces","summary":"  In the ever-changing and dynamic realm of high-end fashion marketplaces,\nproviding accurate and personalized size recommendations has become a critical\naspect. Meeting customer expectations in this regard is not only crucial for\nensuring their satisfaction but also plays a pivotal role in driving customer\nretention, which is a key metric for the success of any fashion retailer. We\npropose a novel sequence classification approach to address this problem,\nintegrating implicit (Add2Bag) and explicit (ReturnReason) user signals. Our\napproach comprises two distinct models: one employs LSTMs to encode the user\nsignals, while the other leverages an Attention mechanism. Our best model\noutperforms SFNet, improving accuracy by 45.7%. By using Add2Bag interactions\nwe increase the user coverage by 24.5% when compared with only using Orders.\nMoreover, we evaluate the models' usability in real-time recommendation\nscenarios by conducting experiments to measure their latency performance.\n","authors":["Alexandre Candeias","Ivo Silva","Vitor Sousa","José Marcelino"],"pdf_url":"https://arxiv.org/pdf/2401.01978v1.pdf","comment":"Accepted in FashionXRecsys23 held at the 17th ACM Conference on\n  Recommender Systems, 18th-22nd September 2023"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2401.01883v1","updated":"2024-01-03T18:53:22Z","published":"2024-01-03T18:53:22Z","title":"Mining Temporal Attack Patterns from Cyberthreat Intelligence Reports","summary":"  Defending from cyberattacks requires practitioners to operate on high-level\nadversary behavior. Cyberthreat intelligence (CTI) reports on past cyberattack\nincidents describe the chain of malicious actions with respect to time. To\navoid repeating cyberattack incidents, practitioners must proactively identify\nand defend against recurring chain of actions - which we refer to as temporal\nattack patterns. Automatically mining the patterns among actions provides\nstructured and actionable information on the adversary behavior of past\ncyberattacks. The goal of this paper is to aid security practitioners in\nprioritizing and proactive defense against cyberattacks by mining temporal\nattack patterns from cyberthreat intelligence reports. To this end, we propose\nChronoCTI, an automated pipeline for mining temporal attack patterns from\ncyberthreat intelligence (CTI) reports of past cyberattacks. To construct\nChronoCTI, we build the ground truth dataset of temporal attack patterns and\napply state-of-the-art large language models, natural language processing, and\nmachine learning techniques. We apply ChronoCTI on a set of 713 CTI reports,\nwhere we identify 124 temporal attack patterns - which we categorize into nine\npattern categories. We identify that the most prevalent pattern category is to\ntrick victim users into executing malicious code to initiate the attack,\nfollowed by bypassing the anti-malware system in the victim network. Based on\nthe observed patterns, we advocate organizations to train users about\ncybersecurity best practices, introduce immutable operating systems with\nlimited functionalities, and enforce multi-user authentications. Moreover, we\nadvocate practitioners to leverage the automated mining capability of ChronoCTI\nand design countermeasures against the recurring attack patterns.\n","authors":["Md Rayhanur Rahman","Brandon Wroblewski","Quinn Matthews","Brantley Morgan","Tim Menzies","Laurie Williams"],"pdf_url":"https://arxiv.org/pdf/2401.01883v1.pdf","comment":"A modified version of this pre-print is submitted to IEEE\n  Transactions on Software Engineering, and is under review"},{"id":"http://arxiv.org/abs/2305.17033v3","updated":"2024-01-03T18:41:04Z","published":"2023-05-26T15:40:11Z","title":"The Brain Tumor Segmentation (BraTS) Challenge 2023: Focus on Pediatrics\n  (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs)","summary":"  Pediatric tumors of the central nervous system are the most common cause of\ncancer-related death in children. The five-year survival rate for high-grade\ngliomas in children is less than 20\\%. Due to their rarity, the diagnosis of\nthese entities is often delayed, their treatment is mainly based on historic\ntreatment concepts, and clinical trials require multi-institutional\ncollaborations. The MICCAI Brain Tumor Segmentation (BraTS) Challenge is a\nlandmark community benchmark event with a successful history of 12 years of\nresource creation for the segmentation and analysis of adult glioma. Here we\npresent the CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023 challenge, which\nrepresents the first BraTS challenge focused on pediatric brain tumors with\ndata acquired across multiple international consortia dedicated to pediatric\nneuro-oncology and clinical trials. The BraTS-PEDs 2023 challenge focuses on\nbenchmarking the development of volumentric segmentation algorithms for\npediatric brain glioma through standardized quantitative performance evaluation\nmetrics utilized across the BraTS 2023 cluster of challenges. Models gaining\nknowledge from the BraTS-PEDs multi-parametric structural MRI (mpMRI) training\ndata will be evaluated on separate validation and unseen test mpMRI dataof\nhigh-grade pediatric glioma. The CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023\nchallenge brings together clinicians and AI/imaging scientists to lead to\nfaster development of automated segmentation techniques that could benefit\nclinical trials, and ultimately the care of children with brain tumors.\n","authors":["Anahita Fathi Kazerooni","Nastaran Khalili","Xinyang Liu","Debanjan Haldar","Zhifan Jiang","Syed Muhammed Anwar","Jake Albrecht","Maruf Adewole","Udunna Anazodo","Hannah Anderson","Sina Bagheri","Ujjwal Baid","Timothy Bergquist","Austin J. Borja","Evan Calabrese","Verena Chung","Gian-Marco Conte","Farouk Dako","James Eddy","Ivan Ezhov","Ariana Familiar","Keyvan Farahani","Shuvanjan Haldar","Juan Eugenio Iglesias","Anastasia Janas","Elaine Johansen","Blaise V Jones","Florian Kofler","Dominic LaBella","Hollie Anne Lai","Koen Van Leemput","Hongwei Bran Li","Nazanin Maleki","Aaron S McAllister","Zeke Meier","Bjoern Menze","Ahmed W Moawad","Khanak K Nandolia","Julija Pavaine","Marie Piraud","Tina Poussaint","Sanjay P Prabhu","Zachary Reitman","Andres Rodriguez","Jeffrey D Rudie","Ibraheem Salman Shaikh","Lubdha M. Shah","Nakul Sheth","Russel Taki Shinohara","Wenxin Tu","Karthik Viswanathan","Chunhao Wang","Jeffrey B Ware","Benedikt Wiestler","Walter Wiggins","Anna Zapaishchykova","Mariam Aboian","Miriam Bornhorst","Peter de Blank","Michelle Deutsch","Maryam Fouladi","Lindsey Hoffman","Benjamin Kann","Margot Lazow","Leonie Mikael","Ali Nabavizadeh","Roger Packer","Adam Resnick","Brian Rood","Arastoo Vossough","Spyridon Bakas","Marius George Linguraru"],"pdf_url":"https://arxiv.org/pdf/2305.17033v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01879v1","updated":"2024-01-03T18:39:13Z","published":"2024-01-03T18:39:13Z","title":"Theoretical guarantees on the best-of-n alignment policy","summary":"  A simple and effective method for the alignment of generative models is the\nbest-of-$n$ policy, where $n$ samples are drawn from a base policy, and ranked\nbased on a reward function, and the highest ranking one is selected. A commonly\nused analytical expression in the literature claims that the KL divergence\nbetween the best-of-$n$ policy and the base policy is equal to $\\log (n) -\n(n-1)/n.$ We disprove the validity of this claim, and show that it is an upper\nbound on the actual KL divergence. We also explore the tightness of this upper\nbound in different regimes. Finally, we propose a new estimator for the KL\ndivergence and empirically show that it provides a tight approximation through\na few examples.\n","authors":["Ahmad Beirami","Alekh Agarwal","Jonathan Berant","Alexander D'Amour","Jacob Eisenstein","Chirag Nagpal","Ananda Theertha Suresh"],"pdf_url":"https://arxiv.org/pdf/2401.01879v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01874v1","updated":"2024-01-03T18:32:25Z","published":"2024-01-03T18:32:25Z","title":"Graph Neural Networks for Surfactant Multi-Property Prediction","summary":"  Surfactants are of high importance in different industrial sectors such as\ncosmetics, detergents, oil recovery and drug delivery systems. Therefore, many\nquantitative structure-property relationship (QSPR) models have been developed\nfor surfactants. Each predictive model typically focuses on one surfactant\nclass, mostly nonionics. Graph Neural Networks (GNNs) have exhibited a great\npredictive performance for property prediction of ionic liquids, polymers and\ndrugs in general. Specifically for surfactants, GNNs can successfully predict\ncritical micelle concentration (CMC), a key surfactant property associated with\nmicellization. A key factor in the predictive ability of QSPR and GNN models is\nthe data available for training. Based on extensive literature search, we\ncreate the largest available CMC database with 429 molecules and the first\nlarge data collection for surface excess concentration ($\\Gamma$$_{m}$),\nanother surfactant property associated with foaming, with 164 molecules. Then,\nwe develop GNN models to predict the CMC and $\\Gamma$$_{m}$ and we explore\ndifferent learning approaches, i.e., single- and multi-task learning, as well\nas different training strategies, namely ensemble and transfer learning. We\nfind that a multi-task GNN with ensemble learning trained on all $\\Gamma$$_{m}$\nand CMC data performs best. Finally, we test the ability of our CMC model to\ngeneralize on industrial grade pure component surfactants. The GNN yields\nhighly accurate predictions for CMC, showing great potential for future\nindustrial applications.\n","authors":["Christoforos Brozos","Jan G. Rittig","Sandip Bhattacharya","Elie Akanny","Christina Kohlmann","Alexander Mitsos"],"pdf_url":"https://arxiv.org/pdf/2401.01874v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.03421v5","updated":"2024-01-03T18:32:00Z","published":"2023-02-07T12:11:59Z","title":"A unified recipe for deriving (time-uniform) PAC-Bayes bounds","summary":"  We present a unified framework for deriving PAC-Bayesian generalization\nbounds. Unlike most previous literature on this topic, our bounds are\nanytime-valid (i.e., time-uniform), meaning that they hold at all stopping\ntimes, not only for a fixed sample size. Our approach combines four tools in\nthe following order: (a) nonnegative supermartingales or reverse\nsubmartingales, (b) the method of mixtures, (c) the Donsker-Varadhan formula\n(or other convex duality principles), and (d) Ville's inequality. Our main\nresult is a PAC-Bayes theorem which holds for a wide class of discrete\nstochastic processes. We show how this result implies time-uniform versions of\nwell-known classical PAC-Bayes bounds, such as those of Seeger, McAllester,\nMaurer, and Catoni, in addition to many recent bounds. We also present several\nnovel bounds. Our framework also enables us to relax traditional assumptions;\nin particular, we consider nonstationary loss functions and non-i.i.d. data. In\nsum, we unify the derivation of past bounds and ease the search for future\nbounds: one may simply check if our supermartingale or submartingale conditions\nare met and, if so, be guaranteed a (time-uniform) PAC-Bayes bound.\n","authors":["Ben Chugg","Hongjian Wang","Aaditya Ramdas"],"pdf_url":"https://arxiv.org/pdf/2302.03421v5.pdf","comment":"56 pages. Published in the Journal of Machine Learning Research,\n  Volume 24 Issue 372"},{"id":"http://arxiv.org/abs/2401.01869v1","updated":"2024-01-03T18:24:18Z","published":"2024-01-03T18:24:18Z","title":"On the hardness of learning under symmetries","summary":"  We study the problem of learning equivariant neural networks via gradient\ndescent. The incorporation of known symmetries (\"equivariance\") into neural\nnets has empirically improved the performance of learning pipelines, in domains\nranging from biology to computer vision. However, a rich yet separate line of\nlearning theoretic research has demonstrated that actually learning shallow,\nfully-connected (i.e. non-symmetric) networks has exponential complexity in the\ncorrelational statistical query (CSQ) model, a framework encompassing gradient\ndescent. In this work, we ask: are known problem symmetries sufficient to\nalleviate the fundamental hardness of learning neural nets with gradient\ndescent? We answer this question in the negative. In particular, we give lower\nbounds for shallow graph neural networks, convolutional networks, invariant\npolynomials, and frame-averaged networks for permutation subgroups, which all\nscale either superpolynomially or exponentially in the relevant input\ndimension. Therefore, in spite of the significant inductive bias imparted via\nsymmetry, actually learning the complete classes of functions represented by\nequivariant neural networks via gradient descent remains hard.\n","authors":["Bobak T. Kiani","Thien Le","Hannah Lawrence","Stefanie Jegelka","Melanie Weber"],"pdf_url":"https://arxiv.org/pdf/2401.01869v1.pdf","comment":"52 pages, 4 figures"},{"id":"http://arxiv.org/abs/2401.01867v1","updated":"2024-01-03T18:19:51Z","published":"2024-01-03T18:19:51Z","title":"Dataset Difficulty and the Role of Inductive Bias","summary":"  Motivated by the goals of dataset pruning and defect identification, a\ngrowing body of methods have been developed to score individual examples within\na dataset. These methods, which we call \"example difficulty scores\", are\ntypically used to rank or categorize examples, but the consistency of rankings\nbetween different training runs, scoring methods, and model architectures is\ngenerally unknown. To determine how example rankings vary due to these random\nand controlled effects, we systematically compare different formulations of\nscores over a range of runs and model architectures. We find that scores\nlargely share the following traits: they are noisy over individual runs of a\nmodel, strongly correlated with a single notion of difficulty, and reveal\nexamples that range from being highly sensitive to insensitive to the inductive\nbiases of certain model architectures. Drawing from statistical genetics, we\ndevelop a simple method for fingerprinting model architectures using a few\nsensitive examples. These findings guide practitioners in maximizing the\nconsistency of their scores (e.g. by choosing appropriate scoring methods,\nnumber of runs, and subsets of examples), and establishes comprehensive\nbaselines for evaluating scores in the future.\n","authors":["Devin Kwok","Nikhil Anand","Jonathan Frankle","Gintare Karolina Dziugaite","David Rolnick"],"pdf_url":"https://arxiv.org/pdf/2401.01867v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2401.01862v1","updated":"2024-01-03T18:09:33Z","published":"2024-01-03T18:09:33Z","title":"A Vision Check-up for Language Models","summary":"  What does learning to model relationships between strings teach large\nlanguage models (LLMs) about the visual world? We systematically evaluate LLMs'\nabilities to generate and recognize an assortment of visual concepts of\nincreasing complexity and then demonstrate how a preliminary visual\nrepresentation learning system can be trained using models of text. As language\nmodels lack the ability to consume or output visual information as pixels, we\nuse code to represent images in our study. Although LLM-generated images do not\nlook like natural images, results on image generation and the ability of models\nto correct these generated images indicate that precise modeling of strings can\nteach language models about numerous aspects of the visual world. Furthermore,\nexperiments on self-supervised visual representation learning, utilizing images\ngenerated with text models, highlight the potential to train vision models\ncapable of making semantic assessments of natural images using just LLMs.\n","authors":["Pratyusha Sharma","Tamar Rott Shaham","Manel Baradad","Stephanie Fu","Adrian Rodriguez-Munoz","Shivam Duggal","Phillip Isola","Antonio Torralba"],"pdf_url":"https://arxiv.org/pdf/2401.01862v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01857v1","updated":"2024-01-03T18:02:13Z","published":"2024-01-03T18:02:13Z","title":"Optimal cross-learning for contextual bandits with unknown context\n  distributions","summary":"  We consider the problem of designing contextual bandit algorithms in the\n``cross-learning'' setting of Balseiro et al., where the learner observes the\nloss for the action they play in all possible contexts, not just the context of\nthe current round. We specifically consider the setting where losses are chosen\nadversarially and contexts are sampled i.i.d. from an unknown distribution. In\nthis setting, we resolve an open problem of Balseiro et al. by providing an\nefficient algorithm with a nearly tight (up to logarithmic factors) regret\nbound of $\\widetilde{O}(\\sqrt{TK})$, independent of the number of contexts. As\na consequence, we obtain the first nearly tight regret bounds for the problems\nof learning to bid in first-price auctions (under unknown value distributions)\nand sleeping bandits with a stochastic action set.\n  At the core of our algorithm is a novel technique for coordinating the\nexecution of a learning algorithm over multiple epochs in such a way to remove\ncorrelations between estimation of the unknown distribution and the actions\nplayed by the algorithm. This technique may be of independent interest for\nother learning problems involving estimation of an unknown context\ndistribution.\n","authors":["Jon Schneider","Julian Zimmert"],"pdf_url":"https://arxiv.org/pdf/2401.01857v1.pdf","comment":"Appeared at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2401.01855v1","updated":"2024-01-03T17:51:16Z","published":"2024-01-03T17:51:16Z","title":"Transformer Neural Autoregressive Flows","summary":"  Density estimation, a central problem in machine learning, can be performed\nusing Normalizing Flows (NFs). NFs comprise a sequence of invertible\ntransformations, that turn a complex target distribution into a simple one, by\nexploiting the change of variables theorem. Neural Autoregressive Flows (NAFs)\nand Block Neural Autoregressive Flows (B-NAFs) are arguably the most perfomant\nmembers of the NF family. However, they suffer scalability issues and training\ninstability due to the constraints imposed on the network structure. In this\npaper, we propose a novel solution to these challenges by exploiting\ntransformers to define a new class of neural flows called Transformer Neural\nAutoregressive Flows (T-NAFs). T-NAFs treat each dimension of a random variable\nas a separate input token, using attention masking to enforce an autoregressive\nconstraint. We take an amortization-inspired approach where the transformer\noutputs the parameters of an invertible transformation. The experimental\nresults demonstrate that T-NAFs consistently match or outperform NAFs and\nB-NAFs across multiple datasets from the UCI benchmark. Remarkably, T-NAFs\nachieve these results using an order of magnitude fewer parameters than\nprevious approaches, without composing multiple flows.\n","authors":["Massimiliano Patacchiola","Aliaksandra Shysheya","Katja Hofmann","Richard E. Turner"],"pdf_url":"https://arxiv.org/pdf/2401.01855v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01854v1","updated":"2024-01-03T17:48:10Z","published":"2024-01-03T17:48:10Z","title":"Multilingual Instruction Tuning With Just a Pinch of Multilinguality","summary":"  As instruction-tuned large language models (LLMs) gain global adoption, their\nability to follow instructions in multiple languages becomes increasingly\ncrucial. One promising approach is cross-lingual transfer, where a model\nacquires specific functionality on some language by finetuning on another\nlanguage. In this work, we investigate how multilinguality during instruction\ntuning of a multilingual LLM affects instruction-following across languages. We\nfirst show that many languages transfer some instruction-following capabilities\nto other languages from even monolingual tuning. Furthermore, we find that only\n40 multilingual examples in an English tuning set substantially improve\nmultilingual instruction-following, both in seen and unseen languages during\ntuning. In general, we observe that models tuned on multilingual mixtures\nexhibit comparable or superior performance in several languages compared to\nmonolingually tuned models, despite training on 10x fewer examples in those\nlanguages. Finally, we find that increasing the number of languages in the\ninstruction tuning set from 1 to only 2, 3, or 4 increases cross-lingual\ngeneralization. Our results suggest that building massively multilingual\ninstruction-tuned models can be done with only a very small set of multilingual\ninstruction-responses.\n","authors":["Uri Shaham","Jonathan Herzig","Roee Aharoni","Idan Szpektor","Reut Tsarfaty","Matan Eyal"],"pdf_url":"https://arxiv.org/pdf/2401.01854v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01851v1","updated":"2024-01-03T17:44:17Z","published":"2024-01-03T17:44:17Z","title":"The Power of Training: How Different Neural Network Setups Influence the\n  Energy Demand","summary":"  This work examines the effects of variations in machine learning training\nregimes and learning paradigms on the corresponding energy consumption. While\nincreasing data availability and innovation in high-performance hardware fuels\nthe training of sophisticated models, it also supports the fading perception of\nenergy consumption and carbon emission. Therefore, the goal of this work is to\ncreate awareness about the energy impact of general training parameters and\nprocesses, from learning rate over batch size to knowledge transfer. Multiple\nsetups with different hyperparameter initializations are evaluated on two\ndifferent hardware configurations to obtain meaningful results. Experiments on\npretraining and multitask training are conducted on top of the baseline results\nto determine their potential towards sustainable machine learning.\n","authors":["Daniel Geißler","Bo Zhou","Mengxi Liu","Sungho Suh","Paul Lukowicz"],"pdf_url":"https://arxiv.org/pdf/2401.01851v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.00859v3","updated":"2024-01-03T17:43:35Z","published":"2023-07-03T08:58:32Z","title":"CardiGraphormer: Unveiling the Power of Self-Supervised Learning in\n  Revolutionizing Drug Discovery","summary":"  In the expansive realm of drug discovery, with approximately 15,000 known\ndrugs and only around 4,200 approved, the combinatorial nature of the chemical\nspace presents a formidable challenge. While Artificial Intelligence (AI) has\nemerged as a powerful ally, traditional AI frameworks face significant hurdles.\nThis manuscript introduces CardiGraphormer, a groundbreaking approach that\nsynergizes self-supervised learning (SSL), Graph Neural Networks (GNNs), and\nCardinality Preserving Attention to revolutionize drug discovery.\nCardiGraphormer, a novel combination of Graphormer and Cardinality Preserving\nAttention, leverages SSL to learn potent molecular representations and employs\nGNNs to extract molecular fingerprints, enhancing predictive performance and\ninterpretability while reducing computation time. It excels in handling complex\ndata like molecular structures and performs tasks associated with nodes, pairs\nof nodes, subgraphs, or entire graph structures. CardiGraphormer's potential\napplications in drug discovery and drug interactions are vast, from identifying\nnew drug targets to predicting drug-to-drug interactions and enabling novel\ndrug discovery. This innovative approach provides an AI-enhanced methodology in\ndrug development, utilizing SSL combined with GNNs to overcome existing\nlimitations and pave the way for a richer exploration of the vast combinatorial\nchemical space in drug discovery.\n","authors":["Abhijit Gupta"],"pdf_url":"https://arxiv.org/pdf/2307.00859v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.10962v2","updated":"2024-01-03T17:38:09Z","published":"2022-08-22T14:27:57Z","title":"Prediction of good reaction coordinates and future evolution of MD\n  trajectories using Regularized Sparse Autoencoders: A novel deep learning\n  approach","summary":"  Identifying reaction coordinates(RCs) is an active area of research, given\nthe crucial role RCs play in determining the progress of a chemical reaction.\nThe choice of the reaction coordinate is often based on heuristic knowledge.\nHowever, an essential criterion for the choice is that the coordinate should\ncapture both the reactant and product states unequivocally. Also, the\ncoordinate should be the slowest one so that all the other degrees of freedom\ncan easily equilibrate along the reaction coordinate. Also, the coordinate\nshould be the slowest one so that all the other degrees of freedom can easily\nequilibrate along the reaction coordinate. We used a regularised sparse\nautoencoder, an energy-based model, to discover a crucial set of reaction\ncoordinates. Along with discovering reaction coordinates, our model also\npredicts the evolution of a molecular dynamics(MD) trajectory. We showcased\nthat including sparsity enforcing regularisation helps in choosing a small but\nimportant set of reaction coordinates. We used two model systems to demonstrate\nour approach: alanine dipeptide system and proflavine and DNA system, which\nexhibited intercalation of proflavine into DNA minor groove in an aqueous\nenvironment. We model MD trajectory as a multivariate time series, and our\nlatent variable model performs the task of multi-step time series prediction.\nThis idea is inspired by the popular sparse coding approach - to represent each\ninput sample as a linear combination of few elements taken from a set of\nrepresentative patterns.\n","authors":["Abhijit Gupta"],"pdf_url":"https://arxiv.org/pdf/2208.10962v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01846v1","updated":"2024-01-03T17:36:27Z","published":"2024-01-03T17:36:27Z","title":"DGDNN: Decoupled Graph Diffusion Neural Network for Stock Movement\n  Prediction","summary":"  Forecasting future stock trends remains challenging for academia and industry\ndue to stochastic inter-stock dynamics and hierarchical intra-stock dynamics\ninfluencing stock prices. In recent years, graph neural networks have achieved\nremarkable performance in this problem by formulating multiple stocks as\ngraph-structured data. However, most of these approaches rely on artificially\ndefined factors to construct static stock graphs, which fail to capture the\nintrinsic interdependencies between stocks that rapidly evolve. In addition,\nthese methods often ignore the hierarchical features of the stocks and lose\ndistinctive information within. In this work, we propose a novel graph learning\napproach implemented without expert knowledge to address these issues. First,\nour approach automatically constructs dynamic stock graphs by entropy-driven\nedge generation from a signal processing perspective. Then, we further learn\ntask-optimal dependencies between stocks via a generalized graph diffusion\nprocess on constructed stock graphs. Last, a decoupled representation learning\nscheme is adopted to capture distinctive hierarchical intra-stock features.\nExperimental results demonstrate substantial improvements over state-of-the-art\nbaselines on real-world datasets. Moreover, the ablation study and sensitivity\nstudy further illustrate the effectiveness of the proposed method in modeling\nthe time-evolving inter-stock and intra-stock dynamics.\n","authors":["Zinuo You","Zijian Shi","Hongbo Bo","John Cartlidge","Li Zhang","Yan Ge"],"pdf_url":"https://arxiv.org/pdf/2401.01846v1.pdf","comment":"12 pages, 5 figures, author manuscript accepted for ICAART 2024\n  (International Conference on Agents and Artificial Intelligence)"},{"id":"http://arxiv.org/abs/2401.01843v1","updated":"2024-01-03T17:22:48Z","published":"2024-01-03T17:22:48Z","title":"Investigating Semi-Supervised Learning Algorithms in Text Datasets","summary":"  Using large training datasets enhances the generalization capabilities of\nneural networks. Semi-supervised learning (SSL) is useful when there are few\nlabeled data and a lot of unlabeled data. SSL methods that use data\naugmentation are most successful for image datasets. In contrast, texts do not\nhave consistent augmentation methods as images. Consequently, methods that use\naugmentation are not as effective in text data as they are in image data. In\nthis study, we compared SSL algorithms that do not require augmentation; these\nare self-training, co-training, tri-training, and tri-training with\ndisagreement. In the experiments, we used 4 different text datasets for\ndifferent tasks. We examined the algorithms from a variety of perspectives by\nasking experiment questions and suggested several improvements. Among the\nalgorithms, tri-training with disagreement showed the closest performance to\nthe Oracle; however, performance gap shows that new semi-supervised algorithms\nor improvements in existing methods are needed.\n","authors":["Himmet Toprak Kesgin","Mehmet Fatih Amasyali"],"pdf_url":"https://arxiv.org/pdf/2401.01843v1.pdf","comment":"Innovations in Intelligent Systems and Applications Conference (ASYU)"},{"id":"http://arxiv.org/abs/2401.01842v1","updated":"2024-01-03T17:20:27Z","published":"2024-01-03T17:20:27Z","title":"Wasserstein Nonnegative Tensor Factorization with Manifold\n  Regularization","summary":"  Nonnegative tensor factorization (NTF) has become an important tool for\nfeature extraction and part-based representation with preserved intrinsic\nstructure information from nonnegative high-order data. However, the original\nNTF methods utilize Euclidean or Kullback-Leibler divergence as the loss\nfunction which treats each feature equally leading to the neglect of the\nside-information of features. To utilize correlation information of features\nand manifold information of samples, we introduce Wasserstein manifold\nnonnegative tensor factorization (WMNTF), which minimizes the Wasserstein\ndistance between the distribution of input tensorial data and the distribution\nof reconstruction. Although some researches about Wasserstein distance have\nbeen proposed in nonnegative matrix factorization (NMF), they ignore the\nspatial structure information of higher-order data. We use Wasserstein distance\n(a.k.a Earth Mover's distance or Optimal Transport distance) as a metric and\nadd a graph regularizer to a latent factor. Experimental results demonstrate\nthe effectiveness of the proposed method compared with other NMF and NTF\nmethods.\n","authors":["Jianyu Wang","Linruize Tang"],"pdf_url":"https://arxiv.org/pdf/2401.01842v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01841v1","updated":"2024-01-03T17:19:54Z","published":"2024-01-03T17:19:54Z","title":"Act as You Learn: Adaptive Decision-Making in Non-Stationary Markov\n  Decision Processes","summary":"  A fundamental (and largely open) challenge in sequential decision-making is\ndealing with non-stationary environments, where exogenous environmental\nconditions change over time. Such problems are traditionally modeled as\nnon-stationary Markov decision processes (NSMDP). However, existing approaches\nfor decision-making in NSMDPs have two major shortcomings: first, they assume\nthat the updated environmental dynamics at the current time are known (although\nfuture dynamics can change); and second, planning is largely pessimistic, i.e.,\nthe agent acts ``safely'' to account for the non-stationary evolution of the\nenvironment. We argue that both these assumptions are invalid in practice --\nupdated environmental conditions are rarely known, and as the agent interacts\nwith the environment, it can learn about the updated dynamics and avoid being\npessimistic, at least in states whose dynamics it is confident about. We\npresent a heuristic search algorithm called \\textit{Adaptive Monte Carlo Tree\nSearch (ADA-MCTS)} that addresses these challenges. We show that the agent can\nlearn the updated dynamics of the environment over time and then act as it\nlearns, i.e., if the agent is in a region of the state space about which it has\nupdated knowledge, it can avoid being pessimistic. To quantify ``updated\nknowledge,'' we disintegrate the aleatoric and epistemic uncertainty in the\nagent's updated belief and show how the agent can use these estimates for\ndecision-making. We compare the proposed approach with the multiple\nstate-of-the-art approaches in decision-making across multiple well-established\nopen-source problems and empirically show that our approach is faster and\nhighly adaptive without sacrificing safety.\n","authors":["Baiting Luo","Yunuo Zhang","Abhishek Dubey","Ayan Mukhopadhyay"],"pdf_url":"https://arxiv.org/pdf/2401.01841v1.pdf","comment":"Accepted for publication at the International Conference on\n  Autonomous Agents and MultiAgent Systems (AAMAS), 2024"},{"id":"http://arxiv.org/abs/2210.12282v2","updated":"2024-01-03T17:02:21Z","published":"2022-10-21T22:27:07Z","title":"Bridging the Gap Between Target Networks and Functional Regularization","summary":"  Bootstrapping is behind much of the successes of Deep Reinforcement Learning.\nHowever, learning the value function via bootstrapping often leads to unstable\ntraining due to fast-changing target values. Target Networks are employed to\nstabilize training by using an additional set of lagging parameters to estimate\nthe target values. Despite the popularity of Target Networks, their effect on\nthe optimization is still misunderstood. In this work, we show that they act as\nan implicit regularizer. This regularizer has disadvantages such as being\ninflexible and non convex. To overcome these issues, we propose an explicit\nFunctional Regularization that is a convex regularizer in function space and\ncan easily be tuned. We analyze the convergence of our method theoretically and\nempirically demonstrate that replacing Target Networks with the more\ntheoretically grounded Functional Regularization approach leads to better\nsample efficiency and performance improvements.\n","authors":["Alexandre Piche","Valentin Thomas","Joseph Marino","Rafael Pardinas","Gian Maria Marconi","Christopher Pal","Mohammad Emtiyaz Khan"],"pdf_url":"https://arxiv.org/pdf/2210.12282v2.pdf","comment":"The published version of this paper (TMLR 2023) is available at\n  arXiv:2106.02613 and https://openreview.net/forum?id=BFvoemrmqX"},{"id":"http://arxiv.org/abs/2401.01830v1","updated":"2024-01-03T16:47:13Z","published":"2024-01-03T16:47:13Z","title":"Iterative Mask Filling: An Effective Text Augmentation Method Using\n  Masked Language Modeling","summary":"  Data augmentation is an effective technique for improving the performance of\nmachine learning models. However, it has not been explored as extensively in\nnatural language processing (NLP) as it has in computer vision. In this paper,\nwe propose a novel text augmentation method that leverages the Fill-Mask\nfeature of the transformer-based BERT model. Our method involves iteratively\nmasking words in a sentence and replacing them with language model predictions.\nWe have tested our proposed method on various NLP tasks and found it to be\neffective in many cases. Our results are presented along with a comparison to\nexisting augmentation methods. Experimental results show that our proposed\nmethod significantly improves performance, especially on topic classification\ndatasets.\n","authors":["Himmet Toprak Kesgin","Mehmet Fatih Amasyali"],"pdf_url":"https://arxiv.org/pdf/2401.01830v1.pdf","comment":"Published in International Conference on Advanced Engineering,\n  Technology and Applications (ICAETA 2023). The final version is available\n  online at https://link.springer.com/chapter/10.1007/978-3-031-50920-9_35"},{"id":"http://arxiv.org/abs/2312.15927v2","updated":"2024-01-03T16:43:33Z","published":"2023-12-26T07:45:32Z","title":"M3D: Dataset Condensation by Minimizing Maximum Mean Discrepancy","summary":"  Training state-of-the-art (SOTA) deep models often requires extensive data,\nresulting in substantial training and storage costs. To address these\nchallenges, dataset condensation has been developed to learn a small synthetic\nset that preserves essential information from the original large-scale dataset.\nNowadays, optimization-oriented methods have been the primary method in the\nfield of dataset condensation for achieving SOTA results. However, the bi-level\noptimization process hinders the practical application of such methods to\nrealistic and larger datasets. To enhance condensation efficiency, previous\nworks proposed Distribution-Matching (DM) as an alternative, which\nsignificantly reduces the condensation cost. Nonetheless, current DM-based\nmethods have yielded less comparable results to optimization-oriented methods\ndue to their focus on aligning only the first moment of the distributions. In\nthis paper, we present a novel DM-based method named M3D for dataset\ncondensation by Minimizing the Maximum Mean Discrepancy between feature\nrepresentations of the synthetic and real images. By embedding their\ndistributions in a reproducing kernel Hilbert space, we align all orders of\nmoments of the distributions of real and synthetic images, resulting in a more\ngeneralized condensed set. Notably, our method even surpasses the SOTA\noptimization-oriented method IDC on the high-resolution ImageNet dataset.\nExtensive analysis is conducted to verify the effectiveness of the proposed\nmethod.\n","authors":["Hansong Zhang","Shikun Li","Pengju Wang","Dan Zeng","Shiming Ge"],"pdf_url":"https://arxiv.org/pdf/2312.15927v2.pdf","comment":"This work has been accepted in AAAI-24"},{"id":"http://arxiv.org/abs/2307.06263v2","updated":"2024-01-03T16:38:27Z","published":"2023-07-12T16:03:34Z","title":"On the hierarchical Bayesian modelling of frequency response functions","summary":"  For situations that may benefit from information sharing among datasets,\ne.g., population-based SHM of similar structures, the hierarchical Bayesian\napproach provides a useful modelling structure. Hierarchical Bayesian models\nlearn statistical distributions at the population (or parent) and the domain\nlevels simultaneously, to bolster statistical strength among the parameters. As\na result, variance is reduced among the parameter estimates, particularly when\ndata are limited. In this paper, a combined probabilistic FRF model is\ndeveloped for a small population of nominally-identical helicopter blades,\nusing a hierarchical Bayesian structure, to support information transfer in the\ncontext of sparse data. The modelling approach is also demonstrated in a\ntraditional SHM context, for a single helicopter blade exposed to varying\ntemperatures, to show how the inclusion of physics-based knowledge can improve\ngeneralisation beyond the training data, in the context of scarce data. These\nmodels address critical challenges in SHM, by accommodating benign variations\nthat present as differences in the underlying dynamics, while also considering\n(and utilising), the similarities among the domains.\n","authors":["T. A. Dardeno","K. Worden","N. Dervilis","R. S. Mills","L. A. Bull"],"pdf_url":"https://arxiv.org/pdf/2307.06263v2.pdf","comment":"Published in Mechanical Systems and Signal Processing"},{"id":"http://arxiv.org/abs/2401.01813v1","updated":"2024-01-03T16:15:22Z","published":"2024-01-03T16:15:22Z","title":"Signal Processing in the Retina: Interpretable Graph Classifier to\n  Predict Ganglion Cell Responses","summary":"  It is a popular hypothesis in neuroscience that ganglion cells in the retina\nare activated by selectively detecting visual features in an observed scene.\nWhile ganglion cell firings can be predicted via data-trained deep neural nets,\nthe networks remain indecipherable, thus providing little understanding of the\ncells' underlying operations. To extract knowledge from the cell firings, in\nthis paper we learn an interpretable graph-based classifier from data to\npredict the firings of ganglion cells in response to visual stimuli.\nSpecifically, we learn a positive semi-definite (PSD) metric matrix $\\mathbf{M}\n\\succeq 0$ that defines Mahalanobis distances between graph nodes (visual\nevents) endowed with pre-computed feature vectors; the computed inter-node\ndistances lead to edge weights and a combinatorial graph that is amenable to\nbinary classification. Mathematically, we define the objective of metric matrix\n$\\mathbf{M}$ optimization using a graph adaptation of large margin nearest\nneighbor (LMNN), which is rewritten as a semi-definite programming (SDP)\nproblem. We solve it efficiently via a fast approximation called Gershgorin\ndisc perfect alignment (GDPA) linearization. The learned metric matrix\n$\\mathbf{M}$ provides interpretability: important features are identified along\n$\\mathbf{M}$'s diagonal, and their mutual relationships are inferred from\noff-diagonal terms. Our fast metric learning framework can be applied to other\nbiological systems with pre-chosen features that require interpretation.\n","authors":["Yasaman Parhizkar","Gene Cheung","Andrew W. Eckford"],"pdf_url":"https://arxiv.org/pdf/2401.01813v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.12061v2","updated":"2024-01-03T16:10:50Z","published":"2022-10-21T15:51:54Z","title":"Validation of Composite Systems by Discrepancy Propagation","summary":"  Assessing the validity of a real-world system with respect to given quality\ncriteria is a common yet costly task in industrial applications due to the vast\nnumber of required real-world tests. Validating such systems by means of\nsimulation offers a promising and less expensive alternative, but requires an\nassessment of the simulation accuracy and therefore end-to-end measurements.\nAdditionally, covariate shifts between simulations and actual usage can cause\ndifficulties for estimating the reliability of such systems. In this work, we\npresent a validation method that propagates bounds on distributional\ndiscrepancy measures through a composite system, thereby allowing us to derive\nan upper bound on the failure probability of the real system from potentially\ninaccurate simulations. Each propagation step entails an optimization problem,\nwhere -- for measures such as maximum mean discrepancy (MMD) -- we develop\ntight convex relaxations based on semidefinite programs. We demonstrate that\nour propagation method yields valid and useful bounds for composite systems\nexhibiting a variety of realistic effects. In particular, we show that the\nproposed method can successfully account for data shifts within the\nexperimental design as well as model inaccuracies within the simulation.\n","authors":["David Reeb","Kanil Patel","Karim Barsim","Martin Schiegg","Sebastian Gerwinn"],"pdf_url":"https://arxiv.org/pdf/2210.12061v2.pdf","comment":"21 pages incl. 11 pages appendix; camera-ready version at UAI 2023"},{"id":"http://arxiv.org/abs/2401.01801v1","updated":"2024-01-03T15:59:35Z","published":"2024-01-03T15:59:35Z","title":"A quatum inspired neural network for geometric modeling","summary":"  By conceiving physical systems as 3D many-body point clouds, geometric graph\nneural networks (GNNs), such as SE(3)/E(3) equivalent GNNs, have showcased\npromising performance. In particular, their effective message-passing mechanics\nmake them adept at modeling molecules and crystalline materials. However,\ncurrent geometric GNNs only offer a mean-field approximation of the many-body\nsystem, encapsulated within two-body message passing, thus falling short in\ncapturing intricate relationships within these geometric graphs. To address\nthis limitation, tensor networks, widely employed by computational physics to\nhandle manybody systems using high-order tensors, have been introduced.\nNevertheless, integrating these tensorized networks into the message-passing\nframework of GNNs faces scalability and symmetry conservation (e.g.,\npermutation and rotation) challenges. In response, we introduce an innovative\nequivariant Matrix Product State (MPS)-based message-passing strategy, through\nachieving an efficient implementation of the tensor contraction operation. Our\nmethod effectively models complex many-body relationships, suppressing\nmean-field approximations, and captures symmetries within geometric graphs.\nImportantly, it seamlessly replaces the standard message-passing and\nlayer-aggregation modules intrinsic to geometric GNNs. We empirically validate\nthe superior accuracy of our approach on benchmark tasks, including predicting\nclassical Newton systems and quantum tensor Hamiltonian matrices. To our\nknowledge, our approach represents the inaugural utilization of parameterized\ngeometric tensor networks.\n","authors":["Weitao Du","Shengchao Liu","Hongyu Guo"],"pdf_url":"https://arxiv.org/pdf/2401.01801v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05840v2","updated":"2024-01-03T15:58:21Z","published":"2023-12-10T09:50:57Z","title":"Topological Data Analysis for Neural Network Analysis: A Comprehensive\n  Survey","summary":"  This survey provides a comprehensive exploration of applications of\nTopological Data Analysis (TDA) within neural network analysis. Using TDA tools\nsuch as persistent homology and Mapper, we delve into the intricate structures\nand behaviors of neural networks and their datasets. We discuss different\nstrategies to obtain topological information from data and neural networks by\nmeans of TDA. Additionally, we review how topological information can be\nleveraged to analyze properties of neural networks, such as their\ngeneralization capacity or expressivity. We explore practical implications of\ndeep learning, specifically focusing on areas like adversarial detection and\nmodel selection. Our survey organizes the examined works into four broad\ndomains: 1. Characterization of neural network architectures; 2. Analysis of\ndecision regions and boundaries; 3. Study of internal representations,\nactivations, and parameters; 4. Exploration of training dynamics and loss\nfunctions. Within each category, we discuss several articles, offering\nbackground information to aid in understanding the various methodologies. We\nconclude with a synthesis of key insights gained from our study, accompanied by\na discussion of challenges and potential advancements in the field.\n","authors":["Rubén Ballester","Carles Casacuberta","Sergio Escalera"],"pdf_url":"https://arxiv.org/pdf/2312.05840v2.pdf","comment":"70 pages, 7 figures. 4 references added. Minor changes in the text.\n  Part of generative models reestructured to improve generality and clarity of\n  exposition"},{"id":"http://arxiv.org/abs/2312.13863v2","updated":"2024-01-03T15:52:24Z","published":"2023-12-21T14:01:51Z","title":"Manipulating Trajectory Prediction with Backdoors","summary":"  Autonomous vehicles ought to predict the surrounding agents' trajectories to\nallow safe maneuvers in uncertain and complex traffic situations. As companies\nincreasingly apply trajectory prediction in the real world, security becomes a\nrelevant concern. In this paper, we focus on backdoors - a security threat\nacknowledged in other fields but so far overlooked for trajectory prediction.\nTo this end, we describe and investigate four triggers that could affect\ntrajectory prediction. We then show that these triggers (for example, a braking\nvehicle), when correlated with a desired output (for example, a curve) during\ntraining, cause the desired output of a state-of-the-art trajectory prediction\nmodel. In other words, the model has good benign performance but is vulnerable\nto backdoors. This is the case even if the trigger maneuver is performed by a\nnon-casual agent behind the target vehicle. As a side-effect, our analysis\nreveals interesting limitations within trajectory prediction models. Finally,\nwe evaluate a range of defenses against backdoors. While some, like simple\noffroad checks, do not enable detection for all triggers, clustering is a\npromising candidate to support manual inspection to find backdoors.\n","authors":["Kaouther Messaoud","Kathrin Grosse","Mickael Chen","Matthieu Cord","Patrick Pérez","Alexandre Alahi"],"pdf_url":"https://arxiv.org/pdf/2312.13863v2.pdf","comment":"9 pages, 7 figures"},{"id":"http://arxiv.org/abs/2306.00732v2","updated":"2024-01-03T15:47:40Z","published":"2023-06-01T14:27:28Z","title":"Sharper Bounds for $\\ell_p$ Sensitivity Sampling","summary":"  In large scale machine learning, random sampling is a popular way to\napproximate datasets by a small representative subset of examples. In\nparticular, sensitivity sampling is an intensely studied technique which\nprovides provable guarantees on the quality of approximation, while reducing\nthe number of examples to the product of the VC dimension $d$ and the total\nsensitivity $\\mathfrak S$ in remarkably general settings. However, guarantees\ngoing beyond this general bound of $\\mathfrak S d$ are known in perhaps only\none setting, for $\\ell_2$ subspace embeddings, despite intense study of\nsensitivity sampling in prior work. In this work, we show the first bounds for\nsensitivity sampling for $\\ell_p$ subspace embeddings for $p > 2$ that improve\nover the general $\\mathfrak S d$ bound, achieving a bound of roughly $\\mathfrak\nS^{2-2/p}$ for $2<p<\\infty$. Furthermore, our techniques yield further new\nresults in the study of sampling algorithms, showing that the root leverage\nscore sampling algorithm achieves a bound of roughly $d$ for $1\\leq p<2$, and\nthat a combination of leverage score and sensitivity sampling achieves an\nimproved bound of roughly $d^{2/p}\\mathfrak S^{2-4/p}$ for $2<p<\\infty$. Our\nsensitivity sampling results yield the best known sample complexity for a wide\nclass of structured matrices that have small $\\ell_p$ sensitivity.\n","authors":["David P. Woodruff","Taisuke Yasuda"],"pdf_url":"https://arxiv.org/pdf/2306.00732v2.pdf","comment":"To appear in ICML 2023; added discussion of prior work"},{"id":"http://arxiv.org/abs/2401.01792v1","updated":"2024-01-03T15:47:17Z","published":"2024-01-03T15:47:17Z","title":"CoMoSVC: Consistency Model-based Singing Voice Conversion","summary":"  The diffusion-based Singing Voice Conversion (SVC) methods have achieved\nremarkable performances, producing natural audios with high similarity to the\ntarget timbre. However, the iterative sampling process results in slow\ninference speed, and acceleration thus becomes crucial. In this paper, we\npropose CoMoSVC, a consistency model-based SVC method, which aims to achieve\nboth high-quality generation and high-speed sampling. A diffusion-based teacher\nmodel is first specially designed for SVC, and a student model is further\ndistilled under self-consistency properties to achieve one-step sampling.\nExperiments on a single NVIDIA GTX4090 GPU reveal that although CoMoSVC has a\nsignificantly faster inference speed than the state-of-the-art (SOTA)\ndiffusion-based SVC system, it still achieves comparable or superior conversion\nperformance based on both subjective and objective metrics. Audio samples and\ncodes are available at https://comosvc.github.io/.\n","authors":["Yiwen Lu","Zhen Ye","Wei Xue","Xu Tan","Qifeng Liu","Yike Guo"],"pdf_url":"https://arxiv.org/pdf/2401.01792v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01789v1","updated":"2024-01-03T15:42:45Z","published":"2024-01-03T15:42:45Z","title":"Deep learning the Hurst parameter of linear fractional processes and\n  assessing its reliability","summary":"  This research explores the reliability of deep learning, specifically Long\nShort-Term Memory (LSTM) networks, for estimating the Hurst parameter in\nfractional stochastic processes. The study focuses on three types of processes:\nfractional Brownian motion (fBm), fractional Ornstein-Uhlenbeck (fOU) process,\nand linear fractional stable motions (lfsm). The work involves a fast\ngeneration of extensive datasets for fBm and fOU to train the LSTM network on a\nlarge volume of data in a feasible time. The study analyses the accuracy of the\nLSTM network's Hurst parameter estimation regarding various performance\nmeasures like RMSE, MAE, MRE, and quantiles of the absolute and relative\nerrors. It finds that LSTM outperforms the traditional statistical methods in\nthe case of fBm and fOU processes; however, it has limited accuracy on lfsm\nprocesses. The research also delves into the implications of training length\nand valuation sequence length on the LSTM's performance. The methodology is\napplied by estimating the Hurst parameter in Li-ion battery degradation data\nand obtaining confidence bounds for the estimation. The study concludes that\nwhile deep learning methods show promise in parameter estimation of fractional\nprocesses, their effectiveness is contingent on the process type and the\nquality of training data.\n","authors":["Dániel Boros","Bálint Csanády","Iván Ivkovic","Lóránt Nagy","András Lukács","László Márkus"],"pdf_url":"https://arxiv.org/pdf/2401.01789v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01788v1","updated":"2024-01-03T15:36:33Z","published":"2024-01-03T15:36:33Z","title":"Applications of machine learning and IoT for Outdoor Air Pollution\n  Monitoring and Prediction: A Systematic Literature Review","summary":"  According to the World Health Organization (WHO), air pollution kills seven\nmillion people every year. Outdoor air pollution is a major environmental\nhealth problem affecting low, middle, and high-income countries. In the past\nfew years, the research community has explored IoT-enabled machine learning\napplications for outdoor air pollution prediction. The general objective of\nthis paper is to systematically review applications of machine learning and\nInternet of Things (IoT) for outdoor air pollution prediction and the\ncombination of monitoring sensors and input features used. Two research\nquestions were formulated for this review. 1086 publications were collected in\nthe initial PRISMA stage. After the screening and eligibility phases, 37 papers\nwere selected for inclusion. A cost-based analysis was conducted on the\nfindings to highlight high-cost monitoring, low-cost IoT and hybrid enabled\nprediction. Three methods of prediction were identified: time series,\nfeature-based and spatio-temporal. This review's findings identify major\nlimitations in applications found in the literature, namely lack of coverage,\nlack of diversity of data and lack of inclusion of context-specific features.\nThis review proposes directions for future research and underlines practical\nimplications in healthcare, urban planning, global synergy and smart cities.\n","authors":["Ihsane Gryech","Chaimae Assad","Mounir Ghogho","Abdellatif Kobbane"],"pdf_url":"https://arxiv.org/pdf/2401.01788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2108.02497v4","updated":"2024-01-03T15:35:30Z","published":"2021-08-05T10:15:17Z","title":"How to avoid machine learning pitfalls: a guide for academic researchers","summary":"  This document outlines some of the common mistakes that occur when using\nmachine learning, and what can be done to avoid them. Whilst it should be\naccessible to anyone with a basic understanding of machine learning techniques,\nit was originally written for research students, and focuses on issues that are\nof particular concern within academic research, such as the need to do rigorous\ncomparisons and reach valid conclusions. It covers five stages of the machine\nlearning process: what to do before model building, how to reliably build\nmodels, how to robustly evaluate models, how to compare models fairly, and how\nto report results.\n","authors":["Michael A. Lones"],"pdf_url":"https://arxiv.org/pdf/2108.02497v4.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2307.05520v3","updated":"2024-01-03T15:20:31Z","published":"2023-07-07T12:07:59Z","title":"Do DL models and training environments have an impact on energy\n  consumption?","summary":"  Current research in the computer vision field mainly focuses on improving\nDeep Learning (DL) correctness and inference time performance. However, there\nis still little work on the huge carbon footprint that has training DL models.\nThis study aims to analyze the impact of the model architecture and training\nenvironment when training greener computer vision models. We divide this goal\ninto two research questions. First, we analyze the effects of model\narchitecture on achieving greener models while keeping correctness at optimal\nlevels. Second, we study the influence of the training environment on producing\ngreener models. To investigate these relationships, we collect multiple metrics\nrelated to energy efficiency and model correctness during the models' training.\nThen, we outline the trade-offs between the measured energy efficiency and the\nmodels' correctness regarding model architecture, and their relationship with\nthe training environment. We conduct this research in the context of a computer\nvision system for image classification. In conclusion, we show that selecting\nthe proper model architecture and training environment can reduce energy\nconsumption dramatically (up to 81.38%) at the cost of negligible decreases in\ncorrectness. Also, we find evidence that GPUs should scale with the models'\ncomputational complexity for better energy efficiency.\n","authors":["Santiago del Rey","Silverio Martínez-Fernández","Luís Cruz","Xavier Franch"],"pdf_url":"https://arxiv.org/pdf/2307.05520v3.pdf","comment":"49th Euromicro Conference Series on Software Engineering and Advanced\n  Applications (SEAA). 8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2207.14650v3","updated":"2024-01-03T15:18:44Z","published":"2022-07-29T12:50:32Z","title":"SYNTA: A novel approach for deep learning-based image analysis in muscle\n  histopathology using photo-realistic synthetic data","summary":"  Artificial intelligence (AI), machine learning, and deep learning (DL)\nmethods are becoming increasingly important in the field of biomedical image\nanalysis. However, to exploit the full potential of such methods, a\nrepresentative number of experimentally acquired images containing a\nsignificant number of manually annotated objects is needed as training data.\nHere we introduce SYNTA (synthetic data) as a novel approach for the generation\nof synthetic, photo-realistic, and highly complex biomedical images as training\ndata for DL systems. We show the versatility of our approach in the context of\nmuscle fiber and connective tissue analysis in histological sections. We\ndemonstrate that it is possible to perform robust and expert-level segmentation\ntasks on previously unseen real-world data, without the need for manual\nannotations using synthetic training data alone. Being a fully parametric\ntechnique, our approach poses an interpretable and controllable alternative to\nGenerative Adversarial Networks (GANs) and has the potential to significantly\naccelerate quantitative image analysis in a variety of biomedical applications\nin microscopy and beyond.\n","authors":["Leonid Mill","Oliver Aust","Jochen A. Ackermann","Philipp Burger","Monica Pascual","Katrin Palumbo-Zerr","Gerhard Krönke","Stefan Uderhardt","Georg Schett","Christoph S. Clemen","Rolf Schröder","Christian Holtzhausen","Samir Jabari","Andreas Maier","Anika Grüneboom"],"pdf_url":"https://arxiv.org/pdf/2207.14650v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01783v1","updated":"2024-01-03T15:16:25Z","published":"2024-01-03T15:16:25Z","title":"Approximating Numerical Flux by Fourier Neural Operators for the\n  Hyperbolic Conservation Laws","summary":"  Classical numerical schemes exist for solving PDEs numerically, and recently,\nneural network-based methods have been developed. However, methodologies using\nneural networks, such as PINN and neural operators, lack robustness and\ngeneralization power. To compensate for such drawbacks, there are many types of\nresearch combining classical numerical schemes and machine learning methods by\nreplacing a small portion of the numerical schemes with neural networks. In\nthis work, we focus on hyperbolic conservation laws and replace numerical\nfluxes in the numerical schemes by neural operator. For this, we construct\nlosses that are motivated by numerical schemes for conservation laws and\napproximate numerical flux by FNO. Through experiments, we show that our\nmethodology has advantages of both numerical schemes and FNO by comparing with\noriginal methods. For instance, we demonstrate our method gains robustness,\nresolution invariance property, and feasibility of a data-driven method. Our\nmethod especially has the ability to predict continuously in time and\ngeneralization power on the out-of-distribution samples, which are challenges\nto be tackled for existing neural operator methods.\n","authors":["Taeyoung Kim","Myungjoo Sang"],"pdf_url":"https://arxiv.org/pdf/2401.01783v1.pdf","comment":"23 pages, 28 figures"},{"id":"http://arxiv.org/abs/2312.15097v2","updated":"2024-01-03T15:02:24Z","published":"2023-12-22T22:33:39Z","title":"Recourse under Model Multiplicity via Argumentative Ensembling\n  (Technical Report)","summary":"  Model Multiplicity (MM) arises when multiple, equally performing machine\nlearning models can be trained to solve the same prediction task. Recent\nstudies show that models obtained under MM may produce inconsistent predictions\nfor the same input. When this occurs, it becomes challenging to provide\ncounterfactual explanations (CEs), a common means for offering recourse\nrecommendations to individuals negatively affected by models' predictions. In\nthis paper, we formalise this problem, which we name recourse-aware ensembling,\nand identify several desirable properties which methods for solving it should\nsatisfy. We show that existing ensembling methods, naturally extended in\ndifferent ways to provide CEs, fail to satisfy these properties. We then\nintroduce argumentative ensembling, deploying computational argumentation to\nguarantee robustness of CEs to MM, while also accommodating customisable user\npreferences. We show theoretically and experimentally that argumentative\nensembling satisfies properties which the existing methods lack, and that the\ntrade-offs are minimal wrt accuracy.\n","authors":["Junqi Jiang","Antonio Rago","Francesco Leofante","Francesca Toni"],"pdf_url":"https://arxiv.org/pdf/2312.15097v2.pdf","comment":"Accepted at AAMAS 2024"},{"id":"http://arxiv.org/abs/2307.14823v2","updated":"2024-01-03T14:38:02Z","published":"2023-07-27T13:00:21Z","title":"Fading memory as inductive bias in residual recurrent networks","summary":"  Residual connections have been proposed as an architecture-based inductive\nbias to mitigate the problem of exploding and vanishing gradients and increased\ntask performance in both feed-forward and recurrent networks (RNNs) when\ntrained with the backpropagation algorithm. Yet, little is known about how\nresidual connections in RNNs influence their dynamics and fading memory\nproperties. Here, we introduce weakly coupled residual recurrent networks\n(WCRNNs) in which residual connections result in well-defined Lyapunov\nexponents and allow for studying properties of fading memory. We investigate\nhow the residual connections of WCRNNs influence their performance, network\ndynamics, and memory properties on a set of benchmark tasks. We show that\nseveral distinct forms of residual connections yield effective inductive biases\nthat result in increased network expressivity. In particular, those are\nresidual connections that (i) result in network dynamics at the proximity of\nthe edge of chaos, (ii) allow networks to capitalize on characteristic spectral\nproperties of the data, and (iii) result in heterogeneous memory properties. In\naddition, we demonstrate how our results can be extended to non-linear\nresiduals and introduce a weakly coupled residual initialization scheme that\ncan be used for Elman RNNs.\n","authors":["Igor Dubinin","Felix Effenberger"],"pdf_url":"https://arxiv.org/pdf/2307.14823v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.15580v4","updated":"2024-01-03T14:21:38Z","published":"2022-05-31T07:41:20Z","title":"A Computation and Communication Efficient Method for Distributed\n  Nonconvex Problems in the Partial Participation Setting","summary":"  We present a new method that includes three key components of distributed\noptimization and federated learning: variance reduction of stochastic\ngradients, partial participation, and compressed communication. We prove that\nthe new method has optimal oracle complexity and state-of-the-art communication\ncomplexity in the partial participation setting. Regardless of the\ncommunication compression feature, our method successfully combines variance\nreduction and partial participation: we get the optimal oracle complexity,\nnever need the participation of all nodes, and do not require the bounded\ngradients (dissimilarity) assumption.\n","authors":["Alexander Tyurin","Peter Richtárik"],"pdf_url":"https://arxiv.org/pdf/2205.15580v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.00086v2","updated":"2024-01-03T14:20:41Z","published":"2022-10-31T18:37:22Z","title":"Disentangled (Un)Controllable Features","summary":"  In the context of MDPs with high-dimensional states, downstream tasks are\npredominantly applied on a compressed, low-dimensional representation of the\noriginal input space. A variety of learning objectives have therefore been used\nto attain useful representations. However, these representations usually lack\ninterpretability of the different features. We present a novel approach that is\nable to disentangle latent features into a controllable and an uncontrollable\npartition. We illustrate that the resulting partitioned representations are\neasily interpretable on three types of environments and show that, in a\ndistribution of procedurally generated maze environments, it is feasible to\ninterpretably employ a planning algorithm in the isolated controllable latent\npartition.\n","authors":["Jacob E. Kooi","Mark Hoogendoorn","Vincent François-Lavet"],"pdf_url":"https://arxiv.org/pdf/2211.00086v2.pdf","comment":"14 pages (8 main paper pages), 15 figures"},{"id":"http://arxiv.org/abs/2305.00011v2","updated":"2024-01-03T13:51:05Z","published":"2023-04-29T08:39:55Z","title":"Adversarial Representation Learning for Robust Privacy Preservation in\n  Audio","summary":"  Sound event detection systems are widely used in various applications such as\nsurveillance and environmental monitoring where data is automatically\ncollected, processed, and sent to a cloud for sound recognition. However, this\nprocess may inadvertently reveal sensitive information about users or their\nsurroundings, hence raising privacy concerns. In this study, we propose a novel\nadversarial training method for learning representations of audio recordings\nthat effectively prevents the detection of speech activity from the latent\nfeatures of the recordings. The proposed method trains a model to generate\ninvariant latent representations of speech-containing audio recordings that\ncannot be distinguished from non-speech recordings by a speech classifier. The\nnovelty of our work is in the optimization algorithm, where the speech\nclassifier's weights are regularly replaced with the weights of classifiers\ntrained in a supervised manner. This increases the discrimination power of the\nspeech classifier constantly during the adversarial training, motivating the\nmodel to generate latent representations in which speech is not\ndistinguishable, even using new speech classifiers trained outside the\nadversarial training loop. The proposed method is evaluated against a baseline\napproach with no privacy measures and a prior adversarial training method,\ndemonstrating a significant reduction in privacy violations compared to the\nbaseline approach. Additionally, we show that the prior adversarial method is\npractically ineffective for this purpose.\n","authors":["Shayan Gharib","Minh Tran","Diep Luong","Konstantinos Drossos","Tuomas Virtanen"],"pdf_url":"https://arxiv.org/pdf/2305.00011v2.pdf","comment":"Published in IEEE Open Journal of Signal Processing"},{"id":"http://arxiv.org/abs/2308.00583v2","updated":"2024-01-03T13:26:44Z","published":"2023-08-01T15:00:14Z","title":"Semisupervised Anomaly Detection using Support Vector Regression with\n  Quantum Kernel","summary":"  Anomaly detection (AD) involves identifying observations or events that\ndeviate in some way from the rest of the data. Machine learning techniques have\nshown success in automating this process by detecting hidden patterns and\ndeviations in large-scale data. The potential of quantum computing for machine\nlearning has been widely recognized, leading to extensive research efforts to\ndevelop suitable quantum machine learning (QML) algorithms. In particular, the\nsearch for QML algorithms for near-term NISQ devices is in full swing. However,\nNISQ devices pose additional challenges due to their limited qubit coherence\ntimes, low number of qubits, and high error rates. Kernel methods based on\nquantum kernel estimation have emerged as a promising approach to QML on NISQ\ndevices, offering theoretical guarantees, versatility, and compatibility with\nNISQ constraints. Especially support vector machines (SVM) utilizing quantum\nkernel estimation have shown success in various supervised learning tasks.\nHowever, in the context of AD, semisupervised learning is of great relevance,\nand yet there is limited research published in this area. This paper introduces\nan approach to semisupervised AD based on the reconstruction loss of a support\nvector regression (SVR) with quantum kernel. This novel model is an alternative\nto the variational quantum and quantum kernel one-class classifiers, and is\ncompared to a quantum autoencoder as quantum baseline and a SVR with\nradial-basis-function (RBF) kernel as well as a classical autoencoder as\nclassical baselines. The models are benchmarked extensively on 10 real-world AD\ndata sets and one toy data set, and it is shown that our SVR model with quantum\nkernel performs better than the SVR with RBF kernel as well as all other\nmodels, achieving highest mean AUC over all data sets. In addition, our QSVR\noutperforms the quantum autoencoder on 9 out of 11 data sets.\n","authors":["Kilian Tscharke","Sebastian Issel","Pascal Debus"],"pdf_url":"https://arxiv.org/pdf/2308.00583v2.pdf","comment":"Accepted to IEEE International Conference on Quantum Computing and\n  Engineering (QCE) 2023"},{"id":"http://arxiv.org/abs/2310.19923v3","updated":"2024-01-03T13:26:41Z","published":"2023-10-30T18:35:30Z","title":"Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long\n  Documents","summary":"  Text embedding models have emerged as powerful tools for transforming\nsentences into fixed-sized feature vectors that encapsulate semantic\ninformation. While these models are essential for tasks like information\nretrieval, semantic clustering, and text re-ranking, most existing open-source\nmodels, especially those built on architectures like BERT, struggle to\nrepresent lengthy documents and often resort to truncation. One common approach\nto mitigate this challenge involves splitting documents into smaller paragraphs\nfor embedding. However, this strategy results in a much larger set of vectors,\nconsequently leading to increased memory consumption and computationally\nintensive vector searches with elevated latency.\n  To address these challenges, we introduce Jina Embeddings 2, an open-source\ntext embedding model capable of accommodating up to 8192 tokens. This model is\ndesigned to transcend the conventional 512-token limit and adeptly process long\ndocuments. Jina Embeddings 2 not only achieves state-of-the-art performance on\na range of embedding-related tasks in the MTEB benchmark but also matches the\nperformance of OpenAI's proprietary ada-002 model. Additionally, our\nexperiments indicate that an extended context can enhance performance in tasks\nsuch as NarrativeQA.\n","authors":["Michael Günther","Jackmin Ong","Isabelle Mohr","Alaeddine Abdessalem","Tanguy Abel","Mohammad Kalim Akram","Susana Guzman","Georgios Mastrapas","Saba Sturua","Bo Wang","Maximilian Werk","Nan Wang","Han Xiao"],"pdf_url":"https://arxiv.org/pdf/2310.19923v3.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2310.00488v2","updated":"2024-01-03T13:23:26Z","published":"2023-09-30T20:59:07Z","title":"On Memorization and Privacy Risks of Sharpness Aware Minimization","summary":"  In many recent works, there is an increased focus on designing algorithms\nthat seek flatter optima for neural network loss optimization as there is\nempirical evidence that it leads to better generalization performance in many\ndatasets. In this work, we dissect these performance gains through the lens of\ndata memorization in overparameterized models. We define a new metric that\nhelps us identify which data points specifically do algorithms seeking flatter\noptima do better when compared to vanilla SGD. We find that the generalization\ngains achieved by Sharpness Aware Minimization (SAM) are particularly\npronounced for atypical data points, which necessitate memorization. This\ninsight helps us unearth higher privacy risks associated with SAM, which we\nverify through exhaustive empirical evaluations. Finally, we propose mitigation\nstrategies to achieve a more desirable accuracy vs privacy tradeoff.\n","authors":["Young In Kim","Pratiksha Agrawal","Johannes O. Royset","Rajiv Khanna"],"pdf_url":"https://arxiv.org/pdf/2310.00488v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01733v1","updated":"2024-01-03T13:12:04Z","published":"2024-01-03T13:12:04Z","title":"Investigating the Suitability of Concept Drift Detection for Detecting\n  Leakages in Water Distribution Networks","summary":"  Leakages are a major risk in water distribution networks as they cause water\nloss and increase contamination risks. Leakage detection is a difficult task\ndue to the complex dynamics of water distribution networks. In particular,\nsmall leakages are hard to detect. From a machine-learning perspective,\nleakages can be modeled as concept drift. Thus, a wide variety of drift\ndetection schemes seems to be a suitable choice for detecting leakages. In this\nwork, we explore the potential of model-loss-based and distribution-based drift\ndetection methods to tackle leakage detection. We additionally discuss the\nissue of temporal dependencies in the data and propose a way to cope with it\nwhen applying distribution-based detection. We evaluate different methods\nsystematically for leakages of different sizes and detection times.\nAdditionally, we propose a first drift-detection-based technique for localizing\nleakages.\n","authors":["Valerie Vaquet","Fabian Hinder","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2401.01733v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01732v1","updated":"2024-01-03T13:11:59Z","published":"2024-01-03T13:11:59Z","title":"Task and Explanation Network","summary":"  Explainability in deep networks has gained increased importance in recent\nyears. We argue herein that an AI must be tasked not just with a task but also\nwith an explanation of why said task was accomplished as such. We present a\nbasic framework -- Task and Explanation Network (TENet) -- which fully\nintegrates task completion and its explanation. We believe that the field of AI\nas a whole should insist -- quite emphatically -- on explainability.\n","authors":["Moshe Sipper"],"pdf_url":"https://arxiv.org/pdf/2401.01732v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01728v1","updated":"2024-01-03T13:07:07Z","published":"2024-01-03T13:07:07Z","title":"Ravnest: Decentralized Asynchronous Training on Heterogeneous Devices","summary":"  Modern deep learning models, growing larger and more complex, have\ndemonstrated exceptional generalization and accuracy due to training on huge\ndatasets. This trend is expected to continue. However, the increasing size of\nthese models poses challenges in training, as traditional centralized methods\nare limited by memory constraints at such scales. This paper proposes an\nasynchronous decentralized training paradigm for large modern deep learning\nmodels that harnesses the compute power of regular heterogeneous PCs with\nlimited resources connected across the internet to achieve favourable\nperformance metrics. Ravnest facilitates decentralized training by efficiently\norganizing compute nodes into clusters with similar data transfer rates and\ncompute capabilities, without necessitating that each node hosts the entire\nmodel. These clusters engage in $\\textit{Zero-Bubble Asynchronous Model\nParallel}$ training, and a $\\textit{Parallel Multi-Ring All-Reduce}$ method is\nemployed to effectively execute global parameter averaging across all clusters.\nWe have framed our asynchronous SGD loss function as a block structured\noptimization problem with delayed updates and derived an optimal convergence\nrate of $O\\left(\\frac{1}{\\sqrt{K}}\\right)$. We further discuss linear speedup\nwith respect to the number of participating clusters and the bound on the\nstaleness parameter.\n","authors":["Anirudh Rajiv Menon","Unnikrishnan Menon","Kailash Ahirwar"],"pdf_url":"https://arxiv.org/pdf/2401.01728v1.pdf","comment":"28 pages, 5 figures"},{"id":"http://arxiv.org/abs/2309.15216v2","updated":"2024-01-03T12:52:42Z","published":"2023-09-26T19:21:09Z","title":"Diabetic Retinopathy Using Gaussian Filter","summary":"  The retina is an essential component of the visual system, and maintaining\neyesight depends on the timely and correct detection of disorders. This\nresearch specifically addresses the early-stage detection and severity\nclassification of diabetic retinopathy (DR), a serious public health hazard. We\ncompare the results of different deep learning models such as InceptionV3,\nDenseNet121 and other CNN based models by using different image filters, such\nas Gaussian, grayscale and Gabor. These models could detect subtle pathological\nalterations and use that information to estimate the risk of retinal illnesses.\nThe objective is to improve the diagnostic processes for diabetic retinopathy,\nthe primary cause of diabetes-related blindness, by utilizing deep learning\nmodels. A comparative analysis between Greyscale, Gaussian and Gabor filters\nhas been provided after applying these filters on the retinal images. The\nGaussian filter resulted to be the most promising filter giving the best\naccuracies for all the models. The best performing model was InceptionV3 which\ngave an accuracy of 96% on Gaussian images, therefore Gaussian filter emerged\nas our most promising filter.\n","authors":["Roshan Vasu Muddaluru","Sharvaani Ravikumar Thoguluva","Shruti Prabha","Tanuja Konda Reddy","Dr. Suja P"],"pdf_url":"https://arxiv.org/pdf/2309.15216v2.pdf","comment":"6 pages, 5 figures, conference, 2 tables"},{"id":"http://arxiv.org/abs/2307.04049v2","updated":"2024-01-03T12:34:37Z","published":"2023-07-08T21:28:20Z","title":"Parallel Algorithms Align with Neural Execution","summary":"  Neural algorithmic reasoners are parallel processors. Teaching them\nsequential algorithms contradicts this nature, rendering a significant share of\ntheir computations redundant. Parallel algorithms however may exploit their\nfull computational power, therefore requiring fewer layers to be executed. This\ndrastically reduces training times, as we observe when comparing parallel\nimplementations of searching, sorting and finding strongly connected components\nto their sequential counterparts on the CLRS framework. Additionally, parallel\nversions achieve (often strongly) superior predictive performance.\n","authors":["Valerie Engelmayer","Dobrik Georgiev","Petar Veličković"],"pdf_url":"https://arxiv.org/pdf/2307.04049v2.pdf","comment":"13 pages, 7 figures, Proceedings of the Second Learning on Graphs\n  Conference (LoG 2023), PMLR 231"},{"id":"http://arxiv.org/abs/2401.01710v1","updated":"2024-01-03T12:25:18Z","published":"2024-01-03T12:25:18Z","title":"EPA: Neural Collapse Inspired Robust Out-of-Distribution Detector","summary":"  Out-of-distribution (OOD) detection plays a crucial role in ensuring the\nsecurity of neural networks. Existing works have leveraged the fact that\nIn-distribution (ID) samples form a subspace in the feature space, achieving\nstate-of-the-art (SOTA) performance. However, the comprehensive characteristics\nof the ID subspace still leave under-explored. Recently, the discovery of\nNeural Collapse ($\\mathcal{NC}$) sheds light on novel properties of the ID\nsubspace. Leveraging insight from $\\mathcal{NC}$, we observe that the Principal\nAngle between the features and the ID feature subspace forms a superior\nrepresentation for measuring the likelihood of OOD. Building upon this\nobservation, we propose a novel $\\mathcal{NC}$-inspired OOD scoring function,\nnamed Entropy-enhanced Principal Angle (EPA), which integrates both the global\ncharacteristic of the ID subspace and its inner property. We experimentally\ncompare EPA with various SOTA approaches, validating its superior performance\nand robustness across different network architectures and OOD datasets.\n","authors":["Jiawei Zhang","Yufan Chen","Cheng Jin","Lei Zhu","Yuantao Gu"],"pdf_url":"https://arxiv.org/pdf/2401.01710v1.pdf","comment":"Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2308.08469v4","updated":"2024-01-03T12:24:57Z","published":"2023-08-16T16:19:50Z","title":"LLM4TS: Aligning Pre-Trained LLMs as Data-Efficient Time-Series\n  Forecasters","summary":"  Multivariate time-series forecasting is vital in various domains, e.g.,\neconomic planning and weather prediction. Deep train-from-scratch models have\nexhibited effective performance yet require large amounts of data, which limits\nreal-world applicability. Recently, researchers have explored pre-trained Large\nLanguage Models (LLMs) for limited non-linguistic datasets. However,\nincorporating LLMs with time-series data presents challenges of limited\nadaptation due to different compositions between time-series and linguistic\ndata, and the inability to process multi-scale temporal information. To tackle\nthese challenges, we propose LLM4TS, a framework for time-series forecasting\nwith pre-trained LLMs. LLM4TS consists of a two-stage fine-tuning strategy: the\ntime-series alignment stage to align LLMs with the nuances of time-series data,\nand the forecasting fine-tuning stage, which is specifically designed for\ntime-series forecasting tasks. Furthermore, our framework features a novel\ntwo-level aggregation method that integrates multi-scale temporal data within\npre-trained LLMs, enhancing their ability to interpret time-specific\ninformation. In experiments across 7 time-series forecasting datasets, LLM4TS\nis superior to existing state-of-the-art methods, including those trained from\nscratch, in full-shot scenarios, and also achieves an average improvement of\n6.84% in MSE in few-shot scenarios. In addition, evaluations compared with\ndifferent self-supervised learning approaches highlight LLM4TS's effectiveness\nwith representation learning in forecasting scenarios.\n","authors":["Ching Chang","Wei-Yao Wang","Wen-Chih Peng","Tien-Fu Chen"],"pdf_url":"https://arxiv.org/pdf/2308.08469v4.pdf","comment":"This paper is currently under review. The code will be made available\n  upon acceptance"},{"id":"http://arxiv.org/abs/2212.08123v3","updated":"2024-01-03T12:22:46Z","published":"2022-12-15T20:23:09Z","title":"Bayesian posterior approximation with stochastic ensembles","summary":"  We introduce ensembles of stochastic neural networks to approximate the\nBayesian posterior, combining stochastic methods such as dropout with deep\nensembles. The stochastic ensembles are formulated as families of distributions\nand trained to approximate the Bayesian posterior with variational inference.\nWe implement stochastic ensembles based on Monte Carlo dropout, DropConnect and\na novel non-parametric version of dropout and evaluate them on a toy problem\nand CIFAR image classification. For both tasks, we test the quality of the\nposteriors directly against Hamiltonian Monte Carlo simulations. Our results\nshow that stochastic ensembles provide more accurate posterior estimates than\nother popular baselines for Bayesian inference.\n","authors":["Oleksandr Balabanov","Bernhard Mehlig","Hampus Linander"],"pdf_url":"https://arxiv.org/pdf/2212.08123v3.pdf","comment":"19 pages, CVPR 2023"},{"id":"http://arxiv.org/abs/2310.06452v2","updated":"2024-01-03T11:58:42Z","published":"2023-10-10T09:25:44Z","title":"Understanding the Effects of RLHF on LLM Generalisation and Diversity","summary":"  Large language models (LLMs) fine-tuned with reinforcement learning from\nhuman feedback (RLHF) have been used in some of the most widely deployed AI\nmodels to date, such as OpenAI's ChatGPT or Anthropic's Claude. % , or Meta's\nLLaMA-2. While there has been significant work developing these methods, our\nunderstanding of the benefits and downsides of each stage in RLHF is still\nlimited. To fill this gap, we present an extensive analysis of how each stage\nof the process (i.e.~supervised fine-tuning (SFT), reward modelling, and RLHF)\naffects two key properties: out-of-distribution (OOD) generalisation and output\ndiversity. OOD generalisation is crucial given the wide range of real-world\nscenarios in which these models are being used, while output diversity refers\nto the model's ability to generate varied outputs and is important for a\nvariety of use cases. We perform our analysis across two base models on both\nsummarisation and instruction following tasks, the latter being highly relevant\nfor current LLM use cases. We find that RLHF generalises better than SFT to new\ninputs, particularly as the distribution shift between train and test becomes\nlarger. However, RLHF significantly reduces output diversity compared to SFT\nacross a variety of measures, implying a tradeoff in current LLM fine-tuning\nmethods between generalisation and diversity. Our results provide guidance on\nwhich fine-tuning method should be used depending on the application, and show\nthat more research is needed to improve the tradeoff between generalisation and\ndiversity.\n","authors":["Robert Kirk","Ishita Mediratta","Christoforos Nalmpantis","Jelena Luketina","Eric Hambro","Edward Grefenstette","Roberta Raileanu"],"pdf_url":"https://arxiv.org/pdf/2310.06452v2.pdf","comment":"Code available here: https://github.com/facebookresearch/rlfh-gen-div"},{"id":"http://arxiv.org/abs/2401.01690v1","updated":"2024-01-03T11:49:07Z","published":"2024-01-03T11:49:07Z","title":"Zero-shot Active Learning Using Self Supervised Learning","summary":"  Deep learning algorithms are often said to be data hungry. The performance of\nsuch algorithms generally improve as more and more annotated data is fed into\nthe model. While collecting unlabelled data is easier (as they can be scraped\neasily from the internet), annotating them is a tedious and expensive task.\nGiven a fixed budget available for data annotation, Active Learning helps\nselecting the best subset of data for annotation, such that the deep learning\nmodel when trained over that subset will have maximum generalization\nperformance under this budget. In this work, we aim to propose a new Active\nLearning approach which is model agnostic as well as one doesn't require an\niterative process. We aim to leverage self-supervised learnt features for the\ntask of Active Learning. The benefit of self-supervised learning, is that one\ncan get useful feature representation of the input data, without having any\nannotation.\n","authors":["Abhishek Sinha","Shreya Singh"],"pdf_url":"https://arxiv.org/pdf/2401.01690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.07618v3","updated":"2024-01-03T11:22:21Z","published":"2023-06-13T08:22:18Z","title":"Hyperbolic Graph Diffusion Model","summary":"  Diffusion generative models (DMs) have achieved promising results in image\nand graph generation. However, real-world graphs, such as social networks,\nmolecular graphs, and traffic graphs, generally share non-Euclidean topologies\nand hidden hierarchies. For example, the degree distributions of graphs are\nmostly power-law distributions. The current latent diffusion model embeds the\nhierarchical data in a Euclidean space, which leads to distortions and\ninterferes with modeling the distribution. Instead, hyperbolic space has been\nfound to be more suitable for capturing complex hierarchical structures due to\nits exponential growth property. In order to simultaneously utilize the data\ngeneration capabilities of diffusion models and the ability of hyperbolic\nembeddings to extract latent hierarchical distributions, we propose a novel\ngraph generation method called, Hyperbolic Graph Diffusion Model (HGDM), which\nconsists of an auto-encoder to encode nodes into successive hyperbolic\nembeddings, and a DM that operates in the hyperbolic latent space. HGDM\ncaptures the crucial graph structure distributions by constructing a hyperbolic\npotential node space that incorporates edge information. Extensive experiments\nshow that HGDM achieves better performance in generic graph and molecule\ngeneration benchmarks, with a $48\\%$ improvement in the quality of graph\ngeneration with highly hierarchical structures.\n","authors":["Lingfeng Wen","Xuan Tang","Mingjie Ouyang","Xiangxiang Shen","Jian Yang","Daxin Zhu","Mingsong Chen","Xian Wei"],"pdf_url":"https://arxiv.org/pdf/2306.07618v3.pdf","comment":"accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2401.01654v1","updated":"2024-01-03T10:22:13Z","published":"2024-01-03T10:22:13Z","title":"LESEN: Label-Efficient deep learning for Multi-parametric MRI-based\n  Visual Pathway Segmentation","summary":"  Recent research has shown the potential of deep learning in multi-parametric\nMRI-based visual pathway (VP) segmentation. However, obtaining labeled data for\ntraining is laborious and time-consuming. Therefore, it is crucial to develop\neffective algorithms in situations with limited labeled samples. In this work,\nwe propose a label-efficient deep learning method with self-ensembling (LESEN).\nLESEN incorporates supervised and unsupervised losses, enabling the student and\nteacher models to mutually learn from each other, forming a self-ensembling\nmean teacher framework. Additionally, we introduce a reliable unlabeled sample\nselection (RUSS) mechanism to further enhance LESEN's effectiveness. Our\nexperiments on the human connectome project (HCP) dataset demonstrate the\nsuperior performance of our method when compared to state-of-the-art\ntechniques, advancing multimodal VP segmentation for comprehensive analysis in\nclinical and research settings. The implementation code will be available at:\nhttps://github.com/aldiak/Semi-Supervised-Multimodal-Visual-Pathway-\nDelineation.\n","authors":["Alou Diakite","Cheng Li","Lei Xie","Yuanjing Feng","Hua Han","Shanshan Wang"],"pdf_url":"https://arxiv.org/pdf/2401.01654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00110v2","updated":"2024-01-03T10:12:30Z","published":"2023-12-30T01:24:25Z","title":"Diffusion Model with Perceptual Loss","summary":"  Diffusion models trained with mean squared error loss tend to generate\nunrealistic samples. Current state-of-the-art models rely on classifier-free\nguidance to improve sample quality, yet its surprising effectiveness is not\nfully understood. In this paper, We show that the effectiveness of\nclassifier-free guidance partly originates from it being a form of implicit\nperceptual guidance. As a result, we can directly incorporate perceptual loss\nin diffusion training to improve sample quality. Since the score matching\nobjective used in diffusion training strongly resembles the denoising\nautoencoder objective used in unsupervised training of perceptual networks, the\ndiffusion model itself is a perceptual network and can be used to generate\nmeaningful perceptual loss. We propose a novel self-perceptual objective that\nresults in diffusion models capable of generating more realistic samples. For\nconditional generation, our method only improves sample quality without\nentanglement with the conditional input and therefore does not sacrifice sample\ndiversity. Our method can also improve sample quality for unconditional\ngeneration, which was not possible with classifier-free guidance before.\n","authors":["Shanchuan Lin","Xiao Yang"],"pdf_url":"https://arxiv.org/pdf/2401.00110v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.12511v2","updated":"2024-01-03T10:00:28Z","published":"2022-08-26T09:09:14Z","title":"Lower Difficulty and Better Robustness: A Bregman Divergence Perspective\n  for Adversarial Training","summary":"  In this paper, we investigate on improving the adversarial robustness\nobtained in adversarial training (AT) via reducing the difficulty of\noptimization. To better study this problem, we build a novel Bregman divergence\nperspective for AT, in which AT can be viewed as the sliding process of the\ntraining data points on the negative entropy curve. Based on this perspective,\nwe analyze the learning objectives of two typical AT methods, i.e., PGD-AT and\nTRADES, and we find that the optimization process of TRADES is easier than\nPGD-AT for that TRADES separates PGD-AT. In addition, we discuss the function\nof entropy in TRADES, and we find that models with high entropy can be better\nrobustness learners. Inspired by the above findings, we propose two methods,\ni.e., FAIT and MER, which can both not only reduce the difficulty of\noptimization under the 10-step PGD adversaries, but also provide better\nrobustness. Our work suggests that reducing the difficulty of optimization\nunder the 10-step PGD adversaries is a promising approach for enhancing the\nadversarial robustness in AT.\n","authors":["Zihui Wu","Haichang Gao","Bingqian Zhou","Xiaoyan Guo","Shudong Zhang"],"pdf_url":"https://arxiv.org/pdf/2208.12511v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.07774v3","updated":"2024-01-03T09:48:07Z","published":"2023-06-13T13:50:31Z","title":"The Rank-Reduced Kalman Filter: Approximate Dynamical-Low-Rank Filtering\n  In High Dimensions","summary":"  Inference and simulation in the context of high-dimensional dynamical systems\nremain computationally challenging problems. Some form of dimensionality\nreduction is required to make the problem tractable in general. In this paper,\nwe propose a novel approximate Gaussian filtering and smoothing method which\npropagates low-rank approximations of the covariance matrices. This is\naccomplished by projecting the Lyapunov equations associated with the\nprediction step to a manifold of low-rank matrices, which are then solved by a\nrecently developed, numerically stable, dynamical low-rank integrator.\nMeanwhile, the update steps are made tractable by noting that the covariance\nupdate only transforms the column space of the covariance matrix, which is\nlow-rank by construction. The algorithm differentiates itself from existing\nensemble-based approaches in that the low-rank approximations of the covariance\nmatrices are deterministic, rather than stochastic. Crucially, this enables the\nmethod to reproduce the exact Kalman filter as the low-rank dimension\napproaches the true dimensionality of the problem. Our method reduces\ncomputational complexity from cubic (for the Kalman filter) to \\emph{quadratic}\nin the state-space size in the worst-case, and can achieve \\emph{linear}\ncomplexity if the state-space model satisfies certain criteria. Through a set\nof experiments in classical data-assimilation and spatio-temporal regression,\nwe show that the proposed method consistently outperforms the ensemble-based\nmethods in terms of error in the mean and covariance with respect to the exact\nKalman filter. This comes at no additional cost in terms of asymptotic\ncomputational complexity.\n","authors":["Jonathan Schmidt","Philipp Hennig","Jörg Nick","Filip Tronarp"],"pdf_url":"https://arxiv.org/pdf/2306.07774v3.pdf","comment":"12 pages main text (including references) + 9 pages appendix, 6\n  figures"},{"id":"http://arxiv.org/abs/2312.13763v2","updated":"2024-01-03T09:40:56Z","published":"2023-12-21T11:41:02Z","title":"Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed\n  Diffusion Models","summary":"  Text-guided diffusion models have revolutionized image and video generation\nand have also been successfully used for optimization-based 3D object\nsynthesis. Here, we instead focus on the underexplored text-to-4D setting and\nsynthesize dynamic, animated 3D objects using score distillation methods with\nan additional temporal dimension. Compared to previous work, we pursue a novel\ncompositional generation-based approach, and combine text-to-image,\ntext-to-video, and 3D-aware multiview diffusion models to provide feedback\nduring 4D object optimization, thereby simultaneously enforcing temporal\nconsistency, high-quality visual appearance and realistic geometry. Our method,\ncalled Align Your Gaussians (AYG), leverages dynamic 3D Gaussian Splatting with\ndeformation fields as 4D representation. Crucial to AYG is a novel method to\nregularize the distribution of the moving 3D Gaussians and thereby stabilize\nthe optimization and induce motion. We also propose a motion amplification\nmechanism as well as a new autoregressive synthesis scheme to generate and\ncombine multiple 4D sequences for longer generation. These techniques allow us\nto synthesize vivid dynamic scenes, outperform previous work qualitatively and\nquantitatively and achieve state-of-the-art text-to-4D performance. Due to the\nGaussian 4D representation, different 4D animations can be seamlessly combined,\nas we demonstrate. AYG opens up promising avenues for animation, simulation and\ndigital content creation as well as synthetic data generation.\n","authors":["Huan Ling","Seung Wook Kim","Antonio Torralba","Sanja Fidler","Karsten Kreis"],"pdf_url":"https://arxiv.org/pdf/2312.13763v2.pdf","comment":"Project page:\n  https://research.nvidia.com/labs/toronto-ai/AlignYourGaussians/"},{"id":"http://arxiv.org/abs/2401.01641v1","updated":"2024-01-03T09:32:48Z","published":"2024-01-03T09:32:48Z","title":"Towards a Foundation Purchasing Model: Pretrained Generative\n  Autoregression on Transaction Sequences","summary":"  Machine learning models underpin many modern financial systems for use cases\nsuch as fraud detection and churn prediction. Most are based on supervised\nlearning with hand-engineered features, which relies heavily on the\navailability of labelled data. Large self-supervised generative models have\nshown tremendous success in natural language processing and computer vision,\nyet so far they haven't been adapted to multivariate time series of financial\ntransactions. In this paper, we present a generative pretraining method that\ncan be used to obtain contextualised embeddings of financial transactions.\nBenchmarks on public datasets demonstrate that it outperforms state-of-the-art\nself-supervised methods on a range of downstream tasks. We additionally perform\nlarge-scale pretraining of an embedding model using a corpus of data from 180\nissuing banks containing 5.1 billion transactions and apply it to the card\nfraud detection problem on hold-out datasets. The embedding model significantly\nimproves value detection rate at high precision thresholds and transfers well\nto out-of-domain distributions.\n","authors":["Piotr Skalski","David Sutton","Stuart Burrell","Iker Perez","Jason Wong"],"pdf_url":"https://arxiv.org/pdf/2401.01641v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01640v1","updated":"2024-01-03T09:31:43Z","published":"2024-01-03T09:31:43Z","title":"Evaluating Fairness in Self-supervised and Supervised Models for\n  Sequential Data","summary":"  Self-supervised learning (SSL) has become the de facto training paradigm of\nlarge models where pre-training is followed by supervised fine-tuning using\ndomain-specific data and labels. Hypothesizing that SSL models would learn more\ngeneric, hence less biased, representations, this study explores the impact of\npre-training and fine-tuning strategies on fairness (i.e., performing equally\non different demographic breakdowns). Motivated by human-centric applications\non real-world timeseries data, we interpret inductive biases on the model,\nlayer, and metric levels by systematically comparing SSL models to their\nsupervised counterparts. Our findings demonstrate that SSL has the capacity to\nachieve performance on par with supervised methods while significantly\nenhancing fairness--exhibiting up to a 27% increase in fairness with a mere 1%\nloss in performance through self-supervision. Ultimately, this work underscores\nSSL's potential in human-centric computing, particularly high-stakes,\ndata-scarce application domains like healthcare.\n","authors":["Sofia Yfantidou","Dimitris Spathis","Marios Constantinides","Athena Vakali","Daniele Quercia","Fahim Kawsar"],"pdf_url":"https://arxiv.org/pdf/2401.01640v1.pdf","comment":"Paper accepted in Human-Centric Representation Learning workshop at\n  AAAI 2024 (https://hcrl-workshop.github.io/2024/)"},{"id":"http://arxiv.org/abs/2401.01629v1","updated":"2024-01-03T09:03:30Z","published":"2024-01-03T09:03:30Z","title":"Synthetic Data in AI: Challenges, Applications, and Ethical Implications","summary":"  In the rapidly evolving field of artificial intelligence, the creation and\nutilization of synthetic datasets have become increasingly significant. This\nreport delves into the multifaceted aspects of synthetic data, particularly\nemphasizing the challenges and potential biases these datasets may harbor. It\nexplores the methodologies behind synthetic data generation, spanning\ntraditional statistical models to advanced deep learning techniques, and\nexamines their applications across diverse domains. The report also critically\naddresses the ethical considerations and legal implications associated with\nsynthetic datasets, highlighting the urgent need for mechanisms to ensure\nfairness, mitigate biases, and uphold ethical standards in AI development.\n","authors":["Shuang Hao","Wenfeng Han","Tao Jiang","Yiping Li","Haonan Wu","Chunlin Zhong","Zhangjun Zhou","He Tang"],"pdf_url":"https://arxiv.org/pdf/2401.01629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01626v1","updated":"2024-01-03T08:54:56Z","published":"2024-01-03T08:54:56Z","title":"On the Expressive Power of Graph Neural Networks","summary":"  The study of Graph Neural Networks has received considerable interest in the\npast few years. By extending deep learning to graph-structured data, GNNs can\nsolve a diverse set of tasks in fields including social science, chemistry, and\nmedicine. The development of GNN architectures has largely been focused on\nimproving empirical performance on tasks like node or graph classification.\nHowever, a line of recent work has instead sought to find GNN architectures\nthat have desirable theoretical properties - by studying their expressive power\nand designing architectures that maximize this expressiveness.\n  While there is no consensus on the best way to define the expressiveness of a\nGNN, it can be viewed from several well-motivated perspectives. Perhaps the\nmost natural approach is to study the universal approximation properties of\nGNNs, much in the way that this has been studied extensively for MLPs. Another\ndirection focuses on the extent to which GNNs can distinguish between different\ngraph structures, relating this to the graph isomorphism test. Besides, a GNN's\nability to compute graph properties such as graph moments has been suggested as\nanother form of expressiveness. All of these different definitions are\ncomplementary and have yielded different recommendations for GNN architecture\nchoices. In this paper, we would like to give an overview of the notion of\n\"expressive power\" of GNNs and provide some valuable insights regarding the\ndesign choices of GNNs.\n","authors":["Ashwin Nalwade","Kelly Marshall","Axel Eladi","Umang Sharma"],"pdf_url":"https://arxiv.org/pdf/2401.01626v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01625v1","updated":"2024-01-03T08:51:18Z","published":"2024-01-03T08:51:18Z","title":"SCALA: Sparsification-based Contrastive Learning for Anomaly Detection\n  on Attributed Networks","summary":"  Anomaly detection on attributed networks aims to find the nodes whose\nbehaviors are significantly different from other majority nodes. Generally,\nnetwork data contains information about relationships between entities, and the\nanomaly is usually embodied in these relationships. Therefore, how to\ncomprehensively model complex interaction patterns in networks is still a major\nfocus. It can be observed that anomalies in networks violate the homophily\nassumption. However, most existing studies only considered this phenomenon\nobliquely rather than explicitly. Besides, the node representation of normal\nentities can be perturbed easily by the noise relationships introduced by\nanomalous nodes. To address the above issues, we present a novel contrastive\nlearning framework for anomaly detection on attributed networks,\n\\textbf{SCALA}, aiming to improve the embedding quality of the network and\nprovide a new measurement of qualifying the anomaly score for each node by\nintroducing sparsification into the conventional method. Extensive experiments\nare conducted on five benchmark real-world datasets and the results show that\nSCALA consistently outperforms all baseline methods significantly.\n","authors":["Enbo He","Yitong Hao","Yue Zhang","Guisheng Yin","Lina Yao"],"pdf_url":"https://arxiv.org/pdf/2401.01625v1.pdf","comment":"9 pages, 14 figures"},{"id":"http://arxiv.org/abs/2206.04688v2","updated":"2024-01-03T08:32:44Z","published":"2022-06-09T08:27:59Z","title":"A New Frontier of AI: On-Device AI Training and Personalization","summary":"  Modern consumer electronic devices have started executing deep learning-based\nintelligence services on devices, not cloud servers, to keep personal data on\ndevices and to reduce network and cloud costs. We find such a trend as the\nopportunity to personalize intelligence services by updating neural networks\nwith user data without exposing the data out of devices: on-device training.\nHowever, the limited resources of devices incurs significant difficulties. We\npropose a light-weight on-device training framework, NNTrainer, which provides\nhighly memory-efficient neural network training techniques and proactive\nswapping based on fine-grained execution order analysis for neural networks.\nMoreover, its optimizations do not sacrifice accuracy and are transparent to\ntraining algorithms; thus, prior algorithmic studies may be implemented on top\nof NNTrainer. The evaluations show that NNTrainer can reduce memory consumption\ndown to 1/20 (saving 95%!) and effectively personalizes intelligence services\non devices. NNTrainer is cross-platform and practical open-source software,\nwhich is being deployed to millions of mobile devices.\n","authors":["Ji Joong Moon","Hyeonseok Lee","Jiho Chu","Donghak Park","Seungbaek Hong","Hyungjun Seo","Donghyeon Jeong","Sungsik Kong","MyungJoo Ham"],"pdf_url":"https://arxiv.org/pdf/2206.04688v2.pdf","comment":"12 pages, 16 figures, Accepted in ICSE 2024"},{"id":"http://arxiv.org/abs/2401.01262v2","updated":"2024-01-03T08:17:53Z","published":"2024-01-02T16:09:36Z","title":"Fairness Certification for Natural Language Processing and Large\n  Language Models","summary":"  Natural Language Processing (NLP) plays an important role in our daily lives,\nparticularly due to the enormous progress of Large Language Models (LLM).\nHowever, NLP has many fairness-critical use cases, e.g., as an expert system in\nrecruitment or as an LLM-based tutor in education. Since NLP is based on human\nlanguage, potentially harmful biases can diffuse into NLP systems and produce\nunfair results, discriminate against minorities or generate legal issues.\nHence, it is important to develop a fairness certification for NLP approaches.\nWe follow a qualitative research approach towards a fairness certification for\nNLP. In particular, we have reviewed a large body of literature on algorithmic\nfairness, and we have conducted semi-structured expert interviews with a wide\nrange of experts from that area. We have systematically devised six fairness\ncriteria for NLP, which can be further refined into 18 sub-categories. Our\ncriteria offer a foundation for operationalizing and testing processes to\ncertify fairness, both from the perspective of the auditor and the audited\norganization.\n","authors":["Vincent Freiberger","Erik Buchmann"],"pdf_url":"https://arxiv.org/pdf/2401.01262v2.pdf","comment":"In depth discussion of our results can be found in the Appendix B"},{"id":"http://arxiv.org/abs/2401.01600v1","updated":"2024-01-03T08:06:26Z","published":"2024-01-03T08:06:26Z","title":"PLLaMa: An Open-source Large Language Model for Plant Science","summary":"  Large Language Models (LLMs) have exhibited remarkable capabilities in\nunderstanding and interacting with natural language across various sectors.\nHowever, their effectiveness is limited in specialized areas requiring high\naccuracy, such as plant science, due to a lack of specific expertise in these\nfields. This paper introduces PLLaMa, an open-source language model that\nevolved from LLaMa-2. It's enhanced with a comprehensive database, comprising\nmore than 1.5 million scholarly articles in plant science. This development\nsignificantly enriches PLLaMa with extensive knowledge and proficiency in plant\nand agricultural sciences. Our initial tests, involving specific datasets\nrelated to plants and agriculture, show that PLLaMa substantially improves its\nunderstanding of plant science-related topics. Moreover, we have formed an\ninternational panel of professionals, including plant scientists, agricultural\nengineers, and plant breeders. This team plays a crucial role in verifying the\naccuracy of PLLaMa's responses to various academic inquiries, ensuring its\neffective and reliable application in the field. To support further research\nand development, we have made the model's checkpoints and source codes\naccessible to the scientific community. These resources are available for\ndownload at \\url{https://github.com/Xianjun-Yang/PLLaMa}.\n","authors":["Xianjun Yang","Junfeng Gao","Wenxin Xue","Erik Alexandersson"],"pdf_url":"https://arxiv.org/pdf/2401.01600v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2401.01599v1","updated":"2024-01-03T08:00:50Z","published":"2024-01-03T08:00:50Z","title":"Generalization Error Curves for Analytic Spectral Algorithms under\n  Power-law Decay","summary":"  The generalization error curve of certain kernel regression method aims at\ndetermining the exact order of generalization error with various source\ncondition, noise level and choice of the regularization parameter rather than\nthe minimax rate. In this work, under mild assumptions, we rigorously provide a\nfull characterization of the generalization error curves of the kernel gradient\ndescent method (and a large class of analytic spectral algorithms) in kernel\nregression. Consequently, we could sharpen the near inconsistency of kernel\ninterpolation and clarify the saturation effects of kernel regression\nalgorithms with higher qualification, etc. Thanks to the neural tangent kernel\ntheory, these results greatly improve our understanding of the generalization\nbehavior of training the wide neural networks. A novel technical contribution,\nthe analytic functional argument, might be of independent interest.\n","authors":["Yicheng Li","Weiye Gan","Zuoqiang Shi","Qian Lin"],"pdf_url":"https://arxiv.org/pdf/2401.01599v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.17207v3","updated":"2024-01-03T07:53:35Z","published":"2023-09-29T12:59:28Z","title":"Memory Gym: Towards Endless Tasks to Benchmark Memory Capabilities of\n  Agents","summary":"  Memory Gym presents a suite of 2D partially observable environments, namely\nMortar Mayhem, Mystery Path, and Searing Spotlights, designed to benchmark\nmemory capabilities in decision-making agents. These environments, originally\nwith finite tasks, are expanded into innovative, endless formats, mirroring the\nescalating challenges of cumulative memory games such as ``I packed my bag''.\nThis progression in task design shifts the focus from merely assessing sample\nefficiency to also probing the levels of memory effectiveness in dynamic,\nprolonged scenarios. To address the gap in available memory-based Deep\nReinforcement Learning baselines, we introduce an implementation that\nintegrates Transformer-XL (TrXL) with Proximal Policy Optimization. This\napproach utilizes TrXL as a form of episodic memory, employing a sliding window\ntechnique. Our comparative study between the Gated Recurrent Unit (GRU) and\nTrXL reveals varied performances across different settings. TrXL, on the finite\nenvironments, demonstrates superior sample efficiency in Mystery Path and\noutperforms in Mortar Mayhem. However, GRU is more efficient on Searing\nSpotlights. Most notably, in all endless tasks, GRU makes a remarkable\nresurgence, consistently outperforming TrXL by significant margins. Website and\nSource Code: https://github.com/MarcoMeter/endless-memory-gym/\n","authors":["Marco Pleines","Matthias Pallasch","Frank Zimmer","Mike Preuss"],"pdf_url":"https://arxiv.org/pdf/2309.17207v3.pdf","comment":"40 pages, 17 figures, 5 tables, under review"},{"id":"http://arxiv.org/abs/2310.04171v3","updated":"2024-01-03T07:32:11Z","published":"2023-10-06T11:41:38Z","title":"Dynamic Relation-Attentive Graph Neural Networks for Fraud Detection","summary":"  Fraud detection aims to discover fraudsters deceiving other users by, for\nexample, leaving fake reviews or making abnormal transactions. Graph-based\nfraud detection methods consider this task as a classification problem with two\nclasses: frauds or normal. We address this problem using Graph Neural Networks\n(GNNs) by proposing a dynamic relation-attentive aggregation mechanism. Based\non the observation that many real-world graphs include different types of\nrelations, we propose to learn a node representation per relation and aggregate\nthe node representations using a learnable attention function that assigns a\ndifferent attention coefficient to each relation. Furthermore, we combine the\nnode representations from different layers to consider both the local and\nglobal structures of a target node, which is beneficial to improving the\nperformance of fraud detection on graphs with heterophily. By employing dynamic\ngraph attention in all the aggregation processes, our method adaptively\ncomputes the attention coefficients for each node. Experimental results show\nthat our method, DRAG, outperforms state-of-the-art fraud detection methods on\nreal-world benchmark datasets.\n","authors":["Heehyeon Kim","Jinhyeok Choi","Joyce Jiyoung Whang"],"pdf_url":"https://arxiv.org/pdf/2310.04171v3.pdf","comment":"5 pages, 3 figures, 3 tables. Machine Learning on Graphs (MLoG)\n  Workshop at the 23rd IEEE International Conference on Data Mining (ICDM 2023)"},{"id":"http://arxiv.org/abs/2401.01579v1","updated":"2024-01-03T07:06:26Z","published":"2024-01-03T07:06:26Z","title":"An Invariant Information Geometric Method for High-Dimensional Online\n  Optimization","summary":"  Sample efficiency is crucial in optimization, particularly in black-box\nscenarios characterized by expensive evaluations and zeroth-order feedback.\nWhen computing resources are plentiful, Bayesian optimization is often favored\nover evolution strategies. In this paper, we introduce a full invariance\noriented evolution strategies algorithm, derived from its corresponding\nframework, that effectively rivals the leading Bayesian optimization method in\ntasks with dimensions at the upper limit of Bayesian capability. Specifically,\nwe first build the framework InvIGO that fully incorporates historical\ninformation while retaining the full invariant and computational complexity. We\nthen exemplify InvIGO on multi-dimensional Gaussian, which gives an invariant\nand scalable optimizer SynCMA . The theoretical behavior and advantages of our\nalgorithm over other Gaussian-based evolution strategies are further analyzed.\nFinally, We benchmark SynCMA against leading algorithms in Bayesian\noptimization and evolution strategies on various high dimension tasks, in\ncluding Mujoco locomotion tasks, rover planning task and synthetic functions.\nIn all scenarios, SynCMA demonstrates great competence, if not dominance, over\nother algorithms in sample efficiency, showing the underdeveloped potential of\nproperty oriented evolution strategies.\n","authors":["Zhengfei Zhang","Yunyue Wei","Yanan Sui"],"pdf_url":"https://arxiv.org/pdf/2401.01579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04444v3","updated":"2024-01-03T06:38:36Z","published":"2023-10-02T22:35:40Z","title":"What's the Magic Word? A Control Theory of LLM Prompting","summary":"  Prompt engineering is crucial for deploying LLMs but is poorly understood\nmathematically. We formalize LLM systems as a class of discrete stochastic\ndynamical systems to explore prompt engineering through the lens of control\ntheory. We investigate the reachable set of output token sequences $R_y(\\mathbf\nx_0)$ for which there exists a control input sequence $\\mathbf u$ for each\n$\\mathbf y \\in R_y(\\mathbf x_0)$ that steers the LLM to output $\\mathbf y$ from\ninitial state sequence $\\mathbf x_0$. We offer analytic analysis on the\nlimitations on the controllability of self-attention in terms of reachable set,\nwhere we prove an upper bound on the reachable set of outputs $R_y(\\mathbf\nx_0)$ as a function of the singular values of the parameter matrices. We\npresent complementary empirical analysis on the controllability of a panel of\nLLMs, including Falcon-7b, Llama-7b, and Falcon-40b. Our results demonstrate a\nlower bound on the reachable set of outputs $R_y(\\mathbf x_0)$ w.r.t. initial\nstate sequences $\\mathbf x_0$ sampled from the Wikitext dataset. We find that\nthe correct next Wikitext token following sequence $\\mathbf x_0$ is reachable\nover 97% of the time with prompts of $k\\leq 10$ tokens. We also establish that\nthe top 75 most likely next tokens, as estimated by the LLM itself, are\nreachable at least 85% of the time with prompts of $k\\leq 10$ tokens.\nIntriguingly, short prompt sequences can dramatically alter the likelihood of\nspecific outputs, even making the least likely tokens become the most likely\nones. This control-centric analysis of LLMs demonstrates the significant and\npoorly understood role of input sequences in steering output probabilities,\noffering a foundational perspective for enhancing language model system\ncapabilities.\n","authors":["Aman Bhargava","Cameron Witkowski","Manav Shah","Matt Thomson"],"pdf_url":"https://arxiv.org/pdf/2310.04444v3.pdf","comment":"23 pages, 8 figures. Under review for ICLR 2024"},{"id":"http://arxiv.org/abs/2309.02084v3","updated":"2024-01-03T06:21:35Z","published":"2023-09-05T09:42:15Z","title":"Unsupervised Out-of-Distribution Detection by Restoring Lossy Inputs\n  with Variational Autoencoder","summary":"  Deep generative models have been demonstrated as problematic in the\nunsupervised out-of-distribution (OOD) detection task, where they tend to\nassign higher likelihoods to OOD samples. Previous studies on this issue are\nusually not applicable to the Variational Autoencoder (VAE). As a popular\nsubclass of generative models, the VAE can be effective with a relatively\nsmaller model size and be more stable and faster in training and inference,\nwhich can be more advantageous in real-world applications. In this paper, We\npropose a novel VAE-based score called Error Reduction (ER) for OOD detection,\nwhich is based on a VAE that takes a lossy version of the training set as\ninputs and the original set as targets. Experiments are carried out on various\ndatasets to show the effectiveness of our method, we also present the effect of\ndesign choices with ablation experiments. Our code is available at:\nhttps://github.com/ZJLAB-AMMI/VAE4OOD.\n","authors":["Zezhen Zeng","Bin Liu"],"pdf_url":"https://arxiv.org/pdf/2309.02084v3.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2311.12564v3","updated":"2024-01-03T05:57:32Z","published":"2023-11-21T12:23:58Z","title":"Summary of the DISPLACE Challenge 2023 -- DIarization of SPeaker and\n  LAnguage in Conversational Environments","summary":"  In multi-lingual societies, where multiple languages are spoken in a small\ngeographic vicinity, informal conversations often involve mix of languages.\nExisting speech technologies may be inefficient in extracting information from\nsuch conversations, where the speech data is rich in diversity with multiple\nlanguages and speakers. The DISPLACE (DIarization of SPeaker and LAnguage in\nConversational Environments) challenge constitutes an open-call for evaluating\nand bench-marking the speaker and language diarization technologies on this\nchallenging condition. The challenge entailed two tracks: Track-1 focused on\nspeaker diarization (SD) in multilingual situations while, Track-2 addressed\nthe language diarization (LD) in a multi-speaker scenario. Both the tracks were\nevaluated using the same underlying audio data. To facilitate this evaluation,\na real-world dataset featuring multilingual, multi-speaker conversational\nfar-field speech was recorded and distributed. Furthermore, a baseline system\nwas made available for both SD and LD task which mimicked the state-of-art in\nthese tasks. The challenge garnered a total of $42$ world-wide registrations\nand received a total of $19$ combined submissions for Track-1 and Track-2. This\npaper describes the challenge, details of the datasets, tasks, and the baseline\nsystem. Additionally, the paper provides a concise overview of the submitted\nsystems in both tracks, with an emphasis given to the top performing systems.\nThe paper also presents insights and future perspectives for SD and LD tasks,\nfocusing on the key challenges that the systems need to overcome before\nwide-spread commercial deployment on such conversations.\n","authors":["Shikha Baghel","Shreyas Ramoji","Somil Jain","Pratik Roy Chowdhuri","Prachi Singh","Deepu Vijayasenan","Sriram Ganapathy"],"pdf_url":"https://arxiv.org/pdf/2311.12564v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01549v1","updated":"2024-01-03T05:51:49Z","published":"2024-01-03T05:51:49Z","title":"Towards Modeling Uncertainties of Self-explaining Neural Networks via\n  Conformal Prediction","summary":"  Despite the recent progress in deep neural networks (DNNs), it remains\nchallenging to explain the predictions made by DNNs. Existing explanation\nmethods for DNNs mainly focus on post-hoc explanations where another\nexplanatory model is employed to provide explanations. The fact that post-hoc\nmethods can fail to reveal the actual original reasoning process of DNNs raises\nthe need to build DNNs with built-in interpretability. Motivated by this, many\nself-explaining neural networks have been proposed to generate not only\naccurate predictions but also clear and intuitive insights into why a\nparticular decision was made. However, existing self-explaining networks are\nlimited in providing distribution-free uncertainty quantification for the two\nsimultaneously generated prediction outcomes (i.e., a sample's final prediction\nand its corresponding explanations for interpreting that prediction).\nImportantly, they also fail to establish a connection between the confidence\nvalues assigned to the generated explanations in the interpretation layer and\nthose allocated to the final predictions in the ultimate prediction layer. To\ntackle the aforementioned challenges, in this paper, we design a novel\nuncertainty modeling framework for self-explaining networks, which not only\ndemonstrates strong distribution-free uncertainty modeling performance for the\ngenerated explanations in the interpretation layer but also excels in producing\nefficient and effective prediction sets for the final predictions based on the\ninformative high-level basis explanations. We perform the theoretical analysis\nfor the proposed framework. Extensive experimental evaluation demonstrates the\neffectiveness of the proposed uncertainty framework.\n","authors":["Wei Qian","Chenxu Zhao","Yangyi Li","Fenglong Ma","Chao Zhang","Mengdi Huai"],"pdf_url":"https://arxiv.org/pdf/2401.01549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.10253v3","updated":"2024-01-03T04:51:27Z","published":"2023-07-17T09:35:18Z","title":"Efficient selective attention LSTM for well log curve synthesis","summary":"  Non-core drilling has gradually become the primary exploration method in\ngeological exploration engineering, and well logging curves have increasingly\ngained importance as the main carriers of geological information. However,\nfactors such as geological environment, logging equipment, borehole quality,\nand unexpected events can all impact the quality of well logging curves.\nPrevious methods of re-logging or manual corrections have been associated with\nhigh costs and low efficiency. This paper proposes a machine learning method\nthat utilizes existing data to predict missing data, and its effectiveness and\nfeasibility have been validated through field experiments. The proposed method\nbuilds on the traditional Long Short-Term Memory (LSTM) neural network by\nincorporating a self-attention mechanism to analyze the sequential dependencies\nof the data. It selects the dominant computational results in the LSTM,\nreducing the computational complexity from O(n^2) to O(nlogn) and improving\nmodel efficiency. Experimental results demonstrate that the proposed method\nachieves higher accuracy compared to traditional curve synthesis methods based\non Fully Connected Neural Networks (FCNN) and vanilla LSTM. This accurate,\nefficient, and cost-effective prediction method holds a practical value in\nengineering applications.\n","authors":["Yuankai Zhou","Huanyu Li"],"pdf_url":"https://arxiv.org/pdf/2307.10253v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01537v1","updated":"2024-01-03T04:31:59Z","published":"2024-01-03T04:31:59Z","title":"The Art of Deception: Robust Backdoor Attack using Dynamic Stacking of\n  Triggers","summary":"  The area of Machine Learning as a Service (MLaaS) is experiencing increased\nimplementation due to recent advancements in the AI (Artificial Intelligence)\nindustry. However, this spike has prompted concerns regarding AI defense\nmechanisms, specifically regarding potential covert attacks from third-party\nproviders that cannot be entirely trusted. Recent research has uncovered that\nauditory backdoors may use certain modifications as their initiating mechanism.\nDynamicTrigger is introduced as a methodology for carrying out dynamic backdoor\nattacks that use cleverly designed tweaks to ensure that corrupted samples are\nindistinguishable from clean. By utilizing fluctuating signal sampling rates\nand masking speaker identities through dynamic sound triggers (such as the\nclapping of hands), it is possible to deceive speech recognition systems (ASR).\nOur empirical testing demonstrates that DynamicTrigger is both potent and\nstealthy, achieving impressive success rates during covert attacks while\nmaintaining exceptional accuracy with non-poisoned datasets.\n","authors":["Orson Mengara"],"pdf_url":"https://arxiv.org/pdf/2401.01537v1.pdf","comment":"Accepted by AAAI Workshop 2024, 8 pages"},{"id":"http://arxiv.org/abs/2312.14705v2","updated":"2024-01-03T04:14:07Z","published":"2023-12-22T14:06:03Z","title":"SCUNet++: Swin-UNet and CNN Bottleneck Hybrid Architecture with\n  Multi-Fusion Dense Skip Connection for Pulmonary Embolism CT Image\n  Segmentation","summary":"  Pulmonary embolism (PE) is a prevalent lung disease that can lead to right\nventricular hypertrophy and failure in severe cases, ranking second in severity\nonly to myocardial infarction and sudden death. Pulmonary artery CT angiography\n(CTPA) is a widely used diagnostic method for PE. However, PE detection\npresents challenges in clinical practice due to limitations in imaging\ntechnology. CTPA can produce noises similar to PE, making confirmation of its\npresence time-consuming and prone to overdiagnosis. Nevertheless, the\ntraditional segmentation method of PE can not fully consider the hierarchical\nstructure of features, local and global spatial features of PE CT images. In\nthis paper, we propose an automatic PE segmentation method called SCUNet++\n(Swin Conv UNet++). This method incorporates multiple fusion dense skip\nconnections between the encoder and decoder, utilizing the Swin Transformer as\nthe encoder. And fuses features of different scales in the decoder subnetwork\nto compensate for spatial information loss caused by the inevitable\ndownsampling in Swin-UNet or other state-of-the-art methods, effectively\nsolving the above problem. We provide a theoretical analysis of this method in\ndetail and validate it on publicly available PE CT image datasets FUMPE and\nCAD-PE. The experimental results indicate that our proposed method achieved a\nDice similarity coefficient (DSC) of 83.47% and a Hausdorff distance 95th\npercentile (HD95) of 3.83 on the FUMPE dataset, as well as a DSC of 83.42% and\nan HD95 of 5.10 on the CAD-PE dataset. These findings demonstrate that our\nmethod exhibits strong performance in PE segmentation tasks, potentially\nenhancing the accuracy of automatic segmentation of PE and providing a powerful\ndiagnostic tool for clinical physicians. Our source code and new FUMPE dataset\nare available at https://github.com/JustlfC03/SCUNet-plusplus.\n","authors":["Yifei Chen","Binfeng Zou","Zhaoxin Guo","Yiyu Huang","Yifan Huang","Feiwei Qin","Qinhai Li","Changmiao Wang"],"pdf_url":"https://arxiv.org/pdf/2312.14705v2.pdf","comment":"10 pages, 7 figures, accept WACV2024"},{"id":"http://arxiv.org/abs/2401.01531v1","updated":"2024-01-03T04:01:20Z","published":"2024-01-03T04:01:20Z","title":"Will 6G be Semantic Communications? Opportunities and Challenges from\n  Task Oriented and Secure Communications to Integrated Sensing","summary":"  This paper explores opportunities and challenges of task (goal)-oriented and\nsemantic communications for next-generation (NextG) communication networks\nthrough the integration of multi-task learning. This approach employs deep\nneural networks representing a dedicated encoder at the transmitter and\nmultiple task-specific decoders at the receiver, collectively trained to handle\ndiverse tasks including semantic information preservation, source input\nreconstruction, and integrated sensing and communications. To extend the\napplicability from point-to-point links to multi-receiver settings, we envision\nthe deployment of decoders at various receivers, where decentralized learning\naddresses the challenges of communication load and privacy concerns, leveraging\nfederated learning techniques that distribute model updates across\ndecentralized nodes. However, the efficacy of this approach is contingent on\nthe robustness of the employed deep learning models. We scrutinize potential\nvulnerabilities stemming from adversarial attacks during both training and\ntesting phases. These attacks aim to manipulate both the inputs at the encoder\nat the transmitter and the signals received over the air on the receiver side,\nhighlighting the importance of fortifying semantic communications against\npotential multi-domain exploits. Overall, the joint and robust design of\ntask-oriented communications, semantic communications, and integrated sensing\nand communications in a multi-task learning framework emerges as the key\nenabler for context-aware, resource-efficient, and secure communications\nultimately needed in NextG network systems.\n","authors":["Yalin E. Sagduyu","Tugba Erpek","Aylin Yener","Sennur Ulukus"],"pdf_url":"https://arxiv.org/pdf/2401.01531v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.11348v2","updated":"2024-01-03T04:00:15Z","published":"2023-05-18T23:47:00Z","title":"In the Name of Fairness: Assessing the Bias in Clinical Record\n  De-identification","summary":"  Data sharing is crucial for open science and reproducible research, but the\nlegal sharing of clinical data requires the removal of protected health\ninformation from electronic health records. This process, known as\nde-identification, is often achieved through the use of machine learning\nalgorithms by many commercial and open-source systems. While these systems have\nshown compelling results on average, the variation in their performance across\ndifferent demographic groups has not been thoroughly examined. In this work, we\ninvestigate the bias of de-identification systems on names in clinical notes\nvia a large-scale empirical analysis. To achieve this, we create 16 name sets\nthat vary along four demographic dimensions: gender, race, name popularity, and\nthe decade of popularity. We insert these names into 100 manually curated\nclinical templates and evaluate the performance of nine public and private\nde-identification methods. Our findings reveal that there are statistically\nsignificant performance gaps along a majority of the demographic dimensions in\nmost methods. We further illustrate that de-identification quality is affected\nby polysemy in names, gender context, and clinical note characteristics. To\nmitigate the identified gaps, we propose a simple and method-agnostic solution\nby fine-tuning de-identification methods with clinical context and diverse\nnames. Overall, it is imperative to address the bias in existing methods\nimmediately so that downstream stakeholders can build high-quality systems to\nserve all demographic parties fairly.\n","authors":["Yuxin Xiao","Shulammite Lim","Tom Joseph Pollard","Marzyeh Ghassemi"],"pdf_url":"https://arxiv.org/pdf/2305.11348v2.pdf","comment":"Accepted by FAccT 2023; updated appendix with the de-identification\n  performance of GPT-4"},{"id":"http://arxiv.org/abs/2401.01528v1","updated":"2024-01-03T03:45:35Z","published":"2024-01-03T03:45:35Z","title":"Improved Bandits in Many-to-one Matching Markets with Incentive\n  Compatibility","summary":"  Two-sided matching markets have been widely studied in the literature due to\ntheir rich applications. Since participants are usually uncertain about their\npreferences, online algorithms have recently been adopted to learn them through\niterative interactions. \\citet{wang2022bandit} initiate the study of this\nproblem in a many-to-one setting with \\textit{responsiveness}. However, their\nresults are far from optimal and lack guarantees of incentive compatibility. An\nextension of \\citet{kong2023player} to this more general setting achieves a\nnear-optimal bound for player-optimal regret. Nevertheless, due to the\nsubstantial requirement for collaboration, a single player's deviation could\nlead to a huge increase in its own cumulative rewards and an $O(T)$ regret for\nothers. In this paper, we aim to enhance the regret bound in many-to-one\nmarkets while ensuring incentive compatibility. We first propose the adaptively\nexplore-then-deferred-acceptance (AETDA) algorithm for responsiveness setting\nand derive an $O(N\\min\\left\\{N,K\\right\\}C\\log T/\\Delta^2)$ upper bound for\nplayer-optimal stable regret while demonstrating its guarantee of incentive\ncompatibility, where $N$ represents the number of players, $K$ is the number of\narms, $T$ denotes the time horizon, $C$ is arms' total capacities and $\\Delta$\nsignifies the minimum preference gap among players. This result is a\nsignificant improvement over \\citet{wang2022bandit}. And to the best of our\nknowledge, it constitutes the first player-optimal guarantee in matching\nmarkets that offers such robust assurances. We also consider broader\n\\textit{substitutable} preferences, one of the most general conditions to\nensure the existence of a stable matching and cover responsiveness. We devise\nan online DA (ODA) algorithm and establish an $O(NK\\log T/\\Delta^2)$\nplayer-pessimal stable regret bound for this setting.\n","authors":["Fang Kong","Shuai Li"],"pdf_url":"https://arxiv.org/pdf/2401.01528v1.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2401.01520v1","updated":"2024-01-03T03:08:32Z","published":"2024-01-03T03:08:32Z","title":"S$^{2}$-DMs:Skip-Step Diffusion Models","summary":"  Diffusion models have emerged as powerful generative tools, rivaling GANs in\nsample quality and mirroring the likelihood scores of autoregressive models. A\nsubset of these models, exemplified by DDIMs, exhibit an inherent asymmetry:\nthey are trained over $T$ steps but only sample from a subset of $T$ during\ngeneration. This selective sampling approach, though optimized for speed,\ninadvertently misses out on vital information from the unsampled steps, leading\nto potential compromises in sample quality. To address this issue, we present\nthe S$^{2}$-DMs, which is a new training method by using an innovative\n$L_{skip}$, meticulously designed to reintegrate the information omitted during\nthe selective sampling phase. The benefits of this approach are manifold: it\nnotably enhances sample quality, is exceptionally simple to implement, requires\nminimal code modifications, and is flexible enough to be compatible with\nvarious sampling algorithms. On the CIFAR10 dataset, models trained using our\nalgorithm showed an improvement of 3.27% to 14.06% over models trained with\ntraditional methods across various sampling algorithms (DDIMs, PNDMs, DEIS) and\ndifferent numbers of sampling steps (10, 20, ..., 1000). On the CELEBA dataset,\nthe improvement ranged from 8.97% to 27.08%. Access to the code and additional\nresources is provided in the github.\n","authors":["Yixuan Wang","Shuangyin Li"],"pdf_url":"https://arxiv.org/pdf/2401.01520v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2401.01519v1","updated":"2024-01-03T03:01:29Z","published":"2024-01-03T03:01:29Z","title":"Exploring the Frontiers of LLMs in Psychological Applications: A\n  Comprehensive Review","summary":"  This paper explores the frontiers of large language models (LLMs) in\npsychology applications. Psychology has undergone several theoretical changes,\nand the current use of Artificial Intelligence (AI) and Machine Learning,\nparticularly LLMs, promises to open up new research directions. We provide a\ndetailed exploration of how LLMs like ChatGPT are transforming psychological\nresearch. It discusses the impact of LLMs across various branches of\npsychology, including cognitive and behavioral, clinical and counseling,\neducational and developmental, and social and cultural psychology, highlighting\ntheir potential to simulate aspects of human cognition and behavior. The paper\ndelves into the capabilities of these models to emulate human-like text\ngeneration, offering innovative tools for literature review, hypothesis\ngeneration, experimental design, experimental subjects, data analysis, academic\nwriting, and peer review in psychology. While LLMs are essential in advancing\nresearch methodologies in psychology, the paper also cautions about their\ntechnical and ethical challenges. There are issues like data privacy, the\nethical implications of using LLMs in psychological research, and the need for\na deeper understanding of these models' limitations. Researchers should\nresponsibly use LLMs in psychological studies, adhering to ethical standards\nand considering the potential consequences of deploying these technologies in\nsensitive areas. Overall, the article provides a comprehensive overview of the\ncurrent state of LLMs in psychology, exploring potential benefits and\nchallenges. It serves as a call to action for researchers to leverage LLLs'\nadvantages responsibly while addressing associated risks.\n","authors":["Luoma Ke","Song Tong","Peng Chen","Kaiping Peng"],"pdf_url":"https://arxiv.org/pdf/2401.01519v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01506v1","updated":"2024-01-03T02:22:39Z","published":"2024-01-03T02:22:39Z","title":"AIRI: Predicting Retention Indices and their Uncertainties using\n  Artificial Intelligence","summary":"  The Kov\\'ats Retention index (RI) is a quantity measured using gas\nchromatography and commonly used in the identification of chemical structures.\nCreating libraries of observed RI values is a laborious task, so we explore the\nuse of a deep neural network for predicting RI values from structure for\nstandard semipolar columns. This network generated predictions with a mean\nabsolute error of 15.1 and, in a quantification of the tail of the error\ndistribution, a 95th percentile absolute error of 46.5. Because of the\nArtificial Intelligence Retention Indices (AIRI) network's accuracy, it was\nused to predict RI values for the NIST EI-MS spectral libraries. These RI\nvalues are used to improve chemical identification methods and the quality of\nthe library. Estimating uncertainty is an important practical need when using\nprediction models. To quantify the uncertainty of our network for each\nindividual prediction, we used the outputs of an ensemble of 8 networks to\ncalculate a predicted standard deviation for each RI value prediction. This\npredicted standard deviation was corrected to follow the error between observed\nand predicted RI values. The Z scores using these predicted standard deviations\nhad a standard deviation of 1.52 and a 95th percentile absolute Z score\ncorresponding to a mean RI value of 42.6.\n","authors":["Lewis Y. Geer","Stephen E. Stein","William Gary Mallard","Douglas J. Slotta"],"pdf_url":"https://arxiv.org/pdf/2401.01506v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2108.08454v4","updated":"2024-01-03T02:20:42Z","published":"2021-08-19T02:57:58Z","title":"Improving Human Sequential Decision-Making with Reinforcement Learning","summary":"  Workers spend a significant amount of time learning how to make good\ndecisions. Evaluating the efficacy of a given decision, however, can be\ncomplicated -- e.g., decision outcomes are often long-term and relate to the\noriginal decision in complex ways. Surprisingly, even though learning good\ndecision-making strategies is difficult, they can often be expressed in simple\nand concise forms. Focusing on sequential decision-making, we design a novel\nmachine learning algorithm that is capable of extracting \"best practices\" from\ntrace data and conveying its insights to humans in the form of interpretable\n\"tips\". Our algorithm selects the tip that best bridges the gap between the\nactions taken by human workers and those taken by the optimal policy in a way\nthat accounts for which actions are consequential for achieving higher\nperformance. We evaluate our approach through a series of randomized controlled\nexperiments where participants manage a virtual kitchen. Our experiments show\nthat the tips generated by our algorithm can significantly improve human\nperformance relative to intuitive baselines. In addition, we discuss a number\nof empirical insights that can help inform the design of algorithms intended\nfor human-AI interfaces. For instance, we find evidence that participants do\nnot simply blindly follow our tips; instead, they combine them with their own\nexperience to discover additional strategies for improving performance.\n","authors":["Hamsa Bastani","Osbert Bastani","Wichinpong Park Sinchaisri"],"pdf_url":"https://arxiv.org/pdf/2108.08454v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00744v3","updated":"2024-01-03T02:17:26Z","published":"2024-01-01T12:57:15Z","title":"Harmonizing Covariance and Expressiveness for Deep Hamiltonian\n  Regression in Crystalline Material Research: a Hybrid Cascaded Regression\n  Framework","summary":"  Deep learning for Hamiltonian regression of quantum systems in material\nresearch necessitates satisfying the covariance laws, among which achieving\nSO(3)-equivariance without sacrificing the expressiveness of networks remains\nan elusive challenge due to the restriction to non-linear mappings on\nguaranteeing theoretical equivariance. To alleviate the\ncovariance-expressiveness dilemma, we propose a hybrid framework with two\ncascaded regression stages. The first stage, with a theoretically-guaranteed\ncovariant neural network modeling symmetry properties of 3D atom systems,\nyields theoretically covariant features and baseline Hamiltonian predictions,\nassisting the second stage in learning covariance. Meanwhile, the second stage,\npowered by a non-linear 3D graph Transformer network we propose for structural\nmodeling of 3D atomic systems, refines the first stage's output as a\nfine-grained prediction of Hamiltonians with better expressiveness capability.\nThe combination of a theoretically covariant yet inevitably less expressive\nmodel with a highly expressive non-linear network enables precise,\ngeneralizable predictions while maintaining robust covariance under coordinate\ntransformations. Our method achieves state-of-the-art performance in\nHamiltonian prediction for electronic structure calculations, confirmed through\nexperiments on five crystalline material databases.\n","authors":["Shi Yin","Xudong Zhu","Tianyu Gao","Haochong Zhang","Feng Wu","Lixin He"],"pdf_url":"https://arxiv.org/pdf/2401.00744v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01502v1","updated":"2024-01-03T02:15:32Z","published":"2024-01-03T02:15:32Z","title":"Pontryagin Neural Operator for Solving Parametric General-Sum\n  Differential Games","summary":"  The values of two-player general-sum differential games are viscosity\nsolutions to Hamilton-Jacobi-Isaacs (HJI) equations. Value and policy\napproximations for such games suffer from the curse of dimensionality (CoD).\nAlleviating CoD through physics-informed neural networks (PINN) encounters\nconvergence issues when value discontinuity is present due to state\nconstraints. On top of these challenges, it is often necessary to learn\ngeneralizable values and policies across a parametric space of games, e.g., for\ngame parameter inference when information is incomplete. To address these\nchallenges, we propose in this paper a Pontryagin-mode neural operator that\noutperforms existing state-of-the-art (SOTA) on safety performance across games\nwith parametric state constraints. Our key contribution is the introduction of\na costate loss defined on the discrepancy between forward and backward costate\nrollouts, which are computationally cheap. We show that the discontinuity of\ncostate dynamics (in the presence of state constraints) effectively enables the\nlearning of discontinuous values, without requiring manually supervised data as\nsuggested by the current SOTA. More importantly, we show that the close\nrelationship between costates and policies makes the former critical in\nlearning feedback control policies with generalizable safety performance.\n","authors":["Lei Zhang","Mukesh Ghimire","Zhe Xu","Wenlong Zhang","Yi Ren"],"pdf_url":"https://arxiv.org/pdf/2401.01502v1.pdf","comment":"Submitted to L4DC 2024"},{"id":"http://arxiv.org/abs/2401.01498v1","updated":"2024-01-03T02:03:36Z","published":"2024-01-03T02:03:36Z","title":"Utilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic\n  Token Prediction","summary":"  We propose a novel text-to-speech (TTS) framework centered around a neural\ntransducer. Our approach divides the whole TTS pipeline into semantic-level\nsequence-to-sequence (seq2seq) modeling and fine-grained acoustic modeling\nstages, utilizing discrete semantic tokens obtained from wav2vec2.0 embeddings.\nFor a robust and efficient alignment modeling, we employ a neural transducer\nnamed token transducer for the semantic token prediction, benefiting from its\nhard monotonic alignment constraints. Subsequently, a non-autoregressive (NAR)\nspeech generator efficiently synthesizes waveforms from these semantic tokens.\nAdditionally, a reference speech controls temporal dynamics and acoustic\nconditions at each stage. This decoupled framework reduces the training\ncomplexity of TTS while allowing each stage to focus on semantic and acoustic\nmodeling. Our experimental results on zero-shot adaptive TTS demonstrate that\nour model surpasses the baseline in terms of speech quality and speaker\nsimilarity, both objectively and subjectively. We also delve into the inference\nspeed and prosody control capabilities of our approach, highlighting the\npotential of neural transducers in TTS frameworks.\n","authors":["Minchan Kim","Myeonghun Jeong","Byoung Jin Choi","Semin Kim","Joun Yeop Lee","Nam Soo Kim"],"pdf_url":"https://arxiv.org/pdf/2401.01498v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2401.01493v1","updated":"2024-01-03T01:45:00Z","published":"2024-01-03T01:45:00Z","title":"Free Lunch for Federated Remote Sensing Target Fine-Grained\n  Classification: A Parameter-Efficient Framework","summary":"  Remote Sensing Target Fine-grained Classification (TFGC) is of great\nsignificance in both military and civilian fields. Due to location differences,\ngrowth in data size, and centralized server storage constraints, these data are\nusually stored under different databases across regions/countries. However,\nprivacy laws and national security concerns constrain researchers from\naccessing these sensitive remote sensing images for further analysis.\nAdditionally, low-resource remote sensing devices encounter challenges in terms\nof communication overhead and efficiency when dealing with the ever-increasing\ndata and model scales. To solve the above challenges, this paper proposes a\nnovel Privacy-Reserving TFGC Framework based on Federated Learning, dubbed\nPRFL. The proposed framework allows each client to learn global and local\nknowledge to enhance the local representation of private data in environments\nwith extreme statistical heterogeneity (non. Independent and Identically\nDistributed, IID). Thus, it provides highly customized models to clients with\ndifferentiated data distributions. Moreover, the framework minimizes\ncommunication overhead and improves efficiency while ensuring satisfactory\nperformance, thereby enhancing robustness and practical applicability under\nresource-scarce conditions. We demonstrate the effectiveness of the proposed\nPRFL on the classical TFGC task by leveraging four public datasets.\n","authors":["Shengchao Chen","Ting Shu","Huan Zhao","Jiahao Wang","Sufen Ren","Lina Yang"],"pdf_url":"https://arxiv.org/pdf/2401.01493v1.pdf","comment":"Under Review, 23 pages, 3 figures, 12 tables"},{"id":"http://arxiv.org/abs/2401.01487v1","updated":"2024-01-03T01:21:30Z","published":"2024-01-03T01:21:30Z","title":"Natural Language Processing and Multimodal Stock Price Prediction","summary":"  In the realm of financial decision-making, predicting stock prices is\npivotal. Artificial intelligence techniques such as long short-term memory\nnetworks (LSTMs), support-vector machines (SVMs), and natural language\nprocessing (NLP) models are commonly employed to predict said prices. This\npaper utilizes stock percentage change as training data, in contrast to the\ntraditional use of raw currency values, with a focus on analyzing publicly\nreleased news articles. The choice of percentage change aims to provide models\nwith context regarding the significance of price fluctuations and overall price\nchange impact on a given stock. The study employs specialized BERT natural\nlanguage processing models to predict stock price trends, with a particular\nemphasis on various data modalities. The results showcase the capabilities of\nsuch strategies with a small natural language processing model to accurately\npredict overall stock trends, and highlight the effectiveness of certain data\nfeatures and sector-specific data.\n","authors":["Kevin Taylor","Jerry Ng"],"pdf_url":"https://arxiv.org/pdf/2401.01487v1.pdf","comment":"13 pages, 13 figures"},{"id":"http://arxiv.org/abs/2401.01484v1","updated":"2024-01-03T01:18:18Z","published":"2024-01-03T01:18:18Z","title":"Uncertainty Regularized Evidential Regression","summary":"  The Evidential Regression Network (ERN) represents a novel approach that\nintegrates deep learning with Dempster-Shafer's theory to predict a target and\nquantify the associated uncertainty. Guided by the underlying theory, specific\nactivation functions must be employed to enforce non-negative values, which is\na constraint that compromises model performance by limiting its ability to\nlearn from all samples. This paper provides a theoretical analysis of this\nlimitation and introduces an improvement to overcome it. Initially, we define\nthe region where the models can't effectively learn from the samples. Following\nthis, we thoroughly analyze the ERN and investigate this constraint. Leveraging\nthe insights from our analysis, we address the limitation by introducing a\nnovel regularization term that empowers the ERN to learn from the whole\ntraining set. Our extensive experiments substantiate our theoretical findings\nand demonstrate the effectiveness of the proposed solution.\n","authors":["Kai Ye","Tiejin Chen","Hua Wei","Liang Zhan"],"pdf_url":"https://arxiv.org/pdf/2401.01484v1.pdf","comment":"Accepted to AAAI 2024 main track"},{"id":"http://arxiv.org/abs/2205.00147v5","updated":"2024-01-03T01:13:25Z","published":"2022-04-30T03:46:03Z","title":"DIRA: Dynamic Domain Incremental Regularised Adaptation","summary":"  Autonomous systems (AS) often use Deep Neural Network (DNN) classifiers to\nallow them to operate in complex, high-dimensional, non-linear, and dynamically\nchanging environments. Due to the complexity of these environments, DNN\nclassifiers may output misclassifications during operation when they face\ndomains not identified during development. Removing a system from operation for\nretraining becomes impractical as the number of such AS increases. To increase\nAS reliability and overcome this limitation, DNN classifiers need to have the\nability to adapt during operation when faced with different operational domains\nusing a few samples (e.g. 2 to 100 samples). However, retraining DNNs on a few\nsamples is known to cause catastrophic forgetting and poor generalisation. In\nthis paper, we introduce Dynamic Incremental Regularised Adaptation (DIRA), an\napproach for dynamic operational domain adaption of DNNs using regularisation\ntechniques. We show that DIRA improves on the problem of forgetting and\nachieves strong gains in performance when retraining using a few samples from\nthe target domain. Our approach shows improvements on different image\nclassification benchmarks aimed at evaluating robustness to distribution shifts\n(e.g.CIFAR-10C/100C, ImageNet-C), and produces state-of-the-art performance in\ncomparison with other methods from the literature.\n","authors":["Abanoub Ghobrial","Xuan Zheng","Darryl Hond","Hamid Asgari","Kerstin Eder"],"pdf_url":"https://arxiv.org/pdf/2205.00147v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01482v1","updated":"2024-01-03T01:11:16Z","published":"2024-01-03T01:11:16Z","title":"Incorporating Geo-Diverse Knowledge into Prompting for Increased\n  Geographical Robustness in Object Recognition","summary":"  Existing object recognition models have been shown to lack robustness in\ndiverse geographical scenarios due to significant domain shifts in design and\ncontext. Class representations need to be adapted to more accurately reflect an\nobject concept under these shifts. In the absence of training data from target\ngeographies, we hypothesize that geography-specific descriptive knowledge of\nobject categories can be leveraged to enhance robustness. For this purpose, we\nexplore the feasibility of probing a large-language model for\ngeography-specific object knowledge, and we investigate integrating knowledge\nin zero-shot and learnable soft prompting with the CLIP vision-language model.\nIn particular, we propose a geography knowledge regularization method to ensure\nthat soft prompts trained on a source set of geographies generalize to an\nunseen target set of geographies. Our gains on DollarStreet when generalizing\nfrom a model trained only on data from Europe are as large as +2.8 on countries\nfrom Africa, and +4.6 on the hardest classes. We further show competitive\nperformance vs. few-shot target training, and provide insights into how\ndescriptive knowledge captures geographical differences.\n","authors":["Kyle Buettner","Sina Malakouti","Xiang Lorraine Li","Adriana Kovashka"],"pdf_url":"https://arxiv.org/pdf/2401.01482v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01479v1","updated":"2024-01-03T00:49:51Z","published":"2024-01-03T00:49:51Z","title":"Kernel-U-Net: Hierarchical and Symmetrical Framework for Multivariate\n  Time Series Forecasting","summary":"  Time series forecasting task predicts future trends based on historical\ninformation. Recent U-Net-based methods have demonstrated superior performance\nin predicting real-world datasets. However, the performance of these models is\nlower than patch-based models or linear models. In this work, we propose a\nsymmetric and hierarchical framework, Kernel-U-Net, which cuts the input\nsequence into slices at each layer of the network and then computes them using\nkernels. Furthermore, it generalizes the concept of convolutional kernels in\nclassic U-Net to accept custom kernels that follow the same design pattern.\nCompared to the existing linear or transformer-based solution, our model\ncontains 3 advantages: 1) A small number of parameters: the parameters size is\n$O(log(L)^2)$ where $L$ is the look-back window size, 2) Flexibility: its\nkernels can be customized and fitted to the datasets, 3) Computation\nefficiency: the computation complexity of transformer modules is reduced to\n$O(log(L)^2)$ if they are placed close to the latent vector. Kernel-U-Net\naccuracy was greater than or equal to the state-of-the-art model on six (out of\nseven) real-world datasets.\n","authors":["Jiang You","Reńe Natowicz","Arben Cela","Jacob Ouanounou","Patrick Siarry"],"pdf_url":"https://arxiv.org/pdf/2401.01479v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01472v1","updated":"2024-01-03T00:13:52Z","published":"2024-01-03T00:13:52Z","title":"A First Look at Information Highlighting in Stack Overflow Answers","summary":"  Context: Navigating the knowledge of Stack Overflow (SO) remains challenging.\nTo make the posts vivid to users, SO allows users to write and edit posts with\nMarkdown or HTML so that users can leverage various formatting styles (e.g.,\nbold, italic, and code) to highlight the important information. Nonetheless,\nthere have been limited studies on the highlighted information. Objective: We\ncarried out the first large-scale exploratory study on the information\nhighlighted in SO answers in our recent study. To extend our previous study, we\ndevelop approaches to automatically recommend highlighted content with\nformatting styles using neural network architectures initially designed for the\nNamed Entity Recognition task. Method: In this paper, we studied 31,169,429\nanswers of Stack Overflow. For training recommendation models, we choose CNN\nand BERT models for each type of formatting (i.e., Bold, Italic, Code, and\nHeading) using the information highlighting dataset we collected from SO\nanswers. Results: Our models based on CNN architecture achieve precision\nranging from 0.71 to 0.82. The trained model for automatic code content\nhighlighting achieves a recall of 0.73 and an F1 score of 0.71, outperforming\nthe trained models for other formatting styles. The BERT models have even lower\nrecalls and F1 scores than the CNN models. Our analysis of failure cases\nindicates that the majority of the failure cases are missing identification\n(i.e., the model misses the content that is supposed to be highlighted) due to\nthe models tend to learn the frequently highlighted words while struggling to\nlearn less frequent words. Conclusion: Our findings suggest that it is possible\nto develop recommendation models for highlighting information for answers with\ndifferent formatting styles on Stack Overflow.\n","authors":["Shahla Shaan Ahmed","Shaowei Wang","Yuan Tian"," Tse-Hsun"," Chen","Haoxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.01472v1.pdf","comment":"This work is submitted to Information and Software Technology Journal"},{"id":"http://arxiv.org/abs/2401.01470v1","updated":"2024-01-03T00:10:33Z","published":"2024-01-03T00:10:33Z","title":"Token Propagation Controller for Efficient Vision Transformer","summary":"  Vision transformers (ViTs) have achieved promising results on a variety of\nComputer Vision tasks, however their quadratic complexity in the number of\ninput tokens has limited their application specially in resource-constrained\nsettings. Previous approaches that employ gradual token reduction to address\nthis challenge assume that token redundancy in one layer implies redundancy in\nall the following layers. We empirically demonstrate that this assumption is\noften not correct, i.e., tokens that are redundant in one layer can be useful\nin later layers. We employ this key insight to propose a novel token\npropagation controller (TPC) that incorporates two different\ntoken-distributions, i.e., pause probability and restart probability to control\nthe reduction and reuse of tokens respectively, which results in more efficient\ntoken utilization. To improve the estimates of token distributions, we propose\na smoothing mechanism that acts as a regularizer and helps remove noisy\noutliers. Furthermore, to improve the training-stability of our proposed TPC,\nwe introduce a model stabilizer that is able to implicitly encode local image\nstructures and minimize accuracy fluctuations during model training. We present\nextensive experimental results on the ImageNet-1K dataset using DeiT, LV-ViT\nand Swin models to demonstrate the effectiveness of our proposed method. For\nexample, compared to baseline models, our proposed method improves the\ninference speed of the DeiT-S by 250% while increasing the classification\naccuracy by 1.0%.\n","authors":["Wentao Zhu"],"pdf_url":"https://arxiv.org/pdf/2401.01470v1.pdf","comment":"Accepted by WACV 2024"},{"id":"http://arxiv.org/abs/2401.01996v1","updated":"2024-01-03T22:19:57Z","published":"2024-01-03T22:19:57Z","title":"Mean-Field Assisted Deep Boltzmann Learning with Probabilistic Computers","summary":"  Despite their appeal as physics-inspired, energy-based and generative nature,\ngeneral Boltzmann Machines (BM) are considered intractable to train. This\nbelief led to simplified models of BMs with restricted intralayer connections\nor layer-by-layer training of deep BMs. Recent developments in domain-specific\nhardware -- specifically probabilistic computers (p-computer) with\nprobabilistic bits (p-bit) -- may change established wisdom on the tractability\nof deep BMs. In this paper, we show that deep and unrestricted BMs can be\ntrained using p-computers generating hundreds of billions of Markov Chain Monte\nCarlo (MCMC) samples per second, on sparse networks developed originally for\nuse in D-Wave's annealers. To maximize the efficiency of learning the\np-computer, we introduce two families of Mean-Field Theory assisted learning\nalgorithms, or xMFTs (x = Naive and Hierarchical). The xMFTs are used to\nestimate the averages and correlations during the positive phase of the\ncontrastive divergence (CD) algorithm and our custom-designed p-computer is\nused to estimate the averages and correlations in the negative phase. A custom\nField-Programmable-Gate Array (FPGA) emulation of the p-computer architecture\ntakes up to 45 billion flips per second, allowing the implementation of CD-$n$\nwhere $n$ can be of the order of millions, unlike RBMs where $n$ is typically 1\nor 2. Experiments on the full MNIST dataset with the combined algorithm show\nthat the positive phase can be efficiently computed by xMFTs without much\ndegradation when the negative phase is computed by the p-computer. Our\nalgorithm can be used in other scalable Ising machines and its variants can be\nused to train BMs, previously thought to be intractable.\n","authors":["Shuvro Chowdhury","Shaila Niazi","Kerem Y. Camsari"],"pdf_url":"https://arxiv.org/pdf/2401.01996v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01990v1","updated":"2024-01-03T21:39:06Z","published":"2024-01-03T21:39:06Z","title":"GPS-SSL: Guided Positive Sampling to Inject Prior Into Self-Supervised\n  Learning","summary":"  We propose Guided Positive Sampling Self-Supervised Learning (GPS-SSL), a\ngeneral method to inject a priori knowledge into Self-Supervised Learning (SSL)\npositive samples selection. Current SSL methods leverage Data-Augmentations\n(DA) for generating positive samples and incorporate prior knowledge - an\nincorrect, or too weak DA will drastically reduce the quality of the learned\nrepresentation. GPS-SSL proposes instead to design a metric space where\nEuclidean distances become a meaningful proxy for semantic relationship. In\nthat space, it is now possible to generate positive samples from nearest\nneighbor sampling. Any prior knowledge can now be embedded into that metric\nspace independently from the employed DA. From its simplicity, GPS-SSL is\napplicable to any SSL method, e.g. SimCLR or BYOL. A key benefit of GPS-SSL is\nin reducing the pressure in tailoring strong DAs. For example GPS-SSL reaches\n85.58% on Cifar10 with weak DA while the baseline only reaches 37.51%. We\ntherefore move a step forward towards the goal of making SSL less reliant on\nDA. We also show that even when using strong DAs, GPS-SSL outperforms the\nbaselines on under-studied domains. We evaluate GPS-SSL along with multiple\nbaseline SSL methods on numerous downstream datasets from different domains\nwhen the models use strong or minimal data augmentations. We hope that GPS-SSL\nwill open new avenues in studying how to inject a priori knowledge into SSL in\na principled manner.\n","authors":["Aarash Feizi","Randall Balestriero","Adriana Romero-Soriano","Reihaneh Rabbany"],"pdf_url":"https://arxiv.org/pdf/2401.01990v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01987v1","updated":"2024-01-03T21:32:46Z","published":"2024-01-03T21:32:46Z","title":"Representation Learning of Multivariate Time Series using Attention and\n  Adversarial Training","summary":"  A critical factor in trustworthy machine learning is to develop robust\nrepresentations of the training data. Only under this guarantee methods are\nlegitimate to artificially generate data, for example, to counteract imbalanced\ndatasets or provide counterfactual explanations for blackbox decision-making\nsystems. In recent years, Generative Adversarial Networks (GANs) have shown\nconsiderable results in forming stable representations and generating realistic\ndata. While many applications focus on generating image data, less effort has\nbeen made in generating time series data, especially multivariate signals. In\nthis work, a Transformer-based autoencoder is proposed that is regularized\nusing an adversarial training scheme to generate artificial multivariate time\nseries signals. The representation is evaluated using t-SNE visualizations,\nDynamic Time Warping (DTW) and Entropy scores. Our results indicate that the\ngenerated signals exhibit higher similarity to an exemplary dataset than using\na convolutional network approach.\n","authors":["Leon Scharwächter","Sebastian Otte"],"pdf_url":"https://arxiv.org/pdf/2401.01987v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01981v1","updated":"2024-01-03T20:59:52Z","published":"2024-01-03T20:59:52Z","title":"Beyond Regrets: Geometric Metrics for Bayesian Optimization","summary":"  Bayesian optimization is a principled optimization strategy for a black-box\nobjective function. It shows its effectiveness in a wide variety of real-world\napplications such as scientific discovery and experimental design. In general,\nthe performance of Bayesian optimization is assessed by regret-based metrics\nsuch as instantaneous, simple, and cumulative regrets. These metrics only rely\non function evaluations, so that they do not consider geometric relationships\nbetween query points and global solutions, or query points themselves. Notably,\nthey cannot discriminate if multiple global solutions are successfully found.\nMoreover, they do not evaluate Bayesian optimization's abilities to exploit and\nexplore a search space given. To tackle these issues, we propose four new\ngeometric metrics, i.e., precision, recall, average degree, and average\ndistance. These metrics allow us to compare Bayesian optimization algorithms\nconsidering the geometry of both query points and global optima, or query\npoints. However, they are accompanied by an extra parameter, which needs to be\ncarefully determined. We therefore devise the parameter-free forms of the\nrespective metrics by integrating out the additional parameter. Finally, we\nempirically validate that our proposed metrics can provide more convincing\ninterpretation and understanding of Bayesian optimization algorithms from\ndistinct perspectives, compared to the conventional metrics.\n","authors":["Jungtaek Kim"],"pdf_url":"https://arxiv.org/pdf/2401.01981v1.pdf","comment":"29 pages, 25 figures, 2 tables"},{"id":"http://arxiv.org/abs/2401.01978v1","updated":"2024-01-03T20:58:03Z","published":"2024-01-03T20:58:03Z","title":"Tailor: Size Recommendations for High-End Fashion Marketplaces","summary":"  In the ever-changing and dynamic realm of high-end fashion marketplaces,\nproviding accurate and personalized size recommendations has become a critical\naspect. Meeting customer expectations in this regard is not only crucial for\nensuring their satisfaction but also plays a pivotal role in driving customer\nretention, which is a key metric for the success of any fashion retailer. We\npropose a novel sequence classification approach to address this problem,\nintegrating implicit (Add2Bag) and explicit (ReturnReason) user signals. Our\napproach comprises two distinct models: one employs LSTMs to encode the user\nsignals, while the other leverages an Attention mechanism. Our best model\noutperforms SFNet, improving accuracy by 45.7%. By using Add2Bag interactions\nwe increase the user coverage by 24.5% when compared with only using Orders.\nMoreover, we evaluate the models' usability in real-time recommendation\nscenarios by conducting experiments to measure their latency performance.\n","authors":["Alexandre Candeias","Ivo Silva","Vitor Sousa","José Marcelino"],"pdf_url":"https://arxiv.org/pdf/2401.01978v1.pdf","comment":"Accepted in FashionXRecsys23 held at the 17th ACM Conference on\n  Recommender Systems, 18th-22nd September 2023"},{"id":"http://arxiv.org/abs/2204.06863v4","updated":"2024-01-03T20:52:22Z","published":"2022-04-14T10:29:01Z","title":"ULF: Unsupervised Labeling Function Correction using Cross-Validation\n  for Weak Supervision","summary":"  A cost-effective alternative to manual data labeling is weak supervision\n(WS), where data samples are automatically annotated using a predefined set of\nlabeling functions (LFs), rule-based mechanisms that generate artificial labels\nfor the associated classes. In this work, we investigate noise reduction\ntechniques for WS based on the principle of k-fold cross-validation. We\nintroduce a new algorithm ULF for Unsupervised Labeling Function correction,\nwhich denoises WS data by leveraging models trained on all but some LFs to\nidentify and correct biases specific to the held-out LFs. Specifically, ULF\nrefines the allocation of LFs to classes by re-estimating this assignment on\nhighly reliable cross-validated samples. Evaluation on multiple datasets\nconfirms ULF's effectiveness in enhancing WS learning without the need for\nmanual labeling.\n","authors":["Anastasiia Sedova","Benjamin Roth"],"pdf_url":"https://arxiv.org/pdf/2204.06863v4.pdf","comment":"EMNLP'23"},{"id":"http://arxiv.org/abs/2401.01974v1","updated":"2024-01-03T20:48:47Z","published":"2024-01-03T20:48:47Z","title":"Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as\n  Programmers","summary":"  Visual reasoning is dominated by end-to-end neural networks scaled to\nbillions of model parameters and training examples. However, even the largest\nmodels struggle with compositional reasoning, generalization, fine-grained\nspatial and temporal reasoning, and counting. Visual reasoning with large\nlanguage models (LLMs) as controllers can, in principle, address these\nlimitations by decomposing the task and solving subtasks by orchestrating a set\nof (visual) tools. Recently, these models achieved great performance on tasks\nsuch as compositional visual question answering, visual grounding, and video\ntemporal reasoning. Nevertheless, in their current form, these models heavily\nrely on human engineering of in-context examples in the prompt, which are often\ndataset- and task-specific and require significant labor by highly skilled\nprogrammers. In this work, we present a framework that mitigates these issues\nby introducing spatially and temporally abstract routines and by leveraging a\nsmall number of labeled examples to automatically generate in-context examples,\nthereby avoiding human-created in-context examples. On a number of visual\nreasoning tasks, we show that our framework leads to consistent gains in\nperformance, makes LLMs as controllers setup more robust, and removes the need\nfor human engineering of in-context examples.\n","authors":["Aleksandar Stanić","Sergi Caelles","Michael Tschannen"],"pdf_url":"https://arxiv.org/pdf/2401.01974v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.04502v4","updated":"2024-01-03T20:47:56Z","published":"2023-06-07T15:10:01Z","title":"Learning with Noisy Labels by Adaptive Gradient-Based Outlier Removal","summary":"  An accurate and substantial dataset is essential for training a reliable and\nwell-performing model. However, even manually annotated datasets contain label\nerrors, not to mention automatically labeled ones. Previous methods for label\ndenoising have primarily focused on detecting outliers and their permanent\nremoval - a process that is likely to over- or underfilter the dataset. In this\nwork, we propose AGRA: a new method for learning with noisy labels by using\nAdaptive GRAdient-based outlier removal. Instead of cleaning the dataset prior\nto model training, the dataset is dynamically adjusted during the training\nprocess. By comparing the aggregated gradient of a batch of samples and an\nindividual example gradient, our method dynamically decides whether a\ncorresponding example is helpful for the model at this point or is\ncounter-productive and should be left out for the current update. Extensive\nevaluation on several datasets demonstrates AGRA's effectiveness, while a\ncomprehensive results analysis supports our initial hypothesis: permanent hard\noutlier removal is not always what model benefits the most from.\n","authors":["Anastasiia Sedova","Lena Zellinger","Benjamin Roth"],"pdf_url":"https://arxiv.org/pdf/2306.04502v4.pdf","comment":"ECML PKDD 2023"},{"id":"http://arxiv.org/abs/2302.01078v2","updated":"2024-01-03T20:20:46Z","published":"2023-02-01T00:50:16Z","title":"Computational Discovery of Microstructured Composites with Optimal\n  Stiffness-Toughness Trade-Offs","summary":"  The conflict between stiffness and toughness is a fundamental problem in\nengineering materials design. However, the systematic discovery of\nmicrostructured composites with optimal stiffness-toughness trade-offs has\nnever been demonstrated, hindered by the discrepancies between simulation and\nreality and the lack of data-efficient exploration of the entire Pareto front.\nWe introduce a generalizable pipeline that integrates physical experiments,\nnumerical simulations, and artificial neural networks to address both\nchallenges. Without any prescribed expert knowledge of material design, our\napproach implements a nested-loop proposal-validation workflow to bridge the\nsimulation-to-reality gap and discover microstructured composites that are\nstiff and tough with high sample efficiency. Further analysis of Pareto-optimal\ndesigns allows us to automatically identify existing toughness enhancement\nmechanisms, which were previously discovered through trial-and-error or\nbiomimicry. On a broader scale, our method provides a blueprint for\ncomputational design in various research areas beyond solid mechanics, such as\npolymer chemistry, fluid dynamics, meteorology, and robotics.\n","authors":["Beichen Li","Bolei Deng","Wan Shou","Tae-Hyun Oh","Yuanming Hu","Yiyue Luo","Liang Shi","Wojciech Matusik"],"pdf_url":"https://arxiv.org/pdf/2302.01078v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.20065v2","updated":"2024-01-03T19:57:42Z","published":"2023-10-30T22:29:50Z","title":"LinFlo-Net: A two-stage deep learning method to generate simulation\n  ready meshes of the heart","summary":"  We present a deep learning model to automatically generate computer models of\nthe human heart from patient imaging data with an emphasis on its capability to\ngenerate thin-walled cardiac structures. Our method works by deforming a\ntemplate mesh to fit the cardiac structures to the given image. Compared with\nprior deep learning methods that adopted this approach, our framework is\ndesigned to minimize mesh self-penetration, which typically arises when\ndeforming surface meshes separated by small distances. We achieve this by using\na two-stage diffeomorphic deformation process along with a novel loss function\nderived from the kinematics of motion that penalizes surface contact and\ninterpenetration. Our model demonstrates comparable accuracy with\nstate-of-the-art methods while additionally producing meshes free of\nself-intersections. The resultant meshes are readily usable in physics based\nsimulation, minimizing the need for post-processing and cleanup.\n","authors":["Arjun Narayanan","Fanwei Kong","Shawn Shadden"],"pdf_url":"https://arxiv.org/pdf/2310.20065v2.pdf","comment":"Accepted manuscript in the Journal of Biomechanical Engineering"},{"id":"http://arxiv.org/abs/2302.10975v3","updated":"2024-01-03T19:40:07Z","published":"2023-02-21T20:23:56Z","title":"Improved uncertainty quantification for neural networks with Bayesian\n  last layer","summary":"  Uncertainty quantification is an important task in machine learning - a task\nin which standardneural networks (NNs) have traditionally not excelled. This\ncan be a limitation for safety-critical applications, where uncertainty-aware\nmethods like Gaussian processes or Bayesian linear regression are often\npreferred. Bayesian neural networks are an approach to address this limitation.\nThey assume probability distributions for all parameters and yield distributed\npredictions. However, training and inference are typically intractable and\napproximations must be employed. A promising approximation is NNs with Bayesian\nlast layer (BLL). They assume distributed weights only in the linear output\nlayer and yield a normally distributed prediction. To approximate the\nintractable Bayesian neural network, point estimates of the distributed weights\nin all but the last layer should be obtained by maximizing the marginal\nlikelihood. This has previously been challenging, as the marginal likelihood is\nexpensive to evaluate in this setting. We present a reformulation of the\nlog-marginal likelihood of a NN with BLL which allows for efficient training\nusing backpropagation. Furthermore, we address the challenge of uncertainty\nquantification for extrapolation points. We provide a metric to quantify the\ndegree of extrapolation and derive a method to improve the uncertainty\nquantification for these points. Our methods are derived for the multivariate\ncase and demonstrated in a simulation study. In comparison to Bayesian linear\nregression with fixed features, and a Bayesian neural network trained with\nvariational inference, our proposed method achieves the highest log-predictive\ndensity on test data.\n","authors":["Felix Fiedler","Sergio Lucia"],"pdf_url":"https://arxiv.org/pdf/2302.10975v3.pdf","comment":"This work has been published at IEEE Access with Digital Object\n  Identifier 10.1109/ACCESS.2023.3329685 under a Creative Commons Attribution\n  4.0 License"},{"id":"http://arxiv.org/abs/2303.02468v4","updated":"2024-01-03T19:33:57Z","published":"2023-03-04T17:59:43Z","title":"Lon-ea at SemEval-2023 Task 11: A Comparison of Activation Functions for\n  Soft and Hard Label Prediction","summary":"  We study the influence of different activation functions in the output layer\nof deep neural network models for soft and hard label prediction in the\nlearning with disagreement task. In this task, the goal is to quantify the\namount of disagreement via predicting soft labels. To predict the soft labels,\nwe use BERT-based preprocessors and encoders and vary the activation function\nused in the output layer, while keeping other parameters constant. The soft\nlabels are then used for the hard label prediction. The activation functions\nconsidered are sigmoid as well as a step-function that is added to the model\npost-training and a sinusoidal activation function, which is introduced for the\nfirst time in this paper.\n","authors":["Peyman Hosseini","Mehran Hosseini","Sana Sabah Al-Azzawi","Marcus Liwicki","Ignacio Castro","Matthew Purver"],"pdf_url":"https://arxiv.org/pdf/2303.02468v4.pdf","comment":"Accepted in ACL 2023 SemEval Workshop as selected task paper"},{"id":"http://arxiv.org/abs/2401.01951v1","updated":"2024-01-03T19:27:20Z","published":"2024-01-03T19:27:20Z","title":"Can We Generate Realistic Hands Only Using Convolution?","summary":"  The enduring inability of image generative models to recreate intricate\ngeometric features, such as those present in human hands and fingers has been\nan ongoing problem in image generation for nearly a decade. While strides have\nbeen made by increasing model sizes and diversifying training datasets, this\nissue remains prevalent across all models, from denoising diffusion models to\nGenerative Adversarial Networks (GAN), pointing to a fundamental shortcoming in\nthe underlying architectures. In this paper, we demonstrate how this problem\ncan be mitigated by augmenting convolution layers geometric capabilities\nthrough providing them with a single input channel incorporating the relative\n$n$-dimensional Cartesian coordinate system. We show that this drastically\nimproves quality of hand and face images generated by GANs and Variational\nAutoEncoders (VAE).\n","authors":["Mehran Hosseini","Peyman Hosseini"],"pdf_url":"https://arxiv.org/pdf/2401.01951v1.pdf","comment":"Contains 17 pages, 14 figures, and 6 tables"},{"id":"http://arxiv.org/abs/2401.01923v1","updated":"2024-01-03T18:08:57Z","published":"2024-01-03T18:08:57Z","title":"IoT in the Era of Generative AI: Vision and Challenges","summary":"  Equipped with sensing, networking, and computing capabilities, Internet of\nThings (IoT) such as smartphones, wearables, smart speakers, and household\nrobots have been seamlessly weaved into our daily lives. Recent advancements in\nGenerative AI exemplified by GPT, LLaMA, DALL-E, and Stable Difussion hold\nimmense promise to push IoT to the next level. In this article, we share our\nvision and views on the benefits that Generative AI brings to IoT, and discuss\nsome of the most important applications of Generative AI in IoT-related\ndomains. Fully harnessing Generative AI in IoT is a complex challenge. We\nidentify some of the most critical challenges including high resource demands\nof the Generative AI models, prompt engineering, on-device inference,\noffloading, on-device fine-tuning, federated learning, security, as well as\ndevelopment tools and benchmarks, and discuss current gaps as well as promising\nopportunities on enabling Generative AI for IoT. We hope this article can\ninspire new research on IoT in the era of Generative AI.\n","authors":["Xin Wang","Zhongwei Wan","Arvin Hekmati","Mingyu Zong","Samiul Alam","Mi Zhang","Bhaskar Krishnamachari"],"pdf_url":"https://arxiv.org/pdf/2401.01923v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01922v1","updated":"2024-01-03T15:09:25Z","published":"2024-01-03T15:09:25Z","title":"Unsupervised Object-Centric Learning from Multiple Unspecified\n  Viewpoints","summary":"  Visual scenes are extremely diverse, not only because there are infinite\npossible combinations of objects and backgrounds but also because the\nobservations of the same scene may vary greatly with the change of viewpoints.\nWhen observing a multi-object visual scene from multiple viewpoints, humans can\nperceive the scene compositionally from each viewpoint while achieving the\nso-called ``object constancy'' across different viewpoints, even though the\nexact viewpoints are untold. This ability is essential for humans to identify\nthe same object while moving and to learn from vision efficiently. It is\nintriguing to design models that have a similar ability. In this paper, we\nconsider a novel problem of learning compositional scene representations from\nmultiple unspecified (i.e., unknown and unrelated) viewpoints without using any\nsupervision and propose a deep generative model which separates latent\nrepresentations into a viewpoint-independent part and a viewpoint-dependent\npart to solve this problem. During the inference, latent representations are\nrandomly initialized and iteratively updated by integrating the information in\ndifferent viewpoints with neural networks. Experiments on several specifically\ndesigned synthetic datasets have shown that the proposed method can effectively\nlearn from multiple unspecified viewpoints.\n","authors":["Jinyang Yuan","Tonglin Chen","Zhimeng Shen","Bin Li","Xiangyang Xue"],"pdf_url":"https://arxiv.org/pdf/2401.01922v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2112.03568"}],"Multimedia":[{"id":"http://arxiv.org/abs/2401.01759v1","updated":"2024-01-03T14:24:02Z","published":"2024-01-03T14:24:02Z","title":"VGA: Vision and Graph Fused Attention Network for Rumor Detection","summary":"  With the development of social media, rumors have been spread broadly on\nsocial media platforms, causing great harm to society. Beside textual\ninformation, many rumors also use manipulated images or conceal textual\ninformation within images to deceive people and avoid being detected, making\nmultimodal rumor detection be a critical problem. The majority of multimodal\nrumor detection methods mainly concentrate on extracting features of source\nclaims and their corresponding images, while ignoring the comments of rumors\nand their propagation structures. These comments and structures imply the\nwisdom of crowds and are proved to be crucial to debunk rumors. Moreover, these\nmethods usually only extract visual features in a basic manner, seldom consider\ntampering or textual information in images. Therefore, in this study, we\npropose a novel Vision and Graph Fused Attention Network (VGA) for rumor\ndetection to utilize propagation structures among posts so as to obtain the\ncrowd opinions and further explore visual tampering features, as well as the\ntextual information hidden in images. We conduct extensive experiments on three\ndatasets, demonstrating that VGA can effectively detect multimodal rumors and\noutperform state-of-the-art methods significantly.\n","authors":["Lin Bai","Caiyan Jia","Ziying Song","Chaoqun Cui"],"pdf_url":"https://arxiv.org/pdf/2401.01759v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01699v1","updated":"2024-01-03T12:06:02Z","published":"2024-01-03T12:06:02Z","title":"WordArt Designer API: User-Driven Artistic Typography Synthesis with\n  Large Language Models on ModelScope","summary":"  This paper introduces the WordArt Designer API, a novel framework for\nuser-driven artistic typography synthesis utilizing Large Language Models\n(LLMs) on ModelScope. We address the challenge of simplifying artistic\ntypography for non-professionals by offering a dynamic, adaptive, and\ncomputationally efficient alternative to traditional rigid templates. Our\napproach leverages the power of LLMs to understand and interpret user input,\nfacilitating a more intuitive design process. We demonstrate through various\ncase studies how users can articulate their aesthetic preferences and\nfunctional requirements, which the system then translates into unique and\ncreative typographic designs. Our evaluations indicate significant improvements\nin user satisfaction, design flexibility, and creative expression over existing\nsystems. The WordArt Designer API not only democratizes the art of typography\nbut also opens up new possibilities for personalized digital communication and\ndesign.\n","authors":["Jun-Yan He","Zhi-Qi Cheng","Chenyang Li","Jingdong Sun","Wangmeng Xiang","Yusen Hu","Xianhui Lin","Xiaoyang Kang","Zengke Jin","Bin Luo","Yifeng Geng","Xuansong Xie","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2401.01699v1.pdf","comment":"Spotlight Paper from the Workshop on Machine Learning for Creativity\n  and Design at the 37th Conference on Neural Information Processing Systems\n  (NeurIPS 2023). 5 pages, 5 figures"},{"id":"http://arxiv.org/abs/2310.05195v2","updated":"2024-01-03T07:40:15Z","published":"2023-10-08T15:04:50Z","title":"GMMFormer: Gaussian-Mixture-Model Based Transformer for Efficient\n  Partially Relevant Video Retrieval","summary":"  Given a text query, partially relevant video retrieval (PRVR) seeks to find\nuntrimmed videos containing pertinent moments in a database. For PRVR, clip\nmodeling is essential to capture the partial relationship between texts and\nvideos. Current PRVR methods adopt scanning-based clip construction to achieve\nexplicit clip modeling, which is information-redundant and requires a large\nstorage overhead. To solve the efficiency problem of PRVR methods, this paper\nproposes GMMFormer, a Gaussian-Mixture-Model based Transformer which models\nclip representations implicitly. During frame interactions, we incorporate\nGaussian-Mixture-Model constraints to focus each frame on its adjacent frames\ninstead of the whole video. Then generated representations will contain\nmulti-scale clip information, achieving implicit clip modeling. In addition,\nPRVR methods ignore semantic differences between text queries relevant to the\nsame video, leading to a sparse embedding space. We propose a query diverse\nloss to distinguish these text queries, making the embedding space more\nintensive and contain more semantic information. Extensive experiments on three\nlarge-scale video datasets (i.e., TVR, ActivityNet Captions, and Charades-STA)\ndemonstrate the superiority and efficiency of GMMFormer. Code is available at\n\\url{https://github.com/huangmozhi9527/GMMFormer}.\n","authors":["Yuting Wang","Jinpeng Wang","Bin Chen","Ziyun Zeng","Shu-Tao Xia"],"pdf_url":"https://arxiv.org/pdf/2310.05195v2.pdf","comment":"Accepted by AAAI 2024. Code is released at\n  https://github.com/huangmozhi9527/GMMFormer"},{"id":"http://arxiv.org/abs/2305.18891v2","updated":"2024-01-03T06:55:36Z","published":"2023-05-30T09:47:29Z","title":"EmotionGesture: Audio-Driven Diverse Emotional Co-Speech 3D Gesture\n  Generation","summary":"  Generating vivid and diverse 3D co-speech gestures is crucial for various\napplications in animating virtual avatars. While most existing methods can\ngenerate gestures from audio directly, they usually overlook that emotion is\none of the key factors of authentic co-speech gesture generation. In this work,\nwe propose EmotionGesture, a novel framework for synthesizing vivid and diverse\nemotional co-speech 3D gestures from audio. Considering emotion is often\nentangled with the rhythmic beat in speech audio, we first develop an\nEmotion-Beat Mining module (EBM) to extract the emotion and audio beat features\nas well as model their correlation via a transcript-based visual-rhythm\nalignment. Then, we propose an initial pose based Spatial-Temporal Prompter\n(STP) to generate future gestures from the given initial poses. STP effectively\nmodels the spatial-temporal correlations between the initial poses and the\nfuture gestures, thus producing the spatial-temporal coherent pose prompt. Once\nwe obtain pose prompts, emotion, and audio beat features, we will generate 3D\nco-speech gestures through a transformer architecture. However, considering the\nposes of existing datasets often contain jittering effects, this would lead to\ngenerating unstable gestures. To address this issue, we propose an effective\nobjective function, dubbed Motion-Smooth Loss. Specifically, we model motion\noffset to compensate for jittering ground-truth by forcing gestures to be\nsmooth. Last, we present an emotion-conditioned VAE to sample emotion features,\nenabling us to generate diverse emotional results. Extensive experiments\ndemonstrate that our framework outperforms the state-of-the-art, achieving\nvivid and diverse emotional co-speech 3D gestures. Our code and dataset will be\nreleased at the project page:\nhttps://xingqunqi-lab.github.io/Emotion-Gesture-Web/\n","authors":["Xingqun Qi","Chen Liu","Lincheng Li","Jie Hou","Haoran Xin","Xin Yu"],"pdf_url":"https://arxiv.org/pdf/2305.18891v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2401.01470v1","updated":"2024-01-03T00:10:33Z","published":"2024-01-03T00:10:33Z","title":"Token Propagation Controller for Efficient Vision Transformer","summary":"  Vision transformers (ViTs) have achieved promising results on a variety of\nComputer Vision tasks, however their quadratic complexity in the number of\ninput tokens has limited their application specially in resource-constrained\nsettings. Previous approaches that employ gradual token reduction to address\nthis challenge assume that token redundancy in one layer implies redundancy in\nall the following layers. We empirically demonstrate that this assumption is\noften not correct, i.e., tokens that are redundant in one layer can be useful\nin later layers. We employ this key insight to propose a novel token\npropagation controller (TPC) that incorporates two different\ntoken-distributions, i.e., pause probability and restart probability to control\nthe reduction and reuse of tokens respectively, which results in more efficient\ntoken utilization. To improve the estimates of token distributions, we propose\na smoothing mechanism that acts as a regularizer and helps remove noisy\noutliers. Furthermore, to improve the training-stability of our proposed TPC,\nwe introduce a model stabilizer that is able to implicitly encode local image\nstructures and minimize accuracy fluctuations during model training. We present\nextensive experimental results on the ImageNet-1K dataset using DeiT, LV-ViT\nand Swin models to demonstrate the effectiveness of our proposed method. For\nexample, compared to baseline models, our proposed method improves the\ninference speed of the DeiT-S by 250% while increasing the classification\naccuracy by 1.0%.\n","authors":["Wentao Zhu"],"pdf_url":"https://arxiv.org/pdf/2401.01470v1.pdf","comment":"Accepted by WACV 2024"},{"id":"http://arxiv.org/abs/2401.01955v1","updated":"2024-01-03T19:39:23Z","published":"2024-01-03T19:39:23Z","title":"MULTI-CASE: A Transformer-based Ethics-aware Multimodal Investigative\n  Intelligence Framework","summary":"  AI-driven models are increasingly deployed in operational analytics\nsolutions, for instance, in investigative journalism or the intelligence\ncommunity. Current approaches face two primary challenges: ethical and privacy\nconcerns, as well as difficulties in efficiently combining heterogeneous data\nsources for multimodal analytics. To tackle the challenge of multimodal\nanalytics, we present MULTI-CASE, a holistic visual analytics framework\ntailored towards ethics-aware and multimodal intelligence exploration, designed\nin collaboration with domain experts. It leverages an equal joint agency\nbetween human and AI to explore and assess heterogeneous information spaces,\nchecking and balancing automation through Visual Analytics. MULTI-CASE operates\non a fully-integrated data model and features type-specific analysis with\nmultiple linked components, including a combined search, annotated text view,\nand graph-based analysis. Parts of the underlying entity detection are based on\na RoBERTa-based language model, which we tailored towards user requirements\nthrough fine-tuning. An overarching knowledge exploration graph combines all\ninformation streams, provides in-situ explanations, transparent source\nattribution, and facilitates effective exploration. To assess our approach, we\nconducted a comprehensive set of evaluations: We benchmarked the underlying\nlanguage model on relevant NER tasks, achieving state-of-the-art performance.\nThe demonstrator was assessed according to intelligence capability assessments,\nwhile the methodology was evaluated according to ethics design guidelines. As a\ncase study, we present our framework in an investigative journalism setting,\nsupporting war crime investigations. Finally, we conduct a formative user\nevaluation with domain experts in law enforcement. Our evaluations confirm that\nour framework facilitates human agency and steering in security-sensitive\napplications.\n","authors":["Maximilian T. Fischer","Yannick Metz","Lucas Joos","Matthias Miller","Daniel A. Keim"],"pdf_url":"https://arxiv.org/pdf/2401.01955v1.pdf","comment":"6 pages, 3 figures, 1 table"}]},"2024-01-04T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2401.02417v1","updated":"2024-01-04T18:59:31Z","published":"2024-01-04T18:59:31Z","title":"Task Oriented Dialogue as a Catalyst for Self-Supervised Automatic\n  Speech Recognition","summary":"  While word error rates of automatic speech recognition (ASR) systems have\nconsistently fallen, natural language understanding (NLU) applications built on\ntop of ASR systems still attribute significant numbers of failures to\nlow-quality speech recognition results. Existing assistant systems collect\nlarge numbers of these unsuccessful interactions, but these systems usually\nfail to learn from these interactions, even in an offline fashion. In this\nwork, we introduce CLC: Contrastive Learning for Conversations, a family of\nmethods for contrastive fine-tuning of models in a self-supervised fashion,\nmaking use of easily detectable artifacts in unsuccessful conversations with\nassistants. We demonstrate that our CLC family of approaches can improve the\nperformance of ASR models on OD3, a new public large-scale semi-synthetic\nmeta-dataset of audio task-oriented dialogues, by up to 19.2%. These gains\ntransfer to real-world systems as well, where we show that CLC can help to\nimprove performance by up to 6.7% over baselines. We make OD3 publicly\navailable at https://github.com/amazon-science/amazon-od3 .\n","authors":["David M. Chan","Shalini Ghosh","Hitesh Tulsiani","Ariya Rastrow","Björn Hoffmeister"],"pdf_url":"https://arxiv.org/pdf/2401.02417v1.pdf","comment":"To appear in ICASSP 2024"},{"id":"http://arxiv.org/abs/2401.02415v1","updated":"2024-01-04T18:59:12Z","published":"2024-01-04T18:59:12Z","title":"LLaMA Pro: Progressive LLaMA with Block Expansion","summary":"  Humans generally acquire new skills without compromising the old; however,\nthe opposite holds for Large Language Models (LLMs), e.g., from LLaMA to\nCodeLLaMA. To this end, we propose a new post-pretraining method for LLMs with\nan expansion of Transformer blocks. We tune the expanded blocks using only new\ncorpus, efficiently and effectively improving the model's knowledge without\ncatastrophic forgetting. In this paper, we experiment on the corpus of code and\nmath, yielding LLaMA Pro-8.3B, a versatile foundation model initialized from\nLLaMA2-7B, excelling in general tasks, programming, and mathematics. LLaMA Pro\nand its instruction-following counterpart (LLaMA Pro-Instruct) achieve advanced\nperformance among various benchmarks, demonstrating superiority over existing\nopen models in the LLaMA family and the immense potential of reasoning and\naddressing diverse tasks as an intelligent agent. Our findings provide valuable\ninsights into integrating natural and programming languages, laying a solid\nfoundation for developing advanced language agents that operate effectively in\nvarious environments.\n","authors":["Chengyue Wu","Yukang Gan","Yixiao Ge","Zeyu Lu","Jiahao Wang","Ye Feng","Ping Luo","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2401.02415v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02412v1","updated":"2024-01-04T18:53:01Z","published":"2024-01-04T18:53:01Z","title":"LLM Augmented LLMs: Expanding Capabilities through Composition","summary":"  Foundational models with billions of parameters which have been trained on\nlarge corpora of data have demonstrated non-trivial skills in a variety of\ndomains. However, due to their monolithic structure, it is challenging and\nexpensive to augment them or impart new skills. On the other hand, due to their\nadaptation abilities, several new instances of these models are being trained\ntowards new domains and tasks. In this work, we study the problem of efficient\nand practical composition of existing foundation models with more specific\nmodels to enable newer capabilities. To this end, we propose CALM --\nComposition to Augment Language Models -- which introduces cross-attention\nbetween models to compose their representations and enable new capabilities.\nSalient features of CALM are: (i) Scales up LLMs on new tasks by 're-using'\nexisting LLMs along with a few additional parameters and data, (ii) Existing\nmodel weights are kept intact, and hence preserves existing capabilities, and\n(iii) Applies to diverse domains and settings. We illustrate that augmenting\nPaLM2-S with a smaller model trained on low-resource languages results in an\nabsolute improvement of up to 13\\% on tasks like translation into English and\narithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is\naugmented with a code-specific model, we see a relative improvement of 40\\%\nover the base model for code generation and explanation tasks -- on-par with\nfully fine-tuned counterparts.\n","authors":["Rachit Bansal","Bidisha Samanta","Siddharth Dalmia","Nitish Gupta","Shikhar Vashishth","Sriram Ganapathy","Abhishek Bapna","Prateek Jain","Partha Talukdar"],"pdf_url":"https://arxiv.org/pdf/2401.02412v1.pdf","comment":"17 pages, 2 figures, 8 tables"},{"id":"http://arxiv.org/abs/2312.11671v2","updated":"2024-01-04T18:46:39Z","published":"2023-12-18T19:27:09Z","title":"Evaluating Language-Model Agents on Realistic Autonomous Tasks","summary":"  In this report, we explore the ability of language model agents to acquire\nresources, create copies of themselves, and adapt to novel challenges they\nencounter in the wild. We refer to this cluster of capabilities as \"autonomous\nreplication and adaptation\" or ARA. We believe that systems capable of ARA\ncould have wide-reaching and hard-to-anticipate consequences, and that\nmeasuring and forecasting ARA may be useful for informing measures around\nsecurity, monitoring, and alignment. Additionally, once a system is capable of\nARA, placing bounds on a system's capabilities may become significantly more\ndifficult.\n  We construct four simple example agents that combine language models with\ntools that allow them to take actions in the world. We then evaluate these\nagents on 12 tasks relevant to ARA. We find that these language model agents\ncan only complete the easiest tasks from this list, although they make some\nprogress on the more challenging tasks. Unfortunately, these evaluations are\nnot adequate to rule out the possibility that near-future agents will be\ncapable of ARA. In particular, we do not think that these evaluations provide\ngood assurance that the ``next generation'' of language models (e.g. 100x\neffective compute scaleup on existing models) will not yield agents capable of\nARA, unless intermediate evaluations are performed during pretraining.\nRelatedly, we expect that fine-tuning of the existing models could produce\nsubstantially more competent agents, even if the fine-tuning is not directly\ntargeted at ARA.\n","authors":["Megan Kinniment","Lucas Jun Koba Sato","Haoxing Du","Brian Goodrich","Max Hasin","Lawrence Chan","Luke Harold Miles","Tao R. Lin","Hjalmar Wijk","Joel Burget","Aaron Ho","Elizabeth Barnes","Paul Christiano"],"pdf_url":"https://arxiv.org/pdf/2312.11671v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2312.10302v3","updated":"2024-01-04T18:00:11Z","published":"2023-12-16T03:33:12Z","title":"One Shot Learning as Instruction Data Prospector for Large Language\n  Models","summary":"  Aligning large language models(LLMs) with human is a critical step in\neffectively utilizing their pre-trained capabilities across a wide array of\nlanguage tasks. Current instruction tuning practices often rely on expanding\ndataset size without a clear strategy for ensuring data quality, which can\ninadvertently introduce noise and degrade model performance. To address this\nchallenge, we introduce Nuggets, a novel and efficient methodology that employs\none shot learning to select high-quality instruction data from expansive\ndatasets. Nuggets assesses the potential of individual instruction examples to\nact as effective one shot examples, thereby identifying those that can\nsignificantly enhance diverse task performance. Nuggets utilizes a scoring\nsystem based on the impact of candidate examples on the perplexity of a diverse\nanchor set, facilitating the selection of the most beneficial data for\ninstruction tuning. Through rigorous testing on two benchmarks, including\nMT-Bench and Alpaca-Eval, we demonstrate that instruction tuning with the top\n1% of Nuggets-curated examples substantially outperforms conventional methods\nthat use the full dataset. These findings advocate for a data selection\nparadigm that prioritizes quality, offering a more efficient pathway to align\nLLMs with humans.\n","authors":["Yunshui Li","Binyuan Hui","Xiaobo Xia","Jiaxi Yang","Min Yang","Lei Zhang","Shuzheng Si","Junhao Liu","Tongliang Liu","Fei Huang","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2312.10302v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02385v1","updated":"2024-01-04T17:54:59Z","published":"2024-01-04T17:54:59Z","title":"TinyLlama: An Open-Source Small Language Model","summary":"  We present TinyLlama, a compact 1.1B language model pretrained on around 1\ntrillion tokens for approximately 3 epochs. Building on the architecture and\ntokenizer of Llama 2, TinyLlama leverages various advances contributed by the\nopen-source community (e.g., FlashAttention), achieving better computational\nefficiency. Despite its relatively small size, TinyLlama demonstrates\nremarkable performance in a series of downstream tasks. It significantly\noutperforms existing open-source language models with comparable sizes. Our\nmodel checkpoints and code are publicly available on GitHub at\nhttps://github.com/jzhang38/TinyLlama.\n","authors":["Peiyuan Zhang","Guangtao Zeng","Tianduo Wang","Wei Lu"],"pdf_url":"https://arxiv.org/pdf/2401.02385v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2401.01078v3","updated":"2024-01-04T17:29:47Z","published":"2024-01-02T07:46:34Z","title":"Vietnamese Poem Generation & The Prospect Of Cross-Language Poem-To-Poem\n  Translation","summary":"  Poetry generation has been a challenging task in the field of Natural\nLanguage Processing, as it requires the model to understand the nuances of\nlanguage, sentiment, and style. In this paper, we propose using Large Language\nModels to generate Vietnamese poems of various genres from natural language\nprompts, thereby facilitating an intuitive process with enhanced content\ncontrol. Our most efficacious model, the GPT-3 Babbage variant, achieves a\ncustom evaluation score of 0.8, specifically tailored to the \"luc bat\" genre of\nVietnamese poetry. Furthermore, we also explore the idea of paraphrasing poems\ninto normal text prompts and yield a relatively high score of 0.781 in the \"luc\nbat\" genre. This experiment presents the potential for cross-Language\npoem-to-poem translation with translated poems as the inputs while concurrently\nmaintaining complete control over the generated content.\n","authors":["Triet Minh Huynh","Quan Le Bao"],"pdf_url":"https://arxiv.org/pdf/2401.01078v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02369v1","updated":"2024-01-04T17:23:44Z","published":"2024-01-04T17:23:44Z","title":"SPEER: Sentence-Level Planning of Long Clinical Summaries via Embedded\n  Entity Retrieval","summary":"  Clinician must write a lengthy summary each time a patient is discharged from\nthe hospital. This task is time-consuming due to the sheer number of unique\nclinical concepts covered in the admission. Identifying and covering salient\nentities is vital for the summary to be clinically useful. We fine-tune\nopen-source LLMs (Mistral-7B-Instruct and Zephyr-7B-\\b{eta}) on the task and\nfind that they generate incomplete and unfaithful summaries. To increase entity\ncoverage, we train a smaller, encoder-only model to predict salient entities,\nwhich are treated as content-plans to guide the LLM. To encourage the LLM to\nfocus on specific mentions in the source notes, we propose SPEER:\nSentence-level Planning via Embedded Entity Retrieval. Specifically, we mark\neach salient entity span with special \"{{ }}\" boundary tags and instruct the\nLLM to retrieve marked spans before generating each sentence. Sentence-level\nplanning acts as a form of state tracking in that the model is explicitly\nrecording the entities it uses. We fine-tune Mistral and Zephyr variants on a\nlarge-scale, diverse dataset of ~167k in-patient hospital admissions and\nevaluate on 3 datasets. SPEER shows gains in both coverage and faithfulness\nmetrics over non-guided and guided baselines.\n","authors":["Griffin Adams","Jason Zucker","Noémie Elhadad"],"pdf_url":"https://arxiv.org/pdf/2401.02369v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2312.15228v2","updated":"2024-01-04T16:20:42Z","published":"2023-12-23T11:37:09Z","title":"Adversarial Data Poisoning for Fake News Detection: How to Make a Model\n  Misclassify a Target News without Modifying It","summary":"  Fake news detection models are critical to countering disinformation but can\nbe manipulated through adversarial attacks. In this position paper, we analyze\nhow an attacker can compromise the performance of an online learning detector\non specific news content without being able to manipulate the original target\nnews. In some contexts, such as social networks, where the attacker cannot\nexert complete control over all the information, this scenario can indeed be\nquite plausible. Therefore, we show how an attacker could potentially introduce\npoisoning data into the training data to manipulate the behavior of an online\nlearning method. Our initial findings reveal varying susceptibility of logistic\nregression models based on complexity and attack type.\n","authors":["Federico Siciliano","Luca Maiano","Lorenzo Papa","Federica Baccini","Irene Amerini","Fabrizio Silvestri"],"pdf_url":"https://arxiv.org/pdf/2312.15228v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02333v1","updated":"2024-01-04T16:16:14Z","published":"2024-01-04T16:16:14Z","title":"Beyond Extraction: Contextualising Tabular Data for Efficient\n  Summarisation by Language Models","summary":"  The conventional use of the Retrieval-Augmented Generation (RAG) architecture\nhas proven effective for retrieving information from diverse documents.\nHowever, challenges arise in handling complex table queries, especially within\nPDF documents containing intricate tabular structures.This research introduces\nan innovative approach to enhance the accuracy of complex table queries in\nRAG-based systems. Our methodology involves storing PDFs in the retrieval\ndatabase and extracting tabular content separately. The extracted tables\nundergo a process of context enrichment, concatenating headers with\ncorresponding values. To ensure a comprehensive understanding of the enriched\ndata, we employ a fine-tuned version of the Llama-2-chat language model for\nsummarisation within the RAG architecture. Furthermore, we augment the tabular\ndata with contextual sense using the ChatGPT 3.5 API through a one-shot prompt.\nThis enriched data is then fed into the retrieval database alongside other\nPDFs. Our approach aims to significantly improve the precision of complex table\nqueries, offering a promising solution to a longstanding challenge in\ninformation retrieval.\n","authors":["Uday Allu","Biddwan Ahmed","Vishesh Tripathi"],"pdf_url":"https://arxiv.org/pdf/2401.02333v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02330v1","updated":"2024-01-04T16:07:43Z","published":"2024-01-04T16:07:43Z","title":"LLaVA-$φ$: Efficient Multi-Modal Assistant with Small Language Model","summary":"  In this paper, we introduce LLaVA-$\\phi$ (LLaVA-Phi), an efficient\nmulti-modal assistant that harnesses the power of the recently advanced small\nlanguage model, Phi-2, to facilitate multi-modal dialogues. LLaVA-Phi marks a\nnotable advancement in the realm of compact multi-modal models. It demonstrates\nthat even smaller language models, with as few as 2.7B parameters, can\neffectively engage in intricate dialogues that integrate both textual and\nvisual elements, provided they are trained with high-quality corpora. Our model\ndelivers commendable performance on publicly available benchmarks that\nencompass visual comprehension, reasoning, and knowledge-based perception.\nBeyond its remarkable performance in multi-modal dialogue tasks, our model\nopens new avenues for applications in time-sensitive environments and systems\nthat require real-time interaction, such as embodied agents. It highlights the\npotential of smaller language models to achieve sophisticated levels of\nunderstanding and interaction, while maintaining greater resource\nefficiency.The project is available at {https://github.com/zhuyiche/llava-phi}.\n","authors":["Yichen Zhu","Minjie Zhu","Ning Liu","Zhicai Ou","Xiaofeng Mou","Jian Tang"],"pdf_url":"https://arxiv.org/pdf/2401.02330v1.pdf","comment":"technique report"},{"id":"http://arxiv.org/abs/2401.02297v1","updated":"2024-01-04T14:36:38Z","published":"2024-01-04T14:36:38Z","title":"Are LLMs Robust for Spoken Dialogues?","summary":"  Large Pre-Trained Language Models have demonstrated state-of-the-art\nperformance in different downstream tasks, including dialogue state tracking\nand end-to-end response generation. Nevertheless, most of the publicly\navailable datasets and benchmarks on task-oriented dialogues focus on written\nconversations. Consequently, the robustness of the developed models to spoken\ninteractions is unknown. In this work, we have evaluated the performance of\nLLMs for spoken task-oriented dialogues on the DSTC11 test sets. Due to the\nlack of proper spoken dialogue datasets, we have automatically transcribed a\ndevelopment set of spoken dialogues with a state-of-the-art ASR engine. We have\ncharacterized the ASR-error types and their distributions and simulated these\nerrors in a large dataset of dialogues. We report the intrinsic (perplexity)\nand extrinsic (human evaluation) performance of fine-tuned GPT-2 and T5 models\nin two subtasks of response generation and dialogue state tracking,\nrespectively. The results show that LLMs are not robust to spoken noise by\ndefault, however, fine-tuning/training such models on a proper dataset of\nspoken TODs can result in a more robust performance.\n","authors":["Seyed Mahed Mousavi","Gabriel Roccabruna","Simone Alghisi","Massimo Rizzoli","Mirco Ravanelli","Giuseppe Riccardi"],"pdf_url":"https://arxiv.org/pdf/2401.02297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.02106v2","updated":"2024-01-04T13:32:06Z","published":"2022-04-05T10:55:33Z","title":"How do media talk about the Covid-19 pandemic? Metaphorical thematic\n  clustering in Italian online newspapers","summary":"  The contribution presents a study on figurative language of the first months\nof the COVID-19 crisis in Italian online newspapers. Particularly, we contrast\ntopics and metaphorical language used by journalists in the first and second\nphase of the government response to the pandemic in Spring 2020. The analysis\nis conducted on a journalistic corpus collected between February 24th and June\n3rd, 2020. The analysis is performed using both quantitative and qualitative\napproaches, combining Structural Topic Modelling (Roberts et al. 2016),\nConceptual Metaphor Theory (Lakoff & Johnson, 1980), and qualitative-corpus\nbased metaphor analysis (Charteris-Black, 2004). We find a significant shift in\ntopics discussed across Phase 1 and Phase 2, and interesting overlaps in\ntopic-specific metaphors. Using qualitative corpus analysis, we present a more\nin-depth case study discussing metaphorical collocations of the topics of\nEconomy and Society\n","authors":["Lucia Busso","Ottavia Tordini"],"pdf_url":"https://arxiv.org/pdf/2204.02106v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02256v1","updated":"2024-01-04T13:15:41Z","published":"2024-01-04T13:15:41Z","title":"Rethinking Response Evaluation from Interlocutor's Eye for Open-Domain\n  Dialogue Systems","summary":"  Open-domain dialogue systems have started to engage in continuous\nconversations with humans. Those dialogue systems are required to be adjusted\nto the human interlocutor and evaluated in terms of their perspective. However,\nit is questionable whether the current automatic evaluation methods can\napproximate the interlocutor's judgments. In this study, we analyzed and\nexamined what features are needed in an automatic response evaluator from the\ninterlocutor's perspective. The first experiment on the Hazumi dataset revealed\nthat interlocutor awareness plays a critical role in making automatic response\nevaluation correlate with the interlocutor's judgments. The second experiment\nusing massive conversations on X (formerly Twitter) confirmed that dialogue\ncontinuity prediction can train an interlocutor-aware response evaluator\nwithout human feedback while revealing the difficulty in evaluating generated\nresponses compared to human responses.\n","authors":["Yuma Tsuta","Naoki Yoshinaga","Shoetsu Sato","Masashi Toyoda"],"pdf_url":"https://arxiv.org/pdf/2401.02256v1.pdf","comment":"9 pages, 3 figures, 5 tables, Accepted by IJCNLP-AACL 2023 SRW"},{"id":"http://arxiv.org/abs/2401.02254v1","updated":"2024-01-04T13:11:17Z","published":"2024-01-04T13:11:17Z","title":"L3Cube-IndicNews: News-based Short Text and Long Document Classification\n  Datasets in Indic Languages","summary":"  In this work, we introduce L3Cube-IndicNews, a multilingual text\nclassification corpus aimed at curating a high-quality dataset for Indian\nregional languages, with a specific focus on news headlines and articles. We\nhave centered our work on 10 prominent Indic languages, including Hindi,\nBengali, Marathi, Telugu, Tamil, Gujarati, Kannada, Odia, Malayalam, and\nPunjabi. Each of these news datasets comprises 10 or more classes of news\narticles. L3Cube-IndicNews offers 3 distinct datasets tailored to handle\ndifferent document lengths that are classified as: Short Headlines\nClassification (SHC) dataset containing the news headline and news category,\nLong Document Classification (LDC) dataset containing the whole news article\nand the news category, and Long Paragraph Classification (LPC) containing\nsub-articles of the news and the news category. We maintain consistent labeling\nacross all 3 datasets for in-depth length-based analysis. We evaluate each of\nthese Indic language datasets using 4 different models including monolingual\nBERT, multilingual Indic Sentence BERT (IndicSBERT), and IndicBERT. This\nresearch contributes significantly to expanding the pool of available text\nclassification datasets and also makes it possible to develop topic\nclassification models for Indian regional languages. This also serves as an\nexcellent resource for cross-lingual analysis owing to the high overlap of\nlabels among languages. The datasets and models are shared publicly at\nhttps://github.com/l3cube-pune/indic-nlp\n","authors":["Aishwarya Mirashi","Srushti Sonavane","Purva Lingayat","Tejas Padhiyar","Raviraj Joshi"],"pdf_url":"https://arxiv.org/pdf/2401.02254v1.pdf","comment":"Accepted at the International Conference on Natural Language\n  Processing (ICON 2023)"},{"id":"http://arxiv.org/abs/2401.02212v1","updated":"2024-01-04T11:34:39Z","published":"2024-01-04T11:34:39Z","title":"Joint Multi-Facts Reasoning Network For Complex Temporal Question\n  Answering Over Knowledge Graph","summary":"  Temporal Knowledge Graph (TKG) is an extension of regular knowledge graph by\nattaching the time scope. Existing temporal knowledge graph question answering\n(TKGQA) models solely approach simple questions, owing to the prior assumption\nthat each question only contains a single temporal fact with explicit/implicit\ntemporal constraints. Hence, they perform poorly on questions which own\nmultiple temporal facts. In this paper, we propose \\textbf{\\underline{J}}oint\n\\textbf{\\underline{M}}ulti \\textbf{\\underline{F}}acts\n\\textbf{\\underline{R}}easoning \\textbf{\\underline{N}}etwork (JMFRN), to jointly\nreasoning multiple temporal facts for accurately answering \\emph{complex}\ntemporal questions. Specifically, JMFRN first retrieves question-related\ntemporal facts from TKG for each entity of the given complex question. For\njoint reasoning, we design two different attention (\\ie entity-aware and\ntime-aware) modules, which are suitable for universal settings, to aggregate\nentities and timestamps information of retrieved facts. Moreover, to filter\nincorrect type answers, we introduce an additional answer type discrimination\ntask. Extensive experiments demonstrate our proposed method significantly\noutperforms the state-of-art on the well-known complex temporal question\nbenchmark TimeQuestions.\n","authors":["Rikui Huang","Wei Wei","Xiaoye Qu","Wenfeng Xie","Xianling Mao","Dangyang Chen"],"pdf_url":"https://arxiv.org/pdf/2401.02212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02208v1","updated":"2024-01-04T11:27:48Z","published":"2024-01-04T11:27:48Z","title":"DIALIGHT: Lightweight Multilingual Development and Evaluation of\n  Task-Oriented Dialogue Systems with Large Language Models","summary":"  We present DIALIGHT, a toolkit for developing and evaluating multilingual\nTask-Oriented Dialogue (ToD) systems which facilitates systematic evaluations\nand comparisons between ToD systems using fine-tuning of Pretrained Language\nModels (PLMs) and those utilising the zero-shot and in-context learning\ncapabilities of Large Language Models (LLMs). In addition to automatic\nevaluation, this toolkit features (i) a secure, user-friendly web interface for\nfine-grained human evaluation at both local utterance level and global dialogue\nlevel, and (ii) a microservice-based backend, improving efficiency and\nscalability. Our evaluations reveal that while PLM fine-tuning leads to higher\naccuracy and coherence, LLM-based systems excel in producing diverse and\nlikeable responses. However, we also identify significant challenges of LLMs in\nadherence to task-specific instructions and generating outputs in multiple\nlanguages, highlighting areas for future research. We hope this open-sourced\ntoolkit will serve as a valuable resource for researchers aiming to develop and\nproperly evaluate multilingual ToD systems and will lower, currently still\nhigh, entry barriers in the field.\n","authors":["Songbo Hu","Xiaobin Wang","Zhangdie Yuan","Anna Korhonen","Ivan Vulić"],"pdf_url":"https://arxiv.org/pdf/2401.02208v1.pdf","comment":"17 pages, 7 tables, 9 figures"},{"id":"http://arxiv.org/abs/2307.05300v3","updated":"2024-01-04T10:51:24Z","published":"2023-07-11T14:45:19Z","title":"Unleashing the Emergent Cognitive Synergy in Large Language Models: A\n  Task-Solving Agent through Multi-Persona Self-Collaboration","summary":"  Human intelligence thrives on cognitive synergy, where collaboration among\ndifferent minds yield superior outcomes compared to isolated individuals. In\nthis work, we propose Solo Performance Prompting (SPP), which transforms a\nsingle LLM into a cognitive synergist by engaging in multi-turn\nself-collaboration with multiple personas. A cognitive synergist is an\nintelligent agent that collaboratively combines multiple minds' strengths and\nknowledge to enhance problem-solving in complex tasks. By dynamically\nidentifying and simulating different personas based on task inputs, SPP\nunleashes the potential of cognitive synergy in LLMs. Our in-depth analysis\nshows that assigning multiple fine-grained personas in LLMs improves\nproblem-solving abilities compared to using a single or fixed number of\npersonas. We evaluate SPP on three challenging tasks: Trivia Creative Writing,\nCodenames Collaborative, and Logic Grid Puzzle, encompassing both\nknowledge-intensive and reasoning-intensive types. Unlike previous works, such\nas Chain-of-Thought, that solely enhance the reasoning abilities in LLMs,\nexperimental results demonstrate that SPP effectively reduces factual\nhallucination, and maintains strong reasoning capabilities. Additionally,\ncomparative experiments show that cognitive synergy only emerges in GPT-4 and\ndoes not appear in less capable models, such as GPT-3.5-turbo and\nLlama2-13b-chat, which draws an interesting analogy to human development. Code,\ndata, and prompts can be found at:\nhttps://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.\n","authors":["Zhenhailong Wang","Shaoguang Mao","Wenshan Wu","Tao Ge","Furu Wei","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2307.05300v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.03656v3","updated":"2024-01-04T10:41:12Z","published":"2023-08-07T15:18:30Z","title":"Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using\n  EmotionBench","summary":"  Evaluating Large Language Models' (LLMs) anthropomorphic capabilities has\nbecome increasingly important in contemporary discourse. Utilizing the emotion\nappraisal theory from psychology, we propose to evaluate the empathy ability of\nLLMs, i.e., how their feelings change when presented with specific situations.\nAfter a careful and comprehensive survey, we collect a dataset containing over\n400 situations that have proven effective in eliciting the eight emotions\ncentral to our study. Categorizing the situations into 36 factors, we conduct a\nhuman evaluation involving more than 1,200 subjects worldwide. With the human\nevaluation results as references, our evaluation includes five LLMs, covering\nboth commercial and open-source models, including variations in model sizes,\nfeaturing the latest iterations, such as GPT-4 and LLaMA-2. We find that,\ndespite several misalignments, LLMs can generally respond appropriately to\ncertain situations. Nevertheless, they fall short in alignment with the\nemotional behaviors of human beings and cannot establish connections between\nsimilar situations. Our collected dataset of situations, the human evaluation\nresults, and the code of our testing framework, dubbed EmotionBench, is made\nopenly accessible via https://github.com/CUHK-ARISE/EmotionBench. We aspire to\ncontribute to the advancement of LLMs regarding better alignment with the\nemotional behaviors of human beings, thereby enhancing their utility and\napplicability as intelligent assistants.\n","authors":["Jen-tse Huang","Man Ho Lam","Eric John Li","Shujie Ren","Wenxuan Wang","Wenxiang Jiao","Zhaopeng Tu","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2308.03656v3.pdf","comment":"16 pages. Added demographic distribution of the user study. Added\n  ethics statements and limitations"},{"id":"http://arxiv.org/abs/2401.02187v1","updated":"2024-01-04T10:39:58Z","published":"2024-01-04T10:39:58Z","title":"Location Aware Modular Biencoder for Tourism Question Answering","summary":"  Answering real-world tourism questions that seek Point-of-Interest (POI)\nrecommendations is challenging, as it requires both spatial and non-spatial\nreasoning, over a large candidate pool. The traditional method of encoding each\npair of question and POI becomes inefficient when the number of candidates\nincreases, making it infeasible for real-world applications. To overcome this,\nwe propose treating the QA task as a dense vector retrieval problem, where we\nencode questions and POIs separately and retrieve the most relevant POIs for a\nquestion by utilizing embedding space similarity. We use pretrained language\nmodels (PLMs) to encode textual information, and train a location encoder to\ncapture spatial information of POIs. Experiments on a real-world tourism QA\ndataset demonstrate that our approach is effective, efficient, and outperforms\nprevious methods across all metrics. Enabled by the dense retrieval\narchitecture, we further build a global evaluation baseline, expanding the\nsearch space by 20 times compared to previous work. We also explore several\nfactors that impact on the model's performance through follow-up experiments.\nOur code and model are publicly available at https://github.com/haonan-li/LAMB.\n","authors":["Haonan Li","Martin Tomko","Timothy Baldwin"],"pdf_url":"https://arxiv.org/pdf/2401.02187v1.pdf","comment":"Accepted at AACL 2023"},{"id":"http://arxiv.org/abs/2305.08372v2","updated":"2024-01-04T10:15:28Z","published":"2023-05-15T06:14:36Z","title":"Hierarchical Aligned Multimodal Learning for NER on Tweet Posts","summary":"  Mining structured knowledge from tweets using named entity recognition (NER)\ncan be beneficial for many down stream applications such as recommendation and\nintention understanding. With tweet posts tending to be multimodal, multimodal\nnamed entity recognition (MNER) has attracted more attention. In this paper, we\npropose a novel approach, which can dynamically align the image and text\nsequence and achieve the multi-level cross-modal learning to augment textual\nword representation for MNER improvement. To be specific, our framework can be\nsplit into three main stages: the first stage focuses on intra-modality\nrepresentation learning to derive the implicit global and local knowledge of\neach modality, the second evaluates the relevance between the text and its\naccompanying image and integrates different grained visual information based on\nthe relevance, the third enforces semantic refinement via iterative cross-modal\ninteractions and co-attention. We conduct experiments on two open datasets, and\nthe results and detailed analysis demonstrate the advantage of our model.\n","authors":["Peipei Liu","Hong Li","Yimo Ren","Jie Liu","Shuaizong Si","Hongsong Zhu","Limin Sun"],"pdf_url":"https://arxiv.org/pdf/2305.08372v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06374v2","updated":"2024-01-04T10:12:18Z","published":"2023-12-11T13:32:11Z","title":"UstanceBR: a multimodal language resource for stance prediction","summary":"  This work introduces UstanceBR, a multimodal corpus in the Brazilian\nPortuguese Twitter domain for target-based stance prediction. The corpus\ncomprises 86.8 k labelled stances towards selected target topics, and extensive\nnetwork information about the users who published these stances on social\nmedia. In this article we describe the corpus multimodal data, and a number of\nusage examples in both in-domain and zero-shot stance prediction based on text-\nand network-related information, which are intended to provide initial baseline\nresults for future studies in the field.\n","authors":["Camila Pereira","Matheus Pavan","Sungwon Yoon","Ricelli Ramos","Pablo Costa","Lais Cavalheiro","Ivandre Paraboni"],"pdf_url":"https://arxiv.org/pdf/2312.06374v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02158v1","updated":"2024-01-04T09:13:18Z","published":"2024-01-04T09:13:18Z","title":"Shayona@SMM4H23: COVID-19 Self diagnosis classification using BERT and\n  LightGBM models","summary":"  This paper describes approaches and results for shared Task 1 and 4 of\nSMMH4-23 by Team Shayona. Shared Task-1 was binary classification of english\ntweets self-reporting a COVID-19 diagnosis, and Shared Task-4 was Binary\nclassification of English Reddit posts self-reporting a social anxiety disorder\ndiagnosis. Our team has achieved the highest f1-score 0.94 in Task-1 among all\nparticipants. We have leveraged the Transformer model (BERT) in combination\nwith the LightGBM model for both tasks.\n","authors":["Rushi Chavda","Darshan Makwana","Vraj Patel","Anupam Shukla"],"pdf_url":"https://arxiv.org/pdf/2401.02158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02147v1","updated":"2024-01-04T08:53:08Z","published":"2024-01-04T08:53:08Z","title":"Exploring Boundary of GPT-4V on Marine Analysis: A Preliminary Case\n  Study","summary":"  Large language models (LLMs) have demonstrated a powerful ability to answer\nvarious queries as a general-purpose assistant. The continuous multi-modal\nlarge language models (MLLM) empower LLMs with the ability to perceive visual\nsignals. The launch of GPT-4 (Generative Pre-trained Transformers) has\ngenerated significant interest in the research communities. GPT-4V(ison) has\ndemonstrated significant power in both academia and industry fields, as a focal\npoint in a new artificial intelligence generation. Though significant success\nwas achieved by GPT-4V, exploring MLLMs in domain-specific analysis (e.g.,\nmarine analysis) that required domain-specific knowledge and expertise has\ngained less attention. In this study, we carry out the preliminary and\ncomprehensive case study of utilizing GPT-4V for marine analysis. This report\nconducts a systematic evaluation of existing GPT-4V, assessing the performance\nof GPT-4V on marine research and also setting a new standard for future\ndevelopments in MLLMs. The experimental results of GPT-4V show that the\nresponses generated by GPT-4V are still far away from satisfying the\ndomain-specific requirements of the marine professions. All images and prompts\nused in this study will be available at\nhttps://github.com/hkust-vgd/Marine_GPT-4V_Eval\n","authors":["Ziqiang Zheng","Yiwei Chen","Jipeng Zhang","Tuan-Anh Vu","Huimin Zeng","Yue Him Wong Tim","Sai-Kit Yeung"],"pdf_url":"https://arxiv.org/pdf/2401.02147v1.pdf","comment":"51 pages, 36 figures, Repository:\n  https://github.com/hkust-vgd/Marine_GPT-4V_Eval"},{"id":"http://arxiv.org/abs/2401.02132v1","updated":"2024-01-04T08:34:16Z","published":"2024-01-04T08:34:16Z","title":"DCR-Consistency: Divide-Conquer-Reasoning for Consistency Evaluation and\n  Improvement of Large Language Models","summary":"  Evaluating the quality and variability of text generated by Large Language\nModels (LLMs) poses a significant, yet unresolved research challenge.\nTraditional evaluation methods, such as ROUGE and BERTScore, which measure\ntoken similarity, often fail to capture the holistic semantic equivalence. This\nresults in a low correlation with human judgments and intuition, which is\nespecially problematic in high-stakes applications like healthcare and finance\nwhere reliability, safety, and robust decision-making are highly critical. This\nwork proposes DCR, an automated framework for evaluating and improving the\nconsistency of LLM-generated texts using a divide-conquer-reasoning approach.\nUnlike existing LLM-based evaluators that operate at the paragraph level, our\nmethod employs a divide-and-conquer evaluator (DCE) that breaks down the\nparagraph-to-paragraph comparison between two generated responses into\nindividual sentence-to-paragraph comparisons, each evaluated based on\npredefined criteria. To facilitate this approach, we introduce an automatic\nmetric converter (AMC) that translates the output from DCE into an\ninterpretable numeric score. Beyond the consistency evaluation, we further\npresent a reason-assisted improver (RAI) that leverages the analytical reasons\nwith explanations identified by DCE to generate new responses aimed at reducing\nthese inconsistencies. Through comprehensive and systematic empirical analysis,\nwe show that our approach outperforms state-of-the-art methods by a large\nmargin (e.g., +19.3% and +24.3% on the SummEval dataset) in evaluating the\nconsistency of LLM generation across multiple benchmarks in semantic, factual,\nand summarization consistency tasks. Our approach also substantially reduces\nnearly 90% of output inconsistencies, showing promise for effective\nhallucination mitigation.\n","authors":["Wendi Cui","Jiaxin Zhang","Zhuohang Li","Lopez Damien","Kamalika Das","Bradley Malin","Sricharan Kumar"],"pdf_url":"https://arxiv.org/pdf/2401.02132v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02122v1","updated":"2024-01-04T08:11:33Z","published":"2024-01-04T08:11:33Z","title":"PEFT for Speech: Unveiling Optimal Placement, Merging Strategies, and\n  Ensemble Techniques","summary":"  Parameter-Efficient Fine-Tuning (PEFT) is increasingly recognized as an\neffective method in speech processing. However, the optimal approach and the\nplacement of PEFT methods remain inconclusive. Our study conducts extensive\nexperiments to compare different PEFT methods and their layer-wise placement\nadapting Differentiable Architecture Search (DARTS). We also explore the use of\nensemble learning to leverage diverse PEFT strategies. The results reveal that\nDARTS does not outperform the baseline approach, which involves inserting the\nsame PEFT method into all layers of a Self-Supervised Learning (SSL) model. In\ncontrast, an ensemble learning approach, particularly one employing majority\nvoting, demonstrates superior performance. Our statistical evidence indicates\nthat different PEFT methods learn in varied ways. This variation might explain\nwhy the synergistic integration of various PEFT methods through ensemble\nlearning can harness their unique learning capabilities more effectively\ncompared to individual layer-wise optimization.\n","authors":["Tzu-Han Lin","How-Shing Wang","Hao-Yung Weng","Kuang-Chen Peng","Zih-Ching Chen","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2401.02122v1.pdf","comment":"Submitted to ICASSP 2024 Self-supervision in Audio, Speech and Beyond\n  workshop"},{"id":"http://arxiv.org/abs/2401.02115v1","updated":"2024-01-04T07:50:24Z","published":"2024-01-04T07:50:24Z","title":"Using LLM to select the right SQL Query from candidates","summary":"  Text-to-SQL models can generate a list of candidate SQL queries, and the best\nquery is often in the candidate list, but not at the top of the list. An\neffective re-rank method can select the right SQL query from the candidate list\nand improve the model's performance. Previous studies on code generation\nautomatically generate test cases and use them to re-rank candidate codes.\nHowever, automatic test case generation for text-to-SQL is an understudied\nfield. We propose an automatic test case generation method that first generates\na database and then uses LLMs to predict the ground truth, which is the\nexpected execution results of the ground truth SQL query on this database. To\nreduce the difficulty for LLMs to predict, we conduct experiments to search for\nways to generate easy databases for LLMs and design easy-to-understand prompts.\nBased on our test case generation method, we propose a re-rank method to select\nthe right SQL query from the candidate list. Given a candidate list, our method\ncan generate test cases and re-rank the candidate list according to their pass\nnumbers on these test cases and their generation probabilities. The experiment\nresults on the validation dataset of Spider show that the performance of some\nstate-of-the-art models can get a 3.6\\% improvement after applying our re-rank\nmethod.\n","authors":["Zhenwen Li","Tao Xie"],"pdf_url":"https://arxiv.org/pdf/2401.02115v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2311.04589v3","updated":"2024-01-04T07:31:07Z","published":"2023-11-08T10:34:16Z","title":"TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models","summary":"  Despite Multi-modal Large Language Models (MM-LLMs) have made exciting\nstrides recently, they are still struggling to efficiently model the\ninteractions among multi-modal inputs and the generation in non-textual\nmodalities. In this work, we propose TEAL (Tokenize and Embed ALl)}, an\napproach to treat the input from any modality as a token sequence and learn a\njoint embedding space for all modalities. Specifically, for the input from any\nmodality, TEAL first discretizes it into a token sequence with the\noff-the-shelf tokenizer and embeds the token sequence into a joint embedding\nspace with a learnable embedding matrix. MM-LLMs just need to predict the\nmulti-modal tokens autoregressively as the textual LLMs do. Finally, the\ncorresponding de-tokenizer is applied to generate the output in each modality\nbased on the predicted token sequence. With the joint embedding space, TEAL\nenables the frozen LLMs to perform both understanding and generation tasks\ninvolving non-textual modalities, such as image and audio. Thus, the textual\nLLM can just work as an interface and maintain its high performance in textual\nunderstanding and generation. Experiments show that TEAL achieves substantial\nimprovements in multi-modal understanding, and implements a simple scheme for\nmulti-modal generations.\n","authors":["Zhen Yang","Yingxue Zhang","Fandong Meng","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2311.04589v3.pdf","comment":"Multi-modal, Large Language Models, Tokenizer, Understanding and\n  Generation"},{"id":"http://arxiv.org/abs/2401.02088v1","updated":"2024-01-04T06:23:22Z","published":"2024-01-04T06:23:22Z","title":"Re-evaluating the Memory-balanced Pipeline Parallelism: BPipe","summary":"  Pipeline parallelism is an essential technique in the training of large-scale\nTransformer models. However, it suffers from imbalanced memory consumption,\nleading to insufficient memory utilization. The BPipe technique was proposed to\naddress this issue and has proven effective in the GPT-3 model. Nevertheless,\nour experiments have not yielded similar benefits for LLaMA training.\nAdditionally, BPipe only yields negligible benefits for GPT-3 training when\napplying flash attention. We analyze the underlying causes of the divergent\nperformance of BPipe on GPT-3 and LLaMA. Furthermore, we introduce a novel\nmethod to estimate the performance of BPipe.\n","authors":["Mincong Huang","Chao Wang","Chi Ma","Yineng Zhang","Peng Zhang","Lei Yu"],"pdf_url":"https://arxiv.org/pdf/2401.02088v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02072v1","updated":"2024-01-04T05:47:41Z","published":"2024-01-04T05:47:41Z","title":"ICE-GRT: Instruction Context Enhancement by Generative Reinforcement\n  based Transformers","summary":"  The emergence of Large Language Models (LLMs) such as ChatGPT and LLaMA\nencounter limitations in domain-specific tasks, with these models often lacking\ndepth and accuracy in specialized areas, and exhibiting a decrease in general\ncapabilities when fine-tuned, particularly analysis ability in small sized\nmodels. To address these gaps, we introduce ICE-GRT, utilizing Reinforcement\nLearning from Human Feedback (RLHF) grounded in Proximal Policy Optimization\n(PPO), demonstrating remarkable ability in in-domain scenarios without\ncompromising general task performance. Our exploration of ICE-GRT highlights\nits understanding and reasoning ability to not only generate robust answers but\nalso to provide detailed analyses of the reasons behind the answer. This\ncapability marks a significant progression beyond the scope of Supervised\nFine-Tuning models. The success of ICE-GRT is dependent on several crucial\nfactors, including Appropriate Data, Reward Size Scaling, KL-Control, Advantage\nNormalization, etc. The ICE-GRT model exhibits state-of-the-art performance in\ndomain-specific tasks and across 12 general Language tasks against equivalent\nsize and even larger size LLMs, highlighting the effectiveness of our approach.\nWe provide a comprehensive analysis of the ICE-GRT, underscoring the\nsignificant advancements it brings to the field of LLM.\n","authors":["Chen Zheng","Ke Sun","Da Tang","Yukun Ma","Yuyu Zhang","Chenguang Xi","Xun Zhou"],"pdf_url":"https://arxiv.org/pdf/2401.02072v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14033v2","updated":"2024-01-04T05:11:22Z","published":"2023-12-21T17:02:06Z","title":"T-Eval: Evaluating the Tool Utilization Capability Step by Step","summary":"  Large language models (LLM) have achieved remarkable performance on various\nNLP tasks and are augmented by tools for broader applications. Yet, how to\nevaluate and analyze the tool-utilization capability of LLMs is still\nunder-explored. In contrast to previous works that evaluate models\nholistically, we comprehensively decompose the tool utilization into multiple\nsub-processes, including instruction following, planning, reasoning, retrieval,\nunderstanding, and review. Based on that, we further introduce T-Eval to\nevaluate the tool utilization capability step by step. T-Eval disentangles the\ntool utilization evaluation into several sub-domains along model capabilities,\nfacilitating the inner understanding of both holistic and isolated competency\nof LLMs. We conduct extensive experiments on T-Eval and in-depth analysis of\nvarious LLMs. T-Eval not only exhibits consistency with the outcome-oriented\nevaluation but also provides a more fine-grained analysis of the capabilities\nof LLMs, providing a new perspective in LLM evaluation on tool-utilization\nability. The benchmark will be available at\nhttps://github.com/open-compass/T-Eval.\n","authors":["Zehui Chen","Weihua Du","Wenwei Zhang","Kuikun Liu","Jiangning Liu","Miao Zheng","Jingming Zhuo","Songyang Zhang","Dahua Lin","Kai Chen","Feng Zhao"],"pdf_url":"https://arxiv.org/pdf/2312.14033v2.pdf","comment":"Code: https://github.com/open-compass/T-Eval; Website:\n  https://open-compass.github.io/T-Eval"},{"id":"http://arxiv.org/abs/2312.17432v2","updated":"2024-01-04T03:08:53Z","published":"2023-12-29T01:56:17Z","title":"Video Understanding with Large Language Models: A Survey","summary":"  With the burgeoning growth of online video platforms and the escalating\nvolume of video content, the demand for proficient video understanding tools\nhas intensified markedly. Given the remarkable capabilities of Large Language\nModels (LLMs) in language and multimodal tasks, this survey provides a detailed\noverview of the recent advancements in video understanding harnessing the power\nof LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly\nadvanced, particularly their ability for open-ended spatial-temporal reasoning\ncombined with commonsense knowledge, suggesting a promising path for future\nvideo understanding. We examine the unique characteristics and capabilities of\nVid-LLMs, categorizing the approaches into four main types: LLM-based Video\nAgents, Vid-LLMs Pretraining, Vid-LLMs Instruction Tuning, and Hybrid Methods.\nFurthermore, this survey presents a comprehensive study of the tasks, datasets,\nand evaluation methodologies for Vid-LLMs. Additionally, it explores the\nexpansive applications of Vid-LLMs across various domains, highlighting their\nremarkable scalability and versatility in real-world video understanding\nchallenges. Finally, it summarizes the limitations of existing Vid-LLMs and\noutlines directions for future research. For more information, readers are\nrecommended to visit the repository at\nhttps://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding.\n","authors":["Yunlong Tang","Jing Bi","Siting Xu","Luchuan Song","Susan Liang","Teng Wang","Daoan Zhang","Jie An","Jingyang Lin","Rongyi Zhu","Ali Vosoughi","Chao Huang","Zeliang Zhang","Feng Zheng","Jianguo Zhang","Ping Luo","Jiebo Luo","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2312.17432v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14504v2","updated":"2024-01-04T02:49:21Z","published":"2023-12-22T08:08:45Z","title":"Theory of Hallucinations based on Equivariance","summary":"  This study aims to acquire knowledge for creating very large language models\nthat are immune to hallucinations. Hallucinations in contemporary large\nlanguage models are often attributed to a misunderstanding of real-world social\nrelationships. Therefore, I hypothesize that very large language models capable\nof thoroughly grasping all these relationships will be free from\nhallucinations. Additionally, I propose that certain types of equivariant\nlanguage models are adept at learning and understanding these relationships.\nBuilding on this, I have developed a specialized cross-entropy error function\nto create a hallucination scale for language models, which measures their\nextent of equivariance acquisition. Utilizing this scale, I tested language\nmodels for their ability to acquire character-level equivariance. In\nparticular, I introduce and employ a novel technique based on T5 (Text To Text\nTransfer Transformer) that efficiently understands permuted input texts without\nthe need for explicit dictionaries to convert token IDs (integers) to texts\n(strings). This T5 model demonstrated a moderate ability to acquire\ncharacter-level equivariance. Additionally, I discovered scale laws that can\naid in developing hallucination-free language models at the character level.\nThis methodology can be extended to assess equivariance acquisition at the word\nlevel, paving the way for very large language models that can comprehensively\nunderstand relationships and, consequently, avoid hallucinations.\n","authors":["Hisaichi Shibata"],"pdf_url":"https://arxiv.org/pdf/2312.14504v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02038v1","updated":"2024-01-04T02:43:57Z","published":"2024-01-04T02:43:57Z","title":"Understanding LLMs: A Comprehensive Overview from Training to Inference","summary":"  The introduction of ChatGPT has led to a significant increase in the\nutilization of Large Language Models (LLMs) for addressing downstream tasks.\nThere's an increasing focus on cost-efficient training and deployment within\nthis context. Low-cost training and deployment of LLMs represent the future\ndevelopment trend. This paper reviews the evolution of large language model\ntraining techniques and inference deployment technologies aligned with this\nemerging trend. The discussion on training includes various aspects, including\ndata preprocessing, training architecture, pre-training tasks, parallel\ntraining, and relevant content related to model fine-tuning. On the inference\nside, the paper covers topics such as model compression, parallel computation,\nmemory scheduling, and structural optimization. It also explores LLMs'\nutilization and provides insights into their future development.\n","authors":["Yiheng Liu","Hao He","Tianle Han","Xu Zhang","Mengyuan Liu","Jiaming Tian","Yutong Zhang","Jiaqi Wang","Xiaohui Gao","Tianyang Zhong","Yi Pan","Shaochen Xu","Zihao Wu","Zhengliang Liu","Xin Zhang","Shu Zhang","Xintao Hu","Tuo Zhang","Ning Qiang","Tianming Liu","Bao Ge"],"pdf_url":"https://arxiv.org/pdf/2401.02038v1.pdf","comment":"30 pages,6 figures"},{"id":"http://arxiv.org/abs/2401.02034v1","updated":"2024-01-04T02:33:38Z","published":"2024-01-04T02:33:38Z","title":"Text2MDT: Extracting Medical Decision Trees from Medical Texts","summary":"  Knowledge of the medical decision process, which can be modeled as medical\ndecision trees (MDTs), is critical to build clinical decision support systems.\nHowever, the current MDT construction methods rely heavily on time-consuming\nand laborious manual annotation. In this work, we propose a novel task,\nText2MDT, to explore the automatic extraction of MDTs from medical texts such\nas medical guidelines and textbooks. We normalize the form of the MDT and\ncreate an annotated Text-to-MDT dataset in Chinese with the participation of\nmedical experts. We investigate two different methods for the Text2MDT tasks:\n(a) an end-to-end framework which only relies on a GPT style large language\nmodels (LLM) instruction tuning to generate all the node information and tree\nstructures. (b) The pipeline framework which decomposes the Text2MDT task to\nthree subtasks. Experiments on our Text2MDT dataset demonstrate that: (a) the\nend-to-end method basd on LLMs (7B parameters or larger) show promising\nresults, and successfully outperform the pipeline methods. (b) The\nchain-of-thought (COT) prompting method \\cite{Wei2022ChainOT} can improve the\nperformance of the fine-tuned LLMs on the Text2MDT test set. (c) the\nlightweight pipelined method based on encoder-based pretrained models can\nperform comparably with LLMs with model complexity two magnititudes smaller.\nOur Text2MDT dataset is open-sourced at\n\\url{https://tianchi.aliyun.com/dataset/95414}, and the source codes are\nopen-sourced at \\url{https://github.com/michael-wzhu/text2dt}.\n","authors":["Wei Zhu","Wenfeng Li","Xing Tian","Pengfei Wang","Xiaoling Wang","Jin Chen","Yuanbin Wu","Yuan Ni","Guotong Xie"],"pdf_url":"https://arxiv.org/pdf/2401.02034v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.06911v2","updated":"2024-01-04T02:22:07Z","published":"2023-08-14T03:12:29Z","title":"GIT-Mol: A Multi-modal Large Language Model for Molecular Science with\n  Graph, Image, and Text","summary":"  Large language models have made significant strides in natural language\nprocessing, enabling innovative applications in molecular science by processing\ntextual representations of molecules. However, most existing language models\ncannot capture the rich information with complex molecular structures or\nimages. In this paper, we introduce GIT-Mol, a multi-modal large language model\nthat integrates the Graph, Image, and Text information. To facilitate the\nintegration of multi-modal molecular data, we propose GIT-Former, a novel\narchitecture that is capable of aligning all modalities into a unified latent\nspace. We achieve a 5%-10% accuracy increase in properties prediction and a\n20.2% boost in molecule generation validity compared to the baselines. With the\nany-to-language molecular translation strategy, our model has the potential to\nperform more downstream tasks, such as compound name recognition and chemical\nreaction prediction.\n","authors":["Pengfei Liu","Yiming Ren","Zhixiang Ren"],"pdf_url":"https://arxiv.org/pdf/2308.06911v2.pdf","comment":"14 pages, 4 figures"},{"id":"http://arxiv.org/abs/2305.06355v2","updated":"2024-01-04T02:06:07Z","published":"2023-05-10T17:59:04Z","title":"VideoChat: Chat-Centric Video Understanding","summary":"  In this paper, we initiate an attempt of developing an end-to-end\nchat-centric video understanding system, coined as VideoChat. It integrates\nvideo foundation models and large language models via a learnable neural\ninterface, excelling in spatiotemporal reasoning, event localization, and\ncausal relationship inference. To instructively tune this system, we build a\nvideo-centric instruction dataset, composed of thousands of videos associated\nwith detailed descriptions and conversations. This dataset emphasizes\nspatiotemporal reasoning and captures causal relationships, providing a\nvaluable asset for training our chat-centric video understanding system.\nPreliminary qualitative experiments demonstrate the potential of our system\nacross a broad spectrum of video applications, which could serve as a simple\nprototype system for future research on chat-centric video understanding.\nAccess our code and data at https://github.com/OpenGVLab/Ask-Anything\n","authors":["KunChang Li","Yinan He","Yi Wang","Yizhuo Li","Wenhai Wang","Ping Luo","Yali Wang","Limin Wang","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2305.06355v2.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2401.01761v2","updated":"2024-01-04T01:30:36Z","published":"2024-01-03T14:28:55Z","title":"Cross-target Stance Detection by Exploiting Target Analytical\n  Perspectives","summary":"  Cross-target stance detection (CTSD) is an important task, which infers the\nattitude of the destination target by utilizing annotated data derived from the\nsource target. One important approach in CTSD is to extract domain-invariant\nfeatures to bridge the knowledge gap between multiple targets. However, the\nanalysis of informal and short text structure, and implicit expressions,\ncomplicate the extraction of domain-invariant knowledge. In this paper, we\npropose a Multi-Perspective Prompt-Tuning (MPPT) model for CTSD that uses the\nanalysis perspective as a bridge to transfer knowledge. First, we develop a\ntwo-stage instruct-based chain-of-thought method (TsCoT) to elicit target\nanalysis perspectives and provide natural language explanations (NLEs) from\nmultiple viewpoints by formulating instructions based on large language model\n(LLM). Second, we propose a multi-perspective prompt-tuning framework\n(MultiPLN) to fuse the NLEs into the stance predictor. Extensive experiments\nresults demonstrate the superiority of MPPT against the state-of-the-art\nbaseline methods.\n","authors":["Daijun Ding","Rong Chen","Liwen Jing","Bowen Zhang","Xu Huang","Li Dong","Xiaowen Zhao","Ge Song"],"pdf_url":"https://arxiv.org/pdf/2401.01761v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02009v1","updated":"2024-01-04T00:32:33Z","published":"2024-01-04T00:32:33Z","title":"Self-Contrast: Better Reflection Through Inconsistent Solving\n  Perspectives","summary":"  The reflection capacity of Large Language Model (LLM) has garnered extensive\nattention. A post-hoc prompting strategy, e.g., reflexion and self-refine,\nrefines LLM's response based on self-evaluated or external feedback. However,\nrecent research indicates without external feedback, LLM's intrinsic reflection\nis unstable. Our investigation unveils that the key bottleneck is the quality\nof the self-evaluated feedback. We find LLMs often exhibit overconfidence or\nhigh randomness when self-evaluate, offering stubborn or inconsistent feedback,\nwhich causes poor reflection. To remedy this, we advocate Self-Contrast: It\nadaptively explores diverse solving perspectives tailored to the request,\ncontrasts the differences, and summarizes these discrepancies into a checklist\nwhich could be used to re-examine and eliminate discrepancies. Our method\nendows LLM with diverse perspectives to alleviate stubborn biases. Moreover,\ntheir discrepancies indicate potential errors or inherent uncertainties that\nLLM often overlooks. Reflecting upon these can catalyze more accurate and\nstable reflection. Experiments conducted on a series of reasoning and\ntranslation tasks with different LLMs serve to underscore the effectiveness and\ngenerality of our strategy.\n","authors":["Wenqi Zhang","Yongliang Shen","Linjuan Wu","Qiuying Peng","Jun Wang","Yueting Zhuang","Weiming Lu"],"pdf_url":"https://arxiv.org/pdf/2401.02009v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.07372v5","updated":"2024-01-04T23:54:41Z","published":"2023-05-12T10:45:29Z","title":"Interactive Text-to-SQL Generation via Editable Step-by-Step\n  Explanations","summary":"  Relational databases play an important role in business, science, and more.\nHowever, many users cannot fully unleash the analytical power of relational\ndatabases, because they are not familiar with database languages such as SQL.\nMany techniques have been proposed to automatically generate SQL from natural\nlanguage, but they suffer from two issues: (1) they still make many mistakes,\nparticularly for complex queries, and (2) they do not provide a flexible way\nfor non-expert users to validate and refine incorrect queries. To address these\nissues, we introduce a new interaction mechanism that allows users to directly\nedit a step-by-step explanation of a query to fix errors. Our experiments on\nmultiple datasets, as well as a user study with 24 participants, demonstrate\nthat our approach can achieve better performance than multiple SOTA approaches.\nOur code and datasets are available at https://github.com/magic-YuanTian/STEPS.\n","authors":["Yuan Tian","Zheng Zhang","Zheng Ning","Toby Jia-Jun Li","Jonathan K. Kummerfeld","Tianyi Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.07372v5.pdf","comment":"Accepted to EMNLP 2023"},{"id":"http://arxiv.org/abs/2312.11514v2","updated":"2024-01-04T22:28:37Z","published":"2023-12-12T18:57:08Z","title":"LLM in a flash: Efficient Large Language Model Inference with Limited\n  Memory","summary":"  Large language models (LLMs) are central to modern natural language\nprocessing, delivering exceptional performance in various tasks. However, their\nsubstantial computational and memory requirements present challenges,\nespecially for devices with limited DRAM capacity. This paper tackles the\nchallenge of efficiently running LLMs that exceed the available DRAM capacity\nby storing the model parameters in flash memory, but bringing them on demand to\nDRAM. Our method involves constructing an inference cost model that takes into\naccount the characteristics of flash memory, guiding us to optimize in two\ncritical areas: reducing the volume of data transferred from flash and reading\ndata in larger, more contiguous chunks. Within this hardware-informed\nframework, we introduce two principal techniques. First, \"windowing\"\nstrategically reduces data transfer by reusing previously activated neurons,\nand second, \"row-column bundling\", tailored to the sequential data access\nstrengths of flash memory, increases the size of data chunks read from flash\nmemory. These methods collectively enable running models up to twice the size\nof the available DRAM, with a 4-5x and 20-25x increase in inference speed\ncompared to naive loading approaches in CPU and GPU, respectively. Our\nintegration of sparsity awareness, context-adaptive loading, and a\nhardware-oriented design paves the way for effective inference of LLMs on\ndevices with limited memory.\n","authors":["Keivan Alizadeh","Iman Mirzadeh","Dmitry Belenko","Karen Khatamifard","Minsik Cho","Carlo C Del Mundo","Mohammad Rastegari","Mehrdad Farajtabar"],"pdf_url":"https://arxiv.org/pdf/2312.11514v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2401.02509v1","updated":"2024-01-04T19:44:03Z","published":"2024-01-04T19:44:03Z","title":"Memory, Consciousness and Large Language Model","summary":"  With the development in cognitive science and Large Language Models (LLMs),\nincreasing connections have come to light between these two distinct fields.\nBuilding upon these connections, we propose a conjecture suggesting the\nexistence of a duality between LLMs and Tulving's theory of memory. We identify\na potential correspondence between Tulving's synergistic ecphory model (SEM) of\nretrieval and the emergent abilities observed in LLMs, serving as supporting\nevidence for our conjecture. Furthermore, we speculate that consciousness may\nbe considered a form of emergent ability based on this duality. We also discuss\nhow other theories of consciousness intersect with our research.\n","authors":["Jitang Li","Jinzheng Li"],"pdf_url":"https://arxiv.org/pdf/2401.02509v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2301.11329v2","updated":"2024-01-04T18:59:56Z","published":"2023-01-26T18:59:33Z","title":"Anatomy-aware and acquisition-agnostic joint registration with\n  SynthMorph","summary":"  Affine image registration is a cornerstone of medical-image analysis. While\nclassical algorithms can achieve excellent accuracy, they solve a\ntime-consuming optimization for every image pair. Deep-learning (DL) methods\nlearn a function that maps an image pair to an output transform. Evaluating the\nfunction is fast, but capturing large transforms can be challenging, and\nnetworks tend to struggle if a test-image characteristic shifts from the\ntraining domain, such as resolution. Most affine methods are agnostic to\nanatomy, meaning the registration will be inaccurate if algorithms consider all\nstructures in the image.\n  We address these shortcomings with SynthMorph, an easy-to-use DL tool for\njoint affine-deformable registration of any brain image without preprocessing,\nright off the MRI scanner. First, we leverage a strategy to train networks with\nwildly varying images synthesized from label maps, yielding robust performance\nacross acquisition specifics unseen at training. Second, we optimize the\nspatial overlap of select anatomical labels. This enables networks to\ndistinguish anatomy of interest from irrelevant structures, removing the need\nfor preprocessing that excludes content which would impinge on anatomy-specific\nregistration. Third, we combine the affine model with a deformable hypernetwork\nthat lets users choose the optimal deformation-field regularity for their\nspecific data, at registration time, in a fraction of the time required by\nclassical methods.\n  We rigorously analyze how competing architectures learn affine transforms and\ncompare state-of-the-art registration tools across an extremely diverse set of\nneuroimaging data, aiming to truly capture the behavior of methods in the real\nworld. SynthMorph demonstrates consistent and improved accuracy. It is\navailable at https://w3id.org/synthmorph, as a single complete end-to-end\nsolution for registration of brain MRI.\n","authors":["Malte Hoffmann","Andrew Hoopes","Douglas N. Greve","Bruce Fischl","Adrian V. Dalca"],"pdf_url":"https://arxiv.org/pdf/2301.11329v2.pdf","comment":"33 pages, 22 figures, 4 tables, affine registration, deformable\n  registration, deep learning, hypernetwork, domain shift, neuroimaging"},{"id":"http://arxiv.org/abs/2401.02418v1","updated":"2024-01-04T18:59:49Z","published":"2024-01-04T18:59:49Z","title":"Learning to Prompt with Text Only Supervision for Vision-Language Models","summary":"  Foundational vision-language models such as CLIP are becoming a new paradigm\nin vision, due to their excellent generalization abilities. However, adapting\nthese models for downstream tasks while maintaining their generalization\nremains a challenge. In literature, one branch of methods adapts CLIP by\nlearning prompts using visual information. While effective, most of these works\nrequire labeled data which is not practical, and often struggle to generalize\ntowards new datasets due to over-fitting on the source data. An alternative\napproach resorts to training-free methods by generating class descriptions from\nlarge language models (LLMs) and perform prompt ensembling. However, these\nmethods often generate class specific prompts that cannot be transferred to\nother classes, which incur higher costs by generating LLM descriptions for each\nclass separately. In this work, we propose to combine the strengths of these\nboth streams of methods by learning prompts using only text data derived from\nLLMs. As supervised training of prompts is not trivial due to absence of\nimages, we develop a training approach that allows prompts to extract rich\ncontextual knowledge from LLM data. Moreover, with LLM contextual data mapped\nwithin the learned prompts, it enables zero-shot transfer of prompts to new\nclasses and datasets potentially cutting the LLM prompt engineering cost. To\nthe best of our knowledge, this is the first work that learns generalized\nprompts using text only data. We perform extensive evaluations on 4 benchmarks\nwhere our method improves over prior ensembling works while being competitive\nto those utilizing labeled images. Our code and pre-trained models are\navailable at https://github.com/muzairkhattak/ProText.\n","authors":["Muhammad Uzair Khattak","Muhammad Ferjad Naeem","Muzammal Naseer","Luc Van Gool","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2401.02418v1.pdf","comment":"Project Page: https://muzairkhattak.github.io/ProText/"},{"id":"http://arxiv.org/abs/2401.02416v1","updated":"2024-01-04T18:59:25Z","published":"2024-01-04T18:59:25Z","title":"ODIN: A Single Model for 2D and 3D Perception","summary":"  State-of-the-art models on contemporary 3D perception benchmarks like ScanNet\nconsume and label dataset-provided 3D point clouds, obtained through post\nprocessing of sensed multiview RGB-D images. They are typically trained\nin-domain, forego large-scale 2D pre-training and outperform alternatives that\nfeaturize the posed RGB-D multiview images instead. The gap in performance\nbetween methods that consume posed images versus post-processed 3D point clouds\nhas fueled the belief that 2D and 3D perception require distinct model\narchitectures. In this paper, we challenge this view and propose ODIN\n(Omni-Dimensional INstance segmentation), a model that can segment and label\nboth 2D RGB images and 3D point clouds, using a transformer architecture that\nalternates between 2D within-view and 3D cross-view information fusion. Our\nmodel differentiates 2D and 3D feature operations through the positional\nencodings of the tokens involved, which capture pixel coordinates for 2D patch\ntokens and 3D coordinates for 3D feature tokens. ODIN achieves state-of-the-art\nperformance on ScanNet200, Matterport3D and AI2THOR 3D instance segmentation\nbenchmarks, and competitive performance on ScanNet, S3DIS and COCO. It\noutperforms all previous works by a wide margin when the sensed 3D point cloud\nis used in place of the point cloud sampled from 3D mesh. When used as the 3D\nperception engine in an instructable embodied agent architecture, it sets a new\nstate-of-the-art on the TEACh action-from-dialogue benchmark. Our code and\ncheckpoints can be found at the project website: https://odin-seg.github.io.\n","authors":["Ayush Jain","Pushkal Katara","Nikolaos Gkanatsios","Adam W. Harley","Gabriel Sarch","Kriti Aggarwal","Vishrav Chaudhary","Katerina Fragkiadaki"],"pdf_url":"https://arxiv.org/pdf/2401.02416v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02414v1","updated":"2024-01-04T18:55:01Z","published":"2024-01-04T18:55:01Z","title":"Bring Metric Functions into Diffusion Models","summary":"  We introduce a Cascaded Diffusion Model (Cas-DM) that improves a Denoising\nDiffusion Probabilistic Model (DDPM) by effectively incorporating additional\nmetric functions in training. Metric functions such as the LPIPS loss have been\nproven highly effective in consistency models derived from the score matching.\nHowever, for the diffusion counterparts, the methodology and efficacy of adding\nextra metric functions remain unclear. One major challenge is the mismatch\nbetween the noise predicted by a DDPM at each step and the desired clean image\nthat the metric function works well on. To address this problem, we propose\nCas-DM, a network architecture that cascades two network modules to effectively\napply metric functions to the diffusion model training. The first module,\nsimilar to a standard DDPM, learns to predict the added noise and is unaffected\nby the metric function. The second cascaded module learns to predict the clean\nimage, thereby facilitating the metric function computation. Experiment results\nshow that the proposed diffusion model backbone enables the effective use of\nthe LPIPS loss, leading to state-of-the-art image quality (FID, sFID, IS) on\nvarious established benchmarks.\n","authors":["Jie An","Zhengyuan Yang","Jianfeng Wang","Linjie Li","Zicheng Liu","Lijuan Wang","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2401.02414v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02412v1","updated":"2024-01-04T18:53:01Z","published":"2024-01-04T18:53:01Z","title":"LLM Augmented LLMs: Expanding Capabilities through Composition","summary":"  Foundational models with billions of parameters which have been trained on\nlarge corpora of data have demonstrated non-trivial skills in a variety of\ndomains. However, due to their monolithic structure, it is challenging and\nexpensive to augment them or impart new skills. On the other hand, due to their\nadaptation abilities, several new instances of these models are being trained\ntowards new domains and tasks. In this work, we study the problem of efficient\nand practical composition of existing foundation models with more specific\nmodels to enable newer capabilities. To this end, we propose CALM --\nComposition to Augment Language Models -- which introduces cross-attention\nbetween models to compose their representations and enable new capabilities.\nSalient features of CALM are: (i) Scales up LLMs on new tasks by 're-using'\nexisting LLMs along with a few additional parameters and data, (ii) Existing\nmodel weights are kept intact, and hence preserves existing capabilities, and\n(iii) Applies to diverse domains and settings. We illustrate that augmenting\nPaLM2-S with a smaller model trained on low-resource languages results in an\nabsolute improvement of up to 13\\% on tasks like translation into English and\narithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is\naugmented with a code-specific model, we see a relative improvement of 40\\%\nover the base model for code generation and explanation tasks -- on-par with\nfully fine-tuned counterparts.\n","authors":["Rachit Bansal","Bidisha Samanta","Siddharth Dalmia","Nitish Gupta","Shikhar Vashishth","Sriram Ganapathy","Abhishek Bapna","Prateek Jain","Partha Talukdar"],"pdf_url":"https://arxiv.org/pdf/2401.02412v1.pdf","comment":"17 pages, 2 figures, 8 tables"},{"id":"http://arxiv.org/abs/2401.02411v1","updated":"2024-01-04T18:50:38Z","published":"2024-01-04T18:50:38Z","title":"What You See is What You GAN: Rendering Every Pixel for High-Fidelity\n  Geometry in 3D GANs","summary":"  3D-aware Generative Adversarial Networks (GANs) have shown remarkable\nprogress in learning to generate multi-view-consistent images and 3D geometries\nof scenes from collections of 2D images via neural volume rendering. Yet, the\nsignificant memory and computational costs of dense sampling in volume\nrendering have forced 3D GANs to adopt patch-based training or employ\nlow-resolution rendering with post-processing 2D super resolution, which\nsacrifices multiview consistency and the quality of resolved geometry.\nConsequently, 3D GANs have not yet been able to fully resolve the rich 3D\ngeometry present in 2D images. In this work, we propose techniques to scale\nneural volume rendering to the much higher resolution of native 2D images,\nthereby resolving fine-grained 3D geometry with unprecedented detail. Our\napproach employs learning-based samplers for accelerating neural rendering for\n3D GAN training using up to 5 times fewer depth samples. This enables us to\nexplicitly \"render every pixel\" of the full-resolution image during training\nand inference without post-processing superresolution in 2D. Together with our\nstrategy to learn high-quality surface geometry, our method synthesizes\nhigh-resolution 3D geometry and strictly view-consistent images while\nmaintaining image quality on par with baselines relying on post-processing\nsuper resolution. We demonstrate state-of-the-art 3D gemetric quality on FFHQ\nand AFHQ, setting a new standard for unsupervised learning of 3D shapes in 3D\nGANs.\n","authors":["Alex Trevithick","Matthew Chan","Towaki Takikawa","Umar Iqbal","Shalini De Mello","Manmohan Chandraker","Ravi Ramamoorthi","Koki Nagano"],"pdf_url":"https://arxiv.org/pdf/2401.02411v1.pdf","comment":"See our project page: https://research.nvidia.com/labs/nxp/wysiwyg/"},{"id":"http://arxiv.org/abs/2401.02402v1","updated":"2024-01-04T18:39:32Z","published":"2024-01-04T18:39:32Z","title":"3D Open-Vocabulary Panoptic Segmentation with 2D-3D Vision-Language\n  Distillation","summary":"  3D panoptic segmentation is a challenging perception task, which aims to\npredict both semantic and instance annotations for 3D points in a scene.\nAlthough prior 3D panoptic segmentation approaches have achieved great\nperformance on closed-set benchmarks, generalizing to novel categories remains\nan open problem. For unseen object categories, 2D open-vocabulary segmentation\nhas achieved promising results that solely rely on frozen CLIP backbones and\nensembling multiple classification outputs. However, we find that simply\nextending these 2D models to 3D does not achieve good performance due to poor\nper-mask classification quality on novel categories. In this paper, we propose\nthe first method to tackle 3D open-vocabulary panoptic segmentation. Our model\ntakes advantage of the fusion between learnable LiDAR features and dense frozen\nvision CLIP features, using a single classification head to make predictions\nfor both base and novel classes. To further improve the classification\nperformance on novel classes and leverage the CLIP model, we propose two novel\nloss functions: object-level distillation loss and voxel-level distillation\nloss. Our experiments on the nuScenes and SemanticKITTI datasets show that our\nmethod outperforms strong baselines by a large margin.\n","authors":["Zihao Xiao","Longlong Jing","Shangxuan Wu","Alex Zihao Zhu","Jingwei Ji","Chiyu Max Jiang","Wei-Chih Hung","Thomas Funkhouser","Weicheng Kuo","Anelia Angelova","Yin Zhou","Shiwei Sheng"],"pdf_url":"https://arxiv.org/pdf/2401.02402v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.13991v3","updated":"2024-01-04T18:35:21Z","published":"2023-02-27T17:30:00Z","title":"Learning to Generalize towards Unseen Domains via a Content-Aware Style\n  Invariant Model for Disease Detection from Chest X-rays","summary":"  Performance degradation due to distribution discrepancy is a longstanding\nchallenge in intelligent imaging, particularly for chest X-rays (CXRs). Recent\nstudies have demonstrated that CNNs are biased toward styles (e.g.,\nuninformative textures) rather than content (e.g., shape), in stark contrast to\nthe human vision system. Radiologists tend to learn visual cues from CXRs and\nthus perform well across multiple domains. Motivated by this, we employ the\nnovel on-the-fly style randomization modules at both image (SRM-IL) and feature\n(SRM-FL) levels to create rich style perturbed features while keeping the\ncontent intact for robust cross-domain performance. Previous methods simulate\nunseen domains by constructing new styles via interpolation or swapping styles\nfrom existing data, limiting them to available source domains during training.\nHowever, SRM-IL samples the style statistics from the possible value range of a\nCXR image instead of the training data to achieve more diversified\naugmentations. Moreover, we utilize pixel-wise learnable parameters in the\nSRM-FL compared to pre-defined channel-wise mean and standard deviations as\nstyle embeddings for capturing more representative style features.\nAdditionally, we leverage consistency regularizations on global semantic\nfeatures and predictive distributions from with and without style-perturbed\nversions of the same CXR to tweak the model's sensitivity toward content\nmarkers for accurate predictions. Our proposed method, trained on CheXpert and\nMIMIC-CXR datasets, achieves 77.32$\\pm$0.35, 88.38$\\pm$0.19, 82.63$\\pm$0.13\nAUCs(%) on the unseen domain test datasets, i.e., BRAX, VinDr-CXR, and NIH\nchest X-ray14, respectively, compared to 75.56$\\pm$0.80, 87.57$\\pm$0.46,\n82.07$\\pm$0.19 from state-of-the-art models on five-fold cross-validation with\nstatistically significant results in thoracic disease classification.\n","authors":["Mohammad Zunaed","Md. Aynal Haque","Taufiq Hasan"],"pdf_url":"https://arxiv.org/pdf/2302.13991v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02400v1","updated":"2024-01-04T18:32:48Z","published":"2024-01-04T18:32:48Z","title":"Learning the 3D Fauna of the Web","summary":"  Learning 3D models of all animals on the Earth requires massively scaling up\nexisting solutions. With this ultimate goal in mind, we develop 3D-Fauna, an\napproach that learns a pan-category deformable 3D animal model for more than\n100 animal species jointly. One crucial bottleneck of modeling animals is the\nlimited availability of training data, which we overcome by simply learning\nfrom 2D Internet images. We show that prior category-specific attempts fail to\ngeneralize to rare species with limited training images. We address this\nchallenge by introducing the Semantic Bank of Skinned Models (SBSM), which\nautomatically discovers a small set of base animal shapes by combining\ngeometric inductive priors with semantic knowledge implicitly captured by an\noff-the-shelf self-supervised feature extractor. To train such a model, we also\ncontribute a new large-scale dataset of diverse animal species. At inference\ntime, given a single image of any quadruped animal, our model reconstructs an\narticulated 3D mesh in a feed-forward fashion within seconds.\n","authors":["Zizhang Li","Dor Litvak","Ruining Li","Yunzhi Zhang","Tomas Jakab","Christian Rupprecht","Shangzhe Wu","Andrea Vedaldi","Jiajun Wu"],"pdf_url":"https://arxiv.org/pdf/2401.02400v1.pdf","comment":"The first two authors contributed equally to this work. The last\n  three authors contributed equally. Project page:\n  https://kyleleey.github.io/3DFauna/"},{"id":"http://arxiv.org/abs/2312.06661v2","updated":"2024-01-04T17:59:04Z","published":"2023-12-11T18:59:55Z","title":"UpFusion: Novel View Diffusion from Unposed Sparse View Observations","summary":"  We propose UpFusion, a system that can perform novel view synthesis and infer\n3D representations for an object given a sparse set of reference images without\ncorresponding pose information. Current sparse-view 3D inference methods\ntypically rely on camera poses to geometrically aggregate information from\ninput views, but are not robust in-the-wild when such information is\nunavailable/inaccurate. In contrast, UpFusion sidesteps this requirement by\nlearning to implicitly leverage the available images as context in a\nconditional generative model for synthesizing novel views. We incorporate two\ncomplementary forms of conditioning into diffusion models for leveraging the\ninput views: a) via inferring query-view aligned features using a scene-level\ntransformer, b) via intermediate attentional layers that can directly observe\nthe input image tokens. We show that this mechanism allows generating\nhigh-fidelity novel views while improving the synthesis quality given\nadditional (unposed) images. We evaluate our approach on the Co3Dv2 and Google\nScanned Objects datasets and demonstrate the benefits of our method over\npose-reliant sparse-view methods as well as single-view methods that cannot\nleverage additional views. Finally, we also show that our learned model can\ngeneralize beyond the training categories and even allow reconstruction from\nself-captured images of generic objects in-the-wild.\n","authors":["Bharath Raj Nagoor Kani","Hsin-Ying Lee","Sergey Tulyakov","Shubham Tulsiani"],"pdf_url":"https://arxiv.org/pdf/2312.06661v2.pdf","comment":"Project Page: https://upfusion3d.github.io/ v2: Fixed a citation\n  mistake"},{"id":"http://arxiv.org/abs/2401.02384v1","updated":"2024-01-04T17:51:48Z","published":"2024-01-04T17:51:48Z","title":"ChartAssisstant: A Universal Chart Multimodal Language Model via\n  Chart-to-Table Pre-training and Multitask Instruction Tuning","summary":"  Charts play a vital role in data visualization, understanding data patterns,\nand informed decision-making. However, their unique combination of graphical\nelements (e.g., bars, lines) and textual components (e.g., labels, legends)\nposes challenges for general-purpose multimodal models. While vision-language\nmodels trained on chart data excel in comprehension, they struggle with\ngeneralization and require task-specific fine-tuning. To address these\nchallenges, we propose ChartAssistant, a chart-based vision-language model for\nuniversal chart comprehension and reasoning. ChartAssistant leverages ChartSFT,\na comprehensive dataset covering diverse chart-related tasks with basic and\nspecialized chart types. It undergoes a two-stage training process, starting\nwith pre-training on chart-to-table parsing to align chart and text, followed\nby multitask instruction-following fine-tuning. This approach enables\nChartAssistant to achieve competitive performance across various chart tasks\nwithout task-specific fine-tuning. Experimental results demonstrate significant\nperformance gains over the state-of-the-art UniChart method, outperforming\nOpenAI's GPT-4V(ision) on real-world chart data. The code and data are\navailable at https://github.com/OpenGVLab/ChartAst.\n","authors":["Fanqing Meng","Wenqi Shao","Quanfeng Lu","Peng Gao","Kaipeng Zhang","Yu Qiao","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2401.02384v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02383v1","updated":"2024-01-04T17:51:44Z","published":"2024-01-04T17:51:44Z","title":"Survey of 3D Human Body Pose and Shape Estimation Methods for\n  Contemporary Dance Applications","summary":"  3D human body shape and pose estimation from RGB images is a challenging\nproblem with potential applications in augmented/virtual reality, healthcare\nand fitness technology and virtual retail. Recent solutions have focused on\nthree types of inputs: i) single images, ii) multi-view images and iii) videos.\nIn this study, we surveyed and compared 3D body shape and pose estimation\nmethods for contemporary dance and performing arts, with a special focus on\nhuman body pose and dressing, camera viewpoint, illumination conditions and\nbackground conditions. We demonstrated that multi-frame methods, such as PHALP,\nprovide better results than single-frame method for pose estimation when\ndancers are performing contemporary dances.\n","authors":["Darshan Venkatrayappa","Alain Tremeau","Damien Muselet","Philippe Colantoni"],"pdf_url":"https://arxiv.org/pdf/2401.02383v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2008.09062 by other authors"},{"id":"http://arxiv.org/abs/2401.02361v1","updated":"2024-01-04T17:00:49Z","published":"2024-01-04T17:00:49Z","title":"An Open and Comprehensive Pipeline for Unified Object Grounding and\n  Detection","summary":"  Grounding-DINO is a state-of-the-art open-set detection model that tackles\nmultiple vision tasks including Open-Vocabulary Detection (OVD), Phrase\nGrounding (PG), and Referring Expression Comprehension (REC). Its effectiveness\nhas led to its widespread adoption as a mainstream architecture for various\ndownstream applications. However, despite its significance, the original\nGrounding-DINO model lacks comprehensive public technical details due to the\nunavailability of its training code. To bridge this gap, we present\nMM-Grounding-DINO, an open-source, comprehensive, and user-friendly baseline,\nwhich is built with the MMDetection toolbox. It adopts abundant vision datasets\nfor pre-training and various detection and grounding datasets for fine-tuning.\nWe give a comprehensive analysis of each reported result and detailed settings\nfor reproduction. The extensive experiments on the benchmarks mentioned\ndemonstrate that our MM-Grounding-DINO-Tiny outperforms the Grounding-DINO-Tiny\nbaseline. We release all our models to the research community. Codes and\ntrained models are released at\nhttps://github.com/open-mmlab/mmdetection/configs/mm_grounding_dino.\n","authors":["Xiangyu Zhao","Yicheng Chen","Shilin Xu","Xiangtai Li","Xinjiang Wang","Yining Li","Haian Huang"],"pdf_url":"https://arxiv.org/pdf/2401.02361v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2401.02358v1","updated":"2024-01-04T16:58:31Z","published":"2024-01-04T16:58:31Z","title":"A novel method to enhance pneumonia detection via a model-level\n  ensembling of CNN and vision transformer","summary":"  Pneumonia remains a leading cause of morbidity and mortality worldwide. Chest\nX-ray (CXR) imaging is a fundamental diagnostic tool, but traditional analysis\nrelies on time-intensive expert evaluation. Recently, deep learning has shown\nimmense potential for automating pneumonia detection from CXRs. This paper\nexplores applying neural networks to improve CXR-based pneumonia diagnosis. We\ndeveloped a novel model fusing Convolution Neural networks (CNN) and Vision\nTransformer networks via model-level ensembling. Our fusion architecture\ncombines a ResNet34 variant and a Multi-Axis Vision Transformer small model.\nBoth base models are initialized with ImageNet pre-trained weights. The output\nlayers are removed, and features are combined using a flattening layer before\nfinal classification. Experiments used the Kaggle pediatric pneumonia dataset\ncontaining 1,341 normal and 3,875 pneumonia CXR images. We compared our model\nagainst standalone ResNet34, Vision Transformer, and Swin Transformer Tiny\nbaseline models using identical training procedures. Extensive data\naugmentation, Adam optimization, learning rate warmup, and decay were employed.\nThe fusion model achieved a state-of-the-art accuracy of 94.87%, surpassing the\nbaselines. We also attained excellent sensitivity, specificity, kappa score,\nand positive predictive value. Confusion matrix analysis confirms fewer\nmisclassifications. The ResNet34 and Vision Transformer combination enables\njointly learning robust features from CNNs and Transformer paradigms. This\nmodel-level ensemble technique effectively integrates their complementary\nstrengths for enhanced pneumonia classification.\n","authors":["Sandeep Angara","Nishith Reddy Mannuru","Aashrith Mannuru","Sharath Thirunagaru"],"pdf_url":"https://arxiv.org/pdf/2401.02358v1.pdf","comment":"NA"},{"id":"http://arxiv.org/abs/2401.02357v1","updated":"2024-01-04T16:57:56Z","published":"2024-01-04T16:57:56Z","title":"Fit-NGP: Fitting Object Models to Neural Graphics Primitives","summary":"  Accurate 3D object pose estimation is key to enabling many robotic\napplications that involve challenging object interactions. In this work, we\nshow that the density field created by a state-of-the-art efficient radiance\nfield reconstruction method is suitable for highly accurate and robust pose\nestimation for objects with known 3D models, even when they are very small and\nwith challenging reflective surfaces. We present a fully automatic object pose\nestimation system based on a robot arm with a single wrist-mounted camera,\nwhich can scan a scene from scratch, detect and estimate the 6-Degrees of\nFreedom (DoF) poses of multiple objects within a couple of minutes of\noperation. Small objects such as bolts and nuts are estimated with accuracy on\norder of 1mm.\n","authors":["Marwan Taher","Ignacio Alzugaray","Andrew J. Davison"],"pdf_url":"https://arxiv.org/pdf/2401.02357v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05922v3","updated":"2024-01-04T16:52:34Z","published":"2022-12-09T17:34:53Z","title":"Audiovisual Masked Autoencoders","summary":"  Can we leverage the audiovisual information already present in video to\nimprove self-supervised representation learning? To answer this question, we\nstudy various pretraining architectures and objectives within the masked\nautoencoding framework, motivated by the success of similar methods in natural\nlanguage and image understanding. We show that we can achieve significant\nimprovements on audiovisual downstream classification tasks, surpassing the\nstate-of-the-art on VGGSound and AudioSet. Furthermore, we can leverage our\naudiovisual pretraining scheme for multiple unimodal downstream tasks using a\nsingle audiovisual pretrained model. We additionally demonstrate the\ntransferability of our representations, achieving state-of-the-art audiovisual\nresults on Epic Kitchens without pretraining specifically for this dataset.\n","authors":["Mariana-Iuliana Georgescu","Eduardo Fonseca","Radu Tudor Ionescu","Mario Lucic","Cordelia Schmid","Anurag Arnab"],"pdf_url":"https://arxiv.org/pdf/2212.05922v3.pdf","comment":"ICCV 2023"},{"id":"http://arxiv.org/abs/2401.02347v1","updated":"2024-01-04T16:43:46Z","published":"2024-01-04T16:43:46Z","title":"Mining Fine-Grained Image-Text Alignment for Zero-Shot Captioning via\n  Text-Only Training","summary":"  Image captioning aims at generating descriptive and meaningful textual\ndescriptions of images, enabling a broad range of vision-language applications.\nPrior works have demonstrated that harnessing the power of Contrastive Image\nLanguage Pre-training (CLIP) offers a promising approach to achieving zero-shot\ncaptioning, eliminating the need for expensive caption annotations. However,\nthe widely observed modality gap in the latent space of CLIP harms the\nperformance of zero-shot captioning by breaking the alignment between paired\nimage-text features. To address this issue, we conduct an analysis on the CLIP\nlatent space which leads to two findings. Firstly, we observe that the CLIP's\nvisual feature of image subregions can achieve closer proximity to the paired\ncaption due to the inherent information loss in text descriptions. In addition,\nwe show that the modality gap between a paired image-text can be empirically\nmodeled as a zero-mean Gaussian distribution. Motivated by the findings, we\npropose a novel zero-shot image captioning framework with text-only training to\nreduce the modality gap. In particular, we introduce a subregion feature\naggregation to leverage local region information, which produces a compact\nvisual representation for matching text representation. Moreover, we\nincorporate a noise injection and CLIP reranking strategy to boost captioning\nperformance. We also extend our framework to build a zero-shot VQA pipeline,\ndemonstrating its generality. Through extensive experiments on common\ncaptioning and VQA datasets such as MSCOCO, Flickr30k and VQAV2, we show that\nour method achieves remarkable performance improvements. Code is available at\nhttps://github.com/Artanic30/MacCap.\n","authors":["Longtian Qiu","Shan Ning","Xuming He"],"pdf_url":"https://arxiv.org/pdf/2401.02347v1.pdf","comment":"AAAI 2024.Open sourced, Code and Model Available"},{"id":"http://arxiv.org/abs/2401.02335v1","updated":"2024-01-04T16:19:52Z","published":"2024-01-04T16:19:52Z","title":"Linguistic Profiling of Deepfakes: An Open Database for Next-Generation\n  Deepfake Detection","summary":"  The emergence of text-to-image generative models has revolutionized the field\nof deepfakes, enabling the creation of realistic and convincing visual content\ndirectly from textual descriptions. However, this advancement presents\nconsiderably greater challenges in detecting the authenticity of such content.\nExisting deepfake detection datasets and methods often fall short in\neffectively capturing the extensive range of emerging deepfakes and offering\nsatisfactory explanatory information for detection. To address the significant\nissue, this paper introduces a deepfake database (DFLIP-3K) for the development\nof convincing and explainable deepfake detection. It encompasses about 300K\ndiverse deepfake samples from approximately 3K generative models, which boasts\nthe largest number of deepfake models in the literature. Moreover, it collects\naround 190K linguistic footprints of these deepfakes. The two distinguished\nfeatures enable DFLIP-3K to develop a benchmark that promotes progress in\nlinguistic profiling of deepfakes, which includes three sub-tasks namely\ndeepfake detection, model identification, and prompt prediction. The deepfake\nmodel and prompt are two essential components of each deepfake, and thus\ndissecting them linguistically allows for an invaluable exploration of\ntrustworthy and interpretable evidence in deepfake detection, which we believe\nis the key for the next-generation deepfake detection. Furthermore, DFLIP-3K is\nenvisioned as an open database that fosters transparency and encourages\ncollaborative efforts to further enhance its growth. Our extensive experiments\non the developed benchmark verify that our DFLIP-3K database is capable of\nserving as a standardized resource for evaluating and comparing\nlinguistic-based deepfake detection, identification, and prompt prediction\ntechniques.\n","authors":["Yabin Wang","Zhiwu Huang","Zhiheng Ma","Xiaopeng Hong"],"pdf_url":"https://arxiv.org/pdf/2401.02335v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02330v1","updated":"2024-01-04T16:07:43Z","published":"2024-01-04T16:07:43Z","title":"LLaVA-$φ$: Efficient Multi-Modal Assistant with Small Language Model","summary":"  In this paper, we introduce LLaVA-$\\phi$ (LLaVA-Phi), an efficient\nmulti-modal assistant that harnesses the power of the recently advanced small\nlanguage model, Phi-2, to facilitate multi-modal dialogues. LLaVA-Phi marks a\nnotable advancement in the realm of compact multi-modal models. It demonstrates\nthat even smaller language models, with as few as 2.7B parameters, can\neffectively engage in intricate dialogues that integrate both textual and\nvisual elements, provided they are trained with high-quality corpora. Our model\ndelivers commendable performance on publicly available benchmarks that\nencompass visual comprehension, reasoning, and knowledge-based perception.\nBeyond its remarkable performance in multi-modal dialogue tasks, our model\nopens new avenues for applications in time-sensitive environments and systems\nthat require real-time interaction, such as embodied agents. It highlights the\npotential of smaller language models to achieve sophisticated levels of\nunderstanding and interaction, while maintaining greater resource\nefficiency.The project is available at {https://github.com/zhuyiche/llava-phi}.\n","authors":["Yichen Zhu","Minjie Zhu","Ning Liu","Zhicai Ou","Xiaofeng Mou","Jian Tang"],"pdf_url":"https://arxiv.org/pdf/2401.02330v1.pdf","comment":"technique report"},{"id":"http://arxiv.org/abs/2401.02326v1","updated":"2024-01-04T15:54:45Z","published":"2024-01-04T15:54:45Z","title":"ClassWise-SAM-Adapter: Parameter Efficient Fine-tuning Adapts Segment\n  Anything to SAR Domain for Semantic Segmentation","summary":"  In the realm of artificial intelligence, the emergence of foundation models,\nbacked by high computing capabilities and extensive data, has been\nrevolutionary. Segment Anything Model (SAM), built on the Vision Transformer\n(ViT) model with millions of parameters and vast training dataset SA-1B, excels\nin various segmentation scenarios relying on its significance of semantic\ninformation and generalization ability. Such achievement of visual foundation\nmodel stimulates continuous researches on specific downstream tasks in computer\nvision. The ClassWise-SAM-Adapter (CWSAM) is designed to adapt the\nhigh-performing SAM for landcover classification on space-borne Synthetic\nAperture Radar (SAR) images. The proposed CWSAM freezes most of SAM's\nparameters and incorporates lightweight adapters for parameter efficient\nfine-tuning, and a classwise mask decoder is designed to achieve semantic\nsegmentation task. This adapt-tuning method allows for efficient landcover\nclassification of SAR images, balancing the accuracy with computational demand.\nIn addition, the task specific input module injects low frequency information\nof SAR images by MLP-based layers to improve the model performance. Compared to\nconventional state-of-the-art semantic segmentation algorithms by extensive\nexperiments, CWSAM showcases enhanced performance with fewer computing\nresources, highlighting the potential of leveraging foundational models like\nSAM for specific downstream tasks in the SAR domain. The source code is\navailable at: https://github.com/xypu98/CWSAM.\n","authors":["Xinyang Pu","Hecheng Jia","Linghao Zheng","Feng Wang","Feng Xu"],"pdf_url":"https://arxiv.org/pdf/2401.02326v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.03842v3","updated":"2024-01-04T15:50:25Z","published":"2022-04-08T05:11:04Z","title":"From 2D Images to 3D Model:Weakly Supervised Multi-View Face\n  Reconstruction with Deep Fusion","summary":"  While weakly supervised multi-view face reconstruction (MVR) is garnering\nincreased attention, one critical issue still remains open: how to effectively\nfuse multiple image information to reconstruct high-precision 3D models. In\nthis regard, we propose a novel model called Deep Fusion MVR (DF-MVR) and\ndesign a multi-view encoding to single decoding framework with skip\nconnections, able to extract, integrate, and compensate deep features with\nattention from multi-view images. Furthermore, we adopt the involution kernel\nto enrich deep fusion features with channel features. In addition, we develop\nthe face parse network to learn, identify, and emphasize the critical common\nface area within multi-view images. Experiments on Pixel-Face and Bosphorus\ndatasets indicate the superiority of our model. Without 3D annotation, DF-MVR\nachieves 5.2% and 3.0% RMSE improvement over the existing weakly supervised\nMVRs respectively on Pixel-Face and Bosphorus dataset. Code will be available\npublicly at https://github.com/weiguangzhao/DF_MVR.\n","authors":["Weiguang Zhao","Chaolong Yang","Jianan Ye","Rui Zhang","Yuyao Yan","Xi Yang","Bin Dong","Amir Hussain","Kaizhu Huang"],"pdf_url":"https://arxiv.org/pdf/2204.03842v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02317v1","updated":"2024-01-04T15:34:44Z","published":"2024-01-04T15:34:44Z","title":"BA-SAM: Scalable Bias-Mode Attention Mask for Segment Anything Model","summary":"  In this paper, we address the challenge of image resolution variation for the\nSegment Anything Model (SAM). SAM, known for its zero-shot generalizability,\nexhibits a performance degradation when faced with datasets with varying image\nsizes. Previous approaches tend to resize the image to a fixed size or adopt\nstructure modifications, hindering the preservation of SAM's rich prior\nknowledge. Besides, such task-specific tuning necessitates a complete\nretraining of the model, which is cost-expensive and unacceptable for\ndeployment in the downstream tasks. In this paper, we reformulate this issue as\na length extrapolation problem, where token sequence length varies while\nmaintaining a consistent patch size for images of different sizes. To this end,\nwe propose Scalable Bias-Mode Attention Mask (BA-SAM) to enhance SAM's\nadaptability to varying image resolutions while eliminating the need for\nstructure modifications. Firstly, we introduce a new scaling factor to ensure\nconsistent magnitude in the attention layer's dot product values when the token\nsequence length changes. Secondly, we present a bias-mode attention mask that\nallows each token to prioritize neighboring information, mitigating the impact\nof untrained distant information. Our BA-SAM demonstrates efficacy in two\nscenarios: zero-shot and fine-tuning. Extensive evaluation on diverse datasets,\nincluding DIS5K, DUTS, ISIC, COD10K, and COCO, reveals its ability to\nsignificantly mitigate performance degradation in the zero-shot setting and\nachieve state-of-the-art performance with minimal fine-tuning. Furthermore, we\npropose a generalized model and benchmark, showcasing BA-SAM's generalizability\nacross all four datasets simultaneously.\n","authors":["Yiran Song","Qianyu Zhou","Xiangtai Li","Deng-Ping Fan","Xuequan Lu","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2401.02317v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02313v1","updated":"2024-01-04T15:21:53Z","published":"2024-01-04T15:21:53Z","title":"SuperEdge: Towards a Generalization Model for Self-Supervised Edge\n  Detection","summary":"  Edge detection is a fundamental technique in various computer vision tasks.\nEdges are indeed effectively delineated by pixel discontinuity and can offer\nreliable structural information even in textureless areas. State-of-the-art\nheavily relies on pixel-wise annotations, which are labor-intensive and subject\nto inconsistencies when acquired manually. In this work, we propose a novel\nself-supervised approach for edge detection that employs a multi-level,\nmulti-homography technique to transfer annotations from synthetic to real-world\ndatasets. To fully leverage the generated edge annotations, we developed\nSuperEdge, a streamlined yet efficient model capable of concurrently extracting\nedges at pixel-level and object-level granularity. Thanks to self-supervised\ntraining, our method eliminates the dependency on manual annotated edge labels,\nthereby enhancing its generalizability across diverse datasets. Comparative\nevaluations reveal that SuperEdge advances edge detection, demonstrating\nimprovements of 4.9% in ODS and 3.3% in OIS over the existing STEdge method on\nBIPEDv2.\n","authors":["Leng Kai","Zhang Zhijie","Liu Jie","Zed Boukhers","Sui Wei","Cong Yang","Li Zhijun"],"pdf_url":"https://arxiv.org/pdf/2401.02313v1.pdf","comment":"7pages"},{"id":"http://arxiv.org/abs/2305.17033v4","updated":"2024-01-04T15:10:34Z","published":"2023-05-26T15:40:11Z","title":"The Brain Tumor Segmentation (BraTS) Challenge 2023: Focus on Pediatrics\n  (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs)","summary":"  Pediatric tumors of the central nervous system are the most common cause of\ncancer-related death in children. The five-year survival rate for high-grade\ngliomas in children is less than 20\\%. Due to their rarity, the diagnosis of\nthese entities is often delayed, their treatment is mainly based on historic\ntreatment concepts, and clinical trials require multi-institutional\ncollaborations. The MICCAI Brain Tumor Segmentation (BraTS) Challenge is a\nlandmark community benchmark event with a successful history of 12 years of\nresource creation for the segmentation and analysis of adult glioma. Here we\npresent the CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023 challenge, which\nrepresents the first BraTS challenge focused on pediatric brain tumors with\ndata acquired across multiple international consortia dedicated to pediatric\nneuro-oncology and clinical trials. The BraTS-PEDs 2023 challenge focuses on\nbenchmarking the development of volumentric segmentation algorithms for\npediatric brain glioma through standardized quantitative performance evaluation\nmetrics utilized across the BraTS 2023 cluster of challenges. Models gaining\nknowledge from the BraTS-PEDs multi-parametric structural MRI (mpMRI) training\ndata will be evaluated on separate validation and unseen test mpMRI dataof\nhigh-grade pediatric glioma. The CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023\nchallenge brings together clinicians and AI/imaging scientists to lead to\nfaster development of automated segmentation techniques that could benefit\nclinical trials, and ultimately the care of children with brain tumors.\n","authors":["Anahita Fathi Kazerooni","Nastaran Khalili","Xinyang Liu","Debanjan Haldar","Zhifan Jiang","Syed Muhammed Anwar","Jake Albrecht","Maruf Adewole","Udunna Anazodo","Hannah Anderson","Sina Bagheri","Ujjwal Baid","Timothy Bergquist","Austin J. Borja","Evan Calabrese","Verena Chung","Gian-Marco Conte","Farouk Dako","James Eddy","Ivan Ezhov","Ariana Familiar","Keyvan Farahani","Shuvanjan Haldar","Juan Eugenio Iglesias","Anastasia Janas","Elaine Johansen","Blaise V Jones","Florian Kofler","Dominic LaBella","Hollie Anne Lai","Koen Van Leemput","Hongwei Bran Li","Nazanin Maleki","Aaron S McAllister","Zeke Meier","Bjoern Menze","Ahmed W Moawad","Khanak K Nandolia","Julija Pavaine","Marie Piraud","Tina Poussaint","Sanjay P Prabhu","Zachary Reitman","Andres Rodriguez","Jeffrey D Rudie","Ibraheem Salman Shaikh","Lubdha M. Shah","Nakul Sheth","Russel Taki Shinohara","Wenxin Tu","Karthik Viswanathan","Chunhao Wang","Jeffrey B Ware","Benedikt Wiestler","Walter Wiggins","Anna Zapaishchykova","Mariam Aboian","Miriam Bornhorst","Peter de Blank","Michelle Deutsch","Maryam Fouladi","Lindsey Hoffman","Benjamin Kann","Margot Lazow","Leonie Mikael","Ali Nabavizadeh","Roger Packer","Adam Resnick","Brian Rood","Arastoo Vossough","Spyridon Bakas","Marius George Linguraru"],"pdf_url":"https://arxiv.org/pdf/2305.17033v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.02535v3","updated":"2024-01-04T15:06:19Z","published":"2023-08-01T10:02:26Z","title":"Learning to Generate Training Datasets for Robust Semantic Segmentation","summary":"  Semantic segmentation methods have advanced significantly. Still, their\nrobustness to real-world perturbations and object types not seen during\ntraining remains a challenge, particularly in safety-critical applications. We\npropose a novel approach to improve the robustness of semantic segmentation\ntechniques by leveraging the synergy between label-to-image generators and\nimage-to-label segmentation models. Specifically, we design Robusta, a novel\nrobust conditional generative adversarial network to generate realistic and\nplausible perturbed images that can be used to train reliable segmentation\nmodels. We conduct in-depth studies of the proposed generative model, assess\nthe performance and robustness of the downstream segmentation network, and\ndemonstrate that our approach can significantly enhance the robustness in the\nface of real-world perturbations, distribution shifts, and out-of-distribution\nsamples. Our results suggest that this approach could be valuable in\nsafety-critical applications, where the reliability of perception modules such\nas semantic segmentation is of utmost importance and comes with a limited\ncomputational budget in inference. We release our code at\nhttps://github.com/ENSTA-U2IS/robusta.\n","authors":["Marwane Hariat","Olivier Laurent","Rémi Kazmierczak","Shihao Zhang","Andrei Bursuc","Angela Yao","Gianni Franchi"],"pdf_url":"https://arxiv.org/pdf/2308.02535v3.pdf","comment":"Published as a conference paper at WACV 2024"},{"id":"http://arxiv.org/abs/2301.13656v2","updated":"2024-01-04T14:58:13Z","published":"2023-01-31T14:18:19Z","title":"A Survey and Benchmark of Automatic Surface Reconstruction from Point\n  Clouds","summary":"  We present a comprehensive survey and benchmark of both traditional and\nlearning-based methods for surface reconstruction from point clouds. This task\nis particularly challenging for real-world acquisitions due to factors like\nnoise, outliers, non-uniform sampling, and missing data. Traditional approaches\noften simplify the problem by imposing handcrafted priors on either the input\npoint clouds or the resulting surface, a process that can necessitate tedious\nhyperparameter tuning. Conversely, deep learning models have the capability to\ndirectly learn the properties of input point clouds and desired surfaces from\ndata. We study the influence of these handcrafted and learned priors on the\nprecision and robustness of surface reconstruction techniques. We evaluate\nvarious time-tested and contemporary methods in a standardized manner. When\nboth trained and evaluated on point clouds with identical characteristics, the\nlearning-based models consistently produce superior surfaces compared to their\ntraditional counterparts$\\unicode{x2013}$even in scenarios involving novel\nshape categories. However, traditional methods demonstrate greater resilience\nto the diverse array of point cloud anomalies commonly found in real-world 3D\nacquisitions. For the benefit of the research community, we make our code and\ndatasets available, inviting further enhancements to learning-based surface\nreconstruction. This can be accessed at\nhttps://github.com/raphaelsulzer/dsr-benchmark .\n","authors":["Raphael Sulzer","Renaud Marlet","Bruno Vallet","Loic Landrieu"],"pdf_url":"https://arxiv.org/pdf/2301.13656v2.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2401.02309v1","updated":"2024-01-04T14:55:57Z","published":"2024-01-04T14:55:57Z","title":"TR-DETR: Task-Reciprocal Transformer for Joint Moment Retrieval and\n  Highlight Detection","summary":"  Video moment retrieval (MR) and highlight detection (HD) based on natural\nlanguage queries are two highly related tasks, which aim to obtain relevant\nmoments within videos and highlight scores of each video clip. Recently,\nseveral methods have been devoted to building DETR-based networks to solve both\nMR and HD jointly. These methods simply add two separate task heads after\nmulti-modal feature extraction and feature interaction, achieving good\nperformance. Nevertheless, these approaches underutilize the reciprocal\nrelationship between two tasks. In this paper, we propose a task-reciprocal\ntransformer based on DETR (TR-DETR) that focuses on exploring the inherent\nreciprocity between MR and HD. Specifically, a local-global multi-modal\nalignment module is first built to align features from diverse modalities into\na shared latent space. Subsequently, a visual feature refinement is designed to\neliminate query-irrelevant information from visual features for modal\ninteraction. Finally, a task cooperation module is constructed to refine the\nretrieval pipeline and the highlight score prediction process by utilizing the\nreciprocity between MR and HD. Comprehensive experiments on QVHighlights,\nCharades-STA and TVSum datasets demonstrate that TR-DETR outperforms existing\nstate-of-the-art methods. Codes are available at\n\\url{https://github.com/mingyao1120/TR-DETR}.\n","authors":["Hao Sun","Mingyao Zhou","Wenjing Chen","Wei Xie"],"pdf_url":"https://arxiv.org/pdf/2401.02309v1.pdf","comment":"Accepted by AAAI-24"},{"id":"http://arxiv.org/abs/2401.02292v1","updated":"2024-01-04T14:31:56Z","published":"2024-01-04T14:31:56Z","title":"GridFormer: Point-Grid Transformer for Surface Reconstruction","summary":"  Implicit neural networks have emerged as a crucial technology in 3D surface\nreconstruction. To reconstruct continuous surfaces from discrete point clouds,\nencoding the input points into regular grid features (plane or volume) has been\ncommonly employed in existing approaches. However, these methods typically use\nthe grid as an index for uniformly scattering point features. Compared with the\nirregular point features, the regular grid features may sacrifice some\nreconstruction details but improve efficiency. To take full advantage of these\ntwo types of features, we introduce a novel and high-efficiency attention\nmechanism between the grid and point features named Point-Grid Transformer\n(GridFormer). This mechanism treats the grid as a transfer point connecting the\nspace and point cloud. Our method maximizes the spatial expressiveness of grid\nfeatures and maintains computational efficiency. Furthermore, optimizing\npredictions over the entire space could potentially result in blurred\nboundaries. To address this issue, we further propose a boundary optimization\nstrategy incorporating margin binary cross-entropy loss and boundary sampling.\nThis approach enables us to achieve a more precise representation of the object\nstructure. Our experiments validate that our method is effective and\noutperforms the state-of-the-art approaches under widely used benchmarks by\nproducing more precise geometry reconstructions. The code is available at\nhttps://github.com/list17/GridFormer.\n","authors":["Shengtao Li","Ge Gao","Yudong Liu","Yu-Shen Liu","Ming Gu"],"pdf_url":"https://arxiv.org/pdf/2401.02292v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01822v2","updated":"2024-01-04T14:28:02Z","published":"2024-01-03T16:38:56Z","title":"HawkRover: An Autonomous mmWave Vehicular Communication Testbed with\n  Multi-sensor Fusion and Deep Learning","summary":"  Connected and automated vehicles (CAVs) have become a transformative\ntechnology that can change our daily life. Currently, millimeter-wave (mmWave)\nbands are identified as the promising CAV connectivity solution. While it can\nprovide high data rate, their realization faces many challenges such as high\nattenuation during mmWave signal propagation and mobility management. Existing\nsolution has to initiate pilot signal to measure channel information, then\napply signal processing to calculate the best narrow beam towards the receiver\nend to guarantee sufficient signal power. This process takes significant\noverhead and time, hence not suitable for vehicles. In this study, we propose\nan autonomous and low-cost testbed to collect extensive co-located mmWave\nsignal and other sensors data such as LiDAR (Light Detection and Ranging),\ncameras, ultrasonic, etc, traditionally for ``automated'', to facilitate mmWave\nvehicular communications. Intuitively, these sensors can build a 3D map around\nthe vehicle and signal propagation path can be estimated, eliminating iterative\nthe process via pilot signals. This multimodal data fusion, together with AI,\nis expected to bring significant advances in ``connected'' research.\n","authors":["Ethan Zhu","Haijian Sun","Mingyue Ji"],"pdf_url":"https://arxiv.org/pdf/2401.01822v2.pdf","comment":"submitted to IEEE conferences for future publications"},{"id":"http://arxiv.org/abs/2401.02287v1","updated":"2024-01-04T14:10:38Z","published":"2024-01-04T14:10:38Z","title":"Distillation-based fabric anomaly detection","summary":"  Unsupervised texture anomaly detection has been a concerning topic in a vast\namount of industrial processes. Patterned textures inspection, particularly in\nthe context of fabric defect detection, is indeed a widely encountered use\ncase. This task involves handling a diverse spectrum of colors and textile\ntypes, encompassing a wide range of fabrics. Given the extensive variability in\ncolors, textures, and defect types, fabric defect detection poses a complex and\nchallenging problem in the field of patterned textures inspection. In this\narticle, we propose a knowledge distillation-based approach tailored\nspecifically for addressing the challenge of unsupervised anomaly detection in\ntextures resembling fabrics. Our method aims to redefine the recently\nintroduced reverse distillation approach, which advocates for an\nencoder-decoder design to mitigate classifier bias and to prevent the student\nfrom reconstructing anomalies. In this study, we present a new reverse\ndistillation technique for the specific task of fabric defect detection. Our\napproach involves a meticulous design selection that strategically highlights\nhigh-level features. To demonstrate the capabilities of our approach both in\nterms of performance and inference speed, we conducted a series of experiments\non multiple texture datasets, including MVTEC AD, AITEX, and TILDA, alongside\nconducting experiments on a dataset acquired from a textile manufacturing\nfacility. The main contributions of this paper are the following: a robust\ntexture anomaly detector utilizing a reverse knowledge-distillation technique\nsuitable for both anomaly detection and domain generalization and a novel\ndataset encompassing a diverse range of fabrics and defects.\n","authors":["Simon Thomine","Hichem Snoussi"],"pdf_url":"https://arxiv.org/pdf/2401.02287v1.pdf","comment":"Textile Research Journal. 2023;0(0)"},{"id":"http://arxiv.org/abs/2306.07716v3","updated":"2024-01-04T13:58:50Z","published":"2023-06-13T12:07:01Z","title":"Dynamically Masked Discriminator for Generative Adversarial Networks","summary":"  Training Generative Adversarial Networks (GANs) remains a challenging\nproblem. The discriminator trains the generator by learning the distribution of\nreal/generated data. However, the distribution of generated data changes\nthroughout the training process, which is difficult for the discriminator to\nlearn. In this paper, we propose a novel method for GANs from the viewpoint of\nonline continual learning. We observe that the discriminator model, trained on\nhistorically generated data, often slows down its adaptation to the changes in\nthe new arrival generated data, which accordingly decreases the quality of\ngenerated results. By treating the generated data in training as a stream, we\npropose to detect whether the discriminator slows down the learning of new\nknowledge in generated data. Therefore, we can explicitly enforce the\ndiscriminator to learn new knowledge fast. Particularly, we propose a new\ndiscriminator, which automatically detects its retardation and then dynamically\nmasks its features, such that the discriminator can adaptively learn the\ntemporally-vary distribution of generated data. Experimental results show our\nmethod outperforms the state-of-the-art approaches.\n","authors":["Wentian Zhang","Haozhe Liu","Bing Li","Jinheng Xie","Yawen Huang","Yuexiang Li","Yefeng Zheng","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2306.07716v3.pdf","comment":"Updated v2 -- NeurIPS 2023 camera ready version"},{"id":"http://arxiv.org/abs/2401.02281v1","updated":"2024-01-04T13:58:14Z","published":"2024-01-04T13:58:14Z","title":"PEGASUS: Physically Enhanced Gaussian Splatting Simulation System for\n  6DOF Object Pose Dataset Generation","summary":"  We introduce Physically Enhanced Gaussian Splatting Simulation System\n(PEGASUS) for 6DOF object pose dataset generation, a versatile dataset\ngenerator based on 3D Gaussian Splatting. Environment and object\nrepresentations can be easily obtained using commodity cameras to reconstruct\nwith Gaussian Splatting. PEGASUS allows the composition of new scenes by\nmerging the respective underlying Gaussian Splatting point cloud of an\nenvironment with one or multiple objects. Leveraging a physics engine enables\nthe simulation of natural object placement within a scene through interaction\nbetween meshes extracted for the objects and the environment. Consequently, an\nextensive amount of new scenes - static or dynamic - can be created by\ncombining different environments and objects. By rendering scenes from various\nperspectives, diverse data points such as RGB images, depth maps, semantic\nmasks, and 6DoF object poses can be extracted. Our study demonstrates that\ntraining on data generated by PEGASUS enables pose estimation networks to\nsuccessfully transfer from synthetic data to real-world data. Moreover, we\nintroduce the Ramen dataset, comprising 30 Japanese cup noodle items. This\ndataset includes spherical scans that captures images from both object\nhemisphere and the Gaussian Splatting reconstruction, making them compatible\nwith PEGASUS.\n","authors":["Lukas Meyer","Floris Erich","Yusuke Yoshiyasu","Marc Stamminger","Noriaki Ando","Yukiyasu Domae"],"pdf_url":"https://arxiv.org/pdf/2401.02281v1.pdf","comment":"Project Page: https://meyerls.github.io/pegasus_web"},{"id":"http://arxiv.org/abs/2401.02278v1","updated":"2024-01-04T13:56:54Z","published":"2024-01-04T13:56:54Z","title":"Lightweight Fish Classification Model for Sustainable Marine Management:\n  Indonesian Case","summary":"  The enormous demand for seafood products has led to exploitation of marine\nresources and near-extinction of some species. In particular, overfishing is\none the main issues in sustainable marine development. In alignment with the\nprotection of marine resources and sustainable fishing, this study proposes to\nadvance fish classification techniques that support identifying protected fish\nspecies using state-of-the-art machine learning. We use a custom modification\nof the MobileNet model to design a lightweight classifier called M-MobileNet\nthat is capable of running on limited hardware. As part of the study, we\ncompiled a labeled dataset of 37,462 images of fish found in the waters of the\nIndonesian archipelago. The proposed model is trained on the dataset to\nclassify images of the captured fish into their species and give\nrecommendations on whether they are consumable or not. Our modified MobileNet\nmodel uses only 50\\% of the top layer parameters with about 42% GTX 860M\nutility and achieves up to 97% accuracy in fish classification and determining\nits consumability. Given the limited computing capacity available on many\nfishing vessels, the proposed model provides a practical solution to on-site\nfish classification. In addition, synchronized implementation of the proposed\nmodel on multiple vessels can supply valuable information about the movement\nand location of different species of fish.\n","authors":["Febrian Kurniawan","Gandeva Bayu Satrya","Firuz Kamalov"],"pdf_url":"https://arxiv.org/pdf/2401.02278v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02274v1","updated":"2024-01-04T13:49:45Z","published":"2024-01-04T13:49:45Z","title":"ShapeAug: Occlusion Augmentation for Event Camera Data","summary":"  Recently, Dynamic Vision Sensors (DVSs) sparked a lot of interest due to\ntheir inherent advantages over conventional RGB cameras. These advantages\ninclude a low latency, a high dynamic range and a low energy consumption.\nNevertheless, the processing of DVS data using Deep Learning (DL) methods\nremains a challenge, particularly since the availability of event training data\nis still limited. This leads to a need for event data augmentation techniques\nin order to improve accuracy as well as to avoid over-fitting on the training\ndata. Another challenge especially in real world automotive applications is\nocclusion, meaning one object is hindering the view onto the object behind it.\nIn this paper, we present a novel event data augmentation approach, which\naddresses this problem by introducing synthetic events for randomly moving\nobjects in a scene. We test our method on multiple DVS classification datasets,\nresulting in an relative improvement of up to 6.5 % in top1-accuracy. Moreover,\nwe apply our augmentation technique on the real world Gen1 Automotive Event\nDataset for object detection, where we especially improve the detection of\npedestrians by up to 5 %.\n","authors":["Katharina Bendig","René Schuster","Didier Stricker"],"pdf_url":"https://arxiv.org/pdf/2401.02274v1.pdf","comment":"Accepted at ICPRAM 2024"},{"id":"http://arxiv.org/abs/2401.01736v2","updated":"2024-01-04T13:24:48Z","published":"2024-01-03T13:19:14Z","title":"Few-shot Adaptation of Multi-modal Foundation Models: A Survey","summary":"  Multi-modal (vision-language) models, such as CLIP, are replacing traditional\nsupervised pre-training models (e.g., ImageNet-based pre-training) as the new\ngeneration of visual foundation models. These models with robust and aligned\nsemantic representations learned from billions of internet image-text pairs and\ncan be applied to various downstream tasks in a zero-shot manner. However, in\nsome fine-grained domains like medical imaging and remote sensing, the\nperformance of multi-modal foundation models often leaves much to be desired.\nConsequently, many researchers have begun to explore few-shot adaptation\nmethods for these models, gradually deriving three main technical approaches:\n1) prompt-based methods, 2) adapter-based methods, and 3) external\nknowledge-based methods. Nevertheless, this rapidly developing field has\nproduced numerous results without a comprehensive survey to systematically\norganize the research progress. Therefore, in this survey, we introduce and\nanalyze the research advancements in few-shot adaptation methods for\nmulti-modal models, summarizing commonly used datasets and experimental setups,\nand comparing the results of different methods. In addition, due to the lack of\nreliable theoretical support for existing methods, we derive the few-shot\nadaptation generalization error bound for multi-modal models. The theorem\nreveals that the generalization error of multi-modal foundation models is\nconstrained by three factors: domain gap, model capacity, and sample size.\nBased on this, we propose three possible solutions from the following aspects:\n1) adaptive domain generalization, 2) adaptive model selection, and 3) adaptive\nknowledge utilization.\n","authors":["Fan Liu","Tianshu Zhang","Wenwen Dai","Wenwen Cai","Xiaocong Zhou","Delong Chen"],"pdf_url":"https://arxiv.org/pdf/2401.01736v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02241v1","updated":"2024-01-04T12:52:48Z","published":"2024-01-04T12:52:48Z","title":"Slot-guided Volumetric Object Radiance Fields","summary":"  We present a novel framework for 3D object-centric representation learning.\nOur approach effectively decomposes complex scenes into individual objects from\na single image in an unsupervised fashion. This method, called slot-guided\nVolumetric Object Radiance Fields (sVORF), composes volumetric object radiance\nfields with object slots as a guidance to implement unsupervised 3D scene\ndecomposition. Specifically, sVORF obtains object slots from a single image via\na transformer module, maps these slots to volumetric object radiance fields\nwith a hypernetwork and composes object radiance fields with the guidance of\nobject slots at a 3D location. Moreover, sVORF significantly reduces memory\nrequirement due to small-sized pixel rendering during training. We demonstrate\nthe effectiveness of our approach by showing top results in scene decomposition\nand generation tasks of complex synthetic datasets (e.g., Room-Diverse).\nFurthermore, we also confirm the potential of sVORF to segment objects in\nreal-world scenes (e.g., the LLFF dataset). We hope our approach can provide\npreliminary understanding of the physical world and help ease future research\nin 3D object-centric representation learning.\n","authors":["Di Qi","Tong Yang","Xiangyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.02241v1.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2301.02008v2","updated":"2024-01-04T12:20:15Z","published":"2023-01-05T11:17:19Z","title":"Expressive Speech-driven Facial Animation with controllable emotions","summary":"  It is in high demand to generate facial animation with high realism, but it\nremains a challenging task. Existing approaches of speech-driven facial\nanimation can produce satisfactory mouth movement and lip synchronization, but\nshow weakness in dramatic emotional expressions and flexibility in emotion\ncontrol. This paper presents a novel deep learning-based approach for\nexpressive facial animation generation from speech that can exhibit\nwide-spectrum facial expressions with controllable emotion type and intensity.\nWe propose an emotion controller module to learn the relationship between the\nemotion variations (e.g., types and intensity) and the corresponding facial\nexpression parameters. It enables emotion-controllable facial animation, where\nthe target expression can be continuously adjusted as desired. The qualitative\nand quantitative evaluations show that the animation generated by our method is\nrich in facial emotional expressiveness while retaining accurate lip movement,\noutperforming other state-of-the-art methods.\n","authors":["Yutong Chen","Junhong Zhao","Wei-Qiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2301.02008v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.04967v2","updated":"2024-01-04T11:42:20Z","published":"2023-09-10T09:00:28Z","title":"Towards Fully Decoupled End-to-End Person Search","summary":"  End-to-end person search aims to jointly detect and re-identify a target\nperson in raw scene images with a unified model. The detection task unifies all\npersons while the re-id task discriminates different identities, resulting in\nconflict optimal objectives. Existing works proposed to decouple end-to-end\nperson search to alleviate such conflict. Yet these methods are still\nsub-optimal on one or two of the sub-tasks due to their partially decoupled\nmodels, which limits the overall person search performance. In this paper, we\npropose to fully decouple person search towards optimal person search. A\ntask-incremental person search network is proposed to incrementally construct\nan end-to-end model for the detection and re-id sub-task, which decouples the\nmodel architecture for the two sub-tasks. The proposed task-incremental network\nallows task-incremental training for the two conflicting tasks. This enables\nindependent learning for different objectives thus fully decoupled the model\nfor persons earch. Comprehensive experimental evaluations demonstrate the\neffectiveness of the proposed fully decoupled models for end-to-end person\nsearch.\n","authors":["Pengcheng Zhang","Xiao Bai","Jin Zheng","Xin Ning"],"pdf_url":"https://arxiv.org/pdf/2309.04967v2.pdf","comment":"DICTA 2023 Best Student Paper"},{"id":"http://arxiv.org/abs/2401.02192v1","updated":"2024-01-04T10:54:05Z","published":"2024-01-04T10:54:05Z","title":"Nodule detection and generation on chest X-rays: NODE21 Challenge","summary":"  Pulmonary nodules may be an early manifestation of lung cancer, the leading\ncause of cancer-related deaths among both men and women. Numerous studies have\nestablished that deep learning methods can yield high-performance levels in the\ndetection of lung nodules in chest X-rays. However, the lack of gold-standard\npublic datasets slows down the progression of the research and prevents\nbenchmarking of methods for this task. To address this, we organized a public\nresearch challenge, NODE21, aimed at the detection and generation of lung\nnodules in chest X-rays. While the detection track assesses state-of-the-art\nnodule detection systems, the generation track determines the utility of nodule\ngeneration algorithms to augment training data and hence improve the\nperformance of the detection systems. This paper summarizes the results of the\nNODE21 challenge and performs extensive additional experiments to examine the\nimpact of the synthetically generated nodule training images on the detection\nalgorithm performance.\n","authors":["Ecem Sogancioglu","Bram van Ginneken","Finn Behrendt","Marcel Bengs","Alexander Schlaefer","Miron Radu","Di Xu","Ke Sheng","Fabien Scalzo","Eric Marcus","Samuele Papa","Jonas Teuwen","Ernst Th. Scholten","Steven Schalekamp","Nils Hendrix","Colin Jacobs","Ward Hendrix","Clara I Sánchez","Keelin Murphy"],"pdf_url":"https://arxiv.org/pdf/2401.02192v1.pdf","comment":"15 pages, 5 figures"},{"id":"http://arxiv.org/abs/2401.02173v1","updated":"2024-01-04T09:55:15Z","published":"2024-01-04T09:55:15Z","title":"Prompt Decoupling for Text-to-Image Person Re-identification","summary":"  Text-to-image person re-identification (TIReID) aims to retrieve the target\nperson from an image gallery via a textual description query. Recently,\npre-trained vision-language models like CLIP have attracted significant\nattention and have been widely utilized for this task due to their robust\ncapacity for semantic concept learning and rich multi-modal knowledge. However,\nrecent CLIP-based TIReID methods commonly rely on direct fine-tuning of the\nentire network to adapt the CLIP model for the TIReID task. Although these\nmethods show competitive performance on this topic, they are suboptimal as they\nnecessitate simultaneous domain adaptation and task adaptation. To address this\nissue, we attempt to decouple these two processes during the training stage.\nSpecifically, we introduce the prompt tuning strategy to enable domain\nadaptation and propose a two-stage training approach to disentangle domain\nadaptation from task adaptation. In the first stage, we freeze the two encoders\nfrom CLIP and solely focus on optimizing the prompts to alleviate domain gap\nbetween the original training data of CLIP and downstream tasks. In the second\nstage, we maintain the fixed prompts and fine-tune the CLIP model to prioritize\ncapturing fine-grained information, which is more suitable for TIReID task.\nFinally, we evaluate the effectiveness of our method on three widely used\ndatasets. Compared to the directly fine-tuned approach, our method achieves\nsignificant improvements.\n","authors":["Weihao Li","Lei Tan","Pingyang Dai","Yan Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.02173v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09024v2","updated":"2024-01-04T09:54:46Z","published":"2023-11-15T15:14:16Z","title":"Fast Certification of Vision-Language Models Using Incremental\n  Randomized Smoothing","summary":"  A key benefit of deep vision-language models such as CLIP is that they enable\nzero-shot open vocabulary classification; the user has the ability to define\nnovel class labels via natural language prompts at inference time. However,\nwhile CLIP-based zero-shot classifiers have demonstrated competitive\nperformance across a range of domain shifts, they remain highly vulnerable to\nadversarial attacks. Therefore, ensuring the robustness of such models is\ncrucial for their reliable deployment in the wild.\n  In this work, we introduce Open Vocabulary Certification (OVC), a fast\ncertification method designed for open-vocabulary models like CLIP via\nrandomized smoothing techniques. Given a base \"training\" set of prompts and\ntheir corresponding certified CLIP classifiers, OVC relies on the observation\nthat a classifier with a novel prompt can be viewed as a perturbed version of\nnearby classifiers in the base training set. Therefore, OVC can rapidly certify\nthe novel classifier using a variation of incremental randomized smoothing. By\nusing a caching trick, we achieve approximately two orders of magnitude\nacceleration in the certification process for novel prompts. To achieve further\n(heuristic) speedups, OVC approximates the embedding space at a given input\nusing a multivariate normal distribution bypassing the need for sampling via\nforward passes through the vision backbone. We demonstrate the effectiveness of\nOVC on through experimental evaluation using multiple vision-language backbones\non the CIFAR-10 and ImageNet test datasets.\n","authors":["A K Nirala","A Joshi","C Hegde","S Sarkar"],"pdf_url":"https://arxiv.org/pdf/2311.09024v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00440v2","updated":"2024-01-04T09:43:33Z","published":"2023-12-31T09:38:53Z","title":"TSGAN: An Optical-to-SAR Dual Conditional GAN for Optical based SAR\n  Temporal Shifting","summary":"  In contrast to the well-investigated field of SAR-to-Optical translation,\nthis study explores the lesser-investigated domain of Optical-to-SAR\ntranslation, a challenging field due to the ill-posed nature of this\ntranslation. The complexity arises as a single optical data can have multiple\nSAR representations based on the SAR viewing geometry. We propose a novel\napproach, termed SAR Temporal Shifting, which inputs an optical data from the\ndesired timestamp along with a SAR data from a different temporal point but\nwith a consistent viewing geometry as the expected SAR data, both complemented\nwith a change map of optical data during the intervening period. This model\nmodifies the SAR data based on the changes observed in optical data to generate\nthe SAR data for the desired timestamp. Our model, a dual conditional\nGenerative Adversarial Network (GAN), named Temporal Shifting GAN (TSGAN),\nincorporates a siamese encoder in both the Generator and the Discriminator. To\nprevent the model from overfitting on the input SAR data, we employed a change\nweighted loss function. Our approach surpasses traditional translation methods\nby eliminating the GAN's fiction phenomenon, particularly in unchanged regions,\nresulting in higher SSIM and PSNR in these areas. Additionally, modifications\nto the Pix2Pix architecture and the inclusion of attention mechanisms have\nenhanced the model's performance on all regions of the data. This research\npaves the way for leveraging legacy optical datasets, the most abundant and\nlongstanding source of Earth imagery data, extending their use to SAR domains\nand temporal analyses. To foster further research, we provide the code,\ndatasets used in our study, and a framework for generating paired SAR-Optical\ndatasets for new regions of interest. These resources are available on\ngithub.com/moienr/TemporalGAN\n","authors":["Moien Rangzan","Sara Attarchi","Richard Gloaguen","Seyed Kazem Alavipanah"],"pdf_url":"https://arxiv.org/pdf/2401.00440v2.pdf","comment":"Comments: Added acknowledgments and corrected a typo. No changes to\n  the main content"},{"id":"http://arxiv.org/abs/2312.12789v2","updated":"2024-01-04T09:34:08Z","published":"2023-12-20T06:22:21Z","title":"SLP-Net:An efficient lightweight network for segmentation of skin\n  lesions","summary":"  Prompt treatment for melanoma is crucial. To assist physicians in identifying\nlesion areas precisely in a quick manner, we propose a novel skin lesion\nsegmentation technique namely SLP-Net, an ultra-lightweight segmentation\nnetwork based on the spiking neural P(SNP) systems type mechanism. Most\nexisting convolutional neural networks achieve high segmentation accuracy while\nneglecting the high hardware cost. SLP-Net, on the contrary, has a very small\nnumber of parameters and a high computation speed. We design a lightweight\nmulti-scale feature extractor without the usual encoder-decoder structure.\nRather than a decoder, a feature adaptation module is designed to replace it\nand implement multi-scale information decoding. Experiments at the ISIC2018\nchallenge demonstrate that the proposed model has the highest Acc and DSC among\nthe state-of-the-art methods, while experiments on the PH2 dataset also\ndemonstrate a favorable generalization ability. Finally, we compare the\ncomputational complexity as well as the computational speed of the models in\nexperiments, where SLP-Net has the highest overall superiority\n","authors":["Bo Yang","Hong Peng","Chenggang Guo","Xiaohui Luo","Jun Wang","Xianzhong Long"],"pdf_url":"https://arxiv.org/pdf/2312.12789v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12804v2","updated":"2024-01-04T09:28:50Z","published":"2023-12-20T06:52:38Z","title":"Multi-stages attention Breast cancer classification based on nonlinear\n  spiking neural P neurons with autapses","summary":"  Breast cancer(BC) is a prevalent type of malignant tumor in women. Early\ndiagnosis and treatment are vital for enhancing the patients' survival rate.\nDownsampling in deep networks may lead to loss of information, so for\ncompensating the detail and edge information and allowing convolutional neural\nnetworks to pay more attention to seek the lesion region, we propose a\nmulti-stages attention architecture based on NSNP neurons with autapses. First,\nunlike the single-scale attention acquisition methods of existing methods, we\nset up spatial attention acquisition at each feature map scale of the\nconvolutional network to obtain an fusion global information on attention\nguidance. Then we introduce a new type of NSNP variants called NSNP neurons\nwith autapses. Specifically, NSNP systems are modularized as feature encoders,\nrecoding the features extracted from convolutional neural network as well as\nthe fusion of attention information and preserve the key characteristic\nelements in feature maps. This ensures the retention of valuable data while\ngradually transforming high-dimensional complicated info into low-dimensional\nones. The proposed method is evaluated on the public dataset BreakHis at\nvarious magnifications and classification tasks. It achieves a classification\naccuracy of 96.32% at all magnification cases, outperforming state-of-the-art\nmethods. Ablation studies are also performed, verifying the proposed model's\nefficacy. The source code is available at\nXhuBobYoung/Breast-cancer-Classification.\n","authors":["Bo Yang","Hong Peng","Xiaohui Luo","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2312.12804v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15403v2","updated":"2024-01-04T09:23:07Z","published":"2023-03-27T17:19:50Z","title":"Training-free Content Injection using h-space in Diffusion Models","summary":"  Diffusion models (DMs) synthesize high-quality images in various domains.\nHowever, controlling their generative process is still hazy because the\nintermediate variables in the process are not rigorously studied. Recently, the\nbottleneck feature of the U-Net, namely $h$-space, is found to convey the\nsemantics of the resulting image. It enables StyleCLIP-like latent editing\nwithin DMs. In this paper, we explore further usage of $h$-space beyond\nattribute editing, and introduce a method to inject the content of one image\ninto another image by combining their features in the generative processes.\nBriefly, given the original generative process of the other image, 1) we\ngradually blend the bottleneck feature of the content with proper\nnormalization, and 2) we calibrate the skip connections to match the injected\ncontent. Unlike custom-diffusion approaches, our method does not require\ntime-consuming optimization or fine-tuning. Instead, our method manipulates\nintermediate features within a feed-forward generative process. Furthermore,\nour method does not require supervision from external networks. The code is\navailable at https://curryjung.github.io/InjectFusion/\n","authors":["Jaeseok Jeong","Mingi Kwon","Youngjung Uh"],"pdf_url":"https://arxiv.org/pdf/2303.15403v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02162v1","updated":"2024-01-04T09:19:54Z","published":"2024-01-04T09:19:54Z","title":"Frequency Domain Nuances Mining for Visible-Infrared Person\n  Re-identification","summary":"  The key of visible-infrared person re-identification (VIReID) lies in how to\nminimize the modality discrepancy between visible and infrared images. Existing\nmethods mainly exploit the spatial information while ignoring the\ndiscriminative frequency information. To address this issue, this paper aims to\nreduce the modality discrepancy from the frequency domain perspective.\nSpecifically, we propose a novel Frequency Domain Nuances Mining (FDNM) method\nto explore the cross-modality frequency domain information, which mainly\nincludes an amplitude guided phase (AGP) module and an amplitude nuances mining\n(ANM) module. These two modules are mutually beneficial to jointly explore\nfrequency domain visible-infrared nuances, thereby effectively reducing the\nmodality discrepancy in the frequency domain. Besides, we propose a\ncenter-guided nuances mining loss to encourage the ANM module to preserve\ndiscriminative identity information while discovering diverse cross-modality\nnuances. To the best of our knowledge, this is the first work that explores the\npotential frequency information for VIReID research. Extensive experiments show\nthat the proposed FDNM has significant advantages in improving the performance\nof VIReID. Specifically, our method outperforms the second-best method by 5.2\\%\nin Rank-1 accuracy and 5.8\\% in mAP on the SYSU-MM01 dataset under the indoor\nsearch mode, respectively. Besides, we also validate the effectiveness and\ngeneralization of our method on the challenging visible-infrared face\nrecognition task. \\textcolor{magenta}{The code will be available.}\n","authors":["Yukang Zhang","Yang Lu","Yan Yan","Hanzi Wang","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2401.02162v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02161v1","updated":"2024-01-04T09:18:31Z","published":"2024-01-04T09:18:31Z","title":"Enhancing RAW-to-sRGB with Decoupled Style Structure in Fourier Domain","summary":"  RAW to sRGB mapping, which aims to convert RAW images from smartphones into\nRGB form equivalent to that of Digital Single-Lens Reflex (DSLR) cameras, has\nbecome an important area of research. However, current methods often ignore the\ndifference between cell phone RAW images and DSLR camera RGB images, a\ndifference that goes beyond the color matrix and extends to spatial structure\ndue to resolution variations. Recent methods directly rebuild color mapping and\nspatial structure via shared deep representation, limiting optimal performance.\nInspired by Image Signal Processing (ISP) pipeline, which distinguishes image\nrestoration and enhancement, we present a novel Neural ISP framework, named\nFourierISP. This approach breaks the image down into style and structure within\nthe frequency domain, allowing for independent optimization. FourierISP is\ncomprised of three subnetworks: Phase Enhance Subnet for structural refinement,\nAmplitude Refine Subnet for color learning, and Color Adaptation Subnet for\nblending them in a smooth manner. This approach sharpens both color and\nstructure, and extensive evaluations across varied datasets confirm that our\napproach realizes state-of-the-art results. Code will be available at\n~\\url{https://github.com/alexhe101/FourierISP}.\n","authors":["Xuanhua He","Tao Hu","Guoli Wang","Zejin Wang","Run Wang","Qian Zhang","Keyu Yan","Ziyi Chen","Rui Li","Chenjun Xie","Jie Zhang","Man Zhou"],"pdf_url":"https://arxiv.org/pdf/2401.02161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2011.14956v5","updated":"2024-01-04T09:17:42Z","published":"2020-11-25T09:40:34Z","title":"Handling Noisy Labels via One-Step Abductive Multi-Target Learning and\n  Its Application to Helicobacter Pylori Segmentation","summary":"  Learning from noisy labels is an important concern in plenty of real-world\nscenarios. Various approaches for this concern first make corrections\ncorresponding to potentially noisy-labeled instances, and then update\npredictive model with information of the made corrections. However, in specific\nareas, such as medical histopathology whole slide image analysis (MHWSIA), it\nis often difficult or impossible for experts to manually achieve the noisy-free\nground-truth labels which leads to labels with complex noise. This situation\nraises two more difficult problems: 1) the methodology of approaches making\ncorrections corresponding to potentially noisy-labeled instances has\nlimitations due to the complex noise existing in labels; and 2) the appropriate\nevaluation strategy for validation/testing is unclear because of the great\ndifficulty in collecting the noisy-free ground-truth labels. For the problem\n1), we present one-step abductive multi-target learning (OSAMTL) that imposes a\none-step logical reasoning upon machine learning via a multi-target learning\nprocedure to constrain the predictions of the learning model to be subject to\nour prior knowledge about the true target. For the problem 2), we propose a\nlogical assessment formula (LAF) that evaluates the logical rationality of the\noutputs of an approach by estimating the consistencies between the predictions\nof the learning model and the logical facts narrated from the results of the\none-step logical reasoning of OSAMTL. Based on the Helicobacter pylori (H.\npylori) segmentation task in MHWSIA, we show that OSAMTL enables the machine\nlearning model achieving logically more rational predictions, which is beyond\nvarious state-of-the-art approaches in handling complex noisy labels.\n","authors":["Yongquan Yang","Yiming Yang","Jie Chen","Jiayi Zheng","Zhongxi Zheng"],"pdf_url":"https://arxiv.org/pdf/2011.14956v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09077v2","updated":"2024-01-04T09:04:44Z","published":"2023-11-15T16:19:13Z","title":"Spiking NeRF: Representing the Real-World Geometry by a Discontinuous\n  Representation","summary":"  A crucial reason for the success of existing NeRF-based methods is to build a\nneural density field for the geometry representation via multiple perceptron\nlayers (MLPs). MLPs are continuous functions, however, real geometry or density\nfield is frequently discontinuous at the interface between the air and the\nsurface. Such a contrary brings the problem of unfaithful geometry\nrepresentation. To this end, this paper proposes spiking NeRF, which leverages\nspiking neurons and a hybrid Artificial Neural Network (ANN)-Spiking Neural\nNetwork (SNN) framework to build a discontinuous density field for faithful\ngeometry representation. Specifically, we first demonstrate the reason why\ncontinuous density fields will bring inaccuracy. Then, we propose to use the\nspiking neurons to build a discontinuous density field. We conduct a\ncomprehensive analysis for the problem of existing spiking neuron models and\nthen provide the numerical relationship between the parameter of the spiking\nneuron and the theoretical accuracy of geometry. Based on this, we propose a\nbounded spiking neuron to build the discontinuous density field. Our method\nachieves SOTA performance. The source code and the supplementary material are\navailable at https://github.com/liaozhanfeng/Spiking-NeRF.\n","authors":["Zhanfeng Liao","Qian Zheng","Yan Liu","Gang Pan"],"pdf_url":"https://arxiv.org/pdf/2311.09077v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.17358v3","updated":"2024-01-04T09:00:11Z","published":"2023-06-30T01:32:16Z","title":"Shadow Generation with Decomposed Mask Prediction and Attentive Shadow\n  Filling","summary":"  Image composition refers to inserting a foreground object into a background\nimage to obtain a composite image. In this work, we focus on generating\nplausible shadows for the inserted foreground object to make the composite\nimage more realistic. To supplement the existing small-scale dataset, we create\na large-scale dataset called RdSOBA with rendering techniques. Moreover, we\ndesign a two-stage network named DMASNet with decomposed mask prediction and\nattentive shadow filling. Specifically, in the first stage, we decompose shadow\nmask prediction into box prediction and shape prediction. In the second stage,\nwe attend to reference background shadow pixels to fill the foreground shadow.\nAbundant experiments prove that our DMASNet achieves better visual effects and\ngeneralizes well to real composite images.\n","authors":["Xinhao Tao","Junyan Cao","Yan Hong","Li Niu"],"pdf_url":"https://arxiv.org/pdf/2306.17358v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02151v1","updated":"2024-01-04T08:58:25Z","published":"2024-01-04T08:58:25Z","title":"Frequency-Adaptive Pan-Sharpening with Mixture of Experts","summary":"  Pan-sharpening involves reconstructing missing high-frequency information in\nmulti-spectral images with low spatial resolution, using a higher-resolution\npanchromatic image as guidance. Although the inborn connection with frequency\ndomain, existing pan-sharpening research has not almost investigated the\npotential solution upon frequency domain. To this end, we propose a novel\nFrequency Adaptive Mixture of Experts (FAME) learning framework for\npan-sharpening, which consists of three key components: the Adaptive Frequency\nSeparation Prediction Module, the Sub-Frequency Learning Expert Module, and the\nExpert Mixture Module. In detail, the first leverages the discrete cosine\ntransform to perform frequency separation by predicting the frequency mask. On\nthe basis of generated mask, the second with low-frequency MOE and\nhigh-frequency MOE takes account for enabling the effective low-frequency and\nhigh-frequency information reconstruction. Followed by, the final fusion module\ndynamically weights high-frequency and low-frequency MOE knowledge to adapt to\nremote sensing images with significant content variations. Quantitative and\nqualitative experiments over multiple datasets demonstrate that our method\nperforms the best against other state-of-the-art ones and comprises a strong\ngeneralization ability for real-world scenes. Code will be made publicly at\n\\url{https://github.com/alexhe101/FAME-Net}.\n","authors":["Xuanhua He","Keyu Yan","Rui Li","Chengjun Xie","Jie Zhang","Man Zhou"],"pdf_url":"https://arxiv.org/pdf/2401.02151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02150v1","updated":"2024-01-04T08:57:09Z","published":"2024-01-04T08:57:09Z","title":"Marginal Debiased Network for Fair Visual Recognition","summary":"  Deep neural networks (DNNs) are often prone to learn the spurious\ncorrelations between target classes and bias attributes, like gender and race,\ninherent in a major portion of training data (bias-aligned samples), thus\nshowing unfair behavior and arising controversy in the modern pluralistic and\negalitarian society. In this paper, we propose a novel marginal debiased\nnetwork (MDN) to learn debiased representations. More specifically, a marginal\nsoftmax loss (MSL) is designed by introducing the idea of margin penalty into\nthe fairness problem, which assigns a larger margin for bias-conflicting\nsamples (data without spurious correlations) than for bias-aligned ones, so as\nto deemphasize the spurious correlations and improve generalization on unbiased\ntest criteria. To determine the margins, our MDN is optimized through a meta\nlearning framework. We propose a meta equalized loss (MEL) to perceive the\nmodel fairness, and adaptively update the margin parameters by metaoptimization\nwhich requires the trained model guided by the optimal margins should minimize\nMEL computed on an unbiased meta-validation set. Extensive experiments on\nBiasedMNIST, Corrupted CIFAR-10, CelebA and UTK-Face datasets demonstrate that\nour MDN can achieve a remarkable performance on under-represented samples and\nobtain superior debiased results against the previous approaches.\n","authors":["Mei Wang","Weihong Deng","Sen Su"],"pdf_url":"https://arxiv.org/pdf/2401.02150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02147v1","updated":"2024-01-04T08:53:08Z","published":"2024-01-04T08:53:08Z","title":"Exploring Boundary of GPT-4V on Marine Analysis: A Preliminary Case\n  Study","summary":"  Large language models (LLMs) have demonstrated a powerful ability to answer\nvarious queries as a general-purpose assistant. The continuous multi-modal\nlarge language models (MLLM) empower LLMs with the ability to perceive visual\nsignals. The launch of GPT-4 (Generative Pre-trained Transformers) has\ngenerated significant interest in the research communities. GPT-4V(ison) has\ndemonstrated significant power in both academia and industry fields, as a focal\npoint in a new artificial intelligence generation. Though significant success\nwas achieved by GPT-4V, exploring MLLMs in domain-specific analysis (e.g.,\nmarine analysis) that required domain-specific knowledge and expertise has\ngained less attention. In this study, we carry out the preliminary and\ncomprehensive case study of utilizing GPT-4V for marine analysis. This report\nconducts a systematic evaluation of existing GPT-4V, assessing the performance\nof GPT-4V on marine research and also setting a new standard for future\ndevelopments in MLLMs. The experimental results of GPT-4V show that the\nresponses generated by GPT-4V are still far away from satisfying the\ndomain-specific requirements of the marine professions. All images and prompts\nused in this study will be available at\nhttps://github.com/hkust-vgd/Marine_GPT-4V_Eval\n","authors":["Ziqiang Zheng","Yiwei Chen","Jipeng Zhang","Tuan-Anh Vu","Huimin Zeng","Yue Him Wong Tim","Sai-Kit Yeung"],"pdf_url":"https://arxiv.org/pdf/2401.02147v1.pdf","comment":"51 pages, 36 figures, Repository:\n  https://github.com/hkust-vgd/Marine_GPT-4V_Eval"},{"id":"http://arxiv.org/abs/2401.02142v1","updated":"2024-01-04T08:48:21Z","published":"2024-01-04T08:48:21Z","title":"GUESS:GradUally Enriching SyntheSis for Text-Driven Human Motion\n  Generation","summary":"  In this paper, we propose a novel cascaded diffusion-based generative\nframework for text-driven human motion synthesis, which exploits a strategy\nnamed GradUally Enriching SyntheSis (GUESS as its abbreviation). The strategy\nsets up generation objectives by grouping body joints of detailed skeletons in\nclose semantic proximity together and then replacing each of such joint group\nwith a single body-part node. Such an operation recursively abstracts a human\npose to coarser and coarser skeletons at multiple granularity levels. With\ngradually increasing the abstraction level, human motion becomes more and more\nconcise and stable, significantly benefiting the cross-modal motion synthesis\ntask. The whole text-driven human motion synthesis problem is then divided into\nmultiple abstraction levels and solved with a multi-stage generation framework\nwith a cascaded latent diffusion model: an initial generator first generates\nthe coarsest human motion guess from a given text description; then, a series\nof successive generators gradually enrich the motion details based on the\ntextual description and the previous synthesized results. Notably, we further\nintegrate GUESS with the proposed dynamic multi-condition fusion mechanism to\ndynamically balance the cooperative effects of the given textual condition and\nsynthesized coarse motion prompt in different generation stages. Extensive\nexperiments on large-scale datasets verify that GUESS outperforms existing\nstate-of-the-art methods by large margins in terms of accuracy, realisticness,\nand diversity. Code is available at https://github.com/Xuehao-Gao/GUESS.\n","authors":["Xuehao Gao","Yang Yang","Zhenyu Xie","Shaoyi Du","Zhongqian Sun","Yang Wu"],"pdf_url":"https://arxiv.org/pdf/2401.02142v1.pdf","comment":"Accepted by IEEE Transactions on Visualization and Computer Graphics\n  (2024)"},{"id":"http://arxiv.org/abs/2401.02141v1","updated":"2024-01-04T08:46:39Z","published":"2024-01-04T08:46:39Z","title":"Bayesian Intrinsic Groupwise Image Registration: Unsupervised\n  Disentanglement of Anatomy and Geometry","summary":"  This article presents a general Bayesian learning framework for multi-modal\ngroupwise registration on medical images. The method builds on probabilistic\nmodelling of the image generative process, where the underlying common anatomy\nand geometric variations of the observed images are explicitly disentangled as\nlatent variables. Thus, groupwise registration is achieved through the solution\nto Bayesian inference. We propose a novel hierarchical variational\nauto-encoding architecture to realize the inference procedure of the latent\nvariables, where the registration parameters can be calculated in a\nmathematically interpretable fashion. Remarkably, this new paradigm can learn\ngroupwise registration in an unsupervised closed-loop self-reconstruction\nprocess, sparing the burden of designing complex intensity-based similarity\nmeasures. The computationally efficient disentangled architecture is also\ninherently scalable and flexible, allowing for groupwise registration on\nlarge-scale image groups with variable sizes. Furthermore, the inferred\nstructural representations from disentanglement learning are capable of\ncapturing the latent anatomy of the observations with visual semantics.\nExtensive experiments were conducted to validate the proposed framework,\nincluding four datasets from cardiac, brain and abdominal medical images. The\nresults have demonstrated the superiority of our method over conventional\nsimilarity-based approaches in terms of accuracy, efficiency, scalability and\ninterpretability.\n","authors":["Xinzhe Luo","Xin Wang","Linda Shapiro","Chun Yuan","Jianfeng Feng","Xiahai Zhuang"],"pdf_url":"https://arxiv.org/pdf/2401.02141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02138v1","updated":"2024-01-04T08:43:41Z","published":"2024-01-04T08:43:41Z","title":"Explore Human Parsing Modality for Action Recognition","summary":"  Multimodal-based action recognition methods have achieved high success using\npose and RGB modality. However, skeletons sequences lack appearance depiction\nand RGB images suffer irrelevant noise due to modality limitations. To address\nthis, we introduce human parsing feature map as a novel modality, since it can\nselectively retain effective semantic features of the body parts, while\nfiltering out most irrelevant noise. We propose a new dual-branch framework\ncalled Ensemble Human Parsing and Pose Network (EPP-Net), which is the first to\nleverage both skeletons and human parsing modalities for action recognition.\nThe first human pose branch feeds robust skeletons in graph convolutional\nnetwork to model pose features, while the second human parsing branch also\nleverages depictive parsing feature maps to model parsing festures via\nconvolutional backbones. The two high-level features will be effectively\ncombined through a late fusion strategy for better action recognition.\nExtensive experiments on NTU RGB+D and NTU RGB+D 120 benchmarks consistently\nverify the effectiveness of our proposed EPP-Net, which outperforms the\nexisting action recognition methods. Our code is available at:\nhttps://github.com/liujf69/EPP-Net-Action.\n","authors":["Jinfu Liu","Runwei Ding","Yuhang Wen","Nan Dai","Fanyang Meng","Shen Zhao","Mengyuan Liu"],"pdf_url":"https://arxiv.org/pdf/2401.02138v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2307.07977"},{"id":"http://arxiv.org/abs/2401.02137v1","updated":"2024-01-04T08:42:36Z","published":"2024-01-04T08:42:36Z","title":"SyCoCa: Symmetrizing Contrastive Captioners with Attentive Masking for\n  Multimodal Alignment","summary":"  Multimodal alignment between language and vision is the fundamental topic in\ncurrent vision-language model research. Contrastive Captioners (CoCa), as a\nrepresentative method, integrates Contrastive Language-Image Pretraining (CLIP)\nand Image Caption (IC) into a unified framework, resulting in impressive\nresults. CLIP imposes a bidirectional constraints on global representation of\nentire images and sentences. Although IC conducts an unidirectional\nimage-to-text generation on local representation, it lacks any constraint on\nlocal text-to-image reconstruction, which limits the ability to understand\nimages at a fine-grained level when aligned with texts. To achieve multimodal\nalignment from both global and local perspectives, this paper proposes\nSymmetrizing Contrastive Captioners (SyCoCa), which introduces bidirectional\ninteractions on images and texts across the global and local representation\nlevels. Specifically, we expand a Text-Guided Masked Image Modeling (TG-MIM)\nhead based on ITC and IC heads. The improved SyCoCa can further leverage\ntextual cues to reconstruct contextual images and visual cues to predict\ntextual contents. When implementing bidirectional local interactions, the local\ncontents of images tend to be cluttered or unrelated to their textual\ndescriptions. Thus, we employ an attentive masking strategy to select effective\nimage patches for interaction. Extensive experiments on five vision-language\ntasks, including image-text retrieval, image-captioning, visual question\nanswering, and zero-shot/finetuned image classification, validate the\neffectiveness of our proposed method.\n","authors":["Ziping Ma","Furong Xu","Jian Liu","Ming Yang","Qingpei Guo"],"pdf_url":"https://arxiv.org/pdf/2401.02137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02126v1","updated":"2024-01-04T08:21:30Z","published":"2024-01-04T08:21:30Z","title":"Unified Diffusion-Based Rigid and Non-Rigid Editing with Text and Image\n  Guidance","summary":"  Existing text-to-image editing methods tend to excel either in rigid or\nnon-rigid editing but encounter challenges when combining both, resulting in\nmisaligned outputs with the provided text prompts. In addition, integrating\nreference images for control remains challenging. To address these issues, we\npresent a versatile image editing framework capable of executing both rigid and\nnon-rigid edits, guided by either textual prompts or reference images. We\nleverage a dual-path injection scheme to handle diverse editing scenarios and\nintroduce an integrated self-attention mechanism for fusion of appearance and\nstructural information. To mitigate potential visual artifacts, we further\nemploy latent fusion techniques to adjust intermediate latents. Compared to\nprevious work, our approach represents a significant advance in achieving\nprecise and versatile image editing. Comprehensive experiments validate the\nefficacy of our method, showcasing competitive or superior results in\ntext-based editing and appearance transfer tasks, encompassing both rigid and\nnon-rigid settings.\n","authors":["Jiacheng Wang","Ping Liu","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2401.02126v1.pdf","comment":"15 pages, 13 figures"},{"id":"http://arxiv.org/abs/2312.06978v3","updated":"2024-01-04T08:21:26Z","published":"2023-12-12T04:38:30Z","title":"CLASS-M: Adaptive stain separation-based contrastive learning with\n  pseudo-labeling for histopathological image classification","summary":"  Histopathological image classification is an important task in medical image\nanalysis. Recent approaches generally rely on weakly supervised learning due to\nthe ease of acquiring case-level labels from pathology reports. However,\npatch-level classification is preferable in applications where only a limited\nnumber of cases are available or when local prediction accuracy is critical. On\nthe other hand, acquiring extensive datasets with localized labels for training\nis not feasible. In this paper, we propose a semi-supervised patch-level\nhistopathological image classification model, named CLASS-M, that does not\nrequire extensively labeled datasets. CLASS-M is formed by two main parts: a\ncontrastive learning module that uses separated Hematoxylin and Eosin images\ngenerated through an adaptive stain separation process, and a module with\npseudo-labels using MixUp. We compare our model with other state-of-the-art\nmodels on two clear cell renal cell carcinoma datasets. We demonstrate that our\nCLASS-M model has the best performance on both datasets. Our code is available\nat github.com/BzhangURU/Paper_CLASS-M/tree/main\n","authors":["Bodong Zhang","Hamid Manoochehri","Man Minh Ho","Fahimeh Fooladgar","Yosep Chong","Beatrice S. Knudsen","Deepika Sirohi","Tolga Tasdizen"],"pdf_url":"https://arxiv.org/pdf/2312.06978v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.13398v3","updated":"2024-01-04T08:19:16Z","published":"2023-11-22T13:53:04Z","title":"Depth-Regularized Optimization for 3D Gaussian Splatting in Few-Shot\n  Images","summary":"  In this paper, we present a method to optimize Gaussian splatting with a\nlimited number of images while avoiding overfitting. Representing a 3D scene by\ncombining numerous Gaussian splats has yielded outstanding visual quality.\nHowever, it tends to overfit the training views when only a small number of\nimages are available. To address this issue, we introduce a dense depth map as\na geometry guide to mitigate overfitting. We obtained the depth map using a\npre-trained monocular depth estimation model and aligning the scale and offset\nusing sparse COLMAP feature points. The adjusted depth aids in the color-based\noptimization of 3D Gaussian splatting, mitigating floating artifacts, and\nensuring adherence to geometric constraints. We verify the proposed method on\nthe NeRF-LLFF dataset with varying numbers of few images. Our approach\ndemonstrates robust geometry compared to the original method that relies solely\non images. Project page: robot0321.github.io/DepthRegGS\n","authors":["Jaeyoung Chung","Jeongtaek Oh","Kyoung Mu Lee"],"pdf_url":"https://arxiv.org/pdf/2311.13398v3.pdf","comment":"10 pages, 5 figures; Project page: robot0321.github.io/DepthRegGS"},{"id":"http://arxiv.org/abs/2305.17423v3","updated":"2024-01-04T08:10:13Z","published":"2023-05-27T09:14:03Z","title":"Accelerating Text-to-Image Editing via Cache-Enabled Sparse Diffusion\n  Inference","summary":"  Due to the recent success of diffusion models, text-to-image generation is\nbecoming increasingly popular and achieves a wide range of applications. Among\nthem, text-to-image editing, or continuous text-to-image generation, attracts\nlots of attention and can potentially improve the quality of generated images.\nIt's common to see that users may want to slightly edit the generated image by\nmaking minor modifications to their input textual descriptions for several\nrounds of diffusion inference. However, such an image editing process suffers\nfrom the low inference efficiency of many existing diffusion models even using\nGPU accelerators. To solve this problem, we introduce Fast Image Semantically\nEdit (FISEdit), a cached-enabled sparse diffusion model inference engine for\nefficient text-to-image editing. The key intuition behind our approach is to\nutilize the semantic mapping between the minor modifications on the input text\nand the affected regions on the output image. For each text editing step,\nFISEdit can automatically identify the affected image regions and utilize the\ncached unchanged regions' feature map to accelerate the inference process.\nExtensive empirical results show that FISEdit can be $3.4\\times$ and\n$4.4\\times$ faster than existing methods on NVIDIA TITAN RTX and A100 GPUs\nrespectively, and even generates more satisfactory images.\n","authors":["Zihao Yu","Haoyang Li","Fangcheng Fu","Xupeng Miao","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2305.17423v3.pdf","comment":"AAAI 2024"},{"id":"http://arxiv.org/abs/2308.04669v4","updated":"2024-01-04T08:00:37Z","published":"2023-08-09T02:27:23Z","title":"A General Implicit Framework for Fast NeRF Composition and Rendering","summary":"  A variety of Neural Radiance Fields (NeRF) methods have recently achieved\nremarkable success in high render speed. However, current accelerating methods\nare specialized and incompatible with various implicit methods, preventing\nreal-time composition over various types of NeRF works. Because NeRF relies on\nsampling along rays, it is possible to provide general guidance for\nacceleration. To that end, we propose a general implicit pipeline for composing\nNeRF objects quickly. Our method enables the casting of dynamic shadows within\nor between objects using analytical light sources while allowing multiple NeRF\nobjects to be seamlessly placed and rendered together with any arbitrary rigid\ntransformations. Mainly, our work introduces a new surface representation known\nas Neural Depth Fields (NeDF) that quickly determines the spatial relationship\nbetween objects by allowing direct intersection computation between rays and\nimplicit surfaces. It leverages an intersection neural network to query NeRF\nfor acceleration instead of depending on an explicit spatial structure.Our\nproposed method is the first to enable both the progressive and interactive\ncomposition of NeRF objects. Additionally, it also serves as a previewing\nplugin for a range of existing NeRF works.\n","authors":["Xinyu Gao","Ziyi Yang","Yunlu Zhao","Yuxiang Sun","Xiaogang Jin","Changqing Zou"],"pdf_url":"https://arxiv.org/pdf/2308.04669v4.pdf","comment":"AAAI 2024"},{"id":"http://arxiv.org/abs/2401.02117v1","updated":"2024-01-04T07:55:53Z","published":"2024-01-04T07:55:53Z","title":"Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost\n  Whole-Body Teleoperation","summary":"  Imitation learning from human demonstrations has shown impressive performance\nin robotics. However, most results focus on table-top manipulation, lacking the\nmobility and dexterity necessary for generally useful tasks. In this work, we\ndevelop a system for imitating mobile manipulation tasks that are bimanual and\nrequire whole-body control. We first present Mobile ALOHA, a low-cost and\nwhole-body teleoperation system for data collection. It augments the ALOHA\nsystem with a mobile base, and a whole-body teleoperation interface. Using data\ncollected with Mobile ALOHA, we then perform supervised behavior cloning and\nfind that co-training with existing static ALOHA datasets boosts performance on\nmobile manipulation tasks. With 50 demonstrations for each task, co-training\ncan increase success rates by up to 90%, allowing Mobile ALOHA to autonomously\ncomplete complex mobile manipulation tasks such as sauteing and serving a piece\nof shrimp, opening a two-door wall cabinet to store heavy cooking pots, calling\nand entering an elevator, and lightly rinsing a used pan using a kitchen\nfaucet. Project website: https://mobile-aloha.github.io\n","authors":["Zipeng Fu","Tony Z. Zhao","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2401.02117v1.pdf","comment":"Project website: https://mobile-aloha.github.io (Zipeng Fu and Tony\n  Z. Zhao are project co-leads, Chelsea Finn is the advisor)"},{"id":"http://arxiv.org/abs/2401.02113v1","updated":"2024-01-04T07:49:32Z","published":"2024-01-04T07:49:32Z","title":"Source-Free Online Domain Adaptive Semantic Segmentation of Satellite\n  Images under Image Degradation","summary":"  Online adaptation to distribution shifts in satellite image segmentation\nstands as a crucial yet underexplored problem. In this paper, we address\nsource-free and online domain adaptation, i.e., test-time adaptation (TTA), for\nsatellite images, with the focus on mitigating distribution shifts caused by\nvarious forms of image degradation. Towards achieving this goal, we propose a\nnovel TTA approach involving two effective strategies. First, we progressively\nestimate the global Batch Normalization (BN) statistics of the target\ndistribution with incoming data stream. Leveraging these statistics during\ninference has the ability to effectively reduce domain gap. Furthermore, we\nenhance prediction quality by refining the predicted masks using global class\ncenters. Both strategies employ dynamic momentum for fast and stable\nconvergence. Notably, our method is backpropagation-free and hence fast and\nlightweight, making it highly suitable for on-the-fly adaptation to new domain.\nThrough comprehensive experiments across various domain adaptation scenarios,\nwe demonstrate the robust performance of our method.\n","authors":["Fahim Faisal Niloy","Kishor Kumar Bhaumik","Simon S. Woo"],"pdf_url":"https://arxiv.org/pdf/2401.02113v1.pdf","comment":"ICASSP 2024"},{"id":"http://arxiv.org/abs/2401.02110v1","updated":"2024-01-04T07:43:40Z","published":"2024-01-04T07:43:40Z","title":"Significance of Anatomical Constraints in Virtual Try-On","summary":"  The system of Virtual Try-ON (VTON) allows a user to try a product virtually.\nIn general, a VTON system takes a clothing source and a person's image to\npredict the try-on output of the person in the given clothing. Although\nexisting methods perform well for simple poses, in case of bent or crossed arms\nposture or when there is a significant difference between the alignment of the\nsource clothing and the pose of the target person, these methods fail by\ngenerating inaccurate clothing deformations. In the VTON methods that employ\nThin Plate Spline (TPS) based clothing transformations, this mainly occurs for\ntwo reasons - (1)~the second-order smoothness constraint of TPS that restricts\nthe bending of the object plane. (2)~Overlaps among different clothing parts\n(e.g., sleeves and torso) can not be modeled by a single TPS transformation, as\nit assumes the clothing as a single planar object; therefore, disregards the\nindependence of movement of different clothing parts. To this end, we make two\nmajor contributions. Concerning the bending limitations of TPS, we propose a\nhuman AnaTomy-Aware Geometric (ATAG) transformation. Regarding the overlap\nissue, we propose a part-based warping approach that divides the clothing into\nindependently warpable parts to warp them separately and later combine them.\nExtensive analysis shows the efficacy of this approach.\n","authors":["Debapriya Roy","Sanchayan Santra","Diganta Mukherjee","Bhabatosh Chanda"],"pdf_url":"https://arxiv.org/pdf/2401.02110v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2208.08076"},{"id":"http://arxiv.org/abs/2311.17515v2","updated":"2024-01-04T07:42:31Z","published":"2023-11-29T10:38:42Z","title":"Fusion of Single and Integral Multispectral Aerial Images","summary":"  A novel hybrid (model- and learning-based) architecture is presented for\nfusing the most significant features from conventional aerial images with the\nones from integral aerial images that are the result of synthetic aperture\nsensing for removing occlusion. It combines the environment's spatial\nreferences with features of unoccluded targets that would normally be hidden by\ndense vegetation. Our method out-beats state-of-the-art two-channel and\nmulti-channel fusion approaches visually and quantitatively in common metrics,\nsuch as mutual information, visual information fidelity, and peak\nsignal-to-noise ratio. The proposed model does not require manually tuned\nparameters, can be extended to an arbitrary number and combinations of spectral\nchannels, and is reconfigurable for addressing different use cases.\n","authors":["Mohamed Youssef","Oliver Bimber"],"pdf_url":"https://arxiv.org/pdf/2311.17515v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.15707v3","updated":"2024-01-04T07:42:19Z","published":"2023-12-25T12:12:36Z","title":"High-Fidelity Diffusion-based Image Editing","summary":"  Diffusion models have attained remarkable success in the domains of image\ngeneration and editing. It is widely recognized that employing larger inversion\nand denoising steps in diffusion model leads to improved image reconstruction\nquality. However, the editing performance of diffusion models tends to be no\nmore satisfactory even with increasing denoising steps. The deficiency in\nediting could be attributed to the conditional Markovian property of the\nediting process, where errors accumulate throughout denoising steps. To tackle\nthis challenge, we first propose an innovative framework where a rectifier\nmodule is incorporated to modulate diffusion model weights with residual\nfeatures, thereby providing compensatory information to bridge the fidelity\ngap. Furthermore, we introduce a novel learning paradigm aimed at minimizing\nerror propagation during the editing process, which trains the editing\nprocedure in a manner similar to denoising score-matching. Extensive\nexperiments demonstrate that our proposed framework and training strategy\nachieve high-fidelity reconstruction and editing results across various levels\nof denoising steps, meanwhile exhibits exceptional performance in terms of both\nquantitative metric and qualitative assessments. Moreover, we explore our\nmodel's generalization through several applications like image-to-image\ntranslation and out-of-domain image editing.\n","authors":["Chen Hou","Guoqiang Wei","Zhibo Chen"],"pdf_url":"https://arxiv.org/pdf/2312.15707v3.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2401.02099v1","updated":"2024-01-04T07:11:16Z","published":"2024-01-04T07:11:16Z","title":"CLAPP: Contrastive Language-Audio Pre-training in Passive Underwater\n  Vessel Classification","summary":"  Existing research on audio classification faces challenges in recognizing\nattributes of passive underwater vessel scenarios and lacks well-annotated\ndatasets due to data privacy concerns. In this study, we introduce CLAPP\n(Contrastive Language-Audio Pre-training in Passive Underwater Vessel\nClassification), a novel model. Our aim is to train a neural network using a\nwide range of vessel audio and vessel state text pairs obtained from an\noceanship dataset. CLAPP is capable of directly learning from raw vessel audio\ndata and, when available, from carefully curated labels, enabling improved\nrecognition of vessel attributes in passive underwater vessel scenarios.\nModel's zero-shot capability allows predicting the most relevant vessel state\ndescription for a given vessel audio, without directly optimizing for the task.\nOur approach aims to solve 2 challenges: vessel audio-text classification and\npassive underwater vessel audio attribute recognition. The proposed method\nachieves new state-of-the-art results on both Deepship and Shipsear public\ndatasets, with a notable margin of about 7%-13% for accuracy compared to prior\nmethods on zero-shot task.\n","authors":["Zeyu Li","Jingsheng Gao","Tong Yu","Suncheng Xiang","Jiacheng Ruan","Ting Liu","Yuzhuo Fu"],"pdf_url":"https://arxiv.org/pdf/2401.02099v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02097v1","updated":"2024-01-04T06:55:49Z","published":"2024-01-04T06:55:49Z","title":"Preserving Image Properties Through Initializations in Diffusion Models","summary":"  Retail photography imposes specific requirements on images. For instance,\nimages may need uniform background colors, consistent model poses, centered\nproducts, and consistent lighting. Minor deviations from these standards impact\na site's aesthetic appeal, making the images unsuitable for use. We show that\nStable Diffusion methods, as currently applied, do not respect these\nrequirements. The usual practice of training the denoiser with a very noisy\nimage and starting inference with a sample of pure noise leads to inconsistent\ngenerated images during inference. This inconsistency occurs because it is easy\nto tell the difference between samples of the training and inference\ndistributions. As a result, a network trained with centered retail product\nimages with uniform backgrounds generates images with erratic backgrounds. The\nproblem is easily fixed by initializing inference with samples from an\napproximation of noisy images. However, in using such an approximation, the\njoint distribution of text and noisy image at inference time still slightly\ndiffers from that at training time. This discrepancy is corrected by training\nthe network with samples from the approximate noisy image distribution.\nExtensive experiments on real application data show significant qualitative and\nquantitative improvements in performance from adopting these procedures.\nFinally, our procedure can interact well with other control-based methods to\nfurther enhance the controllability of diffusion-based methods.\n","authors":["Jeffrey Zhang","Shao-Yu Chang","Kedan Li","David Forsyth"],"pdf_url":"https://arxiv.org/pdf/2401.02097v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01886v2","updated":"2024-01-04T06:48:15Z","published":"2023-12-04T13:40:05Z","title":"InstructTA: Instruction-Tuned Targeted Attack for Large Vision-Language\n  Models","summary":"  Large vision-language models (LVLMs) have demonstrated their incredible\ncapability in image understanding and response generation. However, this rich\nvisual interaction also makes LVLMs vulnerable to adversarial examples. In this\npaper, we formulate a novel and practical gray-box attack scenario that the\nadversary can only access the visual encoder of the victim LVLM, without the\nknowledge of its prompts (which are often proprietary for service providers and\nnot publicly available) and its underlying large language model (LLM). This\npractical setting poses challenges to the cross-prompt and cross-model\ntransferability of targeted adversarial attack, which aims to confuse the LVLM\nto output a response that is semantically similar to the attacker's chosen\ntarget text. To this end, we propose an instruction-tuned targeted attack\n(dubbed InstructTA) to deliver the targeted adversarial attack on LVLMs with\nhigh transferability. Initially, we utilize a public text-to-image generative\nmodel to \"reverse\" the target response into a target image, and employ GPT-4 to\ninfer a reasonable instruction $\\boldsymbol{p}^\\prime$ from the target\nresponse. We then form a local surrogate model (sharing the same visual encoder\nwith the victim LVLM) to extract instruction-aware features of an adversarial\nimage example and the target image, and minimize the distance between these two\nfeatures to optimize the adversarial example. To further improve the\ntransferability, we augment the instruction $\\boldsymbol{p}^\\prime$ with\ninstructions paraphrased from an LLM. Extensive experiments demonstrate the\nsuperiority of our proposed method in targeted attack performance and\ntransferability.\n","authors":["Xunguang Wang","Zhenlan Ji","Pingchuan Ma","Zongjie Li","Shuai Wang"],"pdf_url":"https://arxiv.org/pdf/2312.01886v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.13505v4","updated":"2024-01-04T06:46:53Z","published":"2023-09-24T00:05:39Z","title":"Rewrite Caption Semantics: Bridging Semantic Gaps for\n  Language-Supervised Semantic Segmentation","summary":"  Vision-Language Pre-training has demonstrated its remarkable zero-shot\nrecognition ability and potential to learn generalizable visual representations\nfrom language supervision. Taking a step ahead, language-supervised semantic\nsegmentation enables spatial localization of textual inputs by learning pixel\ngrouping solely from image-text pairs. Nevertheless, the state-of-the-art\nsuffers from clear semantic gaps between visual and textual modality: plenty of\nvisual concepts appeared in images are missing in their paired captions. Such\nsemantic misalignment circulates in pre-training, leading to inferior zero-shot\nperformance in dense predictions due to insufficient visual concepts captured\nin textual representations. To close such semantic gap, we propose Concept\nCuration (CoCu), a pipeline that leverages CLIP to compensate for the missing\nsemantics. For each image-text pair, we establish a concept archive that\nmaintains potential visually-matched concepts with our proposed vision-driven\nexpansion and text-to-vision-guided ranking. Relevant concepts can thus be\nidentified via cluster-guided sampling and fed into pre-training, thereby\nbridging the gap between visual and textual semantics. Extensive experiments\nover a broad suite of 8 segmentation benchmarks show that CoCu achieves superb\nzero-shot transfer performance and greatly boosts language-supervised\nsegmentation baseline by a large margin, suggesting the value of bridging\nsemantic gap in pre-training data.\n","authors":["Yun Xing","Jian Kang","Aoran Xiao","Jiahao Nie","Ling Shao","Shijian Lu"],"pdf_url":"https://arxiv.org/pdf/2309.13505v4.pdf","comment":"NeurIPS 2023. Code is available at\n  https://github.com/xing0047/rewrite"},{"id":"http://arxiv.org/abs/2401.02094v1","updated":"2024-01-04T06:46:19Z","published":"2024-01-04T06:46:19Z","title":"Federated Class-Incremental Learning with Prototype Guided Transformer","summary":"  Existing federated learning methods have effectively addressed decentralized\nlearning in scenarios involving data privacy and non-IID data. However, in\nreal-world situations, each client dynamically learns new classes, requiring\nthe global model to maintain discriminative capabilities for both new and old\nclasses. To effectively mitigate the effects of catastrophic forgetting and\ndata heterogeneity under low communication costs, we designed a simple and\neffective method named PLoRA. On the one hand, we adopt prototype learning to\nlearn better feature representations and leverage the heuristic information\nbetween prototypes and class features to design a prototype re-weight module to\nsolve the classifier bias caused by data heterogeneity without retraining the\nclassification layer. On the other hand, our approach utilizes a pre-trained\nmodel as the backbone and utilizes LoRA to fine-tune with a tiny amount of\nparameters when learning new classes. Moreover, PLoRA does not rely on\nsimilarity-based module selection strategies, thereby further reducing\ncommunication overhead. Experimental results on standard datasets indicate that\nour method outperforms the state-of-the-art approaches significantly. More\nimportantly, our method exhibits strong robustness and superiority in various\nscenarios and degrees of data heterogeneity. Our code will be publicly\navailable.\n","authors":["Haiyang Guo","Fei Zhu","Wenzhuo Liu","Xu-Yao Zhang","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2401.02094v1.pdf","comment":"11 pages, 4 figures, conference"},{"id":"http://arxiv.org/abs/2312.11973v2","updated":"2024-01-04T06:26:36Z","published":"2023-12-19T09:11:49Z","title":"Continual Learning: Forget-free Winning Subnetworks for Video\n  Representations","summary":"  Inspired by the Lottery Ticket Hypothesis (LTH), which highlights the\nexistence of efficient subnetworks within larger, dense networks, a\nhigh-performing Winning Subnetwork (WSN) in terms of task performance under\nappropriate sparsity conditions is considered for various continual learning\ntasks. It leverages pre-existing weights from dense networks to achieve\nefficient learning in Task Incremental Learning (TIL) scenarios. In Few-Shot\nClass Incremental Learning (FSCIL), a variation of WSN referred to as the Soft\nsubnetwork (SoftNet) is designed to prevent overfitting when the data samples\nare scarce. Furthermore, the sparse reuse of WSN weights is considered for\nVideo Incremental Learning (VIL). The use of Fourier Subneural Operator (FSO)\nwithin WSN is considered. It enables compact encoding of videos and identifies\nreusable subnetworks across varying bandwidths. We have integrated FSO into\ndifferent architectural frameworks for continual learning, including VIL, TIL,\nand FSCIL. Our comprehensive experiments demonstrate FSO's effectiveness,\nsignificantly improving task performance at various convolutional\nrepresentational levels. Specifically, FSO enhances higher-layer performance in\nTIL and FSCIL and lower-layer performance in VIL\n","authors":["Haeyong Kang","Jaehong Yoon","Sung Ju Hwang","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2312.11973v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2303.14962,\n  arXiv:2306.11305"},{"id":"http://arxiv.org/abs/2312.08774v3","updated":"2024-01-04T06:01:35Z","published":"2023-12-14T09:50:09Z","title":"VSFormer: Visual-Spatial Fusion Transformer for Correspondence Pruning","summary":"  Correspondence pruning aims to find correct matches (inliers) from an initial\nset of putative correspondences, which is a fundamental task for many\napplications. The process of finding is challenging, given the varying inlier\nratios between scenes/image pairs due to significant visual differences.\nHowever, the performance of the existing methods is usually limited by the\nproblem of lacking visual cues (\\eg texture, illumination, structure) of\nscenes. In this paper, we propose a Visual-Spatial Fusion Transformer\n(VSFormer) to identify inliers and recover camera poses accurately. Firstly, we\nobtain highly abstract visual cues of a scene with the cross attention between\nlocal features of two-view images. Then, we model these visual cues and\ncorrespondences by a joint visual-spatial fusion module, simultaneously\nembedding visual cues into correspondences for pruning. Additionally, to mine\nthe consistency of correspondences, we also design a novel module that combines\nthe KNN-based graph and the transformer, effectively capturing both local and\nglobal contexts. Extensive experiments have demonstrated that the proposed\nVSFormer outperforms state-of-the-art methods on outdoor and indoor benchmarks.\nOur code is provided at the following repository:\nhttps://github.com/sugar-fly/VSFormer.\n","authors":["Tangfei Liao","Xiaoqin Zhang","Li Zhao","Tao Wang","Guobao Xiao"],"pdf_url":"https://arxiv.org/pdf/2312.08774v3.pdf","comment":"Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2312.17492v2","updated":"2024-01-04T05:57:15Z","published":"2023-12-29T06:46:37Z","title":"HEAP: Unsupervised Object Discovery and Localization with Contrastive\n  Grouping","summary":"  Unsupervised object discovery and localization aims to detect or segment\nobjects in an image without any supervision. Recent efforts have demonstrated a\nnotable potential to identify salient foreground objects by utilizing\nself-supervised transformer features. However, their scopes only build upon\npatch-level features within an image, neglecting region/image-level and\ncross-image relationships at a broader scale. Moreover, these methods cannot\ndifferentiate various semantics from multiple instances. To address these\nproblems, we introduce Hierarchical mErging framework via contrAstive grouPing\n(HEAP). Specifically, a novel lightweight head with cross-attention mechanism\nis designed to adaptively group intra-image patches into semantically coherent\nregions based on correlation among self-supervised features. Further, to ensure\nthe distinguishability among various regions, we introduce a region-level\ncontrastive clustering loss to pull closer similar regions across images. Also,\nan image-level contrastive loss is present to push foreground and background\nrepresentations apart, with which foreground objects and background are\naccordingly discovered. HEAP facilitates efficient hierarchical image\ndecomposition, which contributes to more accurate object discovery while also\nenabling differentiation among objects of various classes. Extensive\nexperimental results on semantic segmentation retrieval, unsupervised object\ndiscovery, and saliency detection tasks demonstrate that HEAP achieves\nstate-of-the-art performance.\n","authors":["Xin Zhang","Jinheng Xie","Yuan Yuan","Michael Bi Mi","Robby T. Tan"],"pdf_url":"https://arxiv.org/pdf/2312.17492v2.pdf","comment":"Accepted by AAAI24"},{"id":"http://arxiv.org/abs/2401.02076v1","updated":"2024-01-04T05:56:38Z","published":"2024-01-04T05:56:38Z","title":"Leveraging SAM for Single-Source Domain Generalization in Medical Image\n  Segmentation","summary":"  Domain Generalization (DG) aims to reduce domain shifts between domains to\nachieve promising performance on the unseen target domain, which has been\nwidely practiced in medical image segmentation. Single-source domain\ngeneralization (SDG) is the most challenging setting that trains on only one\nsource domain. Although existing methods have made considerable progress on SDG\nof medical image segmentation, the performances are still far from the\napplicable standards when faced with a relatively large domain shift. In this\npaper, we leverage the Segment Anything Model (SAM) to SDG to greatly improve\nthe ability of generalization. Specifically, we introduce a parallel framework,\nthe source images are sent into the SAM module and normal segmentation module\nrespectively. To reduce the calculation resources, we apply a merging strategy\nbefore sending images to the SAM module. We extract the bounding boxes from the\nsegmentation module and send the refined version as prompts to the SAM module.\nWe evaluate our model on a classic DG dataset and achieve competitive results\ncompared to other state-of-the-art DG methods. Furthermore, We conducted a\nseries of ablation experiments to prove the effectiveness of the proposed\nmethod. The code is publicly available at https://github.com/SARIHUST/SAMMed.\n","authors":["Hanhui Wang","Huaize Ye","Yi Xia","Xueyan Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.02076v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00926v2","updated":"2024-01-04T05:18:15Z","published":"2024-01-01T16:28:30Z","title":"Accurate Leukocyte Detection Based on Deformable-DETR and Multi-Level\n  Feature Fusion for Aiding Diagnosis of Blood Diseases","summary":"  In standard hospital blood tests, the traditional process requires doctors to\nmanually isolate leukocytes from microscopic images of patients' blood using\nmicroscopes. These isolated leukocytes are then categorized via automatic\nleukocyte classifiers to determine the proportion and volume of different types\nof leukocytes present in the blood samples, aiding disease diagnosis. This\nmethodology is not only time-consuming and labor-intensive, but it also has a\nhigh propensity for errors due to factors such as image quality and\nenvironmental conditions, which could potentially lead to incorrect subsequent\nclassifications and misdiagnosis. To address these issues, this paper proposes\nan innovative method of leukocyte detection: the Multi-level Feature Fusion and\nDeformable Self-attention DETR (MFDS-DETR). To tackle the issue of leukocyte\nscale disparity, we designed the High-level Screening-feature Fusion Pyramid\n(HS-FPN), enabling multi-level fusion. This model uses high-level features as\nweights to filter low-level feature information via a channel attention module\nand then merges the screened information with the high-level features, thus\nenhancing the model's feature expression capability. Further, we address the\nissue of leukocyte feature scarcity by incorporating a multi-scale deformable\nself-attention module in the encoder and using the self-attention and\ncross-deformable attention mechanisms in the decoder, which aids in the\nextraction of the global features of the leukocyte feature maps. The\neffectiveness, superiority, and generalizability of the proposed MFDS-DETR\nmethod are confirmed through comparisons with other cutting-edge leukocyte\ndetection models using the private WBCDD, public LISC and BCCD datasets. Our\nsource code and private WBCCD dataset are available at\nhttps://github.com/JustlfC03/MFDS-DETR.\n","authors":["Yifei Chen","Chenyan Zhang","Ben Chen","Yiyu Huang","Yifei Sun","Changmiao Wang","Xianjun Fu","Yuxing Dai","Feiwei Qin","Yong Peng","Yu Gao"],"pdf_url":"https://arxiv.org/pdf/2401.00926v2.pdf","comment":"15 pages, 11 figures, accept Computers in Biology and Medicine 2024"},{"id":"http://arxiv.org/abs/2307.06942v2","updated":"2024-01-04T05:00:34Z","published":"2023-07-13T17:58:32Z","title":"InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding\n  and Generation","summary":"  This paper introduces InternVid, a large-scale video-centric multimodal\ndataset that enables learning powerful and transferable video-text\nrepresentations for multimodal understanding and generation. The InternVid\ndataset contains over 7 million videos lasting nearly 760K hours, yielding 234M\nvideo clips accompanied by detailed descriptions of total 4.1B words. Our core\ncontribution is to develop a scalable approach to autonomously build a\nhigh-quality video-text dataset with large language models (LLM), thereby\nshowcasing its efficacy in learning video-language representation at scale.\nSpecifically, we utilize a multi-scale approach to generate video-related\ndescriptions. Furthermore, we introduce ViCLIP, a video-text representation\nlearning model based on ViT-L. Learned on InternVid via contrastive learning,\nthis model demonstrates leading zero-shot action recognition and competitive\nvideo retrieval performance. Beyond basic video understanding tasks like\nrecognition and retrieval, our dataset and model have broad applications. They\nare particularly beneficial for generating interleaved video-text data for\nlearning a video-centric dialogue system, advancing video-to-text and\ntext-to-video generation research. These proposed resources provide a tool for\nresearchers and practitioners interested in multimodal video understanding and\ngeneration.\n","authors":["Yi Wang","Yinan He","Yizhuo Li","Kunchang Li","Jiashuo Yu","Xin Ma","Xinhao Li","Guo Chen","Xinyuan Chen","Yaohui Wang","Conghui He","Ping Luo","Ziwei Liu","Yali Wang","Limin Wang","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2307.06942v2.pdf","comment":"Data and Code:\n  https://github.com/OpenGVLab/InternVideo/tree/main/Data/InternVid"},{"id":"http://arxiv.org/abs/2401.01642v2","updated":"2024-01-04T03:23:42Z","published":"2024-01-03T09:37:03Z","title":"BLADE: Box-Level Supervised Amodal Segmentation through Directed\n  Expansion","summary":"  Perceiving the complete shape of occluded objects is essential for human and\nmachine intelligence. While the amodal segmentation task is to predict the\ncomplete mask of partially occluded objects, it is time-consuming and\nlabor-intensive to annotate the pixel-level ground truth amodal masks.\nBox-level supervised amodal segmentation addresses this challenge by relying\nsolely on ground truth bounding boxes and instance classes as supervision,\nthereby alleviating the need for exhaustive pixel-level annotations.\nNevertheless, current box-level methodologies encounter limitations in\ngenerating low-resolution masks and imprecise boundaries, failing to meet the\ndemands of practical real-world applications. We present a novel solution to\ntackle this problem by introducing a directed expansion approach from visible\nmasks to corresponding amodal masks. Our approach involves a hybrid end-to-end\nnetwork based on the overlapping region - the area where different instances\nintersect. Diverse segmentation strategies are applied for overlapping regions\nand non-overlapping regions according to distinct characteristics. To guide the\nexpansion of visible masks, we introduce an elaborately-designed connectivity\nloss for overlapping regions, which leverages correlations with visible masks\nand facilitates accurate amodal segmentation. Experiments are conducted on\nseveral challenging datasets and the results show that our proposed method can\noutperform existing state-of-the-art methods with large margins.\n","authors":["Zhaochen Liu","Zhixuan Li","Tingting Jiang"],"pdf_url":"https://arxiv.org/pdf/2401.01642v2.pdf","comment":"Accepted to AAAI 2024"},{"id":"http://arxiv.org/abs/2401.01839v2","updated":"2024-01-04T03:23:04Z","published":"2024-01-03T17:11:27Z","title":"Frequency Domain Modality-invariant Feature Learning for\n  Visible-infrared Person Re-Identification","summary":"  Visible-infrared person re-identification (VI-ReID) is challenging due to the\nsignificant cross-modality discrepancies between visible and infrared images.\nWhile existing methods have focused on designing complex network architectures\nor using metric learning constraints to learn modality-invariant features, they\noften overlook which specific component of the image causes the modality\ndiscrepancy problem. In this paper, we first reveal that the difference in the\namplitude component of visible and infrared images is the primary factor that\ncauses the modality discrepancy and further propose a novel Frequency Domain\nmodality-invariant feature learning framework (FDMNet) to reduce modality\ndiscrepancy from the frequency domain perspective. Our framework introduces two\nnovel modules, namely the Instance-Adaptive Amplitude Filter (IAF) module and\nthe Phrase-Preserving Normalization (PPNorm) module, to enhance the\nmodality-invariant amplitude component and suppress the modality-specific\ncomponent at both the image- and feature-levels. Extensive experimental results\non two standard benchmarks, SYSU-MM01 and RegDB, demonstrate the superior\nperformance of our FDMNet against state-of-the-art methods.\n","authors":["Yulin Li","Tianzhu Zhang","Yongdong Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.01839v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2401.02044v1","updated":"2024-01-04T03:09:39Z","published":"2024-01-04T03:09:39Z","title":"Generalizable vision-language pre-training for annotation-free pathology\n  localization","summary":"  Locating pathologies automatically from medical images aids the understanding\nof the emergence and progression of diseases, and such an ability can\nsignificantly benefit clinical diagnostics. However, existing deep learning\nmodels heavily rely on expert annotations and lack generalization capabilities\nin open clinical environments. In this study, we present a generalizable\nvision-language pre-training model for Annotation-Free pathology Localization\n(AFLoc). The core strength of AFLoc lies in its image annotation-free\nmulti-level semantic structure-based contrastive learning, which\ncomprehensively aligns multi-granularity medical concepts from reports with\nabundant image features, to adapt to the diverse expressions of observed and\nemerging unseen pathologies. We conducted extensive experimental validation\nacross 4 distinct external datasets, encompassing 11 types of chest\npathologies, to verify its generalization ability. The results demonstrate that\nAFLoc surpasses 6 state-of-the-art methods and even outperforms the human\nbenchmark in locating 5 different pathologies, underscoring its suitability for\ncomplex clinical environments.\n","authors":["Hao Yang","Hong-Yu Zhou","Cheng Li","Weijian Huang","Jiarun Liu","Shanshan Wang"],"pdf_url":"https://arxiv.org/pdf/2401.02044v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.17432v2","updated":"2024-01-04T03:08:53Z","published":"2023-12-29T01:56:17Z","title":"Video Understanding with Large Language Models: A Survey","summary":"  With the burgeoning growth of online video platforms and the escalating\nvolume of video content, the demand for proficient video understanding tools\nhas intensified markedly. Given the remarkable capabilities of Large Language\nModels (LLMs) in language and multimodal tasks, this survey provides a detailed\noverview of the recent advancements in video understanding harnessing the power\nof LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly\nadvanced, particularly their ability for open-ended spatial-temporal reasoning\ncombined with commonsense knowledge, suggesting a promising path for future\nvideo understanding. We examine the unique characteristics and capabilities of\nVid-LLMs, categorizing the approaches into four main types: LLM-based Video\nAgents, Vid-LLMs Pretraining, Vid-LLMs Instruction Tuning, and Hybrid Methods.\nFurthermore, this survey presents a comprehensive study of the tasks, datasets,\nand evaluation methodologies for Vid-LLMs. Additionally, it explores the\nexpansive applications of Vid-LLMs across various domains, highlighting their\nremarkable scalability and versatility in real-world video understanding\nchallenges. Finally, it summarizes the limitations of existing Vid-LLMs and\noutlines directions for future research. For more information, readers are\nrecommended to visit the repository at\nhttps://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding.\n","authors":["Yunlong Tang","Jing Bi","Siting Xu","Luchuan Song","Susan Liang","Teng Wang","Daoan Zhang","Jie An","Jingyang Lin","Rongyi Zhu","Ali Vosoughi","Chao Huang","Zeliang Zhang","Feng Zheng","Jianguo Zhang","Ping Luo","Jiebo Luo","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2312.17432v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.02742v2","updated":"2024-01-04T03:08:36Z","published":"2023-09-06T05:56:30Z","title":"MLN-net: A multi-source medical image segmentation method for clustered\n  microcalcifications using multiple layer normalization","summary":"  Accurate segmentation of clustered microcalcifications in mammography is\ncrucial for the diagnosis and treatment of breast cancer. Despite exhibiting\nexpert-level accuracy, recent deep learning advancements in medical image\nsegmentation provide insufficient contribution to practical applications, due\nto the domain shift resulting from differences in patient postures, individual\ngland density, and imaging modalities of mammography etc. In this paper, a\nnovel framework named MLN-net, which can accurately segment multi-source images\nusing only single source images, is proposed for clustered microcalcification\nsegmentation. We first propose a source domain image augmentation method to\ngenerate multi-source images, leading to improved generalization. And a\nstructure of multiple layer normalization (LN) layers is used to construct the\nsegmentation network, which can be found efficient for clustered\nmicrocalcification segmentation in different domains. Additionally, a branch\nselection strategy is designed for measuring the similarity of the source\ndomain data and the target domain data. To validate the proposed MLN-net,\nextensive analyses including ablation experiments are performed, comparison of\n12 baseline methods. Extensive experiments validate the effectiveness of\nMLN-net in segmenting clustered microcalcifications from different domains and\nthe its segmentation accuracy surpasses state-of-the-art methods. Code will be\navailable at https://github.com/yezanting/MLN-NET-VERSON1.\n","authors":["Ke Wang","Zanting Ye","Xiang Xie","Haidong Cui","Tao Chen","Banteng Liu"],"pdf_url":"https://arxiv.org/pdf/2309.02742v2.pdf","comment":"17 pages, 9 figures, 3 tables"},{"id":"http://arxiv.org/abs/2308.04074v2","updated":"2024-01-04T03:04:44Z","published":"2023-08-08T06:16:37Z","title":"Exploiting Spatial-Temporal Context for Interacting Hand Reconstruction\n  on Monocular RGB Video","summary":"  Reconstructing interacting hands from monocular RGB data is a challenging\ntask, as it involves many interfering factors, e.g. self- and mutual occlusion\nand similar textures. Previous works only leverage information from a single\nRGB image without modeling their physically plausible relation, which leads to\ninferior reconstruction results. In this work, we are dedicated to explicitly\nexploiting spatial-temporal information to achieve better interacting hand\nreconstruction. On one hand, we leverage temporal context to complement\ninsufficient information provided by the single frame, and design a novel\ntemporal framework with a temporal constraint for interacting hand motion\nsmoothness. On the other hand, we further propose an interpenetration detection\nmodule to produce kinetically plausible interacting hands without physical\ncollisions. Extensive experiments are performed to validate the effectiveness\nof our proposed framework, which achieves new state-of-the-art performance on\npublic benchmarks.\n","authors":["Weichao Zhao","Hezhen Hu","Wengang Zhou","Li li","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2308.04074v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2401.02041v1","updated":"2024-01-04T02:56:50Z","published":"2024-01-04T02:56:50Z","title":"Efficient Cloud-edge Collaborative Inference for Object\n  Re-identification","summary":"  Current object re-identification (ReID) system follows the centralized\nprocessing paradigm, i.e., all computations are conducted in the cloud server\nand edge devices are only used to capture and send images. As the number of\nvideos experiences a rapid escalation, this paradigm has become impractical due\nto the finite computational resources. In such a scenario, the ReID system\nshould be converted to fit in the cloud-edge collaborative processing paradigm,\nwhich is crucial to boost the scalability and practicality of ReID systems.\nHowever, current relevant work lacks research on this issue, making it\nchallenging for ReID methods to be adapted effectively. Therefore, we pioneer a\ncloud-edge collaborative inference framework for ReID systems and particularly\npropose a distribution-aware correlation modeling network (DaCM) to make the\ndesired image return to the cloud server as soon as possible via learning to\nmodel the spatial-temporal correlations among instances. DaCM embeds the\nspatial-temporal correlations implicitly included in the timestamps into a\ngraph structure, and it can be applied in the cloud to regulate the size of the\nupload window and on the edge device to adjust the sequence of images,\nrespectively. Traditional ReID methods can be combined with DaCM seamlessly,\nenabling their application within our proposed edge-cloud collaborative\nframework. Extensive experiments demonstrate that our method obviously reduces\ntransmission overhead and significantly improves performance. We will release\nour code and model.\n","authors":["Chuanming Wang","Yuxin Yang","Mengshi Qi","Huadong Ma"],"pdf_url":"https://arxiv.org/pdf/2401.02041v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02032v1","updated":"2024-01-04T02:20:54Z","published":"2024-01-04T02:20:54Z","title":"DiffusionEdge: Diffusion Probabilistic Model for Crisp Edge Detection","summary":"  Limited by the encoder-decoder architecture, learning-based edge detectors\nusually have difficulty predicting edge maps that satisfy both correctness and\ncrispness. With the recent success of the diffusion probabilistic model (DPM),\nwe found it is especially suitable for accurate and crisp edge detection since\nthe denoising process is directly applied to the original image size.\nTherefore, we propose the first diffusion model for the task of general edge\ndetection, which we call DiffusionEdge. To avoid expensive computational\nresources while retaining the final performance, we apply DPM in the latent\nspace and enable the classic cross-entropy loss which is uncertainty-aware in\npixel level to directly optimize the parameters in latent space in a\ndistillation manner. We also adopt a decoupled architecture to speed up the\ndenoising process and propose a corresponding adaptive Fourier filter to adjust\nthe latent features of specific frequencies. With all the technical designs,\nDiffusionEdge can be stably trained with limited resources, predicting crisp\nand accurate edge maps with much fewer augmentation strategies. Extensive\nexperiments on four edge detection benchmarks demonstrate the superiority of\nDiffusionEdge both in correctness and crispness. On the NYUDv2 dataset,\ncompared to the second best, we increase the ODS, OIS (without post-processing)\nand AC by 30.2%, 28.1% and 65.1%, respectively. Code:\nhttps://github.com/GuHuangAI/DiffusionEdge.\n","authors":["Yunfan Ye","Kai Xu","Yuhang Huang","Renjiao Yi","Zhiping Cai"],"pdf_url":"https://arxiv.org/pdf/2401.02032v1.pdf","comment":"AAAI 2024"},{"id":"http://arxiv.org/abs/2401.02031v1","updated":"2024-01-04T02:15:09Z","published":"2024-01-04T02:15:09Z","title":"Spy-Watermark: Robust Invisible Watermarking for Backdoor Attack","summary":"  Backdoor attack aims to deceive a victim model when facing backdoor instances\nwhile maintaining its performance on benign data. Current methods use manual\npatterns or special perturbations as triggers, while they often overlook the\nrobustness against data corruption, making backdoor attacks easy to defend in\npractice. To address this issue, we propose a novel backdoor attack method\nnamed Spy-Watermark, which remains effective when facing data collapse and\nbackdoor defense. Therein, we introduce a learnable watermark embedded in the\nlatent domain of images, serving as the trigger. Then, we search for a\nwatermark that can withstand collapse during image decoding, cooperating with\nseveral anti-collapse operations to further enhance the resilience of our\ntrigger against data corruption. Extensive experiments are conducted on\nCIFAR10, GTSRB, and ImageNet datasets, demonstrating that Spy-Watermark\novertakes ten state-of-the-art methods in terms of robustness and stealthiness.\n","authors":["Ruofei Wang","Renjie Wan","Zongyu Guo","Qing Guo","Rui Huang"],"pdf_url":"https://arxiv.org/pdf/2401.02031v1.pdf","comment":"Accepted by ICASSP2024"},{"id":"http://arxiv.org/abs/2305.06355v2","updated":"2024-01-04T02:06:07Z","published":"2023-05-10T17:59:04Z","title":"VideoChat: Chat-Centric Video Understanding","summary":"  In this paper, we initiate an attempt of developing an end-to-end\nchat-centric video understanding system, coined as VideoChat. It integrates\nvideo foundation models and large language models via a learnable neural\ninterface, excelling in spatiotemporal reasoning, event localization, and\ncausal relationship inference. To instructively tune this system, we build a\nvideo-centric instruction dataset, composed of thousands of videos associated\nwith detailed descriptions and conversations. This dataset emphasizes\nspatiotemporal reasoning and captures causal relationships, providing a\nvaluable asset for training our chat-centric video understanding system.\nPreliminary qualitative experiments demonstrate the potential of our system\nacross a broad spectrum of video applications, which could serve as a simple\nprototype system for future research on chat-centric video understanding.\nAccess our code and data at https://github.com/OpenGVLab/Ask-Anything\n","authors":["KunChang Li","Yinan He","Yi Wang","Yizhuo Li","Wenhai Wang","Ping Luo","Yali Wang","Limin Wang","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2305.06355v2.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2306.00876v2","updated":"2024-01-04T02:02:51Z","published":"2023-06-01T16:37:50Z","title":"Quantifying Deep Learning Model Uncertainty in Conformal Prediction","summary":"  Precise estimation of predictive uncertainty in deep neural networks is a\ncritical requirement for reliable decision-making in machine learning and\nstatistical modeling, particularly in the context of medical AI. Conformal\nPrediction (CP) has emerged as a promising framework for representing the model\nuncertainty by providing well-calibrated confidence levels for individual\npredictions. However, the quantification of model uncertainty in conformal\nprediction remains an active research area, yet to be fully addressed. In this\npaper, we explore state-of-the-art CP methodologies and their theoretical\nfoundations. We propose a probabilistic approach in quantifying the model\nuncertainty derived from the produced prediction sets in conformal prediction\nand provide certified boundaries for the computed uncertainty. By doing so, we\nallow model uncertainty measured by CP to be compared by other uncertainty\nquantification methods such as Bayesian (e.g., MC-Dropout and DeepEnsemble) and\nEvidential approaches.\n","authors":["Hamed Karimi","Reza Samavi"],"pdf_url":"https://arxiv.org/pdf/2306.00876v2.pdf","comment":"Accepted in AAAI Second Symposium on Human Partnership with Medical\n  AI: Design, Operationalization, and Ethics"},{"id":"http://arxiv.org/abs/2401.02020v1","updated":"2024-01-04T01:33:33Z","published":"2024-01-04T01:33:33Z","title":"Spikformer V2: Join the High Accuracy Club on ImageNet with an SNN\n  Ticket","summary":"  Spiking Neural Networks (SNNs), known for their biologically plausible\narchitecture, face the challenge of limited performance. The self-attention\nmechanism, which is the cornerstone of the high-performance Transformer and\nalso a biologically inspired structure, is absent in existing SNNs. To this\nend, we explore the potential of leveraging both self-attention capability and\nbiological properties of SNNs, and propose a novel Spiking Self-Attention (SSA)\nand Spiking Transformer (Spikformer). The SSA mechanism eliminates the need for\nsoftmax and captures the sparse visual feature employing spike-based Query,\nKey, and Value. This sparse computation without multiplication makes SSA\nefficient and energy-saving. Further, we develop a Spiking Convolutional Stem\n(SCS) with supplementary convolutional layers to enhance the architecture of\nSpikformer. The Spikformer enhanced with the SCS is referred to as Spikformer\nV2. To train larger and deeper Spikformer V2, we introduce a pioneering\nexploration of Self-Supervised Learning (SSL) within the SNN. Specifically, we\npre-train Spikformer V2 with masking and reconstruction style inspired by the\nmainstream self-supervised Transformer, and then finetune the Spikformer V2 on\nthe image classification on ImageNet. Extensive experiments show that\nSpikformer V2 outperforms other previous surrogate training and ANN2SNN\nmethods. An 8-layer Spikformer V2 achieves an accuracy of 80.38% using 4 time\nsteps, and after SSL, a 172M 16-layer Spikformer V2 reaches an accuracy of\n81.10% with just 1 time step. To the best of our knowledge, this is the first\ntime that the SNN achieves 80+% accuracy on ImageNet. The code will be\navailable at Spikformer V2.\n","authors":["Zhaokun Zhou","Kaiwei Che","Wei Fang","Keyu Tian","Yuesheng Zhu","Shuicheng Yan","Yonghong Tian","Li Yuan"],"pdf_url":"https://arxiv.org/pdf/2401.02020v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17599v3","updated":"2024-01-04T01:30:50Z","published":"2023-03-30T17:59:25Z","title":"Zero-Shot Video Editing Using Off-The-Shelf Image Diffusion Models","summary":"  Large-scale text-to-image diffusion models achieve unprecedented success in\nimage generation and editing. However, how to extend such success to video\nediting is unclear. Recent initial attempts at video editing require\nsignificant text-to-video data and computation resources for training, which is\noften not accessible. In this work, we propose vid2vid-zero, a simple yet\neffective method for zero-shot video editing. Our vid2vid-zero leverages\noff-the-shelf image diffusion models, and doesn't require training on any\nvideo. At the core of our method is a null-text inversion module for\ntext-to-video alignment, a cross-frame modeling module for temporal\nconsistency, and a spatial regularization module for fidelity to the original\nvideo. Without any training, we leverage the dynamic nature of the attention\nmechanism to enable bi-directional temporal modeling at test time. Experiments\nand analyses show promising results in editing attributes, subjects, places,\netc., in real-world videos. Code is made available at\n\\url{https://github.com/baaivision/vid2vid-zero}.\n","authors":["Wen Wang","Yan Jiang","Kangyang Xie","Zide Liu","Hao Chen","Yue Cao","Xinlong Wang","Chunhua Shen"],"pdf_url":"https://arxiv.org/pdf/2303.17599v3.pdf","comment":"Add customized video editing. Under Review"},{"id":"http://arxiv.org/abs/2401.02015v1","updated":"2024-01-04T01:10:56Z","published":"2024-01-04T01:10:56Z","title":"Improving Diffusion-Based Image Synthesis with Context Prediction","summary":"  Diffusion models are a new class of generative models, and have dramatically\npromoted image generation with unprecedented quality and diversity. Existing\ndiffusion models mainly try to reconstruct input image from a corrupted one\nwith a pixel-wise or feature-wise constraint along spatial axes. However, such\npoint-based reconstruction may fail to make each predicted pixel/feature fully\npreserve its neighborhood context, impairing diffusion-based image synthesis.\nAs a powerful source of automatic supervisory signal, context has been well\nstudied for learning representations. Inspired by this, we for the first time\npropose ConPreDiff to improve diffusion-based image synthesis with context\nprediction. We explicitly reinforce each point to predict its neighborhood\ncontext (i.e., multi-stride features/tokens/pixels) with a context decoder at\nthe end of diffusion denoising blocks in training stage, and remove the decoder\nfor inference. In this way, each point can better reconstruct itself by\npreserving its semantic connections with neighborhood context. This new\nparadigm of ConPreDiff can generalize to arbitrary discrete and continuous\ndiffusion backbones without introducing extra parameters in sampling procedure.\nExtensive experiments are conducted on unconditional image generation,\ntext-to-image generation and image inpainting tasks. Our ConPreDiff\nconsistently outperforms previous methods and achieves a new SOTA text-to-image\ngeneration results on MS-COCO, with a zero-shot FID score of 6.21.\n","authors":["Ling Yang","Jingwei Liu","Shenda Hong","Zhilong Zhang","Zhilin Huang","Zheming Cai","Wentao Zhang","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2401.02015v1.pdf","comment":"Accepted by NeurIPS 2023"},{"id":"http://arxiv.org/abs/2309.09431v4","updated":"2024-01-04T01:05:16Z","published":"2023-09-18T02:05:52Z","title":"FactoFormer: Factorized Hyperspectral Transformers with Self-Supervised\n  Pretraining","summary":"  Hyperspectral images (HSIs) contain rich spectral and spatial information.\nMotivated by the success of transformers in the field of natural language\nprocessing and computer vision where they have shown the ability to learn long\nrange dependencies within input data, recent research has focused on using\ntransformers for HSIs. However, current state-of-the-art hyperspectral\ntransformers only tokenize the input HSI sample along the spectral dimension,\nresulting in the under-utilization of spatial information. Moreover,\ntransformers are known to be data-hungry and their performance relies heavily\non large-scale pretraining, which is challenging due to limited annotated\nhyperspectral data. Therefore, the full potential of HSI transformers has not\nbeen fully realized. To overcome these limitations, we propose a novel\nfactorized spectral-spatial transformer that incorporates factorized\nself-supervised pretraining procedures, leading to significant improvements in\nperformance. The factorization of the inputs allows the spectral and spatial\ntransformers to better capture the interactions within the hyperspectral data\ncubes. Inspired by masked image modeling pretraining, we also devise efficient\nmasking strategies for pretraining each of the spectral and spatial\ntransformers. We conduct experiments on six publicly available datasets for HSI\nclassification task and demonstrate that our model achieves state-of-the-art\nperformance in all the datasets. The code for our model will be made available\nat https://github.com/csiro-robotics/factoformer.\n","authors":["Shaheer Mohamed","Maryam Haghighat","Tharindu Fernando","Sridha Sridharan","Clinton Fookes","Peyman Moghadam"],"pdf_url":"https://arxiv.org/pdf/2309.09431v4.pdf","comment":"Accepted to IEEE Transactions on Geoscience and Remote Sensing in\n  December 2023"},{"id":"http://arxiv.org/abs/2306.17019v2","updated":"2024-01-04T22:54:18Z","published":"2023-06-29T15:11:20Z","title":"Histopathology Slide Indexing and Search: Are We There Yet?","summary":"  The search and retrieval of digital histopathology slides is an important\ntask that has yet to be solved. In this case study, we investigate the clinical\nreadiness of three state-of-the-art histopathology slide search engines,\nYottixel, SISH, and RetCCL, on three patients with solid tumors. We provide a\nqualitative assessment of each model's performance in providing retrieval\nresults that are reliable and useful to pathologists. We found that all three\nimage search engines fail to produce consistently reliable results and have\ndifficulties in capturing granular and subtle features of malignancy, limiting\ntheir diagnostic accuracy. Based on our findings, we also propose a minimal set\nof requirements to further advance the development of accurate and reliable\nhistopathology image search engines for successful clinical adoption.\n","authors":["Helen H. Shang","Mohammad Sadegh Nasr","Jai Prakash Veerla","Parisa Boodaghi Malidarreh","MD Jillur Rahman Saurav","Amir Hajighasemi","Manfred Huber","Chace Moleta","Jitin Makker","Jacob M. Luber"],"pdf_url":"https://arxiv.org/pdf/2306.17019v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02565v1","updated":"2024-01-04T22:49:15Z","published":"2024-01-04T22:49:15Z","title":"Vulnerabilities Unveiled: Adversarially Attacking a Multimodal Vision\n  Langauge Model for Pathology Imaging","summary":"  In the dynamic landscape of medical artificial intelligence, this study\nexplores the vulnerabilities of the Pathology Language-Image Pretraining (PLIP)\nmodel, a Vision Language Foundation model, under targeted adversarial\nconditions. Leveraging the Kather Colon dataset with 7,180 H&E images across\nnine tissue types, our investigation employs Projected Gradient Descent (PGD)\nadversarial attacks to intentionally induce misclassifications. The outcomes\nreveal a 100% success rate in manipulating PLIP's predictions, underscoring its\nsusceptibility to adversarial perturbations. The qualitative analysis of\nadversarial examples delves into the interpretability challenges, shedding\nlight on nuanced changes in predictions induced by adversarial manipulations.\nThese findings contribute crucial insights into the interpretability, domain\nadaptation, and trustworthiness of Vision Language Models in medical imaging.\nThe study emphasizes the pressing need for robust defenses to ensure the\nreliability of AI models.\n","authors":["Jai Prakash Veerla","Poojitha Thota","Partha Sai Guttikonda","Shirin Nilizadeh","Jacob M. Luber"],"pdf_url":"https://arxiv.org/pdf/2401.02565v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02564v1","updated":"2024-01-04T22:37:56Z","published":"2024-01-04T22:37:56Z","title":"Predicting Future States with Spatial Point Processes in Single Molecule\n  Resolution Spatial Transcriptomics","summary":"  In this paper, we introduce a pipeline based on Random Forest Regression to\npredict the future distribution of cells that are expressed by the Sog-D gene\n(active cells) in both the Anterior to posterior (AP) and the Dorsal to Ventral\n(DV) axis of the Drosophila in embryogenesis process. This method provides\ninsights about how cells and living organisms control gene expression in super\nresolution whole embryo spatial transcriptomics imaging at sub cellular, single\nmolecule resolution. A Random Forest Regression model was used to predict the\nnext stage active distribution based on the previous one. To achieve this goal,\nwe leveraged temporally resolved, spatial point processes by including Ripley's\nK-function in conjunction with the cell's state in each stage of embryogenesis,\nand found average predictive accuracy of active cell distribution. This tool is\nanalogous to RNA Velocity for spatially resolved developmental biology, from\none data point we can predict future spatially resolved gene expression using\nfeatures from the spatial point processes.\n","authors":["Parisa Boodaghi Malidarreh","Biraaj Rout","Mohammad Sadegh Nasr","Priyanshi Borad","Jillur Rahman Saurav","Jai Prakash Veerla","Kelli Fenelon","Theodora Koromila","Jacob M. Luber"],"pdf_url":"https://arxiv.org/pdf/2401.02564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.09310v3","updated":"2024-01-04T22:05:15Z","published":"2022-11-17T02:58:55Z","title":"Language-Assisted Deep Learning for Autistic Behaviors Recognition","summary":"  Correctly recognizing the behaviors of children with Autism Spectrum Disorder\n(ASD) is of vital importance for the diagnosis of Autism and timely early\nintervention. However, the observation and recording during the treatment from\nthe parents of autistic children may not be accurate and objective. In such\ncases, automatic recognition systems based on computer vision and machine\nlearning (in particular deep learning) technology can alleviate this issue to a\nlarge extent. Existing human action recognition models can now achieve\npersuasive performance on challenging activity datasets, e.g. daily activity,\nand sports activity. However, problem behaviors in children with ASD are very\ndifferent from these general activities, and recognizing these problem\nbehaviors via computer vision is less studied. In this paper, we first evaluate\na strong baseline for action recognition, i.e. Video Swin Transformer, on two\nautism behaviors datasets (SSBD and ESBD) and show that it can achieve high\naccuracy and outperform the previous methods by a large margin, demonstrating\nthe feasibility of vision-based problem behaviors recognition. Moreover, we\npropose language-assisted training to further enhance the action recognition\nperformance. Specifically, we develop a two-branch multimodal deep learning\nframework by incorporating the \"freely available\" language description for each\ntype of problem behavior. Experimental results demonstrate that incorporating\nadditional language supervision can bring an obvious performance boost for the\nautism problem behaviors recognition task as compared to using the video\ninformation only (i.e. 3.49% improvement on ESBD and 1.46% on SSBD).\n","authors":["Andong Deng","Taojiannan Yang","Chen Chen","Qian Chen","Leslie Neely","Sakiko Oyama"],"pdf_url":"https://arxiv.org/pdf/2211.09310v3.pdf","comment":"Smart Health Journal"},{"id":"http://arxiv.org/abs/2401.02550v1","updated":"2024-01-04T21:47:56Z","published":"2024-01-04T21:47:56Z","title":"OptFlow: Fast Optimization-based Scene Flow Estimation without\n  Supervision","summary":"  Scene flow estimation is a crucial component in the development of autonomous\ndriving and 3D robotics, providing valuable information for environment\nperception and navigation. Despite the advantages of learning-based scene flow\nestimation techniques, their domain specificity and limited generalizability\nacross varied scenarios pose challenges. In contrast, non-learning\noptimization-based methods, incorporating robust priors or regularization,\noffer competitive scene flow estimation performance, require no training, and\nshow extensive applicability across datasets, but suffer from lengthy inference\ntimes. In this paper, we present OptFlow, a fast optimization-based scene flow\nestimation method. Without relying on learning or any labeled datasets, OptFlow\nachieves state-of-the-art performance for scene flow estimation on popular\nautonomous driving benchmarks. It integrates a local correlation weight matrix\nfor correspondence matching, an adaptive correspondence threshold limit for\nnearest-neighbor search, and graph prior rigidity constraints, resulting in\nexpedited convergence and improved point correspondence identification.\nMoreover, we demonstrate how integrating a point cloud registration function\nwithin our objective function bolsters accuracy and differentiates between\nstatic and dynamic points without relying on external odometry data.\nConsequently, OptFlow outperforms the baseline graph-prior method by\napproximately 20% and the Neural Scene Flow Prior method by 5%-7% in accuracy,\nall while offering the fastest inference time among all non-learning scene flow\nestimation methods.\n","authors":["Rahul Ahuja","Chris Baker","Wilko Schwarting"],"pdf_url":"https://arxiv.org/pdf/2401.02550v1.pdf","comment":"Accepted at the proceedings of the IEEE/CVF Winter Conference on\n  Applications of Computer Vision (WACV), 2024"},{"id":"http://arxiv.org/abs/2310.03937v2","updated":"2024-01-04T21:39:25Z","published":"2023-10-05T23:00:27Z","title":"Diffusion Models as Masked Audio-Video Learners","summary":"  Over the past several years, the synchronization between audio and visual\nsignals has been leveraged to learn richer audio-visual representations. Aided\nby the large availability of unlabeled videos, many unsupervised training\nframeworks have demonstrated impressive results in various downstream audio and\nvideo tasks. Recently, Masked Audio-Video Learners (MAViL) has emerged as a\nstate-of-the-art audio-video pre-training framework. MAViL couples contrastive\nlearning with masked autoencoding to jointly reconstruct audio spectrograms and\nvideo frames by fusing information from both modalities. In this paper, we\nstudy the potential synergy between diffusion models and MAViL, seeking to\nderive mutual benefits from these two frameworks. The incorporation of\ndiffusion into MAViL, combined with various training efficiency methodologies\nthat include the utilization of a masking ratio curriculum and adaptive batch\nsizing, results in a notable 32% reduction in pre-training Floating-Point\nOperations (FLOPS) and an 18% decrease in pre-training wall clock time.\nCrucially, this enhanced efficiency does not compromise the model's performance\nin downstream audio-classification tasks when compared to MAViL's performance.\n","authors":["Elvis Nunez","Yanzi Jin","Mohammad Rastegari","Sachin Mehta","Maxwell Horton"],"pdf_url":"https://arxiv.org/pdf/2310.03937v2.pdf","comment":"Camera-ready version for the Machine Learning for Audio Workshop at\n  NeurIPS 2023"},{"id":"http://arxiv.org/abs/2202.02952v3","updated":"2024-01-04T21:34:33Z","published":"2022-02-07T05:29:16Z","title":"Supervision by Denoising for Medical Image Segmentation","summary":"  Learning-based image reconstruction models, such as those based on the U-Net,\nrequire a large set of labeled images if good generalization is to be\nguaranteed. In some imaging domains, however, labeled data with pixel- or\nvoxel-level label accuracy are scarce due to the cost of acquiring them. This\nproblem is exacerbated further in domains like medical imaging, where there is\nno single ground truth label, resulting in large amounts of repeat variability\nin the labels. Therefore, training reconstruction networks to generalize better\nby learning from both labeled and unlabeled examples (called semi-supervised\nlearning) is problem of practical and theoretical interest. However,\ntraditional semi-supervised learning methods for image reconstruction often\nnecessitate handcrafting a differentiable regularizer specific to some given\nimaging problem, which can be extremely time-consuming. In this work, we\npropose \"supervision by denoising\" (SUD), a framework that enables us to\nsupervise reconstruction models using their own denoised output as soft labels.\nSUD unifies stochastic averaging and spatial denoising techniques under a\nspatio-temporal denoising framework and alternates denoising and model weight\nupdate steps in an optimization framework for semi-supervision. As example\napplications, we apply SUD to two problems arising from biomedical imaging --\nanatomical brain reconstruction (3D) and cortical parcellation (2D) -- to\ndemonstrate a significant improvement in the image reconstructions over\nsupervised-only and stochastic averaging baselines.\n","authors":["Sean I. Young","Adrian V. Dalca","Enzo Ferrante","Polina Golland","Christopher A. Metzler","Bruce Fischl","Juan Eugenio Iglesias"],"pdf_url":"https://arxiv.org/pdf/2202.02952v3.pdf","comment":"To appear in the IEEE Transactions on Pattern Analysis and Machine\n  Intelligence"},{"id":"http://arxiv.org/abs/2401.02539v1","updated":"2024-01-04T21:02:39Z","published":"2024-01-04T21:02:39Z","title":"Robot-Assisted Deep Venous Thrombosis Ultrasound Examination using\n  Virtual Fixture","summary":"  Deep Venous Thrombosis (DVT) is a common vascular disease with blood clots\ninside deep veins, which may block blood flow or even cause a life-threatening\npulmonary embolism. A typical exam for DVT using ultrasound (US) imaging is by\npressing the target vein until its lumen is fully compressed. However, the\ncompression exam is highly operator-dependent. To alleviate intra- and\ninter-variations, we present a robotic US system with a novel hybrid force\nmotion control scheme ensuring position and force tracking accuracy, and soft\nlanding of the probe onto the target surface. In addition, a path-based virtual\nfixture is proposed to realize easy human-robot interaction for repeat\ncompression operation at the lesion location. To ensure the biometric\nmeasurements obtained in different examinations are comparable, the 6D scanning\npath is determined in a coarse-to-fine manner using both an external RGBD\ncamera and US images. The RGBD camera is first used to extract a rough scanning\npath on the object. Then, the segmented vascular lumen from US images are used\nto optimize the scanning path to ensure the visibility of the target object. To\ngenerate a continuous scan path for developing virtual fixtures, an arc-length\nbased path fitting model considering both position and orientation is proposed.\nFinally, the whole system is evaluated on a human-like arm phantom with an\nuneven surface.\n","authors":["Dianye Huang","Chenguang Yang","Mingchuan Zhou","Angelos Karlas","Nassir Navab","Zhongliang Jiang"],"pdf_url":"https://arxiv.org/pdf/2401.02539v1.pdf","comment":"Accepted Paper IEEE T-ASE"},{"id":"http://arxiv.org/abs/2401.02537v1","updated":"2024-01-04T20:57:25Z","published":"2024-01-04T20:57:25Z","title":"Using Singular Value Decomposition in a Convolutional Neural Network to\n  Improve Brain Tumor Segmentation Accuracy","summary":"  A brain tumor consists of cells showing abnormal brain growth. The area of\nthe brain tumor significantly affects choosing the type of treatment and\nfollowing the course of the disease during the treatment. At the same time,\npictures of Brain MRIs are accompanied by noise. Eliminating existing noises\ncan significantly impact the better segmentation and diagnosis of brain tumors.\nIn this work, we have tried using the analysis of eigenvalues. We have used the\nMSVD algorithm, reducing the image noise and then using the deep neural network\nto segment the tumor in the images. The proposed method's accuracy was\nincreased by 2.4% compared to using the original images. With Using the MSVD\nmethod, convergence speed has also increased, showing the proposed method's\neffectiveness\n","authors":["Pegah Ahadian","Maryam Babaei","Kourosh Parand"],"pdf_url":"https://arxiv.org/pdf/2401.02537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02536v1","updated":"2024-01-04T20:53:43Z","published":"2024-01-04T20:53:43Z","title":"Novel End-to-End Production-Ready Machine Learning Flow for\n  Nanolithography Modeling and Correction","summary":"  Optical lithography is the main enabler to semiconductor manufacturing. It\nrequires extensive processing to perform the Resolution Enhancement Techniques\n(RETs) required to transfer the design data to a working Integrated Circuits\n(ICs). The processing power and computational runtime for RETs tasks is ever\nincreasing due to the continuous reduction of the feature size and the\nexpansion of the chip area. State-of-the-art research sought Machine Learning\n(ML) technologies to reduce runtime and computational power, however they are\nstill not used in production yet. In this study, we analyze the reasons holding\nback ML computational lithography from being production ready and present a\nnovel highly scalable end-to-end flow that enables production ready ML-RET\ncorrection.\n","authors":["Mohamed S. E. Habib","Hossam A. H. Fahmy","Mohamed F. Abu-ElYazeed"],"pdf_url":"https://arxiv.org/pdf/2401.02536v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02526v1","updated":"2024-01-04T20:29:44Z","published":"2024-01-04T20:29:44Z","title":"Branched Variational Autoencoder Classifiers","summary":"  This paper introduces a modified variational autoencoder (VAEs) that contains\nan additional neural network branch. The resulting branched VAE (BVAE)\ncontributes a classification component based on the class labels to the total\nloss and therefore imparts categorical information to the latent\nrepresentation. As a result, the latent space distributions of the input\nclasses are separated and ordered, thereby enhancing the classification\naccuracy. The degree of improvement is quantified by numerical calculations\nemploying the benchmark MNIST dataset for both unrotated and rotated digits.\nThe proposed technique is then compared to and then incorporated into a VAE\nwith fixed output distributions. This procedure is found to yield improved\nperformance for a wide range of output distributions.\n","authors":["Ahmed Salah","David Yevick"],"pdf_url":"https://arxiv.org/pdf/2401.02526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02524v1","updated":"2024-01-04T20:23:51Z","published":"2024-01-04T20:23:51Z","title":"Comprehensive Exploration of Synthetic Data Generation: A Survey","summary":"  Recent years have witnessed a surge in the popularity of Machine Learning\n(ML), applied across diverse domains. However, progress is impeded by the\nscarcity of training data due to expensive acquisition and privacy legislation.\nSynthetic data emerges as a solution, but the abundance of released models and\nlimited overview literature pose challenges for decision-making. This work\nsurveys 417 Synthetic Data Generation (SDG) models over the last decade,\nproviding a comprehensive overview of model types, functionality, and\nimprovements. Common attributes are identified, leading to a classification and\ntrend analysis. The findings reveal increased model performance and complexity,\nwith neural network-based approaches prevailing, except for privacy-preserving\ndata generation. Computer vision dominates, with GANs as primary generative\nmodels, while diffusion models, transformers, and RNNs compete. Implications\nfrom our performance evaluation highlight the scarcity of common metrics and\ndatasets, making comparisons challenging. Additionally, the neglect of training\nand computational costs in literature necessitates attention in future\nresearch. This work serves as a guide for SDG model selection and identifies\ncrucial areas for future exploration.\n","authors":["André Bauer","Simon Trapp","Michael Stenger","Robert Leppich","Samuel Kounev","Mark Leznik","Kyle Chard","Ian Foster"],"pdf_url":"https://arxiv.org/pdf/2401.02524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.10159v3","updated":"2024-01-04T20:23:39Z","published":"2023-06-16T20:02:51Z","title":"Vision-Language Models can Identify Distracted Driver Behavior from\n  Naturalistic Videos","summary":"  Recognizing the activities causing distraction in real-world driving\nscenarios is critical for ensuring the safety and reliability of both drivers\nand pedestrians on the roadways. Conventional computer vision techniques are\ntypically data-intensive and require a large volume of annotated training data\nto detect and classify various distracted driving behaviors, thereby limiting\ntheir efficiency and scalability. We aim to develop a generalized framework\nthat showcases robust performance with access to limited or no annotated\ntraining data. Recently, vision-language models have offered large-scale\nvisual-textual pretraining that can be adapted to task-specific learning like\ndistracted driving activity recognition. Vision-language pretraining models,\nsuch as CLIP, have shown significant promise in learning natural\nlanguage-guided visual representations. This paper proposes a CLIP-based driver\nactivity recognition approach that identifies driver distraction from\nnaturalistic driving images and videos. CLIP's vision embedding offers\nzero-shot transfer and task-based finetuning, which can classify distracted\nactivities from driving video data. Our results show that this framework offers\nstate-of-the-art performance on zero-shot transfer and video-based CLIP for\npredicting the driver's state on two public datasets. We propose both\nframe-based and video-based frameworks developed on top of the CLIP's visual\nrepresentation for distracted driving detection and classification tasks and\nreport the results.\n","authors":["Md Zahid Hasan","Jiajing Chen","Jiyang Wang","Mohammed Shaiqur Rahman","Ameya Joshi","Senem Velipasalar","Chinmay Hegde","Anuj Sharma","Soumik Sarkar"],"pdf_url":"https://arxiv.org/pdf/2306.10159v3.pdf","comment":"18 pages, 10 figures"},{"id":"http://arxiv.org/abs/2401.02523v1","updated":"2024-01-04T20:17:25Z","published":"2024-01-04T20:17:25Z","title":"Image-based Deep Learning for Smart Digital Twins: a Review","summary":"  Smart Digital twins (SDTs) are being increasingly used to virtually replicate\nand predict the behaviors of complex physical systems through continual data\nassimilation enabling the optimization of the performance of these systems by\ncontrolling the actions of systems. Recently, deep learning (DL) models have\nsignificantly enhanced the capabilities of SDTs, particularly for tasks such as\npredictive maintenance, anomaly detection, and optimization. In many domains,\nincluding medicine, engineering, and education, SDTs use image data\n(image-based SDTs) to observe and learn system behaviors and control their\nbehaviors. This paper focuses on various approaches and associated challenges\nin developing image-based SDTs by continually assimilating image data from\nphysical systems. The paper also discusses the challenges involved in designing\nand implementing DL models for SDTs, including data acquisition, processing,\nand interpretation. In addition, insights into the future directions and\nopportunities for developing new image-based DL approaches to develop robust\nSDTs are provided. This includes the potential for using generative models for\ndata augmentation, developing multi-modal DL models, and exploring the\nintegration of DL with other technologies, including 5G, edge computing, and\nIoT. In this paper, we describe the image-based SDTs, which enable broader\nadoption of the digital twin DT paradigms across a broad spectrum of areas and\nthe development of new methods to improve the abilities of SDTs in replicating,\npredicting, and optimizing the behavior of complex systems.\n","authors":["Md Ruman Islam","Mahadevan Subramaniam","Pei-Chi Huang"],"pdf_url":"https://arxiv.org/pdf/2401.02523v1.pdf","comment":"12 pages, 2 figures, and 3 tables"},{"id":"http://arxiv.org/abs/2306.05411v2","updated":"2024-01-04T19:31:50Z","published":"2023-06-08T17:56:46Z","title":"R-MAE: Regions Meet Masked Autoencoders","summary":"  In this work, we explore regions as a potential visual analogue of words for\nself-supervised image representation learning. Inspired by Masked Autoencoding\n(MAE), a generative pre-training baseline, we propose masked region\nautoencoding to learn from groups of pixels or regions. Specifically, we design\nan architecture which efficiently addresses the one-to-many mapping between\nimages and regions, while being highly effective especially with high-quality\nregions. When integrated with MAE, our approach (R-MAE) demonstrates consistent\nimprovements across various pre-training datasets and downstream detection and\nsegmentation benchmarks, with negligible computational overheads. Beyond the\nquantitative evaluation, our analysis indicates the models pre-trained with\nmasked region autoencoding unlock the potential for interactive segmentation.\nThe code is provided at https://github.com/facebookresearch/r-mae.\n","authors":["Duy-Kien Nguyen","Vaibhav Aggarwal","Yanghao Li","Martin R. Oswald","Alexander Kirillov","Cees G. M. Snoek","Xinlei Chen"],"pdf_url":"https://arxiv.org/pdf/2306.05411v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02501v1","updated":"2024-01-04T19:25:00Z","published":"2024-01-04T19:25:00Z","title":"The cell signaling structure function","summary":"  Live cell microscopy captures 5-D $(x,y,z,channel,time)$ movies that display\npatterns of cellular motion and signaling dynamics. We present here an approach\nto finding spatiotemporal patterns of cell signaling dynamics in 5-D live cell\nmicroscopy movies unique in requiring no \\emph{a priori} knowledge of expected\npattern dynamics, and no training data. The proposed cell signaling structure\nfunction (SSF) is a Kolmogorov structure function that optimally measures cell\nsignaling state as nuclear intensity w.r.t. surrounding cytoplasm, a\nsignificant improvement compared to the current state-of-the-art cytonuclear\nratio. SSF kymographs store at each spatiotemporal cell centroid the SSF value,\nor a functional output such as velocity. Patterns of similarity are identified\nvia the metric normalized compression distance (NCD). The NCD is a reproducing\nkernel for a Hilbert space that represents the input SSF kymographs as points\nin a low dimensional embedding that optimally captures the pattern similarity\nidentified by the NCD throughout the space. The only parameter is the expected\ncell radii ($\\mu m$). A new formulation of the cluster structure function\noptimally estimates how meaningful an embedding from the RKHS representation.\nResults are presented quantifying the impact of ERK and AKT signaling between\ndifferent oncogenic mutations, and by the relation between ERK signaling and\ncellular velocity patterns for movies of 2-D monolayers of human breast\nepithelial (MCF10A) cells, 3-D MCF10A spheroids under optogenetic manipulation\nof ERK, and human induced pluripotent stem cells .\n","authors":["Layton Aho","Mark Winter","Marc DeCarlo","Agne Frismantiene","Yannick Blum","Paolo Armando Gagliardi","Olivier Pertz","Andrew R. Cohen"],"pdf_url":"https://arxiv.org/pdf/2401.02501v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.13301v4","updated":"2024-01-04T19:11:25Z","published":"2023-05-22T17:57:41Z","title":"Training Diffusion Models with Reinforcement Learning","summary":"  Diffusion models are a class of flexible generative models trained with an\napproximation to the log-likelihood objective. However, most use cases of\ndiffusion models are not concerned with likelihoods, but instead with\ndownstream objectives such as human-perceived image quality or drug\neffectiveness. In this paper, we investigate reinforcement learning methods for\ndirectly optimizing diffusion models for such objectives. We describe how\nposing denoising as a multi-step decision-making problem enables a class of\npolicy gradient algorithms, which we refer to as denoising diffusion policy\noptimization (DDPO), that are more effective than alternative reward-weighted\nlikelihood approaches. Empirically, DDPO is able to adapt text-to-image\ndiffusion models to objectives that are difficult to express via prompting,\nsuch as image compressibility, and those derived from human feedback, such as\naesthetic quality. Finally, we show that DDPO can improve prompt-image\nalignment using feedback from a vision-language model without the need for\nadditional data collection or human annotation. The project's website can be\nfound at http://rl-diffusion.github.io .\n","authors":["Kevin Black","Michael Janner","Yilun Du","Ilya Kostrikov","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/2305.13301v4.pdf","comment":"23 pages, 16 figures"},{"id":"http://arxiv.org/abs/2401.02473v1","updated":"2024-01-04T18:59:24Z","published":"2024-01-04T18:59:24Z","title":"VASE: Object-Centric Appearance and Shape Manipulation of Real Videos","summary":"  Recently, several works tackled the video editing task fostered by the\nsuccess of large-scale text-to-image generative models. However, most of these\nmethods holistically edit the frame using the text, exploiting the prior given\nby foundation diffusion models and focusing on improving the temporal\nconsistency across frames. In this work, we introduce a framework that is\nobject-centric and is designed to control both the object's appearance and,\nnotably, to execute precise and explicit structural modifications on the\nobject. We build our framework on a pre-trained image-conditioned diffusion\nmodel, integrate layers to handle the temporal dimension, and propose training\nstrategies and architectural modifications to enable shape control. We evaluate\nour method on the image-driven video editing task showing similar performance\nto the state-of-the-art, and showcasing novel shape-editing capabilities.\nFurther details, code and examples are available on our project page:\nhttps://helia95.github.io/vase-website/\n","authors":["Elia Peruzzo","Vidit Goel","Dejia Xu","Xingqian Xu","Yifan Jiang","Zhangyang Wang","Humphrey Shi","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2401.02473v1.pdf","comment":"Project Page https://helia95.github.io/vase-website/"},{"id":"http://arxiv.org/abs/2401.02329v1","updated":"2024-01-04T16:06:31Z","published":"2024-01-04T16:06:31Z","title":"Not all Minorities are Equal: Empty-Class-Aware Distillation for\n  Heterogeneous Federated Learning","summary":"  Data heterogeneity, characterized by disparities in local data distribution\nacross clients, poses a significant challenge in federated learning.\nSubstantial efforts have been devoted to addressing the heterogeneity in local\nlabel distribution. As minority classes suffer from worse accuracy due to\noverfitting on local imbalanced data, prior methods often incorporate\nclass-balanced learning techniques during local training. Despite the improved\nmean accuracy across all classes, we observe that empty classes-referring to\ncategories absent from a client's data distribution-are still not well\nrecognized. This paper introduces FedED, a novel approach in heterogeneous\nfederated learning that integrates both empty-class distillation and logit\nsuppression simultaneously. Specifically, empty-class distillation leverages\nknowledge distillation during local training on each client to retain essential\ninformation related to empty classes from the global model. Moreover, logit\nsuppression directly penalizes network logits for non-label classes,\neffectively addressing misclassifications in minority classes that may be\nbiased toward majority classes. Extensive experiments validate the efficacy of\nFedED, surpassing previous state-of-the-art methods across diverse datasets\nwith varying degrees of label distribution shift.\n","authors":["Kuangpu Guo","Yuhe Ding","Jian Liang","Ran He","Zilei Wang","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2401.02329v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2401.02143v1","updated":"2024-01-04T08:49:10Z","published":"2024-01-04T08:49:10Z","title":"Graph Neural Networks for Tabular Data Learning: A Survey with Taxonomy\n  and Directions","summary":"  In this survey, we dive into Tabular Data Learning (TDL) using Graph Neural\nNetworks (GNNs), a domain where deep learning-based approaches have\nincreasingly shown superior performance in both classification and regression\ntasks compared to traditional methods. The survey highlights a critical gap in\ndeep neural TDL methods: the underrepresentation of latent correlations among\ndata instances and feature values. GNNs, with their innate capability to model\nintricate relationships and interactions between diverse elements of tabular\ndata, have garnered significant interest and application across various TDL\ndomains. Our survey provides a systematic review of the methods involved in\ndesigning and implementing GNNs for TDL (GNN4TDL). It encompasses a detailed\ninvestigation into the foundational aspects and an overview of GNN-based TDL\nmethods, offering insights into their evolving landscape. We present a\ncomprehensive taxonomy focused on constructing graph structures and\nrepresentation learning within GNN-based TDL methods. In addition, the survey\nexamines various training plans, emphasizing the integration of auxiliary tasks\nto enhance the effectiveness of instance representations. A critical part of\nour discussion is dedicated to the practical application of GNNs across a\nspectrum of GNN4TDL scenarios, demonstrating their versatility and impact.\nLastly, we discuss the limitations and propose future research directions,\naiming to spur advancements in GNN4TDL. This survey serves as a resource for\nresearchers and practitioners, offering a thorough understanding of GNNs' role\nin revolutionizing TDL and pointing towards future innovations in this\npromising area.\n","authors":["Cheng-Te Li","Yu-Che Tsai","Chih-Yao Chen","Jay Chiehen Liao"],"pdf_url":"https://arxiv.org/pdf/2401.02143v1.pdf","comment":"Under review, ongoing work, Github page:\n  https://github.com/Roytsai27/awesome-GNN4TDL"},{"id":"http://arxiv.org/abs/2401.02130v1","updated":"2024-01-04T08:31:47Z","published":"2024-01-04T08:31:47Z","title":"Spectral-based Graph Neutral Networks for Complementary Item\n  Recommendation","summary":"  Modeling complementary relationships greatly helps recommender systems to\naccurately and promptly recommend the subsequent items when one item is\npurchased. Unlike traditional similar relationships, items with complementary\nrelationships may be purchased successively (such as iPhone and Airpods Pro),\nand they not only share relevance but also exhibit dissimilarity. Since the two\nattributes are opposites, modeling complementary relationships is challenging.\nPrevious attempts to exploit these relationships have either ignored or\noversimplified the dissimilarity attribute, resulting in ineffective modeling\nand an inability to balance the two attributes. Since Graph Neural Networks\n(GNNs) can capture the relevance and dissimilarity between nodes in the\nspectral domain, we can leverage spectral-based GNNs to effectively understand\nand model complementary relationships. In this study, we present a novel\napproach called Spectral-based Complementary Graph Neural Networks (SComGNN)\nthat utilizes the spectral properties of complementary item graphs. We make the\nfirst observation that complementary relationships consist of low-frequency and\nmid-frequency components, corresponding to the relevance and dissimilarity\nattributes, respectively. Based on this spectral observation, we design\nspectral graph convolutional networks with low-pass and mid-pass filters to\ncapture the low-frequency and mid-frequency components. Additionally, we\npropose a two-stage attention mechanism to adaptively integrate and balance the\ntwo attributes. Experimental results on four e-commerce datasets demonstrate\nthe effectiveness of our model, with SComGNN significantly outperforming\nexisting baseline models.\n","authors":["Haitong Luo","Xuying Meng","Suhang Wang","Hanyun Cao","Weiyao Zhang","Yequan Wang","Yujun Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.02130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02116v1","updated":"2024-01-04T07:52:00Z","published":"2024-01-04T07:52:00Z","title":"Starling: An I/O-Efficient Disk-Resident Graph Index Framework for\n  High-Dimensional Vector Similarity Search on Data Segment","summary":"  High-dimensional vector similarity search (HVSS) is receiving a spotlight as\na powerful tool for various data science and AI applications. As vector data\ngrows larger, in-memory indexes become extremely expensive because they\nnecessitate substantial expansion of main memory resources. One possible\nsolution is to use disk-based implementation, which stores and searches vector\ndata in high-performance devices like NVMe SSDs. However, HVSS for data\nsegments is still challenging in vector databases, where one machine has\nmultiple segments for system features (like scaling) purposes. In this setting,\neach segment has limited memory and disk space, so HVSS on the data segment\nneeds to balance accuracy, efficiency, and space cost. Existing disk-based\nmethods are sub-optimal because they do not consider all these requirements\ntogether. In this paper, we present Starling, an I/O-efficient disk-resident\ngraph index framework that optimizes data layout and search strategy in the\nsegment. It has two main components: (1) a data layout that includes an\nin-memory navigation graph and a reordered disk-based graph with locality\nenhancement, which reduces the search path length and disk bandwidth wastage;\nand (2) a block search strategy that minimizes expensive disk I/Os when\nexecuting a vector query. We conduct extensive experiments to verify Starling's\neffectiveness, efficiency, and scalability. On a data segment with 2GB memory\nand 10GB disk capacity, Starling can maintain up to 33 million vectors in 128\ndimensions, and serve HVSS with more than 0.9 average precision and top-10\nrecall rate, and latency of under 1 millisecond. The results show that Starling\nexhibits 43.9$\\times$ higher throughput with 98% lower query latency than\nstate-of-the-art methods under the same accuracy.\n","authors":["Mengzhao Wang","Weizhi Xu","Xiaomeng Yi","Songlin Wu","Zhangyang Peng","Xiangyu Ke","Yunjun Gao","Xiaoliang Xu","Rentong Guo","Charles Xie"],"pdf_url":"https://arxiv.org/pdf/2401.02116v1.pdf","comment":"This paper has been accepted by SIGMOD 2024"},{"id":"http://arxiv.org/abs/2312.12728v2","updated":"2024-01-04T06:33:52Z","published":"2023-12-20T02:55:15Z","title":"Lookahead: An Inference Acceleration Framework for Large Language Model\n  with Lossless Generation Accuracy","summary":"  As Large Language Models (LLMs) have made significant advancements across\nvarious tasks, such as question answering, translation, text summarization, and\ndialogue systems, the need for accuracy in information becomes crucial,\nespecially for serious financial products serving billions of users like\nAlipay. To address this, Alipay has developed a Retrieval-Augmented Generation\n(RAG) system that grounds LLMs on the most accurate and up-to-date information.\nHowever, for a real-world product serving millions of users, the inference\nspeed of LLMs becomes a critical factor compared to a mere experimental model.\n  Hence, this paper presents a generic framework for accelerating the inference\nprocess, resulting in a substantial increase in speed and cost reduction for\nour RAG system, with lossless generation accuracy. In the traditional inference\nprocess, each token is generated sequentially by the LLM, leading to a time\nconsumption proportional to the number of generated tokens. To enhance this\nprocess, our framework, named \\textit{lookahead}, introduces a\n\\textit{multi-branch} strategy. Instead of generating a single token at a time,\nwe propose a \\textit{Trie-based Retrieval} (TR) process that enables the\ngeneration of multiple branches simultaneously, each of which is a sequence of\ntokens. Subsequently, for each branch, a \\textit{Verification and Accept} (VA)\nprocess is performed to identify the longest correct sub-sequence as the final\noutput. Our strategy offers two distinct advantages: (1) it guarantees absolute\ncorrectness of the output, avoiding any approximation algorithms, and (2) the\nworst-case performance of our approach is equivalent to the conventional\nprocess. We conduct extensive experiments to demonstrate the significant\nimprovements achieved by applying our inference acceleration framework. Code is\navaliable: https://github.com/alipay/PainlessInferenceAcceleration.\n","authors":["Yao Zhao","Zhitian Xie","Chenyi Zhuang","Jinjie Gu"],"pdf_url":"https://arxiv.org/pdf/2312.12728v2.pdf","comment":"10 pages, 6 figures"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2301.11329v2","updated":"2024-01-04T18:59:56Z","published":"2023-01-26T18:59:33Z","title":"Anatomy-aware and acquisition-agnostic joint registration with\n  SynthMorph","summary":"  Affine image registration is a cornerstone of medical-image analysis. While\nclassical algorithms can achieve excellent accuracy, they solve a\ntime-consuming optimization for every image pair. Deep-learning (DL) methods\nlearn a function that maps an image pair to an output transform. Evaluating the\nfunction is fast, but capturing large transforms can be challenging, and\nnetworks tend to struggle if a test-image characteristic shifts from the\ntraining domain, such as resolution. Most affine methods are agnostic to\nanatomy, meaning the registration will be inaccurate if algorithms consider all\nstructures in the image.\n  We address these shortcomings with SynthMorph, an easy-to-use DL tool for\njoint affine-deformable registration of any brain image without preprocessing,\nright off the MRI scanner. First, we leverage a strategy to train networks with\nwildly varying images synthesized from label maps, yielding robust performance\nacross acquisition specifics unseen at training. Second, we optimize the\nspatial overlap of select anatomical labels. This enables networks to\ndistinguish anatomy of interest from irrelevant structures, removing the need\nfor preprocessing that excludes content which would impinge on anatomy-specific\nregistration. Third, we combine the affine model with a deformable hypernetwork\nthat lets users choose the optimal deformation-field regularity for their\nspecific data, at registration time, in a fraction of the time required by\nclassical methods.\n  We rigorously analyze how competing architectures learn affine transforms and\ncompare state-of-the-art registration tools across an extremely diverse set of\nneuroimaging data, aiming to truly capture the behavior of methods in the real\nworld. SynthMorph demonstrates consistent and improved accuracy. It is\navailable at https://w3id.org/synthmorph, as a single complete end-to-end\nsolution for registration of brain MRI.\n","authors":["Malte Hoffmann","Andrew Hoopes","Douglas N. Greve","Bruce Fischl","Adrian V. Dalca"],"pdf_url":"https://arxiv.org/pdf/2301.11329v2.pdf","comment":"33 pages, 22 figures, 4 tables, affine registration, deformable\n  registration, deep learning, hypernetwork, domain shift, neuroimaging"},{"id":"http://arxiv.org/abs/2401.02417v1","updated":"2024-01-04T18:59:31Z","published":"2024-01-04T18:59:31Z","title":"Task Oriented Dialogue as a Catalyst for Self-Supervised Automatic\n  Speech Recognition","summary":"  While word error rates of automatic speech recognition (ASR) systems have\nconsistently fallen, natural language understanding (NLU) applications built on\ntop of ASR systems still attribute significant numbers of failures to\nlow-quality speech recognition results. Existing assistant systems collect\nlarge numbers of these unsuccessful interactions, but these systems usually\nfail to learn from these interactions, even in an offline fashion. In this\nwork, we introduce CLC: Contrastive Learning for Conversations, a family of\nmethods for contrastive fine-tuning of models in a self-supervised fashion,\nmaking use of easily detectable artifacts in unsuccessful conversations with\nassistants. We demonstrate that our CLC family of approaches can improve the\nperformance of ASR models on OD3, a new public large-scale semi-synthetic\nmeta-dataset of audio task-oriented dialogues, by up to 19.2%. These gains\ntransfer to real-world systems as well, where we show that CLC can help to\nimprove performance by up to 6.7% over baselines. We make OD3 publicly\navailable at https://github.com/amazon-science/amazon-od3 .\n","authors":["David M. Chan","Shalini Ghosh","Hitesh Tulsiani","Ariya Rastrow","Björn Hoffmeister"],"pdf_url":"https://arxiv.org/pdf/2401.02417v1.pdf","comment":"To appear in ICASSP 2024"},{"id":"http://arxiv.org/abs/2401.02416v1","updated":"2024-01-04T18:59:25Z","published":"2024-01-04T18:59:25Z","title":"ODIN: A Single Model for 2D and 3D Perception","summary":"  State-of-the-art models on contemporary 3D perception benchmarks like ScanNet\nconsume and label dataset-provided 3D point clouds, obtained through post\nprocessing of sensed multiview RGB-D images. They are typically trained\nin-domain, forego large-scale 2D pre-training and outperform alternatives that\nfeaturize the posed RGB-D multiview images instead. The gap in performance\nbetween methods that consume posed images versus post-processed 3D point clouds\nhas fueled the belief that 2D and 3D perception require distinct model\narchitectures. In this paper, we challenge this view and propose ODIN\n(Omni-Dimensional INstance segmentation), a model that can segment and label\nboth 2D RGB images and 3D point clouds, using a transformer architecture that\nalternates between 2D within-view and 3D cross-view information fusion. Our\nmodel differentiates 2D and 3D feature operations through the positional\nencodings of the tokens involved, which capture pixel coordinates for 2D patch\ntokens and 3D coordinates for 3D feature tokens. ODIN achieves state-of-the-art\nperformance on ScanNet200, Matterport3D and AI2THOR 3D instance segmentation\nbenchmarks, and competitive performance on ScanNet, S3DIS and COCO. It\noutperforms all previous works by a wide margin when the sensed 3D point cloud\nis used in place of the point cloud sampled from 3D mesh. When used as the 3D\nperception engine in an instructable embodied agent architecture, it sets a new\nstate-of-the-art on the TEACh action-from-dialogue benchmark. Our code and\ncheckpoints can be found at the project website: https://odin-seg.github.io.\n","authors":["Ayush Jain","Pushkal Katara","Nikolaos Gkanatsios","Adam W. Harley","Gabriel Sarch","Kriti Aggarwal","Vishrav Chaudhary","Katerina Fragkiadaki"],"pdf_url":"https://arxiv.org/pdf/2401.02416v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.05408v2","updated":"2024-01-04T18:55:56Z","published":"2022-11-10T08:24:52Z","title":"Controlling Moments with Kernel Stein Discrepancies","summary":"  Kernel Stein discrepancies (KSDs) measure the quality of a distributional\napproximation and can be computed even when the target density has an\nintractable normalizing constant. Notable applications include the diagnosis of\napproximate MCMC samplers and goodness-of-fit tests for unnormalized\nstatistical models. The present work analyzes the convergence control\nproperties of KSDs. We first show that standard KSDs used for weak convergence\ncontrol fail to control moment convergence. To address this limitation, we next\nprovide sufficient conditions under which alternative diffusion KSDs control\nboth moment and weak convergence. As an immediate consequence we develop, for\neach $q > 0$, the first KSDs known to exactly characterize $q$-Wasserstein\nconvergence.\n","authors":["Heishiro Kanagawa","Alessandro Barp","Arthur Gretton","Lester Mackey"],"pdf_url":"https://arxiv.org/pdf/2211.05408v2.pdf","comment":"93 pages, 19 figures"},{"id":"http://arxiv.org/abs/2401.02413v1","updated":"2024-01-04T18:53:50Z","published":"2024-01-04T18:53:50Z","title":"Simulation-Based Inference with Quantile Regression","summary":"  We present Neural Quantile Estimation (NQE), a novel Simulation-Based\nInference (SBI) method based on conditional quantile regression. NQE\nautoregressively learns individual one dimensional quantiles for each posterior\ndimension, conditioned on the data and previous posterior dimensions. Posterior\nsamples are obtained by interpolating the predicted quantiles using monotonic\ncubic Hermite spline, with specific treatment for the tail behavior and\nmulti-modal distributions. We introduce an alternative definition for the\nBayesian credible region using the local Cumulative Density Function (CDF),\noffering substantially faster evaluation than the traditional Highest Posterior\nDensity Region (HPDR). In case of limited simulation budget and/or known model\nmisspecification, a post-processing broadening step can be integrated into NQE\nto ensure the unbiasedness of the posterior estimation with negligible\nadditional computational cost. We demonstrate that the proposed NQE method\nachieves state-of-the-art performance on a variety of benchmark problems.\n","authors":["He Jia"],"pdf_url":"https://arxiv.org/pdf/2401.02413v1.pdf","comment":"8+13 pages, 7+7 figures"},{"id":"http://arxiv.org/abs/2401.02412v1","updated":"2024-01-04T18:53:01Z","published":"2024-01-04T18:53:01Z","title":"LLM Augmented LLMs: Expanding Capabilities through Composition","summary":"  Foundational models with billions of parameters which have been trained on\nlarge corpora of data have demonstrated non-trivial skills in a variety of\ndomains. However, due to their monolithic structure, it is challenging and\nexpensive to augment them or impart new skills. On the other hand, due to their\nadaptation abilities, several new instances of these models are being trained\ntowards new domains and tasks. In this work, we study the problem of efficient\nand practical composition of existing foundation models with more specific\nmodels to enable newer capabilities. To this end, we propose CALM --\nComposition to Augment Language Models -- which introduces cross-attention\nbetween models to compose their representations and enable new capabilities.\nSalient features of CALM are: (i) Scales up LLMs on new tasks by 're-using'\nexisting LLMs along with a few additional parameters and data, (ii) Existing\nmodel weights are kept intact, and hence preserves existing capabilities, and\n(iii) Applies to diverse domains and settings. We illustrate that augmenting\nPaLM2-S with a smaller model trained on low-resource languages results in an\nabsolute improvement of up to 13\\% on tasks like translation into English and\narithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is\naugmented with a code-specific model, we see a relative improvement of 40\\%\nover the base model for code generation and explanation tasks -- on-par with\nfully fine-tuned counterparts.\n","authors":["Rachit Bansal","Bidisha Samanta","Siddharth Dalmia","Nitish Gupta","Shikhar Vashishth","Sriram Ganapathy","Abhishek Bapna","Prateek Jain","Partha Talukdar"],"pdf_url":"https://arxiv.org/pdf/2401.02412v1.pdf","comment":"17 pages, 2 figures, 8 tables"},{"id":"http://arxiv.org/abs/2401.02411v1","updated":"2024-01-04T18:50:38Z","published":"2024-01-04T18:50:38Z","title":"What You See is What You GAN: Rendering Every Pixel for High-Fidelity\n  Geometry in 3D GANs","summary":"  3D-aware Generative Adversarial Networks (GANs) have shown remarkable\nprogress in learning to generate multi-view-consistent images and 3D geometries\nof scenes from collections of 2D images via neural volume rendering. Yet, the\nsignificant memory and computational costs of dense sampling in volume\nrendering have forced 3D GANs to adopt patch-based training or employ\nlow-resolution rendering with post-processing 2D super resolution, which\nsacrifices multiview consistency and the quality of resolved geometry.\nConsequently, 3D GANs have not yet been able to fully resolve the rich 3D\ngeometry present in 2D images. In this work, we propose techniques to scale\nneural volume rendering to the much higher resolution of native 2D images,\nthereby resolving fine-grained 3D geometry with unprecedented detail. Our\napproach employs learning-based samplers for accelerating neural rendering for\n3D GAN training using up to 5 times fewer depth samples. This enables us to\nexplicitly \"render every pixel\" of the full-resolution image during training\nand inference without post-processing superresolution in 2D. Together with our\nstrategy to learn high-quality surface geometry, our method synthesizes\nhigh-resolution 3D geometry and strictly view-consistent images while\nmaintaining image quality on par with baselines relying on post-processing\nsuper resolution. We demonstrate state-of-the-art 3D gemetric quality on FFHQ\nand AFHQ, setting a new standard for unsupervised learning of 3D shapes in 3D\nGANs.\n","authors":["Alex Trevithick","Matthew Chan","Towaki Takikawa","Umar Iqbal","Shalini De Mello","Manmohan Chandraker","Ravi Ramamoorthi","Koki Nagano"],"pdf_url":"https://arxiv.org/pdf/2401.02411v1.pdf","comment":"See our project page: https://research.nvidia.com/labs/nxp/wysiwyg/"},{"id":"http://arxiv.org/abs/2312.11671v2","updated":"2024-01-04T18:46:39Z","published":"2023-12-18T19:27:09Z","title":"Evaluating Language-Model Agents on Realistic Autonomous Tasks","summary":"  In this report, we explore the ability of language model agents to acquire\nresources, create copies of themselves, and adapt to novel challenges they\nencounter in the wild. We refer to this cluster of capabilities as \"autonomous\nreplication and adaptation\" or ARA. We believe that systems capable of ARA\ncould have wide-reaching and hard-to-anticipate consequences, and that\nmeasuring and forecasting ARA may be useful for informing measures around\nsecurity, monitoring, and alignment. Additionally, once a system is capable of\nARA, placing bounds on a system's capabilities may become significantly more\ndifficult.\n  We construct four simple example agents that combine language models with\ntools that allow them to take actions in the world. We then evaluate these\nagents on 12 tasks relevant to ARA. We find that these language model agents\ncan only complete the easiest tasks from this list, although they make some\nprogress on the more challenging tasks. Unfortunately, these evaluations are\nnot adequate to rule out the possibility that near-future agents will be\ncapable of ARA. In particular, we do not think that these evaluations provide\ngood assurance that the ``next generation'' of language models (e.g. 100x\neffective compute scaleup on existing models) will not yield agents capable of\nARA, unless intermediate evaluations are performed during pretraining.\nRelatedly, we expect that fine-tuning of the existing models could produce\nsubstantially more competent agents, even if the fine-tuning is not directly\ntargeted at ARA.\n","authors":["Megan Kinniment","Lucas Jun Koba Sato","Haoxing Du","Brian Goodrich","Max Hasin","Lawrence Chan","Luke Harold Miles","Tao R. Lin","Hjalmar Wijk","Joel Burget","Aaron Ho","Elizabeth Barnes","Paul Christiano"],"pdf_url":"https://arxiv.org/pdf/2312.11671v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2401.02403v1","updated":"2024-01-04T18:42:28Z","published":"2024-01-04T18:42:28Z","title":"Real-Time 2D Temperature Field Prediction in Metal Additive\n  Manufacturing Using Physics-Informed Neural Networks","summary":"  Accurately predicting the temperature field in metal additive manufacturing\n(AM) processes is critical to preventing overheating, adjusting process\nparameters, and ensuring process stability. While physics-based computational\nmodels offer precision, they are often time-consuming and unsuitable for\nreal-time predictions and online control in iterative design scenarios.\nConversely, machine learning models rely heavily on high-quality datasets,\nwhich can be costly and challenging to obtain within the metal AM domain. Our\nwork addresses this by introducing a physics-informed neural network framework\nspecifically designed for temperature field prediction in metal AM. This\nframework incorporates a physics-informed input, physics-informed loss\nfunction, and a Convolutional Long Short-Term Memory (ConvLSTM) architecture.\nUtilizing real-time temperature data from the process, our model predicts 2D\ntemperature fields for future timestamps across diverse geometries, deposition\npatterns, and process parameters. We validate the proposed framework in two\nscenarios: full-field temperature prediction for a thin wall and 2D temperature\nfield prediction for cylinder and cubic parts, demonstrating errors below 3%\nand 1%, respectively. Our proposed framework exhibits the flexibility to be\napplied across diverse scenarios with varying process parameters, geometries,\nand deposition patterns.\n","authors":["Pouyan Sajadi","Mostafa Rahmani Dehaghani","Yifan Tang","G. Gary Wang"],"pdf_url":"https://arxiv.org/pdf/2401.02403v1.pdf","comment":"42 pages, 13 Figures"},{"id":"http://arxiv.org/abs/2302.13991v3","updated":"2024-01-04T18:35:21Z","published":"2023-02-27T17:30:00Z","title":"Learning to Generalize towards Unseen Domains via a Content-Aware Style\n  Invariant Model for Disease Detection from Chest X-rays","summary":"  Performance degradation due to distribution discrepancy is a longstanding\nchallenge in intelligent imaging, particularly for chest X-rays (CXRs). Recent\nstudies have demonstrated that CNNs are biased toward styles (e.g.,\nuninformative textures) rather than content (e.g., shape), in stark contrast to\nthe human vision system. Radiologists tend to learn visual cues from CXRs and\nthus perform well across multiple domains. Motivated by this, we employ the\nnovel on-the-fly style randomization modules at both image (SRM-IL) and feature\n(SRM-FL) levels to create rich style perturbed features while keeping the\ncontent intact for robust cross-domain performance. Previous methods simulate\nunseen domains by constructing new styles via interpolation or swapping styles\nfrom existing data, limiting them to available source domains during training.\nHowever, SRM-IL samples the style statistics from the possible value range of a\nCXR image instead of the training data to achieve more diversified\naugmentations. Moreover, we utilize pixel-wise learnable parameters in the\nSRM-FL compared to pre-defined channel-wise mean and standard deviations as\nstyle embeddings for capturing more representative style features.\nAdditionally, we leverage consistency regularizations on global semantic\nfeatures and predictive distributions from with and without style-perturbed\nversions of the same CXR to tweak the model's sensitivity toward content\nmarkers for accurate predictions. Our proposed method, trained on CheXpert and\nMIMIC-CXR datasets, achieves 77.32$\\pm$0.35, 88.38$\\pm$0.19, 82.63$\\pm$0.13\nAUCs(%) on the unseen domain test datasets, i.e., BRAX, VinDr-CXR, and NIH\nchest X-ray14, respectively, compared to 75.56$\\pm$0.80, 87.57$\\pm$0.46,\n82.07$\\pm$0.19 from state-of-the-art models on five-fold cross-validation with\nstatistically significant results in thoracic disease classification.\n","authors":["Mohammad Zunaed","Md. Aynal Haque","Taufiq Hasan"],"pdf_url":"https://arxiv.org/pdf/2302.13991v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02398v1","updated":"2024-01-04T18:31:21Z","published":"2024-01-04T18:31:21Z","title":"Generating synthetic data for neural operators","summary":"  Numerous developments in the recent literature show the promising potential\nof deep learning in obtaining numerical solutions to partial differential\nequations (PDEs) beyond the reach of current numerical solvers. However,\ndata-driven neural operators all suffer from the same problem: the data needed\nto train a network depends on classical numerical solvers such as finite\ndifference or finite element, among others. In this paper, we propose a new\napproach to generating synthetic functional training data that does not require\nsolving a PDE numerically. The way we do this is simple: we draw a large number\n$N$ of independent and identically distributed `random functions' $u_j$ from\nthe underlying solution space (e.g., $H_0^1(\\Omega)$) in which we know the\nsolution lies according to classical theory. We then plug each such random\ncandidate solution into the equation and get a corresponding right-hand side\nfunction $f_j$ for the equation, and consider $(f_j, u_j)_{j=1}^N$ as\nsupervised training data for learning the underlying inverse problem $f\n\\rightarrow u$. This `backwards' approach to generating training data only\nrequires derivative computations, in contrast to standard `forward' approaches,\nwhich require a numerical PDE solver, enabling us to generate a large number of\nsuch data points quickly and efficiently. While the idea is simple, we hope\nthat this method will expand the potential for developing neural PDE solvers\nthat do not depend on classical numerical solvers.\n","authors":["Erisa Hasani","Rachel A. Ward"],"pdf_url":"https://arxiv.org/pdf/2401.02398v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.00357v2","updated":"2024-01-04T17:51:13Z","published":"2022-11-01T10:03:34Z","title":"Generalized Quadratic Embeddings for Nonlinear Dynamics using Deep\n  Learning","summary":"  The engineering design process often relies on mathematical modeling that can\ndescribe the underlying dynamic behavior. In this work, we present a\ndata-driven methodology for modeling the dynamics of nonlinear systems. To\nsimplify this task, we aim to identify a coordinate transformation that allows\nus to represent the dynamics of nonlinear systems using a common, simple model\nstructure. The advantage of a common simple model is that customized design\ntools developed for it can be applied to study a large variety of nonlinear\nsystems. The simplest common model -- one can think of -- is linear, but linear\nsystems often fall short in accurately capturing the complex dynamics of\nnonlinear systems. In this work, we propose using quadratic systems as the\ncommon structure, inspired by the lifting principle. According to this\nprinciple, smooth nonlinear systems can be expressed as quadratic systems in\nsuitable coordinates without approximation errors. However, finding these\ncoordinates solely from data is challenging. Here, we leverage deep learning to\nidentify such lifted coordinates using only data, enabling a quadratic\ndynamical system to describe the system's dynamics. Additionally, we discuss\nthe asymptotic stability of these quadratic dynamical systems. We illustrate\nthe approach using data collected from various numerical examples,\ndemonstrating its superior performance with the existing well-known techniques.\n","authors":["Pawan Goyal","Peter Benner"],"pdf_url":"https://arxiv.org/pdf/2211.00357v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.06432v2","updated":"2024-01-04T17:19:39Z","published":"2023-05-10T19:44:42Z","title":"A Generalizable Physics-informed Learning Framework for Risk Probability\n  Estimation","summary":"  Accurate estimates of long-term risk probabilities and their gradients are\ncritical for many stochastic safe control methods. However, computing such risk\nprobabilities in real-time and in unseen or changing environments is\nchallenging. Monte Carlo (MC) methods cannot accurately evaluate the\nprobabilities and their gradients as an infinitesimal devisor can amplify the\nsampling noise. In this paper, we develop an efficient method to evaluate the\nprobabilities of long-term risk and their gradients. The proposed method\nexploits the fact that long-term risk probability satisfies certain partial\ndifferential equations (PDEs), which characterize the neighboring relations\nbetween the probabilities, to integrate MC methods and physics-informed neural\nnetworks. We provide theoretical guarantees of the estimation error given\ncertain choices of training configurations. Numerical results show the proposed\nmethod has better sample efficiency, generalizes well to unseen regions, and\ncan adapt to systems with changing parameters. The proposed method can also\naccurately estimate the gradients of risk probabilities, which enables first-\nand second-order techniques on risk probabilities to be used for learning and\ncontrol.\n","authors":["Zhuoyuan Wang","Yorie Nakahira"],"pdf_url":"https://arxiv.org/pdf/2305.06432v2.pdf","comment":"Accepted at the 5th Annual Learning for Dynamics & Control (L4DC)\n  Conference, 2023"},{"id":"http://arxiv.org/abs/2401.02363v1","updated":"2024-01-04T17:01:54Z","published":"2024-01-04T17:01:54Z","title":"Integration of physics-informed operator learning and finite element\n  method for parametric learning of partial differential equations","summary":"  We present a method that employs physics-informed deep learning techniques\nfor parametrically solving partial differential equations. The focus is on the\nsteady-state heat equations within heterogeneous solids exhibiting significant\nphase contrast. Similar equations manifest in diverse applications like\nchemical diffusion, electrostatics, and Darcy flow. The neural network aims to\nestablish the link between the complex thermal conductivity profiles and\ntemperature distributions, as well as heat flux components within the\nmicrostructure, under fixed boundary conditions. A distinctive aspect is our\nindependence from classical solvers like finite element methods for data. A\nnoteworthy contribution lies in our novel approach to defining the loss\nfunction, based on the discretized weak form of the governing equation. This\nnot only reduces the required order of derivatives but also eliminates the need\nfor automatic differentiation in the construction of loss terms, accepting\npotential numerical errors from the chosen discretization method. As a result,\nthe loss function in this work is an algebraic equation that significantly\nenhances training efficiency. We benchmark our methodology against the standard\nfinite element method, demonstrating accurate yet faster predictions using the\ntrained neural network for temperature and flux profiles. We also show higher\naccuracy by using the proposed method compared to purely data-driven approaches\nfor unforeseen scenarios.\n","authors":["Shahed Rezaei","Ahmad Moeineddin","Michael Kaliske","Markus Apel"],"pdf_url":"https://arxiv.org/pdf/2401.02363v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01641v2","updated":"2024-01-04T16:52:11Z","published":"2024-01-03T09:32:48Z","title":"Towards a Foundation Purchasing Model: Pretrained Generative\n  Autoregression on Transaction Sequences","summary":"  Machine learning models underpin many modern financial systems for use cases\nsuch as fraud detection and churn prediction. Most are based on supervised\nlearning with hand-engineered features, which relies heavily on the\navailability of labelled data. Large self-supervised generative models have\nshown tremendous success in natural language processing and computer vision,\nyet so far they haven't been adapted to multivariate time series of financial\ntransactions. In this paper, we present a generative pretraining method that\ncan be used to obtain contextualised embeddings of financial transactions.\nBenchmarks on public datasets demonstrate that it outperforms state-of-the-art\nself-supervised methods on a range of downstream tasks. We additionally perform\nlarge-scale pretraining of an embedding model using a corpus of data from 180\nissuing banks containing 5.1 billion transactions and apply it to the card\nfraud detection problem on hold-out datasets. The embedding model significantly\nimproves value detection rate at high precision thresholds and transfers well\nto out-of-domain distributions.\n","authors":["Piotr Skalski","David Sutton","Stuart Burrell","Iker Perez","Jason Wong"],"pdf_url":"https://arxiv.org/pdf/2401.01641v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02349v1","updated":"2024-01-04T16:45:01Z","published":"2024-01-04T16:45:01Z","title":"A Survey Analyzing Generalization in Deep Reinforcement Learning","summary":"  Reinforcement learning research obtained significant success and attention\nwith the utilization of deep neural networks to solve problems in high\ndimensional state or action spaces. While deep reinforcement learning policies\nare currently being deployed in many different fields from medical applications\nto self driving vehicles, there are still ongoing questions the field is trying\nto answer on the generalization capabilities of deep reinforcement learning\npolicies. In this paper, we will outline the fundamental reasons why deep\nreinforcement learning policies encounter overfitting problems that limit their\nrobustness and generalization capabilities. Furthermore, we will formalize and\nunify the diverse solution approaches to increase generalization, and overcome\noverfitting in state-action value functions. We believe our study can provide a\ncompact systematic unified analysis for the current advancements in deep\nreinforcement learning, and help to construct robust deep neural policies with\nimproved generalization abilities.\n","authors":["Ezgi Korkmaz"],"pdf_url":"https://arxiv.org/pdf/2401.02349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02344v1","updated":"2024-01-04T16:38:47Z","published":"2024-01-04T16:38:47Z","title":"Multi-Source Domain Adaptation with Transformer-based Feature Generation\n  for Subject-Independent EEG-based Emotion Recognition","summary":"  Although deep learning-based algorithms have demonstrated excellent\nperformance in automated emotion recognition via electroencephalogram (EEG)\nsignals, variations across brain signal patterns of individuals can diminish\nthe model's effectiveness when applied across different subjects. While\ntransfer learning techniques have exhibited promising outcomes, they still\nencounter challenges related to inadequate feature representations and may\noverlook the fact that source subjects themselves can possess distinct\ncharacteristics. In this work, we propose a multi-source domain adaptation\napproach with a transformer-based feature generator (MSDA-TF) designed to\nleverage information from multiple sources. The proposed feature generator\nretains convolutional layers to capture shallow spatial, temporal, and spectral\nEEG data representations, while self-attention mechanisms extract global\ndependencies within these features. During the adaptation process, we group the\nsource subjects based on correlation values and aim to align the moments of the\ntarget subject with each source as well as within the sources. MSDA-TF is\nvalidated on the SEED dataset and is shown to yield promising results.\n","authors":["Shadi Sartipi","Mujdat Cetin"],"pdf_url":"https://arxiv.org/pdf/2401.02344v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02342v1","updated":"2024-01-04T16:28:15Z","published":"2024-01-04T16:28:15Z","title":"Evasive Hardware Trojan through Adversarial Power Trace","summary":"  The globalization of the Integrated Circuit (IC) supply chain, driven by\ntime-to-market and cost considerations, has made ICs vulnerable to hardware\nTrojans (HTs). Against this threat, a promising approach is to use Machine\nLearning (ML)-based side-channel analysis, which has the advantage of being a\nnon-intrusive method, along with efficiently detecting HTs under golden\nchip-free settings. In this paper, we question the trustworthiness of ML-based\nHT detection via side-channel analysis. We introduce a HT obfuscation (HTO)\napproach to allow HTs to bypass this detection method. Rather than\ntheoretically misleading the model by simulated adversarial traces, a key\naspect of our approach is the design and implementation of adversarial noise as\npart of the circuitry, alongside the HT. We detail HTO methodologies for ASICs\nand FPGAs, and evaluate our approach using TrustHub benchmark. Interestingly,\nwe found that HTO can be implemented with only a single transistor for ASIC\ndesigns to generate adversarial power traces that can fool the defense with\n100% efficiency. We also efficiently implemented our approach on a Spartan 6\nXilinx FPGA using 2 different variants: (i) DSP slices-based, and (ii)\nring-oscillator-based design. Additionally, we assess the efficiency of\ncountermeasures like spectral domain analysis, and we show that an adaptive\nattacker can still design evasive HTOs by constraining the design with a\nspectral noise budget. In addition, while adversarial training (AT) offers\nhigher protection against evasive HTs, AT models suffer from a considerable\nutility loss, potentially rendering them unsuitable for such security\napplication. We believe this research represents a significant step in\nunderstanding and exploiting ML vulnerabilities in a hardware security context,\nand we make all resources and designs openly available online:\nhttps://dev.d18uu4lqwhbmka.amplifyapp.com\n","authors":["Behnam Omidi","Khaled N. Khasawneh","Ihsen Alouani"],"pdf_url":"https://arxiv.org/pdf/2401.02342v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.15228v2","updated":"2024-01-04T16:20:42Z","published":"2023-12-23T11:37:09Z","title":"Adversarial Data Poisoning for Fake News Detection: How to Make a Model\n  Misclassify a Target News without Modifying It","summary":"  Fake news detection models are critical to countering disinformation but can\nbe manipulated through adversarial attacks. In this position paper, we analyze\nhow an attacker can compromise the performance of an online learning detector\non specific news content without being able to manipulate the original target\nnews. In some contexts, such as social networks, where the attacker cannot\nexert complete control over all the information, this scenario can indeed be\nquite plausible. Therefore, we show how an attacker could potentially introduce\npoisoning data into the training data to manipulate the behavior of an online\nlearning method. Our initial findings reveal varying susceptibility of logistic\nregression models based on complexity and attack type.\n","authors":["Federico Siciliano","Luca Maiano","Lorenzo Papa","Federica Baccini","Irene Amerini","Fabrizio Silvestri"],"pdf_url":"https://arxiv.org/pdf/2312.15228v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02333v1","updated":"2024-01-04T16:16:14Z","published":"2024-01-04T16:16:14Z","title":"Beyond Extraction: Contextualising Tabular Data for Efficient\n  Summarisation by Language Models","summary":"  The conventional use of the Retrieval-Augmented Generation (RAG) architecture\nhas proven effective for retrieving information from diverse documents.\nHowever, challenges arise in handling complex table queries, especially within\nPDF documents containing intricate tabular structures.This research introduces\nan innovative approach to enhance the accuracy of complex table queries in\nRAG-based systems. Our methodology involves storing PDFs in the retrieval\ndatabase and extracting tabular content separately. The extracted tables\nundergo a process of context enrichment, concatenating headers with\ncorresponding values. To ensure a comprehensive understanding of the enriched\ndata, we employ a fine-tuned version of the Llama-2-chat language model for\nsummarisation within the RAG architecture. Furthermore, we augment the tabular\ndata with contextual sense using the ChatGPT 3.5 API through a one-shot prompt.\nThis enriched data is then fed into the retrieval database alongside other\nPDFs. Our approach aims to significantly improve the precision of complex table\nqueries, offering a promising solution to a longstanding challenge in\ninformation retrieval.\n","authors":["Uday Allu","Biddwan Ahmed","Vishesh Tripathi"],"pdf_url":"https://arxiv.org/pdf/2401.02333v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02329v1","updated":"2024-01-04T16:06:31Z","published":"2024-01-04T16:06:31Z","title":"Not all Minorities are Equal: Empty-Class-Aware Distillation for\n  Heterogeneous Federated Learning","summary":"  Data heterogeneity, characterized by disparities in local data distribution\nacross clients, poses a significant challenge in federated learning.\nSubstantial efforts have been devoted to addressing the heterogeneity in local\nlabel distribution. As minority classes suffer from worse accuracy due to\noverfitting on local imbalanced data, prior methods often incorporate\nclass-balanced learning techniques during local training. Despite the improved\nmean accuracy across all classes, we observe that empty classes-referring to\ncategories absent from a client's data distribution-are still not well\nrecognized. This paper introduces FedED, a novel approach in heterogeneous\nfederated learning that integrates both empty-class distillation and logit\nsuppression simultaneously. Specifically, empty-class distillation leverages\nknowledge distillation during local training on each client to retain essential\ninformation related to empty classes from the global model. Moreover, logit\nsuppression directly penalizes network logits for non-label classes,\neffectively addressing misclassifications in minority classes that may be\nbiased toward majority classes. Extensive experiments validate the efficacy of\nFedED, surpassing previous state-of-the-art methods across diverse datasets\nwith varying degrees of label distribution shift.\n","authors":["Kuangpu Guo","Yuhe Ding","Jian Liang","Ran He","Zilei Wang","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2401.02329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02325v1","updated":"2024-01-04T15:51:49Z","published":"2024-01-04T15:51:49Z","title":"A Robust Quantile Huber Loss With Interpretable Parameter Adjustment In\n  Distributional Reinforcement Learning","summary":"  Distributional Reinforcement Learning (RL) estimates return distribution\nmainly by learning quantile values via minimizing the quantile Huber loss\nfunction, entailing a threshold parameter often selected heuristically or via\nhyperparameter search, which may not generalize well and can be suboptimal.\nThis paper introduces a generalized quantile Huber loss function derived from\nWasserstein distance (WD) calculation between Gaussian distributions, capturing\nnoise in predicted (current) and target (Bellman-updated) quantile values.\nCompared to the classical quantile Huber loss, this innovative loss function\nenhances robustness against outliers. Notably, the classical Huber loss\nfunction can be seen as an approximation of our proposed loss, enabling\nparameter adjustment by approximating the amount of noise in the data during\nthe learning process. Empirical tests on Atari games, a common application in\ndistributional RL, and a recent hedging strategy using distributional RL,\nvalidate the effectiveness of our proposed loss function and its potential for\nparameter adjustments in distributional RL.\n","authors":["Parvin Malekzadeh","Konstantinos N. Plataniotis","Zissis Poulos","Zeyu Wang"],"pdf_url":"https://arxiv.org/pdf/2401.02325v1.pdf","comment":"6 pages, 1 figure, to be published in ICASSP 2024"},{"id":"http://arxiv.org/abs/2401.02323v1","updated":"2024-01-04T15:43:55Z","published":"2024-01-04T15:43:55Z","title":"Multi-Agent Context Learning Strategy for Interference-Aware Beam\n  Allocation in mmWave Vehicular Communications","summary":"  Millimeter wave (mmWave) has been recognized as one of key technologies for\n5G and beyond networks due to its potential to enhance channel bandwidth and\nnetwork capacity. The use of mmWave for various applications including\nvehicular communications has been extensively discussed. However, applying\nmmWave to vehicular communications faces challenges of high mobility nodes and\nnarrow coverage along the mmWave beams. Due to high mobility in dense networks,\noverlapping beams can cause strong interference which leads to performance\ndegradation. As a remedy, beam switching capability in mmWave can be utilized.\nThen, frequent beam switching and cell change become inevitable to manage\ninterference, which increase computational and signalling complexity. In order\nto deal with the complexity in interference control, we develop a new strategy\ncalled Multi-Agent Context Learning (MACOL), which utilizes Contextual Bandit\nto manage interference while allocating mmWave beams to serve vehicles in the\nnetwork. Our approach demonstrates that by leveraging knowledge of neighbouring\nbeam status, the machine learning agent can identify and avoid potential\ninterfering transmissions to other ongoing transmissions. Furthermore, we show\nthat even under heavy traffic loads, our proposed MACOL strategy is able to\nmaintain low interference levels at around 10%.\n","authors":["Abdulkadir Kose","Haeyoung Lee","Chuan Heng Foh","Mohammad Shojafar"],"pdf_url":"https://arxiv.org/pdf/2401.02323v1.pdf","comment":"Accepted in IEEE Transactions on Intelligent Transportation Systems"},{"id":"http://arxiv.org/abs/2305.17033v4","updated":"2024-01-04T15:10:34Z","published":"2023-05-26T15:40:11Z","title":"The Brain Tumor Segmentation (BraTS) Challenge 2023: Focus on Pediatrics\n  (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs)","summary":"  Pediatric tumors of the central nervous system are the most common cause of\ncancer-related death in children. The five-year survival rate for high-grade\ngliomas in children is less than 20\\%. Due to their rarity, the diagnosis of\nthese entities is often delayed, their treatment is mainly based on historic\ntreatment concepts, and clinical trials require multi-institutional\ncollaborations. The MICCAI Brain Tumor Segmentation (BraTS) Challenge is a\nlandmark community benchmark event with a successful history of 12 years of\nresource creation for the segmentation and analysis of adult glioma. Here we\npresent the CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023 challenge, which\nrepresents the first BraTS challenge focused on pediatric brain tumors with\ndata acquired across multiple international consortia dedicated to pediatric\nneuro-oncology and clinical trials. The BraTS-PEDs 2023 challenge focuses on\nbenchmarking the development of volumentric segmentation algorithms for\npediatric brain glioma through standardized quantitative performance evaluation\nmetrics utilized across the BraTS 2023 cluster of challenges. Models gaining\nknowledge from the BraTS-PEDs multi-parametric structural MRI (mpMRI) training\ndata will be evaluated on separate validation and unseen test mpMRI dataof\nhigh-grade pediatric glioma. The CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023\nchallenge brings together clinicians and AI/imaging scientists to lead to\nfaster development of automated segmentation techniques that could benefit\nclinical trials, and ultimately the care of children with brain tumors.\n","authors":["Anahita Fathi Kazerooni","Nastaran Khalili","Xinyang Liu","Debanjan Haldar","Zhifan Jiang","Syed Muhammed Anwar","Jake Albrecht","Maruf Adewole","Udunna Anazodo","Hannah Anderson","Sina Bagheri","Ujjwal Baid","Timothy Bergquist","Austin J. Borja","Evan Calabrese","Verena Chung","Gian-Marco Conte","Farouk Dako","James Eddy","Ivan Ezhov","Ariana Familiar","Keyvan Farahani","Shuvanjan Haldar","Juan Eugenio Iglesias","Anastasia Janas","Elaine Johansen","Blaise V Jones","Florian Kofler","Dominic LaBella","Hollie Anne Lai","Koen Van Leemput","Hongwei Bran Li","Nazanin Maleki","Aaron S McAllister","Zeke Meier","Bjoern Menze","Ahmed W Moawad","Khanak K Nandolia","Julija Pavaine","Marie Piraud","Tina Poussaint","Sanjay P Prabhu","Zachary Reitman","Andres Rodriguez","Jeffrey D Rudie","Ibraheem Salman Shaikh","Lubdha M. Shah","Nakul Sheth","Russel Taki Shinohara","Wenxin Tu","Karthik Viswanathan","Chunhao Wang","Jeffrey B Ware","Benedikt Wiestler","Walter Wiggins","Anna Zapaishchykova","Mariam Aboian","Miriam Bornhorst","Peter de Blank","Michelle Deutsch","Maryam Fouladi","Lindsey Hoffman","Benjamin Kann","Margot Lazow","Leonie Mikael","Ali Nabavizadeh","Roger Packer","Adam Resnick","Brian Rood","Arastoo Vossough","Spyridon Bakas","Marius George Linguraru"],"pdf_url":"https://arxiv.org/pdf/2305.17033v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06815v2","updated":"2024-01-04T15:06:41Z","published":"2023-03-13T02:14:42Z","title":"On Model Compression for Neural Networks: Framework, Algorithm, and\n  Convergence Guarantee","summary":"  Model compression is a crucial part of deploying neural networks (NNs),\nespecially when the memory and storage of computing devices are limited in many\napplications. This paper focuses on two model compression techniques: low-rank\napproximation and weight pruning in neural networks, which are very popular\nnowadays. However, training NN with low-rank approximation and weight pruning\nalways suffers significant accuracy loss and convergence issues. In this paper,\na holistic framework is proposed for model compression from a novel perspective\nof nonconvex optimization by designing an appropriate objective function. Then,\nwe introduce NN-BCD, a block coordinate descent (BCD) algorithm to solve the\nnonconvex optimization. One advantage of our algorithm is that an efficient\niteration scheme can be derived with closed-form, which is gradient-free.\nTherefore, our algorithm will not suffer from vanishing/exploding gradient\nproblems. Furthermore, with the Kurdyka-{\\L}ojasiewicz (K{\\L}) property of our\nobjective function, we show that our algorithm globally converges to a critical\npoint at the rate of O(1/k), where k denotes the number of iterations. Lastly,\nextensive experiments with tensor train decomposition and weight pruning\ndemonstrate the efficiency and superior performance of the proposed framework.\nOur code implementation is available at https://github.com/ChenyangLi-97/NN-BCD\n","authors":["Chenyang Li","Jihoon Chung","Biao Cai","Haimin Wang","Xianlian Zhou","Bo Shen"],"pdf_url":"https://arxiv.org/pdf/2303.06815v2.pdf","comment":"43 pages"},{"id":"http://arxiv.org/abs/2308.02535v3","updated":"2024-01-04T15:06:19Z","published":"2023-08-01T10:02:26Z","title":"Learning to Generate Training Datasets for Robust Semantic Segmentation","summary":"  Semantic segmentation methods have advanced significantly. Still, their\nrobustness to real-world perturbations and object types not seen during\ntraining remains a challenge, particularly in safety-critical applications. We\npropose a novel approach to improve the robustness of semantic segmentation\ntechniques by leveraging the synergy between label-to-image generators and\nimage-to-label segmentation models. Specifically, we design Robusta, a novel\nrobust conditional generative adversarial network to generate realistic and\nplausible perturbed images that can be used to train reliable segmentation\nmodels. We conduct in-depth studies of the proposed generative model, assess\nthe performance and robustness of the downstream segmentation network, and\ndemonstrate that our approach can significantly enhance the robustness in the\nface of real-world perturbations, distribution shifts, and out-of-distribution\nsamples. Our results suggest that this approach could be valuable in\nsafety-critical applications, where the reliability of perception modules such\nas semantic segmentation is of utmost importance and comes with a limited\ncomputational budget in inference. We release our code at\nhttps://github.com/ENSTA-U2IS/robusta.\n","authors":["Marwane Hariat","Olivier Laurent","Rémi Kazmierczak","Shihao Zhang","Andrei Bursuc","Angela Yao","Gianni Franchi"],"pdf_url":"https://arxiv.org/pdf/2308.02535v3.pdf","comment":"Published as a conference paper at WACV 2024"},{"id":"http://arxiv.org/abs/2302.00736v4","updated":"2024-01-04T14:51:54Z","published":"2023-02-01T20:14:22Z","title":"Approximating the Shapley Value without Marginal Contributions","summary":"  The Shapley value, which is arguably the most popular approach for assigning\na meaningful contribution value to players in a cooperative game, has recently\nbeen used intensively in explainable artificial intelligence. Its\nmeaningfulness is due to axiomatic properties that only the Shapley value\nsatisfies, which, however, comes at the expense of an exact computation growing\nexponentially with the number of agents. Accordingly, a number of works are\ndevoted to the efficient approximation of the Shapley value, most of them\nrevolve around the notion of an agent's marginal contribution. In this paper,\nwe propose with SVARM and Stratified SVARM two parameter-free and\ndomain-independent approximation algorithms based on a representation of the\nShapley value detached from the notion of marginal contribution. We prove\nunmatched theoretical guarantees regarding their approximation quality and\nprovide empirical results including synthetic games as well as common\nexplainability use cases comparing ourselves with state-of-the-art methods.\n","authors":["Patrick Kolpaczki","Viktor Bengs","Maximilian Muschalik","Eyke Hüllermeier"],"pdf_url":"https://arxiv.org/pdf/2302.00736v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02300v1","updated":"2024-01-04T14:42:29Z","published":"2024-01-04T14:42:29Z","title":"Robust Physics Informed Neural Networks","summary":"  We introduce a Robust version of the Physics-Informed Neural Networks\n(RPINNs) to approximate the Partial Differential Equations (PDEs) solution.\nStandard Physics Informed Neural Networks (PINN) takes into account the\ngoverning physical laws described by PDE during the learning process. The\nnetwork is trained on a data set that consists of randomly selected points in\nthe physical domain and its boundary. PINNs have been successfully applied to\nsolve various problems described by PDEs with boundary conditions. The loss\nfunction in traditional PINNs is based on the strong residuals of the PDEs.\nThis loss function in PINNs is generally not robust with respect to the true\nerror. The loss function in PINNs can be far from the true error, which makes\nthe training process more difficult. In particular, we do not know if the\ntraining process has already converged to the solution with the required\naccuracy. This is especially true if we do not know the exact solution, so we\ncannot estimate the true error during the training. This paper introduces a\ndifferent way of defining the loss function. It incorporates the residual and\nthe inverse of the Gram matrix, computed using the energy norm. We test our\nRPINN algorithm on two Laplace problems and one advection-diffusion problem in\ntwo spatial dimensions. We conclude that RPINN is a robust method. The proposed\nloss coincides well with the true error of the solution, as measured in the\nenergy norm. Thus, we know if our training process goes well, and we know when\nto stop the training to obtain the neural network approximation of the solution\nof the PDE with the true error of required accuracy.\n","authors":["Marcin Łoś","Maciej Paszyński"],"pdf_url":"https://arxiv.org/pdf/2401.02300v1.pdf","comment":"33 pages, 18 figures"},{"id":"http://arxiv.org/abs/2401.02296v1","updated":"2024-01-04T14:34:58Z","published":"2024-01-04T14:34:58Z","title":"Training Single-Layer Morphological Perceptron Using Convex-Concave\n  Programming","summary":"  This paper concerns the training of a single-layer morphological perceptron\nusing disciplined convex-concave programming (DCCP). We introduce an algorithm\nreferred to as K-DDCCP, which combines the existing single-layer morphological\nperceptron (SLMP) model proposed by Ritter and Urcid with the weighted\ndisciplined convex-concave programming (WDCCP) algorithm by Charisopoulos and\nMaragos. The proposed training algorithm leverages the disciplined\nconvex-concave procedure (DCCP) and formulates a non-convex optimization\nproblem for binary classification. To tackle this problem, the constraints are\nexpressed as differences of convex functions, enabling the application of the\nDCCP package. The experimental results confirm the effectiveness of the K-DDCCP\nalgorithm in solving binary classification problems. Overall, this work\ncontributes to the field of morphological neural networks by proposing an\nalgorithm that extends the capabilities of the SLMP model.\n","authors":["Iara Cunha","Marcos Eduardo Valle"],"pdf_url":"https://arxiv.org/pdf/2401.02296v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09015v2","updated":"2024-01-04T14:23:03Z","published":"2023-12-14T15:06:29Z","title":"Uncertainty in GNN Learning Evaluations: A Comparison Between Measures\n  for Quantifying Randomness in GNN Community Detection","summary":"  (1) The enhanced capability of Graph Neural Networks (GNNs) in unsupervised\ncommunity detection of clustered nodes is attributed to their capacity to\nencode both the connectivity and feature information spaces of graphs. The\nidentification of latent communities holds practical significance in various\ndomains, from social networks to genomics. Current real-world performance\nbenchmarks are perplexing due to the multitude of decisions influencing GNN\nevaluations for this task. (2) Three metrics are compared to assess the\nconsistency of algorithm rankings in the presence of randomness. The\nconsistency and quality of performance between the results under a\nhyperparameter optimisation with the default hyperparameters is evaluated. (3)\nThe results compare hyperparameter optimisation with default hyperparameters,\nrevealing a significant performance loss when neglecting hyperparameter\ninvestigation. A comparison of metrics indicates that ties in ranks can\nsubstantially alter the quantification of randomness. (4) Ensuring adherence to\nthe same evaluation criteria may result in notable differences in the reported\nperformance of methods for this task. The $W$ Randomness coefficient, based on\nthe Wasserstein distance, is identified as providing the most robust assessment\nof randomness.\n","authors":["William Leeney","Ryan McConville"],"pdf_url":"https://arxiv.org/pdf/2312.09015v2.pdf","comment":"12 pages, 2 figures, contribution from COMPLEX NETWORKS 2023 selected\n  for a possible publication in the special issue of the journal Entropy\n  dedicated to the conference. arXiv admin note: substantial text overlap with\n  arXiv:2305.06026"},{"id":"http://arxiv.org/abs/2401.02290v1","updated":"2024-01-04T14:19:37Z","published":"2024-01-04T14:19:37Z","title":"Path-based Explanation for Knowledge Graph Completion","summary":"  Graph Neural Networks (GNNs) have achieved great success in Knowledge Graph\nCompletion (KGC) by modelling how entities and relations interact in recent\nyears. However, the explanation of the predicted facts has not caught the\nnecessary attention. Proper explanations for the results of GNN-based KGC\nmodels increase model transparency and help researchers develop more reliable\nmodels. Existing practices for explaining KGC tasks rely on\ninstance/subgraph-based approaches, while in some scenarios, paths can provide\nmore user-friendly and interpretable explanations. Nonetheless, the methods for\ngenerating path-based explanations for KGs have not been well-explored. To\naddress this gap, we propose Power-Link, the first path-based KGC explainer\nthat explores GNN-based models. We design a novel simplified graph-powering\ntechnique, which enables the generation of path-based explanations with a fully\nparallelisable and memory-efficient training scheme. We further introduce three\nnew metrics for quantitative evaluation of the explanations, together with a\nqualitative human evaluation. Extensive experiments demonstrate that Power-Link\noutperforms the SOTA baselines in interpretability, efficiency, and\nscalability.\n","authors":["Heng Chang","Jiangnan Ye","Alejo Lopez Avila","Jinhua Du","Jia Li"],"pdf_url":"https://arxiv.org/pdf/2401.02290v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.10759v4","updated":"2024-01-04T14:19:18Z","published":"2023-06-19T08:03:25Z","title":"SGFormer: Simplifying and Empowering Transformers for Large-Graph\n  Representations","summary":"  Learning representations on large-sized graphs is a long-standing challenge\ndue to the inter-dependence nature involved in massive data points.\nTransformers, as an emerging class of foundation encoders for graph-structured\ndata, have shown promising performance on small graphs due to its global\nattention capable of capturing all-pair influence beyond neighboring nodes.\nEven so, existing approaches tend to inherit the spirit of Transformers in\nlanguage and vision tasks, and embrace complicated models by stacking deep\nmulti-head attentions. In this paper, we critically demonstrate that even using\na one-layer attention can bring up surprisingly competitive performance across\nnode property prediction benchmarks where node numbers range from\nthousand-level to billion-level. This encourages us to rethink the design\nphilosophy for Transformers on large graphs, where the global attention is a\ncomputation overhead hindering the scalability. We frame the proposed scheme as\nSimplified Graph Transformers (SGFormer), which is empowered by a simple\nattention model that can efficiently propagate information among arbitrary\nnodes in one layer. SGFormer requires none of positional encodings,\nfeature/graph pre-processing or augmented loss. Empirically, SGFormer\nsuccessfully scales to the web-scale graph ogbn-papers100M and yields up to\n141x inference acceleration over SOTA Transformers on medium-sized graphs.\nBeyond current results, we believe the proposed methodology alone enlightens a\nnew technical path of independent interest for building Transformers on large\ngraphs.\n","authors":["Qitian Wu","Wentao Zhao","Chenxiao Yang","Hengrui Zhang","Fan Nie","Haitian Jiang","Yatao Bian","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2306.10759v4.pdf","comment":"Accepted to NeurIPS 2023, the codes are available at\n  https://github.com/qitianwu/SGFormer"},{"id":"http://arxiv.org/abs/2302.03390v4","updated":"2024-01-04T14:18:56Z","published":"2023-02-07T10:51:53Z","title":"Learning Discretized Neural Networks under Ricci Flow","summary":"  In this paper, we study Discretized Neural Networks (DNNs) composed of\nlow-precision weights and activations, which suffer from either infinite or\nzero gradients due to the non-differentiable discrete function during training.\nMost training-based DNNs in such scenarios employ the standard Straight-Through\nEstimator (STE) to approximate the gradient w.r.t. discrete values. However,\nthe use of STE introduces the problem of gradient mismatch, arising from\nperturbations in the approximated gradient. To address this problem, this paper\nreveals that this mismatch can be interpreted as a metric perturbation in a\nRiemannian manifold, viewed through the lens of duality theory. Building on\ninformation geometry, we construct the Linearly Nearly Euclidean (LNE) manifold\nfor DNNs, providing a background for addressing perturbations. By introducing a\npartial differential equation on metrics, i.e., the Ricci flow, we establish\nthe dynamical stability and convergence of the LNE metric with the $L^2$-norm\nperturbation. In contrast to previous perturbation theories with convergence\nrates in fractional powers, the metric perturbation under the Ricci flow\nexhibits exponential decay in the LNE manifold. Experimental results across\nvarious datasets demonstrate that our method achieves superior and more stable\nperformance for DNNs compared to other representative training-based methods.\n","authors":["Jun Chen","Hanwen Chen","Mengmeng Wang","Guang Dai","Ivor W. Tsang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2302.03390v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03585v2","updated":"2024-01-04T14:17:30Z","published":"2023-10-05T15:08:37Z","title":"Smoothing Methods for Automatic Differentiation Across Conditional\n  Branches","summary":"  Programs involving discontinuities introduced by control flow constructs such\nas conditional branches pose challenges to mathematical optimization methods\nthat assume a degree of smoothness in the objective function's response\nsurface. Smooth interpretation (SI) is a form of abstract interpretation that\napproximates the convolution of a program's output with a Gaussian kernel, thus\nsmoothing its output in a principled manner. Here, we combine SI with automatic\ndifferentiation (AD) to efficiently compute gradients of smoothed programs. In\ncontrast to AD across a regular program execution, these gradients also capture\nthe effects of alternative control flow paths. The combination of SI with AD\nenables the direct gradient-based parameter synthesis for branching programs,\nallowing for instance the calibration of simulation models or their combination\nwith neural network models in machine learning pipelines. We detail the effects\nof the approximations made for tractability in SI and propose a novel Monte\nCarlo estimator that avoids the underlying assumptions by estimating the\nsmoothed programs' gradients through a combination of AD and sampling. Using\nDiscoGrad, our tool for automatically translating simple C++ programs to a\nsmooth differentiable form, we perform an extensive evaluation. We compare the\ncombination of SI with AD and our Monte Carlo estimator to existing\ngradient-free and stochastic methods on four non-trivial and originally\ndiscontinuous problems ranging from classical simulation-based optimization to\nneural network-driven control. While the optimization progress with the\nSI-based estimator depends on the complexity of the program's control flow, our\nMonte Carlo estimator is competitive in all problems, exhibiting the fastest\nconvergence by a substantial margin in our highest-dimensional problem.\n","authors":["Justin N. Kreikemeyer","Philipp Andelfinger"],"pdf_url":"https://arxiv.org/pdf/2310.03585v2.pdf","comment":"21 pages, 17 figures, updated content to reflect journal version.\n  Published in IEEE Access, available at\n  https://ieeexplore.ieee.org/abstract/document/10356054"},{"id":"http://arxiv.org/abs/2401.02283v1","updated":"2024-01-04T14:01:24Z","published":"2024-01-04T14:01:24Z","title":"DEM: A Method for Certifying Deep Neural Network Classifier Outputs in\n  Aerospace","summary":"  Software development in the aerospace domain requires adhering to strict,\nhigh-quality standards. While there exist regulatory guidelines for commercial\nsoftware in this domain (e.g., ARP-4754 and DO-178), these do not apply to\nsoftware with deep neural network (DNN) components. Consequently, it is unclear\nhow to allow aerospace systems to benefit from the deep learning revolution.\nOur work here seeks to address this challenge with a novel, output-centric\napproach for DNN certification. Our method employs statistical verification\ntechniques, and has the key advantage of being able to flag specific inputs for\nwhich the DNN's output may be unreliable - so that they may be later inspected\nby a human expert. To achieve this, our method conducts a statistical analysis\nof the DNN's predictions for other, nearby inputs, in order to detect\ninconsistencies. This is in contrast to existing techniques, which typically\nattempt to certify the entire DNN, as opposed to individual outputs. Our method\nuses the DNN as a black-box, and makes no assumptions about its topology. We\nhope that this work constitutes another step towards integrating DNNs in\nsafety-critical applications - especially in the aerospace domain, where high\nstandards of quality and reliability are crucial.\n","authors":["Guy Katz","Natan Levy","Idan Refaeli","Raz Yerushalmi"],"pdf_url":"https://arxiv.org/pdf/2401.02283v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2401.02278v1","updated":"2024-01-04T13:56:54Z","published":"2024-01-04T13:56:54Z","title":"Lightweight Fish Classification Model for Sustainable Marine Management:\n  Indonesian Case","summary":"  The enormous demand for seafood products has led to exploitation of marine\nresources and near-extinction of some species. In particular, overfishing is\none the main issues in sustainable marine development. In alignment with the\nprotection of marine resources and sustainable fishing, this study proposes to\nadvance fish classification techniques that support identifying protected fish\nspecies using state-of-the-art machine learning. We use a custom modification\nof the MobileNet model to design a lightweight classifier called M-MobileNet\nthat is capable of running on limited hardware. As part of the study, we\ncompiled a labeled dataset of 37,462 images of fish found in the waters of the\nIndonesian archipelago. The proposed model is trained on the dataset to\nclassify images of the captured fish into their species and give\nrecommendations on whether they are consumable or not. Our modified MobileNet\nmodel uses only 50\\% of the top layer parameters with about 42% GTX 860M\nutility and achieves up to 97% accuracy in fish classification and determining\nits consumability. Given the limited computing capacity available on many\nfishing vessels, the proposed model provides a practical solution to on-site\nfish classification. In addition, synchronized implementation of the proposed\nmodel on multiple vessels can supply valuable information about the movement\nand location of different species of fish.\n","authors":["Febrian Kurniawan","Gandeva Bayu Satrya","Firuz Kamalov"],"pdf_url":"https://arxiv.org/pdf/2401.02278v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02277v1","updated":"2024-01-04T13:56:13Z","published":"2024-01-04T13:56:13Z","title":"Universal Approximation Theorem for Vector- and Hypercomplex-Valued\n  Neural Networks","summary":"  The universal approximation theorem states that a neural network with one\nhidden layer can approximate continuous functions on compact sets with any\ndesired precision. This theorem supports using neural networks for various\napplications, including regression and classification tasks. Furthermore, it is\nvalid for real-valued neural networks and some hypercomplex-valued neural\nnetworks such as complex-, quaternion-, tessarine-, and Clifford-valued neural\nnetworks. However, hypercomplex-valued neural networks are a type of\nvector-valued neural network defined on an algebra with additional algebraic or\ngeometric properties. This paper extends the universal approximation theorem\nfor a wide range of vector-valued neural networks, including\nhypercomplex-valued models as particular instances. Precisely, we introduce the\nconcept of non-degenerate algebra and state the universal approximation theorem\nfor neural networks defined on such algebras.\n","authors":["Marcos Eduardo Valle","Wington L. Vital","Guilherme Vieira"],"pdf_url":"https://arxiv.org/pdf/2401.02277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.09457v2","updated":"2024-01-04T13:43:16Z","published":"2023-02-19T02:12:21Z","title":"Attacks in Adversarial Machine Learning: A Systematic Survey from the\n  Life-cycle Perspective","summary":"  Adversarial machine learning (AML) studies the adversarial phenomenon of\nmachine learning, which may make inconsistent or unexpected predictions with\nhumans. Some paradigms have been recently developed to explore this adversarial\nphenomenon occurring at different stages of a machine learning system, such as\nbackdoor attack occurring at the pre-training, in-training and inference stage;\nweight attack occurring at the post-training, deployment and inference stage;\nadversarial attack occurring at the inference stage. However, although these\nadversarial paradigms share a common goal, their developments are almost\nindependent, and there is still no big picture of AML. In this work, we aim to\nprovide a unified perspective to the AML community to systematically review the\noverall progress of this field. We firstly provide a general definition about\nAML, and then propose a unified mathematical framework to covering existing\nattack paradigms. According to the proposed unified framework, we build a full\ntaxonomy to systematically categorize and review existing representative\nmethods for each paradigm. Besides, using this unified framework, it is easy to\nfigure out the connections and differences among different attack paradigms,\nwhich may inspire future researchers to develop more advanced attack paradigms.\nFinally, to facilitate the viewing of the built taxonomy and the related\nliterature in adversarial machine learning, we further provide a website, \\ie,\n\\url{http://adversarial-ml.com}, where the taxonomies and literature will be\ncontinuously updated.\n","authors":["Baoyuan Wu","Zihao Zhu","Li Liu","Qingshan Liu","Zhaofeng He","Siwei Lyu"],"pdf_url":"https://arxiv.org/pdf/2302.09457v2.pdf","comment":"35 pages, 4 figures, 10 tables, 313 reference papers"},{"id":"http://arxiv.org/abs/2401.02258v1","updated":"2024-01-04T13:21:11Z","published":"2024-01-04T13:21:11Z","title":"Uncertainty-Aware Deep Attention Recurrent Neural Network for\n  Heterogeneous Time Series Imputation","summary":"  Missingness is ubiquitous in multivariate time series and poses an obstacle\nto reliable downstream analysis. Although recurrent network imputation achieved\nthe SOTA, existing models do not scale to deep architectures that can\npotentially alleviate issues arising in complex data. Moreover, imputation\ncarries the risk of biased estimations of the ground truth. Yet, confidence in\nthe imputed values is always unmeasured or computed post hoc from model output.\nWe propose DEep Attention Recurrent Imputation (DEARI), which jointly estimates\nmissing values and their associated uncertainty in heterogeneous multivariate\ntime series. By jointly representing feature-wise correlations and temporal\ndynamics, we adopt a self attention mechanism, along with an effective residual\ncomponent, to achieve a deep recurrent neural network with good imputation\nperformance and stable convergence. We also leverage self-supervised metric\nlearning to boost performance by optimizing sample similarity. Finally, we\ntransform DEARI into a Bayesian neural network through a novel Bayesian\nmarginalization strategy to produce stochastic DEARI, which outperforms its\ndeterministic equivalent. Experiments show that DEARI surpasses the SOTA in\ndiverse imputation tasks using real-world datasets, namely air quality control,\nhealthcare and traffic.\n","authors":["Linglong Qian","Zina Ibrahim","Richard Dobson"],"pdf_url":"https://arxiv.org/pdf/2401.02258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.07520v2","updated":"2024-01-04T13:18:00Z","published":"2023-04-15T10:09:03Z","title":"STAS: Spatial-Temporal Return Decomposition for Multi-agent\n  Reinforcement Learning","summary":"  Centralized Training with Decentralized Execution (CTDE) has been proven to\nbe an effective paradigm in cooperative multi-agent reinforcement learning\n(MARL). One of the major challenges is credit assignment, which aims to credit\nagents by their contributions. While prior studies have shown great success,\ntheir methods typically fail to work in episodic reinforcement learning\nscenarios where global rewards are revealed only at the end of the episode.\nThey lack the functionality to model complicated relations of the delayed\nglobal reward in the temporal dimension and suffer from inefficiencies. To\ntackle this, we introduce Spatial-Temporal Attention with Shapley (STAS), a\nnovel method that learns credit assignment in both temporal and spatial\ndimensions. It first decomposes the global return back to each time step, then\nutilizes the Shapley Value to redistribute the individual payoff from the\ndecomposed global reward. To mitigate the computational complexity of the\nShapley Value, we introduce an approximation of marginal contribution and\nutilize Monte Carlo sampling to estimate it. We evaluate our method on an Alice\n& Bob example and MPE environments across different scenarios. Our results\ndemonstrate that our method effectively assigns spatial-temporal credit,\noutperforming all state-of-the-art baselines.\n","authors":["Sirui Chen","Zhaowei Zhang","Yaodong Yang","Yali Du"],"pdf_url":"https://arxiv.org/pdf/2304.07520v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.16713v2","updated":"2024-01-04T13:17:07Z","published":"2023-12-27T20:42:40Z","title":"Knowledge Enhanced Conditional Imputation for Healthcare Time-series","summary":"  This study presents a novel approach to addressing the challenge of missing\ndata in multivariate time series, with a particular focus on the complexities\nof healthcare data. Our Conditional Self-Attention Imputation (CSAI) model,\ngrounded in a transformer-based framework, introduces a conditional hidden\nstate initialization tailored to the intricacies of medical time series data.\nThis methodology diverges from traditional imputation techniques by\nspecifically targeting the imbalance in missing data distribution, a crucial\naspect often overlooked in healthcare datasets. By integrating advanced\nknowledge embedding and a non-uniform masking strategy, CSAI adeptly adjusts to\nthe distinct patterns of missing data in Electronic Health Records (EHRs).\n","authors":["Linglong Qian","Zina Ibrahim","Hugh Logan Ellis","Ao Zhang","Yuezhou Zhang","Tao Wang","Richard Dobson"],"pdf_url":"https://arxiv.org/pdf/2312.16713v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02255v1","updated":"2024-01-04T13:11:43Z","published":"2024-01-04T13:11:43Z","title":"Balancing Continual Learning and Fine-tuning for Human Activity\n  Recognition","summary":"  Wearable-based Human Activity Recognition (HAR) is a key task in\nhuman-centric machine learning due to its fundamental understanding of human\nbehaviours. Due to the dynamic nature of human behaviours, continual learning\npromises HAR systems that are tailored to users' needs. However, because of the\ndifficulty in collecting labelled data with wearable sensors, existing\napproaches that focus on supervised continual learning have limited\napplicability, while unsupervised continual learning methods only handle\nrepresentation learning while delaying classifier training to a later stage.\nThis work explores the adoption and adaptation of CaSSLe, a continual\nself-supervised learning model, and Kaizen, a semi-supervised continual\nlearning model that balances representation learning and down-stream\nclassification, for the task of wearable-based HAR. These schemes re-purpose\ncontrastive learning for knowledge retention and, Kaizen combines that with\nself-training in a unified scheme that can leverage unlabelled and labelled\ndata for continual learning. In addition to comparing state-of-the-art\nself-supervised continual learning schemes, we further investigated the\nimportance of different loss terms and explored the trade-off between knowledge\nretention and learning from new tasks. In particular, our extensive evaluation\ndemonstrated that the use of a weighting factor that reflects the ratio between\nlearned and new classes achieves the best overall trade-off in continual\nlearning.\n","authors":["Chi Ian Tang","Lorena Qendro","Dimitris Spathis","Fahim Kawsar","Akhil Mathur","Cecilia Mascolo"],"pdf_url":"https://arxiv.org/pdf/2401.02255v1.pdf","comment":"AAAI 2024 HCRL (Human-Centric Representation Learning) Workshop"},{"id":"http://arxiv.org/abs/2401.02254v1","updated":"2024-01-04T13:11:17Z","published":"2024-01-04T13:11:17Z","title":"L3Cube-IndicNews: News-based Short Text and Long Document Classification\n  Datasets in Indic Languages","summary":"  In this work, we introduce L3Cube-IndicNews, a multilingual text\nclassification corpus aimed at curating a high-quality dataset for Indian\nregional languages, with a specific focus on news headlines and articles. We\nhave centered our work on 10 prominent Indic languages, including Hindi,\nBengali, Marathi, Telugu, Tamil, Gujarati, Kannada, Odia, Malayalam, and\nPunjabi. Each of these news datasets comprises 10 or more classes of news\narticles. L3Cube-IndicNews offers 3 distinct datasets tailored to handle\ndifferent document lengths that are classified as: Short Headlines\nClassification (SHC) dataset containing the news headline and news category,\nLong Document Classification (LDC) dataset containing the whole news article\nand the news category, and Long Paragraph Classification (LPC) containing\nsub-articles of the news and the news category. We maintain consistent labeling\nacross all 3 datasets for in-depth length-based analysis. We evaluate each of\nthese Indic language datasets using 4 different models including monolingual\nBERT, multilingual Indic Sentence BERT (IndicSBERT), and IndicBERT. This\nresearch contributes significantly to expanding the pool of available text\nclassification datasets and also makes it possible to develop topic\nclassification models for Indian regional languages. This also serves as an\nexcellent resource for cross-lingual analysis owing to the high overlap of\nlabels among languages. The datasets and models are shared publicly at\nhttps://github.com/l3cube-pune/indic-nlp\n","authors":["Aishwarya Mirashi","Srushti Sonavane","Purva Lingayat","Tejas Padhiyar","Raviraj Joshi"],"pdf_url":"https://arxiv.org/pdf/2401.02254v1.pdf","comment":"Accepted at the International Conference on Natural Language\n  Processing (ICON 2023)"},{"id":"http://arxiv.org/abs/2401.02244v1","updated":"2024-01-04T12:54:10Z","published":"2024-01-04T12:54:10Z","title":"Policy-regularized Offline Multi-objective Reinforcement Learning","summary":"  In this paper, we aim to utilize only offline trajectory data to train a\npolicy for multi-objective RL. We extend the offline policy-regularized method,\na widely-adopted approach for single-objective offline RL problems, into the\nmulti-objective setting in order to achieve the above goal. However, such\nmethods face a new challenge in offline MORL settings, namely the\npreference-inconsistent demonstration problem. We propose two solutions to this\nproblem: 1) filtering out preference-inconsistent demonstrations via\napproximating behavior preferences, and 2) adopting regularization techniques\nwith high policy expressiveness. Moreover, we integrate the\npreference-conditioned scalarized update method into policy-regularized offline\nRL, in order to simultaneously learn a set of policies using a single policy\nnetwork, thus reducing the computational cost induced by the training of a\nlarge number of individual policies for various preferences. Finally, we\nintroduce Regularization Weight Adaptation to dynamically determine appropriate\nregularization weights for arbitrary target preferences during deployment.\nEmpirical results on various multi-objective datasets demonstrate the\ncapability of our approach in solving offline MORL problems.\n","authors":["Qian Lin","Chao Yu","Zongkai Liu","Zifan Wu"],"pdf_url":"https://arxiv.org/pdf/2401.02244v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02236v1","updated":"2024-01-04T12:41:40Z","published":"2024-01-04T12:41:40Z","title":"U-Mixer: An Unet-Mixer Architecture with Stationarity Correction for\n  Time Series Forecasting","summary":"  Time series forecasting is a crucial task in various domains. Caused by\nfactors such as trends, seasonality, or irregular fluctuations, time series\noften exhibits non-stationary. It obstructs stable feature propagation through\ndeep layers, disrupts feature distributions, and complicates learning data\ndistribution changes. As a result, many existing models struggle to capture the\nunderlying patterns, leading to degraded forecasting performance. In this\nstudy, we tackle the challenge of non-stationarity in time series forecasting\nwith our proposed framework called U-Mixer. By combining Unet and Mixer,\nU-Mixer effectively captures local temporal dependencies between different\npatches and channels separately to avoid the influence of distribution\nvariations among channels, and merge low- and high-levels features to obtain\ncomprehensive data representations. The key contribution is a novel\nstationarity correction method, explicitly restoring data distribution by\nconstraining the difference in stationarity between the data before and after\nmodel processing to restore the non-stationarity information, while ensuring\nthe temporal dependencies are preserved. Through extensive experiments on\nvarious real-world time series datasets, U-Mixer demonstrates its effectiveness\nand robustness, and achieves 14.5\\% and 7.7\\% improvements over\nstate-of-the-art (SOTA) methods.\n","authors":["Xiang Ma","Xuemei Li","Lexin Fang","Tianlong Zhao","Caiming Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.02236v1.pdf","comment":"Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2401.02225v1","updated":"2024-01-04T12:21:01Z","published":"2024-01-04T12:21:01Z","title":"Trajectory-Oriented Policy Optimization with Sparse Rewards","summary":"  Deep reinforcement learning (DRL) remains challenging in tasks with sparse\nrewards. These sparse rewards often only indicate whether the task is partially\nor fully completed, meaning that many exploration actions must be performed\nbefore the agent obtains useful feedback. Hence, most existing DRL algorithms\nfail to learn feasible policies within a reasonable time frame. To overcome\nthis problem, we develop an approach that exploits offline demonstration\ntrajectories for faster and more efficient online RL in sparse reward settings.\nOur key insight is that by regarding offline demonstration trajectories as\nguidance, instead of imitating them, our method learns a policy whose\nstate-action visitation marginal distribution matches that of offline\ndemonstrations. Specifically, we introduce a novel trajectory distance based on\nmaximum mean discrepancy (MMD) and formulate policy optimization as a\ndistance-constrained optimization problem. Then, we show that this\ndistance-constrained optimization problem can be reduced into a policy-gradient\nalgorithm with shaped rewards learned from offline demonstrations. The proposed\nalgorithm is evaluated on extensive discrete and continuous control tasks with\nsparse and deceptive rewards. The experimental results indicate that our\nproposed algorithm is significantly better than the baseline methods regarding\ndiverse exploration and learning the optimal policy.\n","authors":["Guojian Wang","Faguo Wu","Xiao Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.02225v1.pdf","comment":"5 pages, 7 figures"},{"id":"http://arxiv.org/abs/2104.10561v2","updated":"2024-01-04T11:57:06Z","published":"2021-04-21T14:32:03Z","title":"Covert Channel Attack to Federated Learning Systems","summary":"  Federated learning (FL) goes beyond traditional, centralized machine learning\nby distributing model training among a large collection of edge clients. These\nclients cooperatively train a global, e.g., cloud-hosted, model without\ndisclosing their local, private training data. The global model is then shared\namong all the participants which use it for local predictions. In this paper,\nwe put forward a novel attacker model aiming at turning FL systems into covert\nchannels to implement a stealth communication infrastructure. The main\nintuition is that, during federated training, a malicious sender can poison the\nglobal model by submitting purposely crafted examples. Although the effect of\nthe model poisoning is negligible to other participants, and does not alter the\noverall model performance, it can be observed by a malicious receiver and used\nto transmit a single bit.\n","authors":["Gabriele Costa","Fabio Pinelli","Simone Soderi","Gabriele Tolomei"],"pdf_url":"https://arxiv.org/pdf/2104.10561v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2107.01752v5","updated":"2024-01-04T11:53:29Z","published":"2021-07-05T00:51:02Z","title":"Dynamic programming by polymorphic semiring algebraic shortcut fusion","summary":"  Dynamic programming (DP) is an algorithmic design paradigm for the efficient,\nexact solution of otherwise intractable, combinatorial problems. However, DP\nalgorithm design is often presented in an ad-hoc manner. It is sometimes\ndifficult to justify algorithm correctness. To address this issue, this paper\npresents a rigorous algebraic formalism for systematically deriving DP\nalgorithms, based on semiring polymorphism. We start with a specification,\nconstruct an algorithm to compute the required solution which is self-evidently\ncorrect because it exhaustively generates and evaluates all possible solutions\nmeeting the specification. We then derive, through the use of shortcut fusion,\nan implementation of this algorithm which is both efficient and correct. We\nalso demonstrate how, with the use of semiring lifting, the specification can\nbe augmented with combinatorial constraints, showing how these constraints can\nbe fused with the algorithm. We furthermore demonstrate how existing DP\nalgorithms for a given combinatorial problem can be abstracted from their\noriginal context and re-purposed.\n  This approach can be applied to the full scope of combinatorial problems\nexpressible in terms of semirings. This includes, for example: optimal\nprobability and Viterbi decoding, probabilistic marginalization, logical\ninference, fuzzy sets, differentiable softmax, relational and provenance\nqueries. The approach, building on ideas from the existing literature on\nconstructive algorithmics, exploits generic properties of polymorphic\nfunctions, tupling and formal sums and algebraic simplifications arising from\nconstraint algebras. We demonstrate the effectiveness of this formalism for\nsome example applications arising in signal processing, bioinformatics and\nreliability engineering. Python software implementing these algorithms can be\ndownloaded from: http://www.maxlittle.net/software/dppolyalg.zip.\n","authors":["Max A. Little","Xi He","Ugur Kayas"],"pdf_url":"https://arxiv.org/pdf/2107.01752v5.pdf","comment":"Updated v22 with revised text"},{"id":"http://arxiv.org/abs/2401.02203v1","updated":"2024-01-04T11:15:44Z","published":"2024-01-04T11:15:44Z","title":"Robust bilinear factor analysis based on the matrix-variate $t$\n  distribution","summary":"  Factor Analysis based on multivariate $t$ distribution ($t$fa) is a useful\nrobust tool for extracting common factors on heavy-tailed or contaminated data.\nHowever, $t$fa is only applicable to vector data. When $t$fa is applied to\nmatrix data, it is common to first vectorize the matrix observations. This\nintroduces two challenges for $t$fa: (i) the inherent matrix structure of the\ndata is broken, and (ii) robustness may be lost, as vectorized matrix data\ntypically results in a high data dimension, which could easily lead to the\nbreakdown of $t$fa. To address these issues, starting from the intrinsic matrix\nstructure of matrix data, a novel robust factor analysis model, namely bilinear\nfactor analysis built on the matrix-variate $t$ distribution ($t$bfa), is\nproposed in this paper. The novelty is that it is capable to simultaneously\nextract common factors for both row and column variables of interest on\nheavy-tailed or contaminated matrix data. Two efficient algorithms for maximum\nlikelihood estimation of $t$bfa are developed. Closed-form expression for the\nFisher information matrix to calculate the accuracy of parameter estimates are\nderived. Empirical studies are conducted to understand the proposed $t$bfa\nmodel and compare with related competitors. The results demonstrate the\nsuperiority and practicality of $t$bfa. Importantly, $t$bfa exhibits a\nsignificantly higher breakdown point than $t$fa, making it more suitable for\nmatrix data.\n","authors":["Xuan Ma","Jianhua Zhao","Changchun Shang","Fen Jiang","Philip L. H. Yu"],"pdf_url":"https://arxiv.org/pdf/2401.02203v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15256v2","updated":"2024-01-04T11:10:57Z","published":"2023-08-29T12:30:53Z","title":"Let There Be Sound: Reconstructing High Quality Speech from Silent\n  Videos","summary":"  The goal of this work is to reconstruct high quality speech from lip motions\nalone, a task also known as lip-to-speech. A key challenge of lip-to-speech\nsystems is the one-to-many mapping caused by (1) the existence of homophenes\nand (2) multiple speech variations, resulting in a mispronounced and\nover-smoothed speech. In this paper, we propose a novel lip-to-speech system\nthat significantly improves the generation quality by alleviating the\none-to-many mapping problem from multiple perspectives. Specifically, we\nincorporate (1) self-supervised speech representations to disambiguate\nhomophenes, and (2) acoustic variance information to model diverse speech\nstyles. Additionally, to better solve the aforementioned problem, we employ a\nflow based post-net which captures and refines the details of the generated\nspeech. We perform extensive experiments on two datasets, and demonstrate that\nour method achieves the generation quality close to that of real human\nutterance, outperforming existing methods in terms of speech naturalness and\nintelligibility by a large margin. Synthesised samples are available at our\ndemo page: https://mm.kaist.ac.kr/projects/LTBS.\n","authors":["Ji-Hoon Kim","Jaehun Kim","Joon Son Chung"],"pdf_url":"https://arxiv.org/pdf/2308.15256v2.pdf","comment":"Accepted to AAAI 2024"},{"id":"http://arxiv.org/abs/2401.02199v1","updated":"2024-01-04T11:09:15Z","published":"2024-01-04T11:09:15Z","title":"LADRI: LeArning-based Dynamic Risk Indicator in Automated Driving System","summary":"  As the horizon of intelligent transportation expands with the evolution of\nAutomated Driving Systems (ADS), ensuring paramount safety becomes more\nimperative than ever. Traditional risk assessment methodologies, primarily\ncrafted for human-driven vehicles, grapple to adequately adapt to the\nmultifaceted, evolving environments of ADS. This paper introduces a framework\nfor real-time Dynamic Risk Assessment (DRA) in ADS, harnessing the potency of\nArtificial Neural Networks (ANNs).\n  Our proposed solution transcends these limitations, drawing upon ANNs, a\ncornerstone of deep learning, to meticulously analyze and categorize risk\ndimensions using real-time On-board Sensor (OBS) data. This learning-centric\napproach not only elevates the ADS's situational awareness but also enriches\nits understanding of immediate operational contexts. By dissecting OBS data,\nthe system is empowered to pinpoint its current risk profile, thereby enhancing\nsafety prospects for onboard passengers and the broader traffic ecosystem.\n  Through this framework, we chart a direction in risk assessment, bridging the\nconventional voids and enhancing the proficiency of ADS. By utilizing ANNs, our\nmethodology offers a perspective, allowing ADS to adeptly navigate and react to\npotential risk factors, ensuring safer and more informed autonomous journeys.\n","authors":["Anil Ranjitbhai Patel","Peter Liggesmeyer"],"pdf_url":"https://arxiv.org/pdf/2401.02199v1.pdf","comment":"2023 IEEE International Test Conference, 8th Edition of Automotive,\n  Reliability, Test & Safety Workshop in Disneyland, Anaheim, CA"},{"id":"http://arxiv.org/abs/2401.02192v1","updated":"2024-01-04T10:54:05Z","published":"2024-01-04T10:54:05Z","title":"Nodule detection and generation on chest X-rays: NODE21 Challenge","summary":"  Pulmonary nodules may be an early manifestation of lung cancer, the leading\ncause of cancer-related deaths among both men and women. Numerous studies have\nestablished that deep learning methods can yield high-performance levels in the\ndetection of lung nodules in chest X-rays. However, the lack of gold-standard\npublic datasets slows down the progression of the research and prevents\nbenchmarking of methods for this task. To address this, we organized a public\nresearch challenge, NODE21, aimed at the detection and generation of lung\nnodules in chest X-rays. While the detection track assesses state-of-the-art\nnodule detection systems, the generation track determines the utility of nodule\ngeneration algorithms to augment training data and hence improve the\nperformance of the detection systems. This paper summarizes the results of the\nNODE21 challenge and performs extensive additional experiments to examine the\nimpact of the synthetically generated nodule training images on the detection\nalgorithm performance.\n","authors":["Ecem Sogancioglu","Bram van Ginneken","Finn Behrendt","Marcel Bengs","Alexander Schlaefer","Miron Radu","Di Xu","Ke Sheng","Fabien Scalzo","Eric Marcus","Samuele Papa","Jonas Teuwen","Ernst Th. Scholten","Steven Schalekamp","Nils Hendrix","Colin Jacobs","Ward Hendrix","Clara I Sánchez","Keelin Murphy"],"pdf_url":"https://arxiv.org/pdf/2401.02192v1.pdf","comment":"15 pages, 5 figures"},{"id":"http://arxiv.org/abs/2306.11586v3","updated":"2024-01-04T10:43:46Z","published":"2023-06-20T15:03:31Z","title":"Provably Powerful Graph Neural Networks for Directed Multigraphs","summary":"  This paper analyses a set of simple adaptations that transform standard\nmessage-passing Graph Neural Networks (GNN) into provably powerful directed\nmultigraph neural networks. The adaptations include multigraph port numbering,\nego IDs, and reverse message passing. We prove that the combination of these\ntheoretically enables the detection of any directed subgraph pattern. To\nvalidate the effectiveness of our proposed adaptations in practice, we conduct\nexperiments on synthetic subgraph detection tasks, which demonstrate\noutstanding performance with almost perfect results. Moreover, we apply our\nproposed adaptations to two financial crime analysis tasks. We observe dramatic\nimprovements in detecting money laundering transactions, improving the\nminority-class F1 score of a standard message-passing GNN by up to 30%, and\nclosely matching or outperforming tree-based and GNN baselines. Similarly\nimpressive results are observed on a real-world phishing detection dataset,\nboosting three standard GNNs' F1 scores by around 15% and outperforming all\nbaselines.\n","authors":["Béni Egressy","Luc von Niederhäusern","Jovan Blanusa","Erik Altman","Roger Wattenhofer","Kubilay Atasu"],"pdf_url":"https://arxiv.org/pdf/2306.11586v3.pdf","comment":"AAAI 2024"},{"id":"http://arxiv.org/abs/2401.02183v1","updated":"2024-01-04T10:29:02Z","published":"2024-01-04T10:29:02Z","title":"FairGridSearch: A Framework to Compare Fairness-Enhancing Models","summary":"  Machine learning models are increasingly used in critical decision-making\napplications. However, these models are susceptible to replicating or even\namplifying bias present in real-world data. While there are various bias\nmitigation methods and base estimators in the literature, selecting the optimal\nmodel for a specific application remains challenging.\n  This paper focuses on binary classification and proposes FairGridSearch, a\nnovel framework for comparing fairness-enhancing models. FairGridSearch enables\nexperimentation with different model parameter combinations and recommends the\nbest one. The study applies FairGridSearch to three popular datasets (Adult,\nCOMPAS, and German Credit) and analyzes the impacts of metric selection, base\nestimator choice, and classification threshold on model fairness.\n  The results highlight the significance of selecting appropriate accuracy and\nfairness metrics for model evaluation. Additionally, different base estimators\nand classification threshold values affect the effectiveness of bias mitigation\nmethods and fairness stability respectively, but the effects are not consistent\nacross all datasets. Based on these findings, future research on fairness in\nmachine learning should consider a broader range of factors when building fair\nmodels, going beyond bias mitigation methods alone.\n","authors":["Shih-Chi Ma","Tatiana Ermakova","Benjamin Fabian"],"pdf_url":"https://arxiv.org/pdf/2401.02183v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12789v2","updated":"2024-01-04T09:34:08Z","published":"2023-12-20T06:22:21Z","title":"SLP-Net:An efficient lightweight network for segmentation of skin\n  lesions","summary":"  Prompt treatment for melanoma is crucial. To assist physicians in identifying\nlesion areas precisely in a quick manner, we propose a novel skin lesion\nsegmentation technique namely SLP-Net, an ultra-lightweight segmentation\nnetwork based on the spiking neural P(SNP) systems type mechanism. Most\nexisting convolutional neural networks achieve high segmentation accuracy while\nneglecting the high hardware cost. SLP-Net, on the contrary, has a very small\nnumber of parameters and a high computation speed. We design a lightweight\nmulti-scale feature extractor without the usual encoder-decoder structure.\nRather than a decoder, a feature adaptation module is designed to replace it\nand implement multi-scale information decoding. Experiments at the ISIC2018\nchallenge demonstrate that the proposed model has the highest Acc and DSC among\nthe state-of-the-art methods, while experiments on the PH2 dataset also\ndemonstrate a favorable generalization ability. Finally, we compare the\ncomputational complexity as well as the computational speed of the models in\nexperiments, where SLP-Net has the highest overall superiority\n","authors":["Bo Yang","Hong Peng","Chenggang Guo","Xiaohui Luo","Jun Wang","Xianzhong Long"],"pdf_url":"https://arxiv.org/pdf/2312.12789v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01520v2","updated":"2024-01-04T09:23:10Z","published":"2023-11-29T15:51:04Z","title":"Entropy and the Kullback-Leibler Divergence for Bayesian Networks:\n  Computational Complexity and Efficient Implementation","summary":"  Bayesian networks (BNs) are a foundational model in machine learning and\ncausal inference. Their graphical structure can handle high-dimensional\nproblems, divide them into a sparse collection of smaller ones, underlies Judea\nPearl's causality, and determines their explainability and interpretability.\nDespite their popularity, there are almost no resources in the literature on\nhow to compute Shannon's entropy and the Kullback-Leibler (KL) divergence for\nBNs under their most common distributional assumptions. In this paper, we\nprovide computationally efficient algorithms for both by leveraging BNs'\ngraphical structure, and we illustrate them with a complete set of numerical\nexamples. In the process, we show it is possible to reduce the computational\ncomplexity of KL from cubic to quadratic for Gaussian BNs.\n","authors":["Marco Scutari"],"pdf_url":"https://arxiv.org/pdf/2312.01520v2.pdf","comment":"32 pages, 4 figures"},{"id":"http://arxiv.org/abs/2011.14956v5","updated":"2024-01-04T09:17:42Z","published":"2020-11-25T09:40:34Z","title":"Handling Noisy Labels via One-Step Abductive Multi-Target Learning and\n  Its Application to Helicobacter Pylori Segmentation","summary":"  Learning from noisy labels is an important concern in plenty of real-world\nscenarios. Various approaches for this concern first make corrections\ncorresponding to potentially noisy-labeled instances, and then update\npredictive model with information of the made corrections. However, in specific\nareas, such as medical histopathology whole slide image analysis (MHWSIA), it\nis often difficult or impossible for experts to manually achieve the noisy-free\nground-truth labels which leads to labels with complex noise. This situation\nraises two more difficult problems: 1) the methodology of approaches making\ncorrections corresponding to potentially noisy-labeled instances has\nlimitations due to the complex noise existing in labels; and 2) the appropriate\nevaluation strategy for validation/testing is unclear because of the great\ndifficulty in collecting the noisy-free ground-truth labels. For the problem\n1), we present one-step abductive multi-target learning (OSAMTL) that imposes a\none-step logical reasoning upon machine learning via a multi-target learning\nprocedure to constrain the predictions of the learning model to be subject to\nour prior knowledge about the true target. For the problem 2), we propose a\nlogical assessment formula (LAF) that evaluates the logical rationality of the\noutputs of an approach by estimating the consistencies between the predictions\nof the learning model and the logical facts narrated from the results of the\none-step logical reasoning of OSAMTL. Based on the Helicobacter pylori (H.\npylori) segmentation task in MHWSIA, we show that OSAMTL enables the machine\nlearning model achieving logically more rational predictions, which is beyond\nvarious state-of-the-art approaches in handling complex noisy labels.\n","authors":["Yongquan Yang","Yiming Yang","Jie Chen","Jiayi Zheng","Zhongxi Zheng"],"pdf_url":"https://arxiv.org/pdf/2011.14956v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02154v1","updated":"2024-01-04T09:05:37Z","published":"2024-01-04T09:05:37Z","title":"Disentangle Estimation of Causal Effects from Cross-Silo Data","summary":"  Estimating causal effects among different events is of great importance to\ncritical fields such as drug development. Nevertheless, the data features\nassociated with events may be distributed across various silos and remain\nprivate within respective parties, impeding direct information exchange between\nthem. This, in turn, can result in biased estimations of local causal effects,\nwhich rely on the characteristics of only a subset of the covariates. To tackle\nthis challenge, we introduce an innovative disentangle architecture designed to\nfacilitate the seamless cross-silo transmission of model parameters, enriched\nwith causal mechanisms, through a combination of shared and private branches.\nBesides, we introduce global constraints into the equation to effectively\nmitigate bias within the various missing domains, thereby elevating the\naccuracy of our causal effect estimation. Extensive experiments conducted on\nnew semi-synthetic datasets show that our method outperforms state-of-the-art\nbaselines.\n","authors":["Yuxuan Liu","Haozhao Wang","Shuang Wang","Zhiming He","Wenchao Xu","Jialiang Zhu","Fan Yang"],"pdf_url":"https://arxiv.org/pdf/2401.02154v1.pdf","comment":"Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2401.02143v1","updated":"2024-01-04T08:49:10Z","published":"2024-01-04T08:49:10Z","title":"Graph Neural Networks for Tabular Data Learning: A Survey with Taxonomy\n  and Directions","summary":"  In this survey, we dive into Tabular Data Learning (TDL) using Graph Neural\nNetworks (GNNs), a domain where deep learning-based approaches have\nincreasingly shown superior performance in both classification and regression\ntasks compared to traditional methods. The survey highlights a critical gap in\ndeep neural TDL methods: the underrepresentation of latent correlations among\ndata instances and feature values. GNNs, with their innate capability to model\nintricate relationships and interactions between diverse elements of tabular\ndata, have garnered significant interest and application across various TDL\ndomains. Our survey provides a systematic review of the methods involved in\ndesigning and implementing GNNs for TDL (GNN4TDL). It encompasses a detailed\ninvestigation into the foundational aspects and an overview of GNN-based TDL\nmethods, offering insights into their evolving landscape. We present a\ncomprehensive taxonomy focused on constructing graph structures and\nrepresentation learning within GNN-based TDL methods. In addition, the survey\nexamines various training plans, emphasizing the integration of auxiliary tasks\nto enhance the effectiveness of instance representations. A critical part of\nour discussion is dedicated to the practical application of GNNs across a\nspectrum of GNN4TDL scenarios, demonstrating their versatility and impact.\nLastly, we discuss the limitations and propose future research directions,\naiming to spur advancements in GNN4TDL. This survey serves as a resource for\nresearchers and practitioners, offering a thorough understanding of GNNs' role\nin revolutionizing TDL and pointing towards future innovations in this\npromising area.\n","authors":["Cheng-Te Li","Yu-Che Tsai","Chih-Yao Chen","Jay Chiehen Liao"],"pdf_url":"https://arxiv.org/pdf/2401.02143v1.pdf","comment":"Under review, ongoing work, Github page:\n  https://github.com/Roytsai27/awesome-GNN4TDL"},{"id":"http://arxiv.org/abs/2401.02135v1","updated":"2024-01-04T08:39:49Z","published":"2024-01-04T08:39:49Z","title":"PosCUDA: Position based Convolution for Unlearnable Audio Datasets","summary":"  Deep learning models require large amounts of clean data to acheive good\nperformance. To avoid the cost of expensive data acquisition, researchers use\nthe abundant data available on the internet. This raises significant privacy\nconcerns on the potential misuse of personal data for model training without\nauthorisation. Recent works such as CUDA propose solutions to this problem by\nadding class-wise blurs to make datasets unlearnable, i.e a model can never use\nthe acquired dataset for learning. However these methods often reduce the\nquality of the data making it useless for practical applications. We introduce\nPosCUDA, a position based convolution for creating unlearnable audio datasets.\nPosCUDA uses class-wise convolutions on small patches of audio. The location of\nthe patches are based on a private key for each class, hence the model learns\nthe relations between positional blurs and labels, while failing to generalize.\nWe empirically show that PosCUDA can achieve unlearnability while maintaining\nthe quality of the original audio datasets. Our proposed method is also robust\nto different audio feature representations such as MFCC, raw audio and\ndifferent architectures such as transformers, convolutional networks etc.\n","authors":["Vignesh Gokul","Shlomo Dubnov"],"pdf_url":"https://arxiv.org/pdf/2401.02135v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02124v1","updated":"2024-01-04T08:19:27Z","published":"2024-01-04T08:19:27Z","title":"ACP-ESM: A novel framework for classification of anticancer peptides\n  using protein-oriented transformer approach","summary":"  Anticancer peptides (ACPs) are a class of molecules that have gained\nsignificant attention in the field of cancer research and therapy. ACPs are\nshort chains of amino acids, the building blocks of proteins, and they possess\nthe ability to selectively target and kill cancer cells. One of the key\nadvantages of ACPs is their ability to selectively target cancer cells while\nsparing healthy cells to a greater extent. This selectivity is often attributed\nto differences in the surface properties of cancer cells compared to normal\ncells. That is why ACPs are being investigated as potential candidates for\ncancer therapy. ACPs may be used alone or in combination with other treatment\nmodalities like chemotherapy and radiation therapy. While ACPs hold promise as\na novel approach to cancer treatment, there are challenges to overcome,\nincluding optimizing their stability, improving selectivity, and enhancing\ntheir delivery to cancer cells, continuous increasing in number of peptide\nsequences, developing a reliable and precise prediction model. In this work, we\npropose an efficient transformer-based framework to identify anticancer\npeptides for by performing accurate a reliable and precise prediction model.\nFor this purpose, four different transformer models, namely ESM, ProtBert,\nBioBERT, and SciBERT are employed to detect anticancer peptides from amino acid\nsequences. To demonstrate the contribution of the proposed framework, extensive\nexperiments are carried on widely-used datasets in the literature, two versions\nof AntiCp2, cACP-DeepGram, ACP-740. Experiment results show the usage of\nproposed model enhances classification accuracy when compared to the\nstate-of-the-art studies. The proposed framework, ESM, exhibits 96.45 of\naccuracy for AntiCp2 dataset, 97.66 of accuracy for cACP-DeepGram dataset, and\n88.51 of accuracy for ACP-740 dataset, thence determining new state-of-the-art.\n","authors":["Zeynep Hilal Kilimci","Mustafa Yalcin"],"pdf_url":"https://arxiv.org/pdf/2401.02124v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.04669v4","updated":"2024-01-04T08:00:37Z","published":"2023-08-09T02:27:23Z","title":"A General Implicit Framework for Fast NeRF Composition and Rendering","summary":"  A variety of Neural Radiance Fields (NeRF) methods have recently achieved\nremarkable success in high render speed. However, current accelerating methods\nare specialized and incompatible with various implicit methods, preventing\nreal-time composition over various types of NeRF works. Because NeRF relies on\nsampling along rays, it is possible to provide general guidance for\nacceleration. To that end, we propose a general implicit pipeline for composing\nNeRF objects quickly. Our method enables the casting of dynamic shadows within\nor between objects using analytical light sources while allowing multiple NeRF\nobjects to be seamlessly placed and rendered together with any arbitrary rigid\ntransformations. Mainly, our work introduces a new surface representation known\nas Neural Depth Fields (NeDF) that quickly determines the spatial relationship\nbetween objects by allowing direct intersection computation between rays and\nimplicit surfaces. It leverages an intersection neural network to query NeRF\nfor acceleration instead of depending on an explicit spatial structure.Our\nproposed method is the first to enable both the progressive and interactive\ncomposition of NeRF objects. Additionally, it also serves as a previewing\nplugin for a range of existing NeRF works.\n","authors":["Xinyu Gao","Ziyi Yang","Yunlu Zhao","Yuxiang Sun","Xiaogang Jin","Changqing Zou"],"pdf_url":"https://arxiv.org/pdf/2308.04669v4.pdf","comment":"AAAI 2024"},{"id":"http://arxiv.org/abs/2401.01783v2","updated":"2024-01-04T07:56:58Z","published":"2024-01-03T15:16:25Z","title":"Approximating Numerical Flux by Fourier Neural Operators for the\n  Hyperbolic Conservation Laws","summary":"  Classical numerical schemes exist for solving PDEs numerically, and recently,\nneural network-based methods have been developed. However, methodologies using\nneural networks, such as PINN and neural operators, lack robustness and\ngeneralization power. To compensate for such drawbacks, there are many types of\nresearch combining classical numerical schemes and machine learning methods by\nreplacing a small portion of the numerical schemes with neural networks. In\nthis work, we focus on hyperbolic conservation laws and replace numerical\nfluxes in the numerical schemes by neural operator. For this, we construct\nlosses that are motivated by numerical schemes for conservation laws and\napproximate numerical flux by FNO. Through experiments, we show that our\nmethodology has advantages of both numerical schemes and FNO by comparing with\noriginal methods. For instance, we demonstrate our method gains robustness,\nresolution invariance property, and feasibility of a data-driven method. Our\nmethod especially has the ability to predict continuously in time and\ngeneralization power on the out-of-distribution samples, which are challenges\nto be tackled for existing neural operator methods.\n","authors":["Taeyoung Kim","Myungjoo Kang"],"pdf_url":"https://arxiv.org/pdf/2401.01783v2.pdf","comment":"23 pages, 28 figures"},{"id":"http://arxiv.org/abs/2401.02117v1","updated":"2024-01-04T07:55:53Z","published":"2024-01-04T07:55:53Z","title":"Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost\n  Whole-Body Teleoperation","summary":"  Imitation learning from human demonstrations has shown impressive performance\nin robotics. However, most results focus on table-top manipulation, lacking the\nmobility and dexterity necessary for generally useful tasks. In this work, we\ndevelop a system for imitating mobile manipulation tasks that are bimanual and\nrequire whole-body control. We first present Mobile ALOHA, a low-cost and\nwhole-body teleoperation system for data collection. It augments the ALOHA\nsystem with a mobile base, and a whole-body teleoperation interface. Using data\ncollected with Mobile ALOHA, we then perform supervised behavior cloning and\nfind that co-training with existing static ALOHA datasets boosts performance on\nmobile manipulation tasks. With 50 demonstrations for each task, co-training\ncan increase success rates by up to 90%, allowing Mobile ALOHA to autonomously\ncomplete complex mobile manipulation tasks such as sauteing and serving a piece\nof shrimp, opening a two-door wall cabinet to store heavy cooking pots, calling\nand entering an elevator, and lightly rinsing a used pan using a kitchen\nfaucet. Project website: https://mobile-aloha.github.io\n","authors":["Zipeng Fu","Tony Z. Zhao","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2401.02117v1.pdf","comment":"Project website: https://mobile-aloha.github.io (Zipeng Fu and Tony\n  Z. Zhao are project co-leads, Chelsea Finn is the advisor)"},{"id":"http://arxiv.org/abs/2310.06289v2","updated":"2024-01-04T07:36:29Z","published":"2023-10-10T04:02:43Z","title":"Better and Simpler Lower Bounds for Differentially Private Statistical\n  Estimation","summary":"  We provide optimal lower bounds for two well-known parameter estimation (also\nknown as statistical estimation) tasks in high dimensions with approximate\ndifferential privacy. First, we prove that for any $\\alpha \\le O(1)$,\nestimating the covariance of a Gaussian up to spectral error $\\alpha$ requires\n$\\tilde{\\Omega}\\left(\\frac{d^{3/2}}{\\alpha \\varepsilon} +\n\\frac{d}{\\alpha^2}\\right)$ samples, which is tight up to logarithmic factors.\nThis result improves over previous work which established this for $\\alpha \\le\nO\\left(\\frac{1}{\\sqrt{d}}\\right)$, and is also simpler than previous work.\nNext, we prove that estimating the mean of a heavy-tailed distribution with\nbounded $k$th moments requires $\\tilde{\\Omega}\\left(\\frac{d}{\\alpha^{k/(k-1)}\n\\varepsilon} + \\frac{d}{\\alpha^2}\\right)$ samples. Previous work for this\nproblem was only able to establish this lower bound against pure differential\nprivacy, or in the special case of $k = 2$.\n  Our techniques follow the method of fingerprinting and are generally quite\nsimple. Our lower bound for heavy-tailed estimation is based on a black-box\nreduction from privately estimating identity-covariance Gaussians. Our lower\nbound for covariance estimation utilizes a Bayesian approach to show that,\nunder an Inverse Wishart prior distribution for the covariance matrix, no\nprivate estimator can be accurate even in expectation, without sufficiently\nmany samples.\n","authors":["Shyam Narayanan"],"pdf_url":"https://arxiv.org/pdf/2310.06289v2.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2401.02106v1","updated":"2024-01-04T07:28:56Z","published":"2024-01-04T07:28:56Z","title":"Cadmium Zinc Telluride (CZT) photon counting detector Characterisation\n  for soft tissue imaging","summary":"  The use of photon counting detection technology has resulted in significant\nX-ray imaging research interest in recent years. Computed Tomography (CT)\nscanners can benefit from photon-counting detectors, which are new technology\nwith the potential to overcome key limitations of conventional CT detectors.\nResearchers are still studying the effectiveness and sensitivity of\nsemiconductor detector materials in photon counting detectors for detecting\nsoft tissue contrasts. This study aimed to characterize the performance of the\nCadmium Zinc Telluride photon counting detector in identifying various tissues.\nAn optimal frame rate per second (FPS) of CZT detector was evaluated by setting\nthe X-ray tube voltage and current at 25 keV, 35 keV and 0.5 mA, 1.0 mA\nrespectively by keeping the optimum FPS fixed, the detector energy thresholds\nwere set in small steps from 15 keV to 35 keV and the Currents were set for\nX-ray tubes in ranges of 0.1 mA to 1.0 mA to find the relationship between\nvoltage and current of the X-ray source and counts per second (CPS). The\nsamples i.e., fat, liver, muscles, paraffin wax, and contrast media were\nstacked at six different thickness levels in a stair-step chamber made from\nPlexi-glass. X-ray transmission at six different thicknesses of tissue samples\nwas also examined for five different energy (regions) thresholds (21 keV, 25\nkeV, 29 keV, 31 keV, and 45 keV) to determine the effect on count per second\n(CPS). In this study, 12 frames per second is found to be the optimum frame\nrate per second (FPS) based on the spectral response of an X-ray source and CPS\nhas a linear relationship with X-ray tube current as well. It was also noted\nthat A sample's thickness also affects its X-ray transmission at different\nenergy thresholds. A high sensitivity and linearity of the detectors make them\nsuitable for use in both preclinical and medical applications.\n","authors":["Kamran Hameed","Rafidah Zainon","Mahbubunnabi Tamal"],"pdf_url":"https://arxiv.org/pdf/2401.02106v1.pdf","comment":"29 pages and 11 figures"},{"id":"http://arxiv.org/abs/2112.08364v3","updated":"2024-01-04T07:19:17Z","published":"2021-12-15T02:42:28Z","title":"Data Valuation for Vertical Federated Learning: A Model-free and\n  Privacy-preserving Method","summary":"  Vertical Federated learning (VFL) is a promising paradigm for predictive\nanalytics, empowering an organization (i.e., task party) to enhance its\npredictive models through collaborations with multiple data suppliers (i.e.,\ndata parties) in a decentralized and privacy-preserving way. Despite the\nfast-growing interest in VFL, the lack of effective and secure tools for\nassessing the value of data owned by data parties hinders the application of\nVFL in business contexts. In response, we propose FedValue, a\nprivacy-preserving, task-specific but model-free data valuation method for VFL,\nwhich consists of a data valuation metric and a federated computation method.\nSpecifically, we first introduce a novel data valuation metric, namely\nMShapley-CMI. The metric evaluates a data party's contribution to a predictive\nanalytics task without the need of executing a machine learning model, making\nit well-suited for real-world applications of VFL. Next, we develop an\ninnovative federated computation method that calculates the MShapley-CMI value\nfor each data party in a privacy-preserving manner. Extensive experiments\nconducted on six public datasets validate the efficacy of FedValue for data\nvaluation in the context of VFL. In addition, we illustrate the practical\nutility of FedValue with a case study involving federated movie\nrecommendations.\n","authors":["Xiao Han","Leye Wang","Junjie Wu","Xiao Fang"],"pdf_url":"https://arxiv.org/pdf/2112.08364v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.16430v3","updated":"2024-01-04T07:07:57Z","published":"2023-12-27T06:34:54Z","title":"Preference as Reward, Maximum Preference Optimization with Importance\n  Sampling","summary":"  Preference learning is a key technology for aligning language models with\nhuman values. Reinforcement Learning from Human Feedback (RLHF) is a model\nbased algorithm to optimize preference learning, which first fitting a reward\nmodel for preference score, and then optimizing generating policy with\non-policy PPO algorithm to maximize the reward. The processing of RLHF is\ncomplex, time-consuming and unstable. Direct Preference Optimization (DPO)\nalgorithm using off-policy algorithm to direct optimize generating policy and\neliminating the need for reward model, which is data efficient and stable. DPO\nuse Bradley-Terry model and log-loss which leads to over-fitting to the\npreference data at the expense of ignoring KL-regularization term when\npreference near deterministic. IPO uses a root-finding pairwise MSE loss to\nsolve the ignoring KL-regularization problem, and learning an optimal policy.\nBut IPO's pairwise loss still can't s make the KL-regularization to work. In\nthis paper, we design a simple and intuitive off-policy preferences\noptimization algorithm from an importance sampling view, and add an off-policy\nKL-regularization term which makes KL-regularization truly effective. To\nsimplify the learning process and save memory usage, we can generate\nregularization data in advance, which eliminate the needs for both reward model\nand reference policy in the stage of optimization.\n","authors":["Zaifan Jiang","Xing Huang","Chao Wei"],"pdf_url":"https://arxiv.org/pdf/2312.16430v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.11319v2","updated":"2024-01-04T06:41:05Z","published":"2023-09-20T13:44:18Z","title":"WFTNet: Exploiting Global and Local Periodicity in Long-term Time Series\n  Forecasting","summary":"  Recent CNN and Transformer-based models tried to utilize frequency and\nperiodicity information for long-term time series forecasting. However, most\nexisting work is based on Fourier transform, which cannot capture fine-grained\nand local frequency structure. In this paper, we propose a Wavelet-Fourier\nTransform Network (WFTNet) for long-term time series forecasting. WFTNet\nutilizes both Fourier and wavelet transforms to extract comprehensive\ntemporal-frequency information from the signal, where Fourier transform\ncaptures the global periodic patterns and wavelet transform captures the local\nones. Furthermore, we introduce a Periodicity-Weighted Coefficient (PWC) to\nadaptively balance the importance of global and local frequency patterns.\nExtensive experiments on various time series datasets show that WFTNet\nconsistently outperforms other state-of-the-art baseline. Code is available at\nhttps://github.com/Hank0626/WFTNet.\n","authors":["Peiyuan Liu","Beiliang Wu","Naiqi Li","Tao Dai","Fengmao Lei","Jigang Bao","Yong Jiang","Shu-Tao Xia"],"pdf_url":"https://arxiv.org/pdf/2309.11319v2.pdf","comment":"Accepted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2312.13143v2","updated":"2024-01-04T06:38:00Z","published":"2023-12-20T16:04:02Z","title":"Underwater Acoustic Signal Recognition Based on Salient Feature","summary":"  With the rapid advancement of technology, the recognition of underwater\nacoustic signals in complex environments has become increasingly crucial.\nCurrently, mainstream underwater acoustic signal recognition relies primarily\non time-frequency analysis to extract spectral features, finding widespread\napplications in the field. However, existing recognition methods heavily depend\non expert systems, facing limitations such as restricted knowledge bases and\nchallenges in handling complex relationships. These limitations stem from the\ncomplexity and maintenance difficulties associated with rules or inference\nengines. Recognizing the potential advantages of deep learning in handling\nintricate relationships, this paper proposes a method utilizing neural networks\nfor underwater acoustic signal recognition. The proposed approach involves\ncontinual learning of features extracted from spectra for the classification of\nunderwater acoustic signals. Deep learning models can automatically learn\nabstract features from data and continually adjust weights during training to\nenhance classification performance.\n","authors":["Minghao Chen"],"pdf_url":"https://arxiv.org/pdf/2312.13143v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12728v2","updated":"2024-01-04T06:33:52Z","published":"2023-12-20T02:55:15Z","title":"Lookahead: An Inference Acceleration Framework for Large Language Model\n  with Lossless Generation Accuracy","summary":"  As Large Language Models (LLMs) have made significant advancements across\nvarious tasks, such as question answering, translation, text summarization, and\ndialogue systems, the need for accuracy in information becomes crucial,\nespecially for serious financial products serving billions of users like\nAlipay. To address this, Alipay has developed a Retrieval-Augmented Generation\n(RAG) system that grounds LLMs on the most accurate and up-to-date information.\nHowever, for a real-world product serving millions of users, the inference\nspeed of LLMs becomes a critical factor compared to a mere experimental model.\n  Hence, this paper presents a generic framework for accelerating the inference\nprocess, resulting in a substantial increase in speed and cost reduction for\nour RAG system, with lossless generation accuracy. In the traditional inference\nprocess, each token is generated sequentially by the LLM, leading to a time\nconsumption proportional to the number of generated tokens. To enhance this\nprocess, our framework, named \\textit{lookahead}, introduces a\n\\textit{multi-branch} strategy. Instead of generating a single token at a time,\nwe propose a \\textit{Trie-based Retrieval} (TR) process that enables the\ngeneration of multiple branches simultaneously, each of which is a sequence of\ntokens. Subsequently, for each branch, a \\textit{Verification and Accept} (VA)\nprocess is performed to identify the longest correct sub-sequence as the final\noutput. Our strategy offers two distinct advantages: (1) it guarantees absolute\ncorrectness of the output, avoiding any approximation algorithms, and (2) the\nworst-case performance of our approach is equivalent to the conventional\nprocess. We conduct extensive experiments to demonstrate the significant\nimprovements achieved by applying our inference acceleration framework. Code is\navaliable: https://github.com/alipay/PainlessInferenceAcceleration.\n","authors":["Yao Zhao","Zhitian Xie","Chenyi Zhuang","Jinjie Gu"],"pdf_url":"https://arxiv.org/pdf/2312.12728v2.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2312.11973v2","updated":"2024-01-04T06:26:36Z","published":"2023-12-19T09:11:49Z","title":"Continual Learning: Forget-free Winning Subnetworks for Video\n  Representations","summary":"  Inspired by the Lottery Ticket Hypothesis (LTH), which highlights the\nexistence of efficient subnetworks within larger, dense networks, a\nhigh-performing Winning Subnetwork (WSN) in terms of task performance under\nappropriate sparsity conditions is considered for various continual learning\ntasks. It leverages pre-existing weights from dense networks to achieve\nefficient learning in Task Incremental Learning (TIL) scenarios. In Few-Shot\nClass Incremental Learning (FSCIL), a variation of WSN referred to as the Soft\nsubnetwork (SoftNet) is designed to prevent overfitting when the data samples\nare scarce. Furthermore, the sparse reuse of WSN weights is considered for\nVideo Incremental Learning (VIL). The use of Fourier Subneural Operator (FSO)\nwithin WSN is considered. It enables compact encoding of videos and identifies\nreusable subnetworks across varying bandwidths. We have integrated FSO into\ndifferent architectural frameworks for continual learning, including VIL, TIL,\nand FSCIL. Our comprehensive experiments demonstrate FSO's effectiveness,\nsignificantly improving task performance at various convolutional\nrepresentational levels. Specifically, FSO enhances higher-layer performance in\nTIL and FSCIL and lower-layer performance in VIL\n","authors":["Haeyong Kang","Jaehong Yoon","Sung Ju Hwang","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2312.11973v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2303.14962,\n  arXiv:2306.11305"},{"id":"http://arxiv.org/abs/2401.00413v2","updated":"2024-01-04T06:25:16Z","published":"2023-12-31T07:10:15Z","title":"Real-Time FJ/MAC PDE Solvers via Tensorized, Back-Propagation-Free\n  Optical PINN Training","summary":"  Solving partial differential equations (PDEs) numerically often requires huge\ncomputing time, energy cost, and hardware resources in practical applications.\nThis has limited their applications in many scenarios (e.g., autonomous\nsystems, supersonic flows) that have a limited energy budget and require near\nreal-time response. Leveraging optical computing, this paper develops an\non-chip training framework for physics-informed neural networks (PINNs), aiming\nto solve high-dimensional PDEs with fJ/MAC photonic power consumption and\nultra-low latency. Despite the ultra-high speed of optical neural networks,\ntraining a PINN on an optical chip is hard due to (1) the large size of\nphotonic devices, and (2) the lack of scalable optical memory devices to store\nthe intermediate results of back-propagation (BP). To enable realistic optical\nPINN training, this paper presents a scalable method to avoid the BP process.\nWe also employ a tensor-compressed approach to improve the convergence and\nscalability of our optical PINN training. This training framework is designed\nwith tensorized optical neural networks (TONN) for scalable inference\nacceleration and MZI phase-domain tuning for \\textit{in-situ} optimization. Our\nsimulation results of a 20-dim HJB PDE show that our photonic accelerator can\nreduce the number of MZIs by a factor of $1.17\\times 10^3$, with only $1.36$ J\nand $1.15$ s to solve this equation. This is the first real-size optical PINN\ntraining framework that can be applied to solve high-dimensional PDEs.\n","authors":["Yequan Zhao","Xian Xiao","Xinling Yu","Ziyue Liu","Zhixiong Chen","Geza Kurczveil","Raymond G. Beausoleil","Zheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.00413v2.pdf","comment":"ML with New Compute Paradigms (MLNCP) at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2401.02088v1","updated":"2024-01-04T06:23:22Z","published":"2024-01-04T06:23:22Z","title":"Re-evaluating the Memory-balanced Pipeline Parallelism: BPipe","summary":"  Pipeline parallelism is an essential technique in the training of large-scale\nTransformer models. However, it suffers from imbalanced memory consumption,\nleading to insufficient memory utilization. The BPipe technique was proposed to\naddress this issue and has proven effective in the GPT-3 model. Nevertheless,\nour experiments have not yielded similar benefits for LLaMA training.\nAdditionally, BPipe only yields negligible benefits for GPT-3 training when\napplying flash attention. We analyze the underlying causes of the divergent\nperformance of BPipe on GPT-3 and LLaMA. Furthermore, we introduce a novel\nmethod to estimate the performance of BPipe.\n","authors":["Mincong Huang","Chao Wang","Chi Ma","Yineng Zhang","Peng Zhang","Lei Yu"],"pdf_url":"https://arxiv.org/pdf/2401.02088v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02086v1","updated":"2024-01-04T06:20:24Z","published":"2024-01-04T06:20:24Z","title":"View-based Explanations for Graph Neural Networks","summary":"  Generating explanations for graph neural networks (GNNs) has been studied to\nunderstand their behavior in analytical tasks such as graph classification.\nExisting approaches aim to understand the overall results of GNNs rather than\nproviding explanations for specific class labels of interest, and may return\nexplanation structures that are hard to access, nor directly queryable.\n  We propose GVEX, a novel paradigm that generates Graph Views for EXplanation.\n(1) We design a two-tier explanation structure called explanation views. An\nexplanation view consists of a set of graph patterns and a set of induced\nexplanation subgraphs. Given a database G of multiple graphs and a specific\nclass label l assigned by a GNN-based classifier M, it concisely describes the\nfraction of G that best explains why l is assigned by M. (2) We propose quality\nmeasures and formulate an optimization problem to compute optimal explanation\nviews for GNN explanation. We show that the problem is $\\Sigma^2_P$-hard. (3)\nWe present two algorithms. The first one follows an explain-and-summarize\nstrategy that first generates high-quality explanation subgraphs which best\nexplain GNNs in terms of feature influence maximization, and then performs a\nsummarization step to generate patterns. We show that this strategy provides an\napproximation ratio of 1/2. Our second algorithm performs a single-pass to an\ninput node stream in batches to incrementally maintain explanation views,\nhaving an anytime quality guarantee of 1/4 approximation. Using real-world\nbenchmark data, we experimentally demonstrate the effectiveness, efficiency,\nand scalability of GVEX. Through case studies, we showcase the practical\napplications of GVEX.\n","authors":["Tingyang Chen","Dazhuo Qiu","Yinghui Wu","Arijit Khan","Xiangyu Ke","Yunjun Gao"],"pdf_url":"https://arxiv.org/pdf/2401.02086v1.pdf","comment":"31 pages, 13 figures"},{"id":"http://arxiv.org/abs/2312.15045v2","updated":"2024-01-04T06:12:44Z","published":"2023-12-22T20:16:10Z","title":"Probabilistic Modeling for Sequences of Sets in Continuous-Time","summary":"  Neural marked temporal point processes have been a valuable addition to the\nexisting toolbox of statistical parametric models for continuous-time event\ndata. These models are useful for sequences where each event is associated with\na single item (a single type of event or a \"mark\") -- but such models are not\nsuited for the practical situation where each event is associated with a set of\nitems. In this work, we develop a general framework for modeling set-valued\ndata in continuous-time, compatible with any intensity-based recurrent neural\npoint process model. In addition, we develop inference methods that can use\nsuch models to answer probabilistic queries such as \"the probability of item\n$A$ being observed before item $B$,\" conditioned on sequence history. Computing\nexact answers for such queries is generally intractable for neural models due\nto both the continuous-time nature of the problem setting and the\ncombinatorially-large space of potential outcomes for each event. To address\nthis, we develop a class of importance sampling methods for querying with\nset-based sequences and demonstrate orders-of-magnitude improvements in\nefficiency over direct sampling via systematic experiments with four real-world\ndatasets. We also illustrate how to use this framework to perform model\nselection using likelihoods that do not involve one-step-ahead prediction.\n","authors":["Yuxin Chang","Alex Boyd","Padhraic Smyth"],"pdf_url":"https://arxiv.org/pdf/2312.15045v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02080v1","updated":"2024-01-04T06:03:46Z","published":"2024-01-04T06:03:46Z","title":"Energy based diffusion generator for efficient sampling of Boltzmann\n  distributions","summary":"  We introduce a novel sampler called the energy based diffusion generator for\ngenerating samples from arbitrary target distributions. The sampling model\nemploys a structure similar to a variational autoencoder, utilizing a decoder\nto transform latent variables from a simple distribution into random variables\napproximating the target distribution, and we design an encoder based on the\ndiffusion model. Leveraging the powerful modeling capacity of the diffusion\nmodel for complex distributions, we can obtain an accurate variational estimate\nof the Kullback-Leibler divergence between the distributions of the generated\nsamples and the target. Moreover, we propose a decoder based on generalized\nHamiltonian dynamics to further enhance sampling performance. Through empirical\nevaluation, we demonstrate the effectiveness of our method across various\ncomplex distribution functions, showcasing its superiority compared to existing\nmethods.\n","authors":["Yan Wang","Ling Guo","Hao Wu","Tao Zhou"],"pdf_url":"https://arxiv.org/pdf/2401.02080v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12517v2","updated":"2024-01-04T05:22:36Z","published":"2023-08-24T03:06:20Z","title":"Not Only Rewards But Also Constraints: Applications on Legged Robot\n  Locomotion","summary":"  Several earlier studies have shown impressive control performance in complex\nrobotic systems by designing the controller using a neural network and training\nit with model-free reinforcement learning. However, these outstanding\ncontrollers with natural motion style and high task performance are developed\nthrough extensive reward engineering, which is a highly laborious and\ntime-consuming process of designing numerous reward terms and determining\nsuitable reward coefficients. In this work, we propose a novel reinforcement\nlearning framework for training neural network controllers for complex robotic\nsystems consisting of both rewards and constraints. To let the engineers\nappropriately reflect their intent to constraints and handle them with minimal\ncomputation overhead, two constraint types and an efficient policy optimization\nalgorithm are suggested. The learning framework is applied to train locomotion\ncontrollers for several legged robots with different morphology and physical\nattributes to traverse challenging terrains. Extensive simulation and\nreal-world experiments demonstrate that performant controllers can be trained\nwith significantly less reward engineering, by tuning only a single reward\ncoefficient. Furthermore, a more straightforward and intuitive engineering\nprocess can be utilized, thanks to the interpretability and generalizability of\nconstraints. The summary video is available at https://youtu.be/KAlm3yskhvM.\n","authors":["Yunho Kim","Hyunsik Oh","Jeonghyun Lee","Jinhyeok Choi","Gwanghyeon Ji","Moonkyu Jung","Donghoon Youm","Jemin Hwangbo"],"pdf_url":"https://arxiv.org/pdf/2308.12517v2.pdf","comment":"Submitted to Transactions on Robotics (T-RO)"},{"id":"http://arxiv.org/abs/2401.02062v1","updated":"2024-01-04T04:58:02Z","published":"2024-01-04T04:58:02Z","title":"U-Trustworthy Models.Reliability, Competence, and Confidence in\n  Decision-Making","summary":"  With growing concerns regarding bias and discrimination in predictive models,\nthe AI community has increasingly focused on assessing AI system\ntrustworthiness. Conventionally, trustworthy AI literature relies on the\nprobabilistic framework and calibration as prerequisites for trustworthiness.\nIn this work, we depart from this viewpoint by proposing a novel trust\nframework inspired by the philosophy literature on trust. We present a precise\nmathematical definition of trustworthiness, termed\n$\\mathcal{U}$-trustworthiness, specifically tailored for a subset of tasks\naimed at maximizing a utility function. We argue that a model's\n$\\mathcal{U}$-trustworthiness is contingent upon its ability to maximize Bayes\nutility within this task subset. Our first set of results challenges the\nprobabilistic framework by demonstrating its potential to favor less\ntrustworthy models and introduce the risk of misleading trustworthiness\nassessments. Within the context of $\\mathcal{U}$-trustworthiness, we prove that\nproperly-ranked models are inherently $\\mathcal{U}$-trustworthy. Furthermore,\nwe advocate for the adoption of the AUC metric as the preferred measure of\ntrustworthiness. By offering both theoretical guarantees and experimental\nvalidation, AUC enables robust evaluation of trustworthiness, thereby enhancing\nmodel selection and hyperparameter tuning to yield more trustworthy outcomes.\n","authors":["Ritwik Vashistha","Arya Farahi"],"pdf_url":"https://arxiv.org/pdf/2401.02062v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02058v1","updated":"2024-01-04T04:53:31Z","published":"2024-01-04T04:53:31Z","title":"Neural Collapse for Cross-entropy Class-Imbalanced Learning with\n  Unconstrained ReLU Feature Model","summary":"  The current paradigm of training deep neural networks for classification\ntasks includes minimizing the empirical risk that pushes the training loss\nvalue towards zero, even after the training error has been vanished. In this\nterminal phase of training, it has been observed that the last-layer features\ncollapse to their class-means and these class-means converge to the vertices of\na simplex Equiangular Tight Frame (ETF). This phenomenon is termed as Neural\nCollapse (NC). To theoretically understand this phenomenon, recent works employ\na simplified unconstrained feature model to prove that NC emerges at the global\nsolutions of the training problem. However, when the training dataset is\nclass-imbalanced, some NC properties will no longer be true. For example, the\nclass-means geometry will skew away from the simplex ETF when the loss\nconverges. In this paper, we generalize NC to imbalanced regime for\ncross-entropy loss under the unconstrained ReLU feature model. We prove that,\nwhile the within-class features collapse property still holds in this setting,\nthe class-means will converge to a structure consisting of orthogonal vectors\nwith different lengths. Furthermore, we find that the classifier weights are\naligned to the scaled and centered class-means with scaling factors depend on\nthe number of training samples of each class, which generalizes NC in the\nclass-balanced setting. We empirically prove our results through experiments on\npractical architectures and dataset.\n","authors":["Hien Dang","Tho Tran","Tan Nguyen","Nhat Ho"],"pdf_url":"https://arxiv.org/pdf/2401.02058v1.pdf","comment":"25 pages, 3 figures"},{"id":"http://arxiv.org/abs/2302.09267v4","updated":"2024-01-04T03:42:11Z","published":"2023-02-18T09:24:15Z","title":"Stochastic Approximation Approaches to Group Distributionally Robust\n  Optimization","summary":"  This paper investigates group distributionally robust optimization (GDRO),\nwith the purpose to learn a model that performs well over $m$ different\ndistributions. First, we formulate GDRO as a stochastic convex-concave\nsaddle-point problem, and demonstrate that stochastic mirror descent (SMD),\nusing $m$ samples in each iteration, achieves an $O(m (\\log m)/\\epsilon^2)$\nsample complexity for finding an $\\epsilon$-optimal solution, which matches the\n$\\Omega(m/\\epsilon^2)$ lower bound up to a logarithmic factor. Then, we make\nuse of techniques from online learning to reduce the number of samples required\nin each round from $m$ to $1$, keeping the same sample complexity.\nSpecifically, we cast GDRO as a two-players game where one player simply\nperforms SMD and the other executes an online algorithm for non-oblivious\nmulti-armed bandits. Next, we consider a more practical scenario where the\nnumber of samples that can be drawn from each distribution is different, and\npropose a novel formulation of weighted GDRO, which allows us to derive\ndistribution-dependent convergence rates. Denote by $n_i$ the sample budget for\nthe $i$-th distribution, and assume $n_1 \\geq n_2 \\geq \\cdots \\geq n_m$. In the\nfirst approach, we incorporate non-uniform sampling into SMD such that the\nsample budget is satisfied in expectation, and prove that the excess risk of\nthe $i$-th distribution decreases at an $O(\\sqrt{n_1 \\log m}/n_i)$ rate. In the\nsecond approach, we use mini-batches to meet the budget exactly and also reduce\nthe variance in stochastic gradients, and then leverage stochastic mirror-prox\nalgorithm, which can exploit small variances, to optimize a carefully designed\nweighted GDRO problem. Under appropriate conditions, it attains an $O((\\log\nm)/\\sqrt{n_i})$ convergence rate, which almost matches the optimal\n$O(\\sqrt{1/n_i})$ rate of only learning from the $i$-th distribution with $n_i$\nsamples.\n","authors":["Lijun Zhang","Peng Zhao","Zhen-Hua Zhuang","Tianbao Yang","Zhi-Hua Zhou"],"pdf_url":"https://arxiv.org/pdf/2302.09267v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.17498v2","updated":"2024-01-04T03:11:14Z","published":"2023-10-26T15:53:18Z","title":"CBD: A Certified Backdoor Detector Based on Local Dominant Probability","summary":"  Backdoor attack is a common threat to deep neural networks. During testing,\nsamples embedded with a backdoor trigger will be misclassified as an\nadversarial target by a backdoored model, while samples without the backdoor\ntrigger will be correctly classified. In this paper, we present the first\ncertified backdoor detector (CBD), which is based on a novel, adjustable\nconformal prediction scheme based on our proposed statistic local dominant\nprobability. For any classifier under inspection, CBD provides 1) a detection\ninference, 2) the condition under which the attacks are guaranteed to be\ndetectable for the same classification domain, and 3) a probabilistic upper\nbound for the false positive rate. Our theoretical results show that attacks\nwith triggers that are more resilient to test-time noise and have smaller\nperturbation magnitudes are more likely to be detected with guarantees.\nMoreover, we conduct extensive experiments on four benchmark datasets\nconsidering various backdoor types, such as BadNet, CB, and Blend. CBD achieves\ncomparable or even higher detection accuracy than state-of-the-art detectors,\nand it in addition provides detection certification. Notably, for backdoor\nattacks with random perturbation triggers bounded by $\\ell_2\\leq0.75$ which\nachieves more than 90\\% attack success rate, CBD achieves 100\\% (98\\%), 100\\%\n(84\\%), 98\\% (98\\%), and 72\\% (40\\%) empirical (certified) detection true\npositive rates on the four benchmark datasets GTSRB, SVHN, CIFAR-10, and\nTinyImageNet, respectively, with low false positive rates.\n","authors":["Zhen Xiang","Zidi Xiong","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2310.17498v2.pdf","comment":"Accepted to NeurIPS 2023"},{"id":"http://arxiv.org/abs/2401.00625v2","updated":"2024-01-04T02:49:50Z","published":"2024-01-01T01:12:42Z","title":"Beyond Efficiency: A Systematic Survey of Resource-Efficient Large\n  Language Models","summary":"  The burgeoning field of Large Language Models (LLMs), exemplified by\nsophisticated models like OpenAI's ChatGPT, represents a significant\nadvancement in artificial intelligence. These models, however, bring forth\nsubstantial challenges in the high consumption of computational, memory,\nenergy, and financial resources, especially in environments with limited\nresource capabilities. This survey aims to systematically address these\nchallenges by reviewing a broad spectrum of techniques designed to enhance the\nresource efficiency of LLMs. We categorize methods based on their optimization\nfocus: computational, memory, energy, financial, and network resources and\ntheir applicability across various stages of an LLM's lifecycle, including\narchitecture design, pretraining, finetuning, and system design. Additionally,\nthe survey introduces a nuanced categorization of resource efficiency\ntechniques by their specific resource types, which uncovers the intricate\nrelationships and mappings between various resources and corresponding\noptimization techniques. A standardized set of evaluation metrics and datasets\nis also presented to facilitate consistent and fair comparisons across\ndifferent models and techniques. By offering a comprehensive overview of the\ncurrent sota and identifying open research avenues, this survey serves as a\nfoundational reference for researchers and practitioners, aiding them in\ndeveloping more sustainable and efficient LLMs in a rapidly evolving landscape.\n","authors":["Guangji Bai","Zheng Chai","Chen Ling","Shiyu Wang","Jiaying Lu","Nan Zhang","Tingwei Shi","Ziyang Yu","Mengdan Zhu","Yifei Zhang","Carl Yang","Yue Cheng","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2401.00625v2.pdf","comment":"Preprint. GitHub repo:\n  https://github.com/tiingweii-shii/Awesome-Resource-Efficient-LLM-Papers"},{"id":"http://arxiv.org/abs/2312.14504v2","updated":"2024-01-04T02:49:21Z","published":"2023-12-22T08:08:45Z","title":"Theory of Hallucinations based on Equivariance","summary":"  This study aims to acquire knowledge for creating very large language models\nthat are immune to hallucinations. Hallucinations in contemporary large\nlanguage models are often attributed to a misunderstanding of real-world social\nrelationships. Therefore, I hypothesize that very large language models capable\nof thoroughly grasping all these relationships will be free from\nhallucinations. Additionally, I propose that certain types of equivariant\nlanguage models are adept at learning and understanding these relationships.\nBuilding on this, I have developed a specialized cross-entropy error function\nto create a hallucination scale for language models, which measures their\nextent of equivariance acquisition. Utilizing this scale, I tested language\nmodels for their ability to acquire character-level equivariance. In\nparticular, I introduce and employ a novel technique based on T5 (Text To Text\nTransfer Transformer) that efficiently understands permuted input texts without\nthe need for explicit dictionaries to convert token IDs (integers) to texts\n(strings). This T5 model demonstrated a moderate ability to acquire\ncharacter-level equivariance. Additionally, I discovered scale laws that can\naid in developing hallucination-free language models at the character level.\nThis methodology can be extended to assess equivariance acquisition at the word\nlevel, paving the way for very large language models that can comprehensively\nunderstand relationships and, consequently, avoid hallucinations.\n","authors":["Hisaichi Shibata"],"pdf_url":"https://arxiv.org/pdf/2312.14504v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.15665v2","updated":"2024-01-04T02:32:33Z","published":"2023-12-25T09:20:26Z","title":"A Multi-Modal Contrastive Diffusion Model for Therapeutic Peptide\n  Generation","summary":"  Therapeutic peptides represent a unique class of pharmaceutical agents\ncrucial for the treatment of human diseases. Recently, deep generative models\nhave exhibited remarkable potential for generating therapeutic peptides, but\nthey only utilize sequence or structure information alone, which hinders the\nperformance in generation. In this study, we propose a Multi-Modal Contrastive\nDiffusion model (MMCD), fusing both sequence and structure modalities in a\ndiffusion framework to co-generate novel peptide sequences and structures.\nSpecifically, MMCD constructs the sequence-modal and structure-modal diffusion\nmodels, respectively, and devises a multi-modal contrastive learning strategy\nwith intercontrastive and intra-contrastive in each diffusion timestep, aiming\nto capture the consistency between two modalities and boost model performance.\nThe inter-contrastive aligns sequences and structures of peptides by maximizing\nthe agreement of their embeddings, while the intra-contrastive differentiates\ntherapeutic and non-therapeutic peptides by maximizing the disagreement of\ntheir sequence/structure embeddings simultaneously. The extensive experiments\ndemonstrate that MMCD performs better than other state-of-theart deep\ngenerative methods in generating therapeutic peptides across various metrics,\nincluding antimicrobial/anticancer score, diversity, and peptide-docking.\n","authors":["Yongkang Wang","Xuan Liu","Feng Huang","Zhankun Xiong","Wen Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.15665v2.pdf","comment":"This paper is accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2308.06911v2","updated":"2024-01-04T02:22:07Z","published":"2023-08-14T03:12:29Z","title":"GIT-Mol: A Multi-modal Large Language Model for Molecular Science with\n  Graph, Image, and Text","summary":"  Large language models have made significant strides in natural language\nprocessing, enabling innovative applications in molecular science by processing\ntextual representations of molecules. However, most existing language models\ncannot capture the rich information with complex molecular structures or\nimages. In this paper, we introduce GIT-Mol, a multi-modal large language model\nthat integrates the Graph, Image, and Text information. To facilitate the\nintegration of multi-modal molecular data, we propose GIT-Former, a novel\narchitecture that is capable of aligning all modalities into a unified latent\nspace. We achieve a 5%-10% accuracy increase in properties prediction and a\n20.2% boost in molecule generation validity compared to the baselines. With the\nany-to-language molecular translation strategy, our model has the potential to\nperform more downstream tasks, such as compound name recognition and chemical\nreaction prediction.\n","authors":["Pengfei Liu","Yiming Ren","Zhixiang Ren"],"pdf_url":"https://arxiv.org/pdf/2308.06911v2.pdf","comment":"14 pages, 4 figures"},{"id":"http://arxiv.org/abs/2306.00876v2","updated":"2024-01-04T02:02:51Z","published":"2023-06-01T16:37:50Z","title":"Quantifying Deep Learning Model Uncertainty in Conformal Prediction","summary":"  Precise estimation of predictive uncertainty in deep neural networks is a\ncritical requirement for reliable decision-making in machine learning and\nstatistical modeling, particularly in the context of medical AI. Conformal\nPrediction (CP) has emerged as a promising framework for representing the model\nuncertainty by providing well-calibrated confidence levels for individual\npredictions. However, the quantification of model uncertainty in conformal\nprediction remains an active research area, yet to be fully addressed. In this\npaper, we explore state-of-the-art CP methodologies and their theoretical\nfoundations. We propose a probabilistic approach in quantifying the model\nuncertainty derived from the produced prediction sets in conformal prediction\nand provide certified boundaries for the computed uncertainty. By doing so, we\nallow model uncertainty measured by CP to be compared by other uncertainty\nquantification methods such as Bayesian (e.g., MC-Dropout and DeepEnsemble) and\nEvidential approaches.\n","authors":["Hamed Karimi","Reza Samavi"],"pdf_url":"https://arxiv.org/pdf/2306.00876v2.pdf","comment":"Accepted in AAAI Second Symposium on Human Partnership with Medical\n  AI: Design, Operationalization, and Ethics"},{"id":"http://arxiv.org/abs/2201.01954v2","updated":"2024-01-04T01:46:17Z","published":"2022-01-06T07:40:51Z","title":"Federated Optimization of Smooth Loss Functions","summary":"  In this work, we study empirical risk minimization (ERM) within a federated\nlearning framework, where a central server minimizes an ERM objective function\nusing training data that is stored across $m$ clients. In this setting, the\nFederated Averaging (FedAve) algorithm is the staple for determining\n$\\epsilon$-approximate solutions to the ERM problem. Similar to standard\noptimization algorithms, the convergence analysis of FedAve only relies on\nsmoothness of the loss function in the optimization parameter. However, loss\nfunctions are often very smooth in the training data too. To exploit this\nadditional smoothness, we propose the Federated Low Rank Gradient Descent\n(FedLRGD) algorithm. Since smoothness in data induces an approximate low rank\nstructure on the loss function, our method first performs a few rounds of\ncommunication between the server and clients to learn weights that the server\ncan use to approximate clients' gradients. Then, our method solves the ERM\nproblem at the server using inexact gradient descent. To show that FedLRGD can\nhave superior performance to FedAve, we present a notion of federated oracle\ncomplexity as a counterpart to canonical oracle complexity. Under some\nassumptions on the loss function, e.g., strong convexity in parameter,\n$\\eta$-H\\\"older smoothness in data, etc., we prove that the federated oracle\ncomplexity of FedLRGD scales like $\\phi m(p/\\epsilon)^{\\Theta(d/\\eta)}$ and\nthat of FedAve scales like $\\phi m(p/\\epsilon)^{3/4}$ (neglecting sub-dominant\nfactors), where $\\phi\\gg 1$ is a \"communication-to-computation ratio,\" $p$ is\nthe parameter dimension, and $d$ is the data dimension. Then, we show that when\n$d$ is small and the loss function is sufficiently smooth in the data, FedLRGD\nbeats FedAve in federated oracle complexity. Finally, in the course of\nanalyzing FedLRGD, we also establish a result on low rank approximation of\nlatent variable models.\n","authors":["Ali Jadbabaie","Anuran Makur","Devavrat Shah"],"pdf_url":"https://arxiv.org/pdf/2201.01954v2.pdf","comment":"31 pages, double column format, 2 figures"},{"id":"http://arxiv.org/abs/2209.00652v2","updated":"2024-01-04T01:41:08Z","published":"2022-09-01T02:18:00Z","title":"Towards Optimization and Model Selection for Domain Generalization: A\n  Mixup-guided Solution","summary":"  The distribution shifts between training and test data typically undermine\nthe performance of models. In recent years, lots of work pays attention to\ndomain generalization (DG) where distribution shifts exist, and target data are\nunseen. Despite the progress in algorithm design, two foundational factors have\nlong been ignored: 1) the optimization for regularization-based objectives, and\n2) the model selection for DG since no knowledge about the target domain can be\nutilized. In this paper, we propose Mixup guided optimization and selection\ntechniques for DG. For optimization, we utilize an adapted Mixup to generate an\nout-of-distribution dataset that can guide the preference direction and\noptimize with Pareto optimization. For model selection, we generate a\nvalidation dataset with a closer distance to the target distribution, and\nthereby it can better represent the target data. We also present some\ntheoretical insights behind our proposals. Comprehensive experiments\ndemonstrate that our model optimization and selection techniques can largely\nimprove the performance of existing domain generalization algorithms and even\nachieve new state-of-the-art results.\n","authors":["Wang Lu","Jindong Wang","Yidong Wang","Xing Xie"],"pdf_url":"https://arxiv.org/pdf/2209.00652v2.pdf","comment":"Accepted by SIAM International Conference on Data Mining (SDM) 2024"},{"id":"http://arxiv.org/abs/2401.02020v1","updated":"2024-01-04T01:33:33Z","published":"2024-01-04T01:33:33Z","title":"Spikformer V2: Join the High Accuracy Club on ImageNet with an SNN\n  Ticket","summary":"  Spiking Neural Networks (SNNs), known for their biologically plausible\narchitecture, face the challenge of limited performance. The self-attention\nmechanism, which is the cornerstone of the high-performance Transformer and\nalso a biologically inspired structure, is absent in existing SNNs. To this\nend, we explore the potential of leveraging both self-attention capability and\nbiological properties of SNNs, and propose a novel Spiking Self-Attention (SSA)\nand Spiking Transformer (Spikformer). The SSA mechanism eliminates the need for\nsoftmax and captures the sparse visual feature employing spike-based Query,\nKey, and Value. This sparse computation without multiplication makes SSA\nefficient and energy-saving. Further, we develop a Spiking Convolutional Stem\n(SCS) with supplementary convolutional layers to enhance the architecture of\nSpikformer. The Spikformer enhanced with the SCS is referred to as Spikformer\nV2. To train larger and deeper Spikformer V2, we introduce a pioneering\nexploration of Self-Supervised Learning (SSL) within the SNN. Specifically, we\npre-train Spikformer V2 with masking and reconstruction style inspired by the\nmainstream self-supervised Transformer, and then finetune the Spikformer V2 on\nthe image classification on ImageNet. Extensive experiments show that\nSpikformer V2 outperforms other previous surrogate training and ANN2SNN\nmethods. An 8-layer Spikformer V2 achieves an accuracy of 80.38% using 4 time\nsteps, and after SSL, a 172M 16-layer Spikformer V2 reaches an accuracy of\n81.10% with just 1 time step. To the best of our knowledge, this is the first\ntime that the SNN achieves 80+% accuracy on ImageNet. The code will be\navailable at Spikformer V2.\n","authors":["Zhaokun Zhou","Kaiwei Che","Wei Fang","Keyu Tian","Yuesheng Zhu","Shuicheng Yan","Yonghong Tian","Li Yuan"],"pdf_url":"https://arxiv.org/pdf/2401.02020v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02019v1","updated":"2024-01-04T01:32:50Z","published":"2024-01-04T01:32:50Z","title":"From Function to Distribution Modeling: A PAC-Generative Approach to\n  Offline Optimization","summary":"  This paper considers the problem of offline optimization, where the objective\nfunction is unknown except for a collection of ``offline\" data examples. While\nrecent years have seen a flurry of work on applying various machine learning\ntechniques to the offline optimization problem, the majority of these work\nfocused on learning a surrogate of the unknown objective function and then\napplying existing optimization algorithms. While the idea of modeling the\nunknown objective function is intuitive and appealing, from the learning point\nof view it also makes it very difficult to tune the objective of the learner\naccording to the objective of optimization. Instead of learning and then\noptimizing the unknown objective function, in this paper we take on a less\nintuitive but more direct view that optimization can be thought of as a process\nof sampling from a generative model. To learn an effective generative model\nfrom the offline data examples, we consider the standard technique of\n``re-weighting\", and our main technical contribution is a probably\napproximately correct (PAC) lower bound on the natural optimization objective,\nwhich allows us to jointly learn a weight function and a score-based generative\nmodel. The robustly competitive performance of the proposed approach is\ndemonstrated via empirical studies using the standard offline optimization\nbenchmarks.\n","authors":["Qiang Zhang","Ruida Zhou","Yang Shen","Tie Liu"],"pdf_url":"https://arxiv.org/pdf/2401.02019v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02013v1","updated":"2024-01-04T01:05:45Z","published":"2024-01-04T01:05:45Z","title":"SwitchTab: Switched Autoencoders Are Effective Tabular Learners","summary":"  Self-supervised representation learning methods have achieved significant\nsuccess in computer vision and natural language processing, where data samples\nexhibit explicit spatial or semantic dependencies. However, applying these\nmethods to tabular data is challenging due to the less pronounced dependencies\namong data samples. In this paper, we address this limitation by introducing\nSwitchTab, a novel self-supervised method specifically designed to capture\nlatent dependencies in tabular data. SwitchTab leverages an asymmetric\nencoder-decoder framework to decouple mutual and salient features among data\npairs, resulting in more representative embeddings. These embeddings, in turn,\ncontribute to better decision boundaries and lead to improved results in\ndownstream tasks. To validate the effectiveness of SwitchTab, we conduct\nextensive experiments across various domains involving tabular data. The\nresults showcase superior performance in end-to-end prediction tasks with\nfine-tuning. Moreover, we demonstrate that pre-trained salient embeddings can\nbe utilized as plug-and-play features to enhance the performance of various\ntraditional classification methods (e.g., Logistic Regression, XGBoost, etc.).\nLastly, we highlight the capability of SwitchTab to create explainable\nrepresentations through visualization of decoupled mutual and salient features\nin the latent space.\n","authors":["Jing Wu","Suiyao Chen","Qi Zhao","Renat Sergazinov","Chen Li","Shengjie Liu","Chongchao Zhao","Tianpei Xie","Hanqing Guo","Cheng Ji","Daniel Cociorva","Hakan Brunzel"],"pdf_url":"https://arxiv.org/pdf/2401.02013v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02012v1","updated":"2024-01-04T01:02:55Z","published":"2024-01-04T01:02:55Z","title":"Fast & Fair: Efficient Second-Order Robust Optimization for Fairness in\n  Machine Learning","summary":"  This project explores adversarial training techniques to develop fairer Deep\nNeural Networks (DNNs) to mitigate the inherent bias they are known to exhibit.\nDNNs are susceptible to inheriting bias with respect to sensitive attributes\nsuch as race and gender, which can lead to life-altering outcomes (e.g.,\ndemographic bias in facial recognition software used to arrest a suspect). We\npropose a robust optimization problem, which we demonstrate can improve\nfairness in several datasets, both synthetic and real-world, using an affine\nlinear model. Leveraging second order information, we are able to find a\nsolution to our optimization problem more efficiently than a purely first order\nmethod.\n","authors":["Allen Minch","Hung Anh Vu","Anne Marie Warren"],"pdf_url":"https://arxiv.org/pdf/2401.02012v1.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2401.02011v1","updated":"2024-01-04T00:57:33Z","published":"2024-01-04T00:57:33Z","title":"Decentralized Multi-Task Online Convex Optimization Under Random Link\n  Failures","summary":"  Decentralized optimization methods often entail information exchange between\nneighbors. Transmission failures can happen due to network congestion,\nhardware/software issues, communication outage, and other factors. In this\npaper, we investigate the random link failure problem in decentralized\nmulti-task online convex optimization, where agents have individual decisions\nthat are coupled with each other via pairwise constraints. Although widely used\nin constrained optimization, conventional saddle-point algorithms are not\ndirectly applicable here because of random packet dropping. To address this\nissue, we develop a robust decentralized saddle-point algorithm against random\nlink failures with heterogeneous probabilities by replacing the missing\ndecisions of neighbors with their latest received values. Then, by judiciously\nbounding the accumulated deviation stemming from this replacement, we first\nestablish that our algorithm achieves $\\mathcal{O}(\\sqrt{T})$ regret and\n$\\mathcal{O}(T^\\frac{3}{4})$ constraint violations for the full information\nscenario, where the complete information on the local cost function is revealed\nto each agent at the end of each time slot. These two bounds match, in order\nsense, the performance bounds of algorithms with perfect communications.\nFurther, we extend our algorithm and analysis to the two-point bandit feedback\nscenario, where only the values of the local cost function at two random points\nare disclosed to each agent sequentially. Performance bounds of the same orders\nas the full information case are derived. Finally, we corroborate the efficacy\nof the proposed algorithms and the analytical results through numerical\nsimulations.\n","authors":["Wenjing Yan","Xuanyu Cao"],"pdf_url":"https://arxiv.org/pdf/2401.02011v1.pdf","comment":"18 pages. 2 figures"},{"id":"http://arxiv.org/abs/2401.02008v1","updated":"2024-01-04T00:25:12Z","published":"2024-01-04T00:25:12Z","title":"Two-Stage Surrogate Modeling for Data-Driven Design Optimization with\n  Application to Composite Microstructure Generation","summary":"  This paper introduces a novel two-stage machine learning-based surrogate\nmodeling framework to address inverse problems in scientific and engineering\nfields. In the first stage of the proposed framework, a machine learning model\ntermed the \"learner\" identifies a limited set of candidates within the input\ndesign space whose predicted outputs closely align with desired outcomes.\nSubsequently, in the second stage, a separate surrogate model, functioning as\nan \"evaluator,\" is employed to assess the reduced candidate space generated in\nthe first stage. This evaluation process eliminates inaccurate and uncertain\nsolutions, guided by a user-defined coverage level. The framework's distinctive\ncontribution is the integration of conformal inference, providing a versatile\nand efficient approach that can be widely applicable. To demonstrate the\neffectiveness of the proposed framework compared to conventional single-stage\ninverse problems, we conduct several benchmark tests and investigate an\nengineering application focused on the micromechanical modeling of\nfiber-reinforced composites. The results affirm the superiority of our proposed\nframework, as it consistently produces more reliable solutions. Therefore, the\nintroduced framework offers a unique perspective on fostering interactions\nbetween machine learning-based surrogate models in real-world applications.\n","authors":["Farhad Pourkamali-Anaraki","Jamal F. Husseini","Evan J. Pineda","Brett A. Bednarcyk","Scott E. Stapleton"],"pdf_url":"https://arxiv.org/pdf/2401.02008v1.pdf","comment":"23 pages, 11 figures"},{"id":"http://arxiv.org/abs/2302.06114v3","updated":"2024-01-04T00:22:41Z","published":"2023-02-13T05:43:24Z","title":"A Comprehensive Survey on Graph Summarization with Graph Neural Networks","summary":"  As large-scale graphs become more widespread, more and more computational\nchallenges with extracting, processing, and interpreting large graph data are\nbeing exposed. It is therefore natural to search for ways to summarize these\nexpansive graphs while preserving their key characteristics. In the past, most\ngraph summarization techniques sought to capture the most important part of a\ngraph statistically. However, today, the high dimensionality and complexity of\nmodern graph data are making deep learning techniques more popular. Hence, this\npaper presents a comprehensive survey of progress in deep learning\nsummarization techniques that rely on graph neural networks (GNNs). Our\ninvestigation includes a review of the current state-of-the-art approaches,\nincluding recurrent GNNs, convolutional GNNs, graph autoencoders, and graph\nattention networks. A new burgeoning line of research is also discussed where\ngraph reinforcement learning is being used to evaluate and improve the quality\nof graph summaries. Additionally, the survey provides details of benchmark\ndatasets, evaluation metrics, and open-source tools that are often employed in\nexperimentation settings, along with a detailed comparison, discussion, and\ntakeaways for the research community focused on graph summarization. Finally,\nthe survey concludes with a number of open research challenges to motivate\nfurther study in this area.\n","authors":["Nasrin Shabani","Jia Wu","Amin Beheshti","Quan Z. Sheng","Jin Foo","Venus Haghighi","Ambreen Hanif","Maryam Shahabikargar"],"pdf_url":"https://arxiv.org/pdf/2302.06114v3.pdf","comment":"21 pages, 4 figures, 9 tables, Journal of IEEE Transactions on\n  Artificial Intelligence"},{"id":"http://arxiv.org/abs/2211.13535v2","updated":"2024-01-04T00:16:22Z","published":"2022-11-24T11:10:54Z","title":"DeepTaster: Adversarial Perturbation-Based Fingerprinting to Identify\n  Proprietary Dataset Use in Deep Neural Networks","summary":"  Training deep neural networks (DNNs) requires large datasets and powerful\ncomputing resources, which has led some owners to restrict redistribution\nwithout permission. Watermarking techniques that embed confidential data into\nDNNs have been used to protect ownership, but these can degrade model\nperformance and are vulnerable to watermark removal attacks. Recently,\nDeepJudge was introduced as an alternative approach to measuring the similarity\nbetween a suspect and a victim model. While DeepJudge shows promise in\naddressing the shortcomings of watermarking, it primarily addresses situations\nwhere the suspect model copies the victim's architecture. In this study, we\nintroduce DeepTaster, a novel DNN fingerprinting technique, to address\nscenarios where a victim's data is unlawfully used to build a suspect model.\nDeepTaster can effectively identify such DNN model theft attacks, even when the\nsuspect model's architecture deviates from the victim's. To accomplish this,\nDeepTaster generates adversarial images with perturbations, transforms them\ninto the Fourier frequency domain, and uses these transformed images to\nidentify the dataset used in a suspect model. The underlying premise is that\nadversarial images can capture the unique characteristics of DNNs built with a\nspecific dataset. To demonstrate the effectiveness of DeepTaster, we evaluated\nthe effectiveness of DeepTaster by assessing its detection accuracy on three\ndatasets (CIFAR10, MNIST, and Tiny-ImageNet) across three model architectures\n(ResNet18, VGG16, and DenseNet161). We conducted experiments under various\nattack scenarios, including transfer learning, pruning, fine-tuning, and data\naugmentation. Specifically, in the Multi-Architecture Attack scenario,\nDeepTaster was able to identify all the stolen cases across all datasets, while\nDeepJudge failed to detect any of the cases.\n","authors":["Seonhye Park","Alsharif Abuadbba","Shuo Wang","Kristen Moore","Yansong Gao","Hyoungshick Kim","Surya Nepal"],"pdf_url":"https://arxiv.org/pdf/2211.13535v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02576v1","updated":"2024-01-04T23:44:35Z","published":"2024-01-04T23:44:35Z","title":"t-DGR: A Trajectory-Based Deep Generative Replay Method for Continual\n  Learning in Decision Making","summary":"  Deep generative replay has emerged as a promising approach for continual\nlearning in decision-making tasks. This approach addresses the problem of\ncatastrophic forgetting by leveraging the generation of trajectories from\npreviously encountered tasks to augment the current dataset. However, existing\ndeep generative replay methods for continual learning rely on autoregressive\nmodels, which suffer from compounding errors in the generated trajectories. In\nthis paper, we propose a simple, scalable, and non-autoregressive method for\ncontinual learning in decision-making tasks using a generative model that\ngenerates task samples conditioned on the trajectory timestep. We evaluate our\nmethod on Continual World benchmarks and find that our approach achieves\nstate-of-the-art performance on the average success rate metric among continual\nlearning methods. Code is available at https://github.com/WilliamYue37/t-DGR .\n","authors":["William Yue","Bo Liu","Peter Stone"],"pdf_url":"https://arxiv.org/pdf/2401.02576v1.pdf","comment":"2nd Workshop on Agent Learning in Open-Endedness (ALOE) at NeurIPS\n  2023"},{"id":"http://arxiv.org/abs/2303.02725v4","updated":"2024-01-04T23:44:12Z","published":"2023-03-05T17:44:23Z","title":"Local Environment Poisoning Attacks on Federated Reinforcement Learning","summary":"  Federated learning (FL) has become a popular tool for solving traditional\nReinforcement Learning (RL) tasks. The multi-agent structure addresses the\nmajor concern of data-hungry in traditional RL, while the federated mechanism\nprotects the data privacy of individual agents. However, the federated\nmechanism also exposes the system to poisoning by malicious agents that can\nmislead the trained policy. Despite the advantage brought by FL, the\nvulnerability of Federated Reinforcement Learning (FRL) has not been\nwell-studied before. In this work, we propose a general framework to\ncharacterize FRL poisoning as an optimization problem and design a poisoning\nprotocol that can be applied to policy-based FRL. Our framework can also be\nextended to FRL with actor-critic as a local RL algorithm by training a pair of\nprivate and public critics. We provably show that our method can strictly hurt\nthe global objective. We verify our poisoning effectiveness by conducting\nextensive experiments targeting mainstream RL algorithms and over various RL\nOpenAI Gym environments covering a wide range of difficulty levels. Within\nthese experiments, we compare clean and baseline poisoning methods against our\nproposed framework. The results show that the proposed framework is successful\nin poisoning FRL systems and reducing performance across various environments\nand does so more effectively than baseline methods. Our work provides new\ninsights into the vulnerability of FL in RL training and poses new challenges\nfor designing robust FRL algorithms\n","authors":["Evelyn Ma","Praneet Rathi","S. Rasoul Etesami"],"pdf_url":"https://arxiv.org/pdf/2303.02725v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02575v1","updated":"2024-01-04T23:37:48Z","published":"2024-01-04T23:37:48Z","title":"Large Language Models for Social Networks: Applications, Challenges, and\n  Solutions","summary":"  Large Language Models (LLMs) are transforming the way people generate,\nexplore, and engage with content. We study how we can develop LLM applications\nfor online social networks. Despite LLMs' successes in other domains, it is\nchallenging to develop LLM-based products for social networks for numerous\nreasons, and it has been relatively under-reported in the research community.\nWe categorize LLM applications for social networks into three categories. First\nis knowledge tasks where users want to find new knowledge and information, such\nas search and question-answering. Second is entertainment tasks where users\nwant to consume interesting content, such as getting entertaining notification\ncontent. Third is foundational tasks that need to be done to moderate and\noperate the social networks, such as content annotation and LLM monitoring. For\neach task, we share the challenges we found, solutions we developed, and\nlessons we learned. To the best of our knowledge, this is the first\ncomprehensive paper about developing LLM applications for social networks.\n","authors":["Jingying Zeng","Richard Huang","Waleed Malik","Langxuan Yin","Bojan Babic","Danny Shacham","Xiao Yan","Jaewon Yang","Qi He"],"pdf_url":"https://arxiv.org/pdf/2401.02575v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.12006v3","updated":"2024-01-04T23:09:40Z","published":"2023-06-21T04:05:10Z","title":"Learning Homogenization for Elliptic Operators","summary":"  Multiscale partial differential equations (PDEs) arise in various\napplications, and several schemes have been developed to solve them\nefficiently. Homogenization theory is a powerful methodology that eliminates\nthe small-scale dependence, resulting in simplified equations that are\ncomputationally tractable while accurately predicting the macroscopic response.\nIn the field of continuum mechanics, homogenization is crucial for deriving\nconstitutive laws that incorporate microscale physics in order to formulate\nbalance laws for the macroscopic quantities of interest. However, obtaining\nhomogenized constitutive laws is often challenging as they do not in general\nhave an analytic form and can exhibit phenomena not present on the microscale.\nIn response, data-driven learning of the constitutive law has been proposed as\nappropriate for this task. However, a major challenge in data-driven learning\napproaches for this problem has remained unexplored: the impact of\ndiscontinuities and corner interfaces in the underlying material. These\ndiscontinuities in the coefficients affect the smoothness of the solutions of\nthe underlying equations. Given the prevalence of discontinuous materials in\ncontinuum mechanics applications, it is important to address the challenge of\nlearning in this context; in particular, to develop underpinning theory that\nestablishes the reliability of data-driven methods in this scientific domain.\nThe paper addresses this unexplored challenge by investigating the learnability\nof homogenized constitutive laws for elliptic operators in the presence of such\ncomplexities. Approximation theory is presented, and numerical experiments are\nperformed which validate the theory in the context of learning the solution\noperator defined by the cell problem arising in homogenization for elliptic\nPDEs.\n","authors":["Kaushik Bhattacharya","Nikola Kovachki","Aakila Rajan","Andrew M. Stuart","Margaret Trautner"],"pdf_url":"https://arxiv.org/pdf/2306.12006v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.04688v3","updated":"2024-01-04T23:07:19Z","published":"2022-06-09T08:27:59Z","title":"A New Frontier of AI: On-Device AI Training and Personalization","summary":"  Modern consumer electronic devices have started executing deep learning-based\nintelligence services on devices, not cloud servers, to keep personal data on\ndevices and to reduce network and cloud costs. We find such a trend as the\nopportunity to personalize intelligence services by updating neural networks\nwith user data without exposing the data out of devices: on-device training.\nHowever, the limited resources of devices incurs significant difficulties. We\npropose a light-weight on-device training framework, NNTrainer, which provides\nhighly memory-efficient neural network training techniques and proactive\nswapping based on fine-grained execution order analysis for neural networks.\nMoreover, its optimizations do not sacrifice accuracy and are transparent to\ntraining algorithms; thus, prior algorithmic studies may be implemented on top\nof NNTrainer. The evaluations show that NNTrainer can reduce memory consumption\ndown to 1/20 (saving 95%!) and effectively personalizes intelligence services\non devices. NNTrainer is cross-platform and practical open-source software,\nwhich is being deployed to millions of mobile devices.\n","authors":["Ji Joong Moon","Hyun Suk Lee","Jiho Chu","Donghak Park","Seungbaek Hong","Hyungjun Seo","Donghyeon Jeong","Sungsik Kong","MyungJoo Ham"],"pdf_url":"https://arxiv.org/pdf/2206.04688v3.pdf","comment":"12 pages, 16 figures, Accepted in ICSE 2024"},{"id":"http://arxiv.org/abs/2401.02566v1","updated":"2024-01-04T22:51:13Z","published":"2024-01-04T22:51:13Z","title":"Siamese Residual Neural Network for Musical Shape Evaluation in Piano\n  Performance Assessment","summary":"  Understanding and identifying musical shape plays an important role in music\neducation and performance assessment. To simplify the otherwise time- and\ncost-intensive musical shape evaluation, in this paper we explore how\nartificial intelligence (AI) driven models can be applied. Considering musical\nshape evaluation as a classification problem, a light-weight Siamese residual\nneural network (S-ResNN) is proposed to automatically identify musical shapes.\nTo assess the proposed approach in the context of piano musical shape\nevaluation, we have generated a new dataset, containing 4116 music pieces\nderived by 147 piano preparatory exercises and performed in 28 categories of\nmusical shapes. The experimental results show that the S-ResNN significantly\noutperforms a number of benchmark methods in terms of the precision, recall and\nF1 score.\n","authors":["Xiaoquan Li","Stephan Weiss","Yijun Yan","Yinhe Li","Jinchang Ren","John Soraghan","Ming Gong"],"pdf_url":"https://arxiv.org/pdf/2401.02566v1.pdf","comment":"X.Li, S.Weiss, Y.Yan, Y.Li, J.Ren, J.Soraghan, M.Gong,\"Siamese\n  residual neural network for musical shape evaluation in piano performance\n  assessment\" in Proc. of the 31st European Signal Processing Conference,\n  Helsinki, Finland"},{"id":"http://arxiv.org/abs/2312.11514v2","updated":"2024-01-04T22:28:37Z","published":"2023-12-12T18:57:08Z","title":"LLM in a flash: Efficient Large Language Model Inference with Limited\n  Memory","summary":"  Large language models (LLMs) are central to modern natural language\nprocessing, delivering exceptional performance in various tasks. However, their\nsubstantial computational and memory requirements present challenges,\nespecially for devices with limited DRAM capacity. This paper tackles the\nchallenge of efficiently running LLMs that exceed the available DRAM capacity\nby storing the model parameters in flash memory, but bringing them on demand to\nDRAM. Our method involves constructing an inference cost model that takes into\naccount the characteristics of flash memory, guiding us to optimize in two\ncritical areas: reducing the volume of data transferred from flash and reading\ndata in larger, more contiguous chunks. Within this hardware-informed\nframework, we introduce two principal techniques. First, \"windowing\"\nstrategically reduces data transfer by reusing previously activated neurons,\nand second, \"row-column bundling\", tailored to the sequential data access\nstrengths of flash memory, increases the size of data chunks read from flash\nmemory. These methods collectively enable running models up to twice the size\nof the available DRAM, with a 4-5x and 20-25x increase in inference speed\ncompared to naive loading approaches in CPU and GPU, respectively. Our\nintegration of sparsity awareness, context-adaptive loading, and a\nhardware-oriented design paves the way for effective inference of LLMs on\ndevices with limited memory.\n","authors":["Keivan Alizadeh","Iman Mirzadeh","Dmitry Belenko","Karen Khatamifard","Minsik Cho","Carlo C Del Mundo","Mohammad Rastegari","Mehrdad Farajtabar"],"pdf_url":"https://arxiv.org/pdf/2312.11514v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2401.02561v1","updated":"2024-01-04T22:23:56Z","published":"2024-01-04T22:23:56Z","title":"MeTA: Multi-source Test Time Adaptation","summary":"  Test time adaptation is the process of adapting, in an unsupervised manner, a\npre-trained source model to each incoming batch of the test data (i.e., without\nrequiring a substantial portion of the test data to be available, as in\ntraditional domain adaptation) and without access to the source data. Since it\nworks with each batch of test data, it is well-suited for dynamic environments\nwhere decisions need to be made as the data is streaming in. Current test time\nadaptation methods are primarily focused on a single source model. We propose\nthe first completely unsupervised Multi-source Test Time Adaptation (MeTA)\nframework that handles multiple source models and optimally combines them to\nadapt to the test data. MeTA has two distinguishing features. First, it\nefficiently obtains the optimal combination weights to combine the source\nmodels to adapt to the test data distribution. Second, it identifies which of\nthe source model parameters to update so that only the model which is most\ncorrelated to the target data is adapted, leaving the less correlated ones\nuntouched; this mitigates the issue of \"forgetting\" the source model parameters\nby focusing only on the source model that exhibits the strongest correlation\nwith the test batch distribution. Experiments on diverse datasets demonstrate\nthat the combination of multiple source models does at least as well as the\nbest source (with hindsight knowledge), and performance does not degrade as the\ntest data distribution changes over time (robust to forgetting).\n","authors":["Sk Miraj Ahmed","Fahim Faisal Niloy","Dripta S. Raychaudhuri","Samet Oymak","Amit K. Roy-Chowdhury"],"pdf_url":"https://arxiv.org/pdf/2401.02561v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2401.02552v1","updated":"2024-01-04T21:55:50Z","published":"2024-01-04T21:55:50Z","title":"Long-term Fairness For Real-time Decision Making: A Constrained Online\n  Optimization Approach","summary":"  Machine learning (ML) has demonstrated remarkable capabilities across many\nreal-world systems, from predictive modeling to intelligent automation.\nHowever, the widespread integration of machine learning also makes it necessary\nto ensure machine learning-driven decision-making systems do not violate\nethical principles and values of society in which they operate. As ML-driven\ndecisions proliferate, particularly in cases involving sensitive attributes\nsuch as gender, race, and age, to name a few, the need for equity and\nimpartiality has emerged as a fundamental concern. In situations demanding\nreal-time decision-making, fairness objectives become more nuanced and complex:\ninstantaneous fairness to ensure equity in every time slot, and long-term\nfairness to ensure fairness over a period of time. There is a growing awareness\nthat real-world systems that operate over long periods and require fairness\nover different timelines. However, existing approaches mainly address dynamic\ncosts with time-invariant fairness constraints, often disregarding the\nchallenges posed by time-varying fairness constraints. To bridge this gap, this\nwork introduces a framework for ensuring long-term fairness within dynamic\ndecision-making systems characterized by time-varying fairness constraints. We\nformulate the decision problem with fairness constraints over a period as a\nconstrained online optimization problem. A novel online algorithm, named\nLoTFair, is presented that solves the problem 'on the fly'. We prove that\nLoTFair can make overall fairness violations negligible while maintaining the\nperformance over the long run.\n","authors":["Ruijie Du","Deepan Muthirayan","Pramod P. Khargonekar","Yanning Shen"],"pdf_url":"https://arxiv.org/pdf/2401.02552v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.02952v3","updated":"2024-01-04T21:34:33Z","published":"2022-02-07T05:29:16Z","title":"Supervision by Denoising for Medical Image Segmentation","summary":"  Learning-based image reconstruction models, such as those based on the U-Net,\nrequire a large set of labeled images if good generalization is to be\nguaranteed. In some imaging domains, however, labeled data with pixel- or\nvoxel-level label accuracy are scarce due to the cost of acquiring them. This\nproblem is exacerbated further in domains like medical imaging, where there is\nno single ground truth label, resulting in large amounts of repeat variability\nin the labels. Therefore, training reconstruction networks to generalize better\nby learning from both labeled and unlabeled examples (called semi-supervised\nlearning) is problem of practical and theoretical interest. However,\ntraditional semi-supervised learning methods for image reconstruction often\nnecessitate handcrafting a differentiable regularizer specific to some given\nimaging problem, which can be extremely time-consuming. In this work, we\npropose \"supervision by denoising\" (SUD), a framework that enables us to\nsupervise reconstruction models using their own denoised output as soft labels.\nSUD unifies stochastic averaging and spatial denoising techniques under a\nspatio-temporal denoising framework and alternates denoising and model weight\nupdate steps in an optimization framework for semi-supervision. As example\napplications, we apply SUD to two problems arising from biomedical imaging --\nanatomical brain reconstruction (3D) and cortical parcellation (2D) -- to\ndemonstrate a significant improvement in the image reconstructions over\nsupervised-only and stochastic averaging baselines.\n","authors":["Sean I. Young","Adrian V. Dalca","Enzo Ferrante","Polina Golland","Christopher A. Metzler","Bruce Fischl","Juan Eugenio Iglesias"],"pdf_url":"https://arxiv.org/pdf/2202.02952v3.pdf","comment":"To appear in the IEEE Transactions on Pattern Analysis and Machine\n  Intelligence"},{"id":"http://arxiv.org/abs/2401.02544v1","updated":"2024-01-04T21:24:01Z","published":"2024-01-04T21:24:01Z","title":"Hyperparameter Estimation for Sparse Bayesian Learning Models","summary":"  Sparse Bayesian Learning (SBL) models are extensively used in signal\nprocessing and machine learning for promoting sparsity through hierarchical\npriors. The hyperparameters in SBL models are crucial for the model's\nperformance, but they are often difficult to estimate due to the non-convexity\nand the high-dimensionality of the associated objective function. This paper\npresents a comprehensive framework for hyperparameter estimation in SBL models,\nencompassing well-known algorithms such as the expectation-maximization (EM),\nMacKay, and convex bounding (CB) algorithms. These algorithms are cohesively\ninterpreted within an alternating minimization and linearization (AML)\nparadigm, distinguished by their unique linearized surrogate functions.\nAdditionally, a novel algorithm within the AML framework is introduced, showing\nenhanced efficiency, especially under low signal noise ratios. This is further\nimproved by a new alternating minimization and quadratic approximation (AMQ)\nparadigm, which includes a proximal regularization term. The paper\nsubstantiates these advancements with thorough convergence analysis and\nnumerical experiments, demonstrating the algorithm's effectiveness in various\nnoise conditions and signal-to-noise ratios.\n","authors":["Feng Yu","Lixin Shen","Guohui Song"],"pdf_url":"https://arxiv.org/pdf/2401.02544v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02536v1","updated":"2024-01-04T20:53:43Z","published":"2024-01-04T20:53:43Z","title":"Novel End-to-End Production-Ready Machine Learning Flow for\n  Nanolithography Modeling and Correction","summary":"  Optical lithography is the main enabler to semiconductor manufacturing. It\nrequires extensive processing to perform the Resolution Enhancement Techniques\n(RETs) required to transfer the design data to a working Integrated Circuits\n(ICs). The processing power and computational runtime for RETs tasks is ever\nincreasing due to the continuous reduction of the feature size and the\nexpansion of the chip area. State-of-the-art research sought Machine Learning\n(ML) technologies to reduce runtime and computational power, however they are\nstill not used in production yet. In this study, we analyze the reasons holding\nback ML computational lithography from being production ready and present a\nnovel highly scalable end-to-end flow that enables production ready ML-RET\ncorrection.\n","authors":["Mohamed S. E. Habib","Hossam A. H. Fahmy","Mohamed F. Abu-ElYazeed"],"pdf_url":"https://arxiv.org/pdf/2401.02536v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14303v2","updated":"2024-01-04T20:41:48Z","published":"2023-12-21T21:26:09Z","title":"Geo2SigMap: High-Fidelity RF Signal Mapping Using Geographic Databases","summary":"  Radio frequency (RF) signal mapping, which is the process of analyzing and\npredicting the RF signal strength and distribution across specific areas, is\ncrucial for cellular network planning and deployment. Traditional approaches to\nRF signal mapping rely on statistical models constructed based on measurement\ndata, which offer low complexity but often lack accuracy, or ray tracing tools,\nwhich provide enhanced precision for the target area but suffer from increased\ncomputational complexity. Recently, machine learning (ML) has emerged as a\ndata-driven method for modeling RF signal propagation, which leverages models\ntrained on synthetic datasets to perform RF signal mapping in \"unseen\" areas.\n  In this paper, we present Geo2SigMap, an ML-based framework for efficient and\nhigh-fidelity RF signal mapping using geographic databases. First, we develop\nan automated framework that seamlessly integrates three open-source tools:\nOpenStreetMap (geographic databases), Blender (computer graphics), and Sionna\n(ray tracing), enabling the efficient generation of large-scale 3D building\nmaps and ray tracing models. Second, we propose a cascaded U-Net model, which\nis pre-trained on synthetic datasets and employed to generate detailed RF\nsignal maps, leveraging environmental information and sparse measurement data.\nFinally, we evaluate the performance of Geo2SigMap via a real-world measurement\ncampaign, where three types of user equipment (UE) collect over 45,000 data\npoints related to cellular information from six LTE cells operating in the\ncitizens broadband radio service (CBRS) band. Our results show that Geo2SigMap\nachieves an average root-mean-square-error (RMSE) of 6.04 dB for predicting the\nreference signal received power (RSRP) at the UE, representing an average RMSE\nimprovement of 3.59 dB compared to existing methods.\n","authors":["Yiming Li","Zeyu Li","Zhihui Gao","Tingjun Chen"],"pdf_url":"https://arxiv.org/pdf/2312.14303v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.15325v5","updated":"2024-01-04T20:38:03Z","published":"2023-09-27T00:12:07Z","title":"Neural Operators for Accelerating Scientific Simulations and Design","summary":"  Scientific discovery and engineering design are currently limited by the time\nand cost of physical experiments, selected mostly through trial-and-error and\nintuition that require deep domain expertise. Numerical simulations present an\nalternative to physical experiments but are usually infeasible for complex\nreal-world domains due to the computational requirements of existing numerical\nmethods. Artificial intelligence (AI) presents a potential paradigm shift by\ndeveloping fast data-driven surrogate models. In particular, an AI framework,\nknown as Neural Operators, presents a principled framework for learning\nmappings between functions defined on continuous domains, e.g., spatiotemporal\nprocesses and partial differential equations (PDE). They can extrapolate and\npredict solutions at new locations unseen during training, i.e., perform\nzero-shot super-resolution. Neural Operators can augment or even replace\nexisting simulators in many applications, such as computational fluid dynamics,\nweather forecasting, and material modeling, while being 4-5 orders of magnitude\nfaster. Further, Neural Operators can be integrated with physics and other\ndomain constraints enforced at finer resolutions to obtain high-fidelity\nsolutions and good generalization. Since Neural Operators are differentiable,\nthey can directly optimize parameters for inverse design and other inverse\nproblems. We believe that Neural Operators present a transformative approach to\nsimulation and design, enabling rapid research and development.\n","authors":["Kamyar Azizzadenesheli","Nikola Kovachki","Zongyi Li","Miguel Liu-Schiaffini","Jean Kossaifi","Anima Anandkumar"],"pdf_url":"https://arxiv.org/pdf/2309.15325v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02526v1","updated":"2024-01-04T20:29:44Z","published":"2024-01-04T20:29:44Z","title":"Branched Variational Autoencoder Classifiers","summary":"  This paper introduces a modified variational autoencoder (VAEs) that contains\nan additional neural network branch. The resulting branched VAE (BVAE)\ncontributes a classification component based on the class labels to the total\nloss and therefore imparts categorical information to the latent\nrepresentation. As a result, the latent space distributions of the input\nclasses are separated and ordered, thereby enhancing the classification\naccuracy. The degree of improvement is quantified by numerical calculations\nemploying the benchmark MNIST dataset for both unrotated and rotated digits.\nThe proposed technique is then compared to and then incorporated into a VAE\nwith fixed output distributions. This procedure is found to yield improved\nperformance for a wide range of output distributions.\n","authors":["Ahmed Salah","David Yevick"],"pdf_url":"https://arxiv.org/pdf/2401.02526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02524v1","updated":"2024-01-04T20:23:51Z","published":"2024-01-04T20:23:51Z","title":"Comprehensive Exploration of Synthetic Data Generation: A Survey","summary":"  Recent years have witnessed a surge in the popularity of Machine Learning\n(ML), applied across diverse domains. However, progress is impeded by the\nscarcity of training data due to expensive acquisition and privacy legislation.\nSynthetic data emerges as a solution, but the abundance of released models and\nlimited overview literature pose challenges for decision-making. This work\nsurveys 417 Synthetic Data Generation (SDG) models over the last decade,\nproviding a comprehensive overview of model types, functionality, and\nimprovements. Common attributes are identified, leading to a classification and\ntrend analysis. The findings reveal increased model performance and complexity,\nwith neural network-based approaches prevailing, except for privacy-preserving\ndata generation. Computer vision dominates, with GANs as primary generative\nmodels, while diffusion models, transformers, and RNNs compete. Implications\nfrom our performance evaluation highlight the scarcity of common metrics and\ndatasets, making comparisons challenging. Additionally, the neglect of training\nand computational costs in literature necessitates attention in future\nresearch. This work serves as a guide for SDG model selection and identifies\ncrucial areas for future exploration.\n","authors":["André Bauer","Simon Trapp","Michael Stenger","Robert Leppich","Samuel Kounev","Mark Leznik","Kyle Chard","Ian Foster"],"pdf_url":"https://arxiv.org/pdf/2401.02524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02523v1","updated":"2024-01-04T20:17:25Z","published":"2024-01-04T20:17:25Z","title":"Image-based Deep Learning for Smart Digital Twins: a Review","summary":"  Smart Digital twins (SDTs) are being increasingly used to virtually replicate\nand predict the behaviors of complex physical systems through continual data\nassimilation enabling the optimization of the performance of these systems by\ncontrolling the actions of systems. Recently, deep learning (DL) models have\nsignificantly enhanced the capabilities of SDTs, particularly for tasks such as\npredictive maintenance, anomaly detection, and optimization. In many domains,\nincluding medicine, engineering, and education, SDTs use image data\n(image-based SDTs) to observe and learn system behaviors and control their\nbehaviors. This paper focuses on various approaches and associated challenges\nin developing image-based SDTs by continually assimilating image data from\nphysical systems. The paper also discusses the challenges involved in designing\nand implementing DL models for SDTs, including data acquisition, processing,\nand interpretation. In addition, insights into the future directions and\nopportunities for developing new image-based DL approaches to develop robust\nSDTs are provided. This includes the potential for using generative models for\ndata augmentation, developing multi-modal DL models, and exploring the\nintegration of DL with other technologies, including 5G, edge computing, and\nIoT. In this paper, we describe the image-based SDTs, which enable broader\nadoption of the digital twin DT paradigms across a broad spectrum of areas and\nthe development of new methods to improve the abilities of SDTs in replicating,\npredicting, and optimizing the behavior of complex systems.\n","authors":["Md Ruman Islam","Mahadevan Subramaniam","Pei-Chi Huang"],"pdf_url":"https://arxiv.org/pdf/2401.02523v1.pdf","comment":"12 pages, 2 figures, and 3 tables"},{"id":"http://arxiv.org/abs/2401.02520v1","updated":"2024-01-04T20:13:23Z","published":"2024-01-04T20:13:23Z","title":"Structured Matrix Learning under Arbitrary Entrywise Dependence and\n  Estimation of Markov Transition Kernel","summary":"  The problem of structured matrix estimation has been studied mostly under\nstrong noise dependence assumptions. This paper considers a general framework\nof noisy low-rank-plus-sparse matrix recovery, where the noise matrix may come\nfrom any joint distribution with arbitrary dependence across entries. We\npropose an incoherent-constrained least-square estimator and prove its\ntightness both in the sense of deterministic lower bound and matching minimax\nrisks under various noise distributions. To attain this, we establish a novel\nresult asserting that the difference between two arbitrary low-rank incoherent\nmatrices must spread energy out across its entries, in other words cannot be\ntoo sparse, which sheds light on the structure of incoherent low-rank matrices\nand may be of independent interest. We then showcase the applications of our\nframework to several important statistical machine learning problems. In the\nproblem of estimating a structured Markov transition kernel, the proposed\nmethod achieves the minimax optimality and the result can be extended to\nestimating the conditional mean operator, a crucial component in reinforcement\nlearning. The applications to multitask regression and structured covariance\nestimation are also presented. We propose an alternating minimization algorithm\nto approximately solve the potentially hard optimization problem. Numerical\nresults corroborate the effectiveness of our method which typically converges\nin a few steps.\n","authors":["Jinhang Chai","Jianqing Fan"],"pdf_url":"https://arxiv.org/pdf/2401.02520v1.pdf","comment":"55 pages, 4 figures"},{"id":"http://arxiv.org/abs/2401.02511v1","updated":"2024-01-04T19:45:27Z","published":"2024-01-04T19:45:27Z","title":"Gain Scheduling with a Neural Operator for a Transport PDE with\n  Nonlinear Recirculation","summary":"  To stabilize PDE models, control laws require space-dependent functional\ngains mapped by nonlinear operators from the PDE functional coefficients. When\na PDE is nonlinear and its \"pseudo-coefficient\" functions are state-dependent,\na gain-scheduling (GS) nonlinear design is the simplest approach to the design\nof nonlinear feedback. The GS version of PDE backstepping employs gains\nobtained by solving a PDE at each value of the state. Performing such PDE\ncomputations in real time may be prohibitive. The recently introduced neural\noperators (NO) can be trained to produce the gain functions, rapidly in real\ntime, for each state value, without requiring a PDE solution. In this paper we\nintroduce NOs for GS-PDE backstepping. GS controllers act on the premise that\nthe state change is slow and, as a result, guarantee only local stability, even\nfor ODEs. We establish local stabilization of hyperbolic PDEs with nonlinear\nrecirculation using both a \"full-kernel\" approach and the \"gain-only\" approach\nto gain operator approximation. Numerical simulations illustrate stabilization\nand demonstrate speedup by three orders of magnitude over traditional PDE\ngain-scheduling. Code (Github) for the numerical implementation is published to\nenable exploration.\n","authors":["Maxence Lamarque","Luke Bhan","Rafael Vazquez","Miroslav Krstic"],"pdf_url":"https://arxiv.org/pdf/2401.02511v1.pdf","comment":"16 pages, 5 figures"},{"id":"http://arxiv.org/abs/2401.02508v1","updated":"2024-01-04T19:41:33Z","published":"2024-01-04T19:41:33Z","title":"Towards an Adaptable and Generalizable Optimization Engine in Decision\n  and Control: A Meta Reinforcement Learning Approach","summary":"  Sampling-based model predictive control (MPC) has found significant success\nin optimal control problems with non-smooth system dynamics and cost function.\nMany machine learning-based works proposed to improve MPC by a) learning or\nfine-tuning the dynamics/ cost function, or b) learning to optimize for the\nupdate of the MPC controllers. For the latter, imitation learning-based\noptimizers are trained to update the MPC controller by mimicking the expert\ndemonstrations, which, however, are expensive or even unavailable. More\nsignificantly, many sequential decision-making problems are in non-stationary\nenvironments, requiring that an optimizer should be adaptable and generalizable\nto update the MPC controller for solving different tasks. To address those\nissues, we propose to learn an optimizer based on meta-reinforcement learning\n(RL) to update the controllers. This optimizer does not need expert\ndemonstration and can enable fast adaptation (e.g., few-shots) when it is\ndeployed in unseen control tasks. Experimental results validate the\neffectiveness of the learned optimizer regarding fast adaptation.\n","authors":["Sungwook Yang","Chaoying Pei","Ran Dai","Chuangchuang Sun"],"pdf_url":"https://arxiv.org/pdf/2401.02508v1.pdf","comment":"3 pages"},{"id":"http://arxiv.org/abs/2401.02501v1","updated":"2024-01-04T19:25:00Z","published":"2024-01-04T19:25:00Z","title":"The cell signaling structure function","summary":"  Live cell microscopy captures 5-D $(x,y,z,channel,time)$ movies that display\npatterns of cellular motion and signaling dynamics. We present here an approach\nto finding spatiotemporal patterns of cell signaling dynamics in 5-D live cell\nmicroscopy movies unique in requiring no \\emph{a priori} knowledge of expected\npattern dynamics, and no training data. The proposed cell signaling structure\nfunction (SSF) is a Kolmogorov structure function that optimally measures cell\nsignaling state as nuclear intensity w.r.t. surrounding cytoplasm, a\nsignificant improvement compared to the current state-of-the-art cytonuclear\nratio. SSF kymographs store at each spatiotemporal cell centroid the SSF value,\nor a functional output such as velocity. Patterns of similarity are identified\nvia the metric normalized compression distance (NCD). The NCD is a reproducing\nkernel for a Hilbert space that represents the input SSF kymographs as points\nin a low dimensional embedding that optimally captures the pattern similarity\nidentified by the NCD throughout the space. The only parameter is the expected\ncell radii ($\\mu m$). A new formulation of the cluster structure function\noptimally estimates how meaningful an embedding from the RKHS representation.\nResults are presented quantifying the impact of ERK and AKT signaling between\ndifferent oncogenic mutations, and by the relation between ERK signaling and\ncellular velocity patterns for movies of 2-D monolayers of human breast\nepithelial (MCF10A) cells, 3-D MCF10A spheroids under optogenetic manipulation\nof ERK, and human induced pluripotent stem cells .\n","authors":["Layton Aho","Mark Winter","Marc DeCarlo","Agne Frismantiene","Yannick Blum","Paolo Armando Gagliardi","Olivier Pertz","Andrew R. Cohen"],"pdf_url":"https://arxiv.org/pdf/2401.02501v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.13301v4","updated":"2024-01-04T19:11:25Z","published":"2023-05-22T17:57:41Z","title":"Training Diffusion Models with Reinforcement Learning","summary":"  Diffusion models are a class of flexible generative models trained with an\napproximation to the log-likelihood objective. However, most use cases of\ndiffusion models are not concerned with likelihoods, but instead with\ndownstream objectives such as human-perceived image quality or drug\neffectiveness. In this paper, we investigate reinforcement learning methods for\ndirectly optimizing diffusion models for such objectives. We describe how\nposing denoising as a multi-step decision-making problem enables a class of\npolicy gradient algorithms, which we refer to as denoising diffusion policy\noptimization (DDPO), that are more effective than alternative reward-weighted\nlikelihood approaches. Empirically, DDPO is able to adapt text-to-image\ndiffusion models to objectives that are difficult to express via prompting,\nsuch as image compressibility, and those derived from human feedback, such as\naesthetic quality. Finally, we show that DDPO can improve prompt-image\nalignment using feedback from a vision-language model without the need for\nadditional data collection or human annotation. The project's website can be\nfound at http://rl-diffusion.github.io .\n","authors":["Kevin Black","Michael Janner","Yilun Du","Ilya Kostrikov","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/2305.13301v4.pdf","comment":"23 pages, 16 figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2401.02309v1","updated":"2024-01-04T14:55:57Z","published":"2024-01-04T14:55:57Z","title":"TR-DETR: Task-Reciprocal Transformer for Joint Moment Retrieval and\n  Highlight Detection","summary":"  Video moment retrieval (MR) and highlight detection (HD) based on natural\nlanguage queries are two highly related tasks, which aim to obtain relevant\nmoments within videos and highlight scores of each video clip. Recently,\nseveral methods have been devoted to building DETR-based networks to solve both\nMR and HD jointly. These methods simply add two separate task heads after\nmulti-modal feature extraction and feature interaction, achieving good\nperformance. Nevertheless, these approaches underutilize the reciprocal\nrelationship between two tasks. In this paper, we propose a task-reciprocal\ntransformer based on DETR (TR-DETR) that focuses on exploring the inherent\nreciprocity between MR and HD. Specifically, a local-global multi-modal\nalignment module is first built to align features from diverse modalities into\na shared latent space. Subsequently, a visual feature refinement is designed to\neliminate query-irrelevant information from visual features for modal\ninteraction. Finally, a task cooperation module is constructed to refine the\nretrieval pipeline and the highlight score prediction process by utilizing the\nreciprocity between MR and HD. Comprehensive experiments on QVHighlights,\nCharades-STA and TVSum datasets demonstrate that TR-DETR outperforms existing\nstate-of-the-art methods. Codes are available at\n\\url{https://github.com/mingyao1120/TR-DETR}.\n","authors":["Hao Sun","Mingyao Zhou","Wenjing Chen","Wei Xie"],"pdf_url":"https://arxiv.org/pdf/2401.02309v1.pdf","comment":"Accepted by AAAI-24"},{"id":"http://arxiv.org/abs/2305.08372v2","updated":"2024-01-04T10:15:28Z","published":"2023-05-15T06:14:36Z","title":"Hierarchical Aligned Multimodal Learning for NER on Tweet Posts","summary":"  Mining structured knowledge from tweets using named entity recognition (NER)\ncan be beneficial for many down stream applications such as recommendation and\nintention understanding. With tweet posts tending to be multimodal, multimodal\nnamed entity recognition (MNER) has attracted more attention. In this paper, we\npropose a novel approach, which can dynamically align the image and text\nsequence and achieve the multi-level cross-modal learning to augment textual\nword representation for MNER improvement. To be specific, our framework can be\nsplit into three main stages: the first stage focuses on intra-modality\nrepresentation learning to derive the implicit global and local knowledge of\neach modality, the second evaluates the relevance between the text and its\naccompanying image and integrates different grained visual information based on\nthe relevance, the third enforces semantic refinement via iterative cross-modal\ninteractions and co-attention. We conduct experiments on two open datasets, and\nthe results and detailed analysis demonstrate the advantage of our model.\n","authors":["Peipei Liu","Hong Li","Yimo Ren","Jie Liu","Shuaizong Si","Hongsong Zhu","Limin Sun"],"pdf_url":"https://arxiv.org/pdf/2305.08372v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02566v1","updated":"2024-01-04T22:51:13Z","published":"2024-01-04T22:51:13Z","title":"Siamese Residual Neural Network for Musical Shape Evaluation in Piano\n  Performance Assessment","summary":"  Understanding and identifying musical shape plays an important role in music\neducation and performance assessment. To simplify the otherwise time- and\ncost-intensive musical shape evaluation, in this paper we explore how\nartificial intelligence (AI) driven models can be applied. Considering musical\nshape evaluation as a classification problem, a light-weight Siamese residual\nneural network (S-ResNN) is proposed to automatically identify musical shapes.\nTo assess the proposed approach in the context of piano musical shape\nevaluation, we have generated a new dataset, containing 4116 music pieces\nderived by 147 piano preparatory exercises and performed in 28 categories of\nmusical shapes. The experimental results show that the S-ResNN significantly\noutperforms a number of benchmark methods in terms of the precision, recall and\nF1 score.\n","authors":["Xiaoquan Li","Stephan Weiss","Yijun Yan","Yinhe Li","Jinchang Ren","John Soraghan","Ming Gong"],"pdf_url":"https://arxiv.org/pdf/2401.02566v1.pdf","comment":"X.Li, S.Weiss, Y.Yan, Y.Li, J.Ren, J.Soraghan, M.Gong,\"Siamese\n  residual neural network for musical shape evaluation in piano performance\n  assessment\" in Proc. of the 31st European Signal Processing Conference,\n  Helsinki, Finland"},{"id":"http://arxiv.org/abs/2310.03937v2","updated":"2024-01-04T21:39:25Z","published":"2023-10-05T23:00:27Z","title":"Diffusion Models as Masked Audio-Video Learners","summary":"  Over the past several years, the synchronization between audio and visual\nsignals has been leveraged to learn richer audio-visual representations. Aided\nby the large availability of unlabeled videos, many unsupervised training\nframeworks have demonstrated impressive results in various downstream audio and\nvideo tasks. Recently, Masked Audio-Video Learners (MAViL) has emerged as a\nstate-of-the-art audio-video pre-training framework. MAViL couples contrastive\nlearning with masked autoencoding to jointly reconstruct audio spectrograms and\nvideo frames by fusing information from both modalities. In this paper, we\nstudy the potential synergy between diffusion models and MAViL, seeking to\nderive mutual benefits from these two frameworks. The incorporation of\ndiffusion into MAViL, combined with various training efficiency methodologies\nthat include the utilization of a masking ratio curriculum and adaptive batch\nsizing, results in a notable 32% reduction in pre-training Floating-Point\nOperations (FLOPS) and an 18% decrease in pre-training wall clock time.\nCrucially, this enhanced efficiency does not compromise the model's performance\nin downstream audio-classification tasks when compared to MAViL's performance.\n","authors":["Elvis Nunez","Yanzi Jin","Mohammad Rastegari","Sachin Mehta","Maxwell Horton"],"pdf_url":"https://arxiv.org/pdf/2310.03937v2.pdf","comment":"Camera-ready version for the Machine Learning for Audio Workshop at\n  NeurIPS 2023"}]},"2024-01-05T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2401.02954v1","updated":"2024-01-05T18:59:13Z","published":"2024-01-05T18:59:13Z","title":"DeepSeek LLM: Scaling Open-Source Language Models with Longtermism","summary":"  The rapid development of open-source large language models (LLMs) has been\ntruly remarkable. However, the scaling law described in previous literature\npresents varying conclusions, which casts a dark cloud over scaling LLMs. We\ndelve into the study of scaling laws and present our distinctive findings that\nfacilitate scaling of large scale models in two commonly used open-source\nconfigurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek\nLLM, a project dedicated to advancing open-source language models with a\nlong-term perspective. To support the pre-training phase, we have developed a\ndataset that currently consists of 2 trillion tokens and is continuously\nexpanding. We further conduct supervised fine-tuning (SFT) and Direct\nPreference Optimization (DPO) on DeepSeek LLM Base models, resulting in the\ncreation of DeepSeek Chat models. Our evaluation results demonstrate that\nDeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in\nthe domains of code, mathematics, and reasoning. Furthermore, open-ended\nevaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance\ncompared to GPT-3.5.\n","authors":[" DeepSeek-AI"," :","Xiao Bi","Deli Chen","Guanting Chen","Shanhuang Chen","Damai Dai","Chengqi Deng","Honghui Ding","Kai Dong","Qiushi Du","Zhe Fu","Huazuo Gao","Kaige Gao","Wenjun Gao","Ruiqi Ge","Kang Guan","Daya Guo","Jianzhong Guo","Guangbo Hao","Zhewen Hao","Ying He","Wenjie Hu","Panpan Huang","Erhang Li","Guowei Li","Jiashi Li","Yao Li","Y. K. Li","Wenfeng Liang","Fangyun Lin","A. X. Liu","Bo Liu","Wen Liu","Xiaodong Liu","Xin Liu","Yiyuan Liu","Haoyu Lu","Shanghao Lu","Fuli Luo","Shirong Ma","Xiaotao Nie","Tian Pei","Yishi Piao","Junjie Qiu","Hui Qu","Tongzheng Ren","Zehui Ren","Chong Ruan","Zhangli Sha","Zhihong Shao","Junxiao Song","Xuecheng Su","Jingxiang Sun","Yaofeng Sun","Minghui Tang","Bingxuan Wang","Peiyi Wang","Shiyu Wang","Yaohui Wang","Yongji Wang","Tong Wu","Y. Wu","Xin Xie","Zhenda Xie","Ziwei Xie","Yiliang Xiong","Hanwei Xu","R. X. Xu","Yanhong Xu","Dejian Yang","Yuxiang You","Shuiping Yu","Xingkai Yu","B. Zhang","Haowei Zhang","Lecong Zhang","Liyue Zhang","Mingchuan Zhang","Minghua Zhang","Wentao Zhang","Yichao Zhang","Chenggang Zhao","Yao Zhao","Shangyan Zhou","Shunfeng Zhou","Qihao Zhu","Yuheng Zou"],"pdf_url":"https://arxiv.org/pdf/2401.02954v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02921v1","updated":"2024-01-05T17:58:10Z","published":"2024-01-05T17:58:10Z","title":"Towards ASR Robust Spoken Language Understanding Through In-Context\n  Learning With Word Confusion Networks","summary":"  In the realm of spoken language understanding (SLU), numerous natural\nlanguage understanding (NLU) methodologies have been adapted by supplying large\nlanguage models (LLMs) with transcribed speech instead of conventional written\ntext. In real-world scenarios, prior to input into an LLM, an automated speech\nrecognition (ASR) system generates an output transcript hypothesis, where\ninherent errors can degrade subsequent SLU tasks. Here we introduce a method\nthat utilizes the ASR system's lattice output instead of relying solely on the\ntop hypothesis, aiming to encapsulate speech ambiguities and enhance SLU\noutcomes. Our in-context learning experiments, covering spoken question\nanswering and intent classification, underline the LLM's resilience to noisy\nspeech transcripts with the help of word confusion networks from lattices,\nbridging the SLU performance gap between using the top ASR hypothesis and an\noracle upper bound. Additionally, we delve into the LLM's robustness to varying\nASR performance conditions and scrutinize the aspects of in-context learning\nwhich prove the most influential.\n","authors":["Kevin Everson","Yile Gu","Huck Yang","Prashanth Gurunath Shivakumar","Guan-Ting Lin","Jari Kolehmainen","Ivan Bulyko","Ankur Gandhe","Shalini Ghosh","Wael Hamza","Hung-yi Lee","Ariya Rastrow","Andreas Stolcke"],"pdf_url":"https://arxiv.org/pdf/2401.02921v1.pdf","comment":"Accepted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2401.02909v1","updated":"2024-01-05T17:15:01Z","published":"2024-01-05T17:15:01Z","title":"Introducing Bode: A Fine-Tuned Large Language Model for Portuguese\n  Prompt-Based Task","summary":"  Large Language Models (LLMs) are increasingly bringing advances to Natural\nLanguage Processing. However, low-resource languages, those lacking extensive\nprominence in datasets for various NLP tasks, or where existing datasets are\nnot as substantial, such as Portuguese, already obtain several benefits from\nLLMs, but not to the same extent. LLMs trained on multilingual datasets\nnormally struggle to respond to prompts in Portuguese satisfactorily,\npresenting, for example, code switching in their responses. This work proposes\na fine-tuned LLaMA 2-based model for Portuguese prompts named Bode in two\nversions: 7B and 13B. We evaluate the performance of this model in\nclassification tasks using the zero-shot approach with in-context learning, and\ncompare it with other LLMs. Our main contribution is to bring an LLM with\nsatisfactory results in the Portuguese language, as well as to provide a model\nthat is free for research or commercial purposes.\n","authors":["Gabriel Lino Garcia","Pedro Henrique Paiola","Luis Henrique Morelli","Giovani Candido","Arnaldo Cândido Júnior","Danilo Samuel Jodas","Luis C. S. Afonso","Ivan Rizzo Guilherme","Bruno Elias Penteado","João Paulo Papa"],"pdf_url":"https://arxiv.org/pdf/2401.02909v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2401.02906v1","updated":"2024-01-05T17:05:42Z","published":"2024-01-05T17:05:42Z","title":"MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance","summary":"  The deployment of multimodal large language models (MLLMs) has brought forth\na unique vulnerability: susceptibility to malicious attacks through visual\ninputs. We delve into the novel challenge of defending MLLMs against such\nattacks. We discovered that images act as a \"foreign language\" that is not\nconsidered during alignment, which can make MLLMs prone to producing harmful\nresponses. Unfortunately, unlike the discrete tokens considered in text-based\nLLMs, the continuous nature of image signals presents significant alignment\nchallenges, which poses difficulty to thoroughly cover the possible scenarios.\nThis vulnerability is exacerbated by the fact that open-source MLLMs are\npredominantly fine-tuned on limited image-text pairs that is much less than the\nextensive text-based pretraining corpus, which makes the MLLMs more prone to\ncatastrophic forgetting of their original abilities during explicit alignment\ntuning. To tackle these challenges, we introduce MLLM-Protector, a\nplug-and-play strategy combining a lightweight harm detector and a response\ndetoxifier. The harm detector's role is to identify potentially harmful outputs\nfrom the MLLM, while the detoxifier corrects these outputs to ensure the\nresponse stipulates to the safety standards. This approach effectively\nmitigates the risks posed by malicious visual inputs without compromising the\nmodel's overall performance. Our results demonstrate that MLLM-Protector offers\na robust solution to a previously unaddressed aspect of MLLM security.\n","authors":["Renjie Pi","Tianyang Han","Yueqi Xie","Rui Pan","Qing Lian","Hanze Dong","Jipeng Zhang","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.02906v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.17213v2","updated":"2024-01-05T16:10:15Z","published":"2022-03-31T17:41:03Z","title":"Analyzing Wrap-Up Effects through an Information-Theoretic Lens","summary":"  Numerous analyses of reading time (RT) data have been implemented -- all in\nan effort to better understand the cognitive processes driving reading\ncomprehension. However, data measured on words at the end of a sentence -- or\neven at the end of a clause -- is often omitted due to the confounding factors\nintroduced by so-called \"wrap-up effects,\" which manifests as a skewed\ndistribution of RTs for these words. Consequently, the understanding of the\ncognitive processes that might be involved in these wrap-up effects is limited.\nIn this work, we attempt to learn more about these processes by examining the\nrelationship between wrap-up effects and information-theoretic quantities, such\nas word and context surprisals. We find that the distribution of information in\nprior contexts is often predictive of sentence- and clause-final RTs (while not\nof sentence-medial RTs). This lends support to several prior hypotheses about\nthe processes involved in wrap-up effects.\n","authors":["Clara Meister","Tiago Pimentel","Thomas Hikaru Clark","Ryan Cotterell","Roger Levy"],"pdf_url":"https://arxiv.org/pdf/2203.17213v2.pdf","comment":"ACL 2022 (main conference)"},{"id":"http://arxiv.org/abs/2307.03749v2","updated":"2024-01-05T15:55:23Z","published":"2023-07-07T17:59:12Z","title":"On the Efficacy of Sampling Adapters","summary":"  Sampling is a common strategy for generating text from probabilistic models,\nyet standard ancestral sampling often results in text that is incoherent or\nungrammatical. To alleviate this issue, various modifications to a model's\nsampling distribution, such as nucleus or top-k sampling, have been introduced\nand are now ubiquitously used in language generation systems. We propose a\nunified framework for understanding these techniques, which we term sampling\nadapters. Sampling adapters often lead to qualitatively better text, which\nraises the question: From a formal perspective, how are they changing the\n(sub)word-level distributions of language generation models? And why do these\nlocal changes lead to higher-quality text? We argue that the shift they enforce\ncan be viewed as a trade-off between precision and recall: while the model\nloses its ability to produce certain strings, its precision rate on desirable\ntext increases. While this trade-off is not reflected in standard metrics of\ndistribution quality (such as perplexity), we find that several\nprecision-emphasizing measures indeed indicate that sampling adapters can lead\nto probability distributions more aligned with the true distribution. Further,\nthese measures correlate with higher sequence-level quality scores,\nspecifically, Mauve.\n","authors":["Clara Meister","Tiago Pimentel","Luca Malagutti","Ethan G. Wilcox","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2307.03749v2.pdf","comment":"ACL 2023 Main Conference Proceedings"},{"id":"http://arxiv.org/abs/2401.02870v1","updated":"2024-01-05T15:52:59Z","published":"2024-01-05T15:52:59Z","title":"AFSPP: Agent Framework for Shaping Preference and Personality with Large\n  Language Models","summary":"  The evolution of Large Language Models (LLMs) has introduced a new paradigm\nfor investigating human behavior emulation. Recent research has employed\nLLM-based Agents to create a sociological research environment, in which agents\nexhibit behavior based on the unfiltered characteristics of large language\nmodels. However, these studies overlook the iterative development within a\nhuman-like setting - Human preferences and personalities are complex, shaped by\nvarious factors and subject to ongoing change as a result of environmental and\nsubjective influences. In light of this observation, we propose Agent Framework\nfor Shaping Preference and Personality (AFSPP), exploring the multifaceted\nimpact of social networks and subjective consciousness on LLM-based Agents'\npreference and personality formation. With AFSPP, we have, for the first time,\nsuccessfully replicated several key findings from human personality\nexperiments. And other AFSPP-based experimental results indicate that plan\nmaking, sensory perceptions and social networking with subjective information,\nwield the most pronounced influence on preference shaping. AFSPP can\nsignificantly enhance the efficiency and scope of psychological experiments,\nwhile yielding valuable insights for Trustworthy Artificial Intelligence\nresearch for strategies to prevent undesirable preference and personality\ndevelopment.\n","authors":["Zihong He","Changwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.02870v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01698v2","updated":"2024-01-05T15:33:40Z","published":"2024-01-03T12:05:38Z","title":"Patterns of Persistence and Diffusibility across the World's Languages","summary":"  Language similarities can be caused by genetic relatedness, areal contact,\nuniversality, or chance. Colexification, i.e. a type of similarity where a\nsingle lexical form is used to convey multiple meanings, is underexplored. In\nour work, we shed light on the linguistic causes of cross-lingual similarity in\ncolexification and phonology, by exploring genealogical stability (persistence)\nand contact-induced change (diffusibility). We construct large-scale graphs\nincorporating semantic, genealogical, phonological and geographical data for\n1,966 languages. We then show the potential of this resource, by investigating\nseveral established hypotheses from previous work in linguistics, while\nproposing new ones. Our results strongly support a previously established\nhypothesis in the linguistic literature, while offering contradicting evidence\nto another. Our large scale resource opens for further research across\ndisciplines, e.g.~in multilingual NLP and comparative linguistics.\n","authors":["Yiyi Chen","Johannes Bjerva"],"pdf_url":"https://arxiv.org/pdf/2401.01698v2.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2401.02839v1","updated":"2024-01-05T14:47:20Z","published":"2024-01-05T14:47:20Z","title":"Pheme: Efficient and Conversational Speech Generation","summary":"  In recent years, speech generation has seen remarkable progress, now\nachieving one-shot generation capability that is often virtually\nindistinguishable from real human voice. Integrating such advancements in\nspeech generation with large language models might revolutionize a wide range\nof applications. However, certain applications, such as assistive\nconversational systems, require natural and conversational speech generation\ntools that also operate efficiently in real time. Current state-of-the-art\nmodels like VALL-E and SoundStorm, powered by hierarchical neural audio codecs,\nrequire large neural components and extensive training data to work well. In\ncontrast, MQTTS aims to build more compact conversational TTS models while\ncapitalizing on smaller-scale real-life conversational speech data. However,\nits autoregressive nature yields high inference latency and thus limits its\nreal-time usage. In order to mitigate the current limitations of the\nstate-of-the-art TTS models while capitalizing on their strengths, in this work\nwe introduce the Pheme model series that 1) offers compact yet high-performing\nmodels, 2) allows for parallel speech generation of 3) natural conversational\nspeech, and 4) it can be trained efficiently on smaller-scale conversational\ndata, cutting data demands by more than 10x but still matching the quality of\nthe autoregressive TTS models. We also show that through simple teacher-student\ndistillation we can meet significant improvements in voice quality for\nsingle-speaker setups on top of pretrained Pheme checkpoints, relying solely on\nsynthetic speech generated by much larger teacher models. Audio samples and\npretrained models are available online.\n","authors":["Paweł Budzianowski","Taras Sereda","Tomasz Cichy","Ivan Vulić"],"pdf_url":"https://arxiv.org/pdf/2401.02839v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07910v2","updated":"2024-01-05T14:45:00Z","published":"2023-12-13T05:58:34Z","title":"PromptBench: A Unified Library for Evaluation of Large Language Models","summary":"  The evaluation of large language models (LLMs) is crucial to assess their\nperformance and mitigate potential security risks. In this paper, we introduce\nPromptBench, a unified library to evaluate LLMs. It consists of several key\ncomponents that are easily used and extended by researchers: prompt\nconstruction, prompt engineering, dataset and model loading, adversarial prompt\nattack, dynamic evaluation protocols, and analysis tools. PromptBench is\ndesigned to be an open, general, and flexible codebase for research purposes\nthat can facilitate original study in creating new benchmarks, deploying\ndownstream applications, and designing new evaluation protocols. The code is\navailable at: https://github.com/microsoft/promptbench and will be continuously\nsupported.\n","authors":["Kaijie Zhu","Qinlin Zhao","Hao Chen","Jindong Wang","Xing Xie"],"pdf_url":"https://arxiv.org/pdf/2312.07910v2.pdf","comment":"An extension to PromptBench (arXiv:2306.04528) for unified evaluation\n  of LLMs using the same name; code: https://github.com/microsoft/promptbench"},{"id":"http://arxiv.org/abs/2311.14212v2","updated":"2024-01-05T14:18:35Z","published":"2023-11-23T21:54:22Z","title":"Annotation Sensitivity: Training Data Collection Methods Affect Model\n  Performance","summary":"  When training data are collected from human annotators, the design of the\nannotation instrument, the instructions given to annotators, the\ncharacteristics of the annotators, and their interactions can impact training\ndata. This study demonstrates that design choices made when creating an\nannotation instrument also impact the models trained on the resulting\nannotations. We introduce the term annotation sensitivity to refer to the\nimpact of annotation data collection methods on the annotations themselves and\non downstream model performance and predictions. We collect annotations of hate\nspeech and offensive language in five experimental conditions of an annotation\ninstrument, randomly assigning annotators to conditions. We then fine-tune BERT\nmodels on each of the five resulting datasets and evaluate model performance on\na holdout portion of each condition. We find considerable differences between\nthe conditions for 1) the share of hate speech/offensive language annotations,\n2) model performance, 3) model predictions, and 4) model learning curves. Our\nresults emphasize the crucial role played by the annotation instrument which\nhas received little attention in the machine learning literature. We call for\nadditional research into how and why the instrument impacts the annotations to\ninform the development of best practices in instrument design.\n","authors":["Christoph Kern","Stephanie Eckman","Jacob Beck","Rob Chew","Bolei Ma","Frauke Kreuter"],"pdf_url":"https://arxiv.org/pdf/2311.14212v2.pdf","comment":"EMNLP 2023 Findings:\n  https://aclanthology.org/2023.findings-emnlp.992/"},{"id":"http://arxiv.org/abs/2401.02823v1","updated":"2024-01-05T14:15:36Z","published":"2024-01-05T14:15:36Z","title":"DocGraphLM: Documental Graph Language Model for Information Extraction","summary":"  Advances in Visually Rich Document Understanding (VrDU) have enabled\ninformation extraction and question answering over documents with complex\nlayouts. Two tropes of architectures have emerged -- transformer-based models\ninspired by LLMs, and Graph Neural Networks. In this paper, we introduce\nDocGraphLM, a novel framework that combines pre-trained language models with\ngraph semantics. To achieve this, we propose 1) a joint encoder architecture to\nrepresent documents, and 2) a novel link prediction approach to reconstruct\ndocument graphs. DocGraphLM predicts both directions and distances between\nnodes using a convergent joint loss function that prioritizes neighborhood\nrestoration and downweighs distant node detection. Our experiments on three\nSotA datasets show consistent improvement on IE and QA tasks with the adoption\nof graph features. Moreover, we report that adopting the graph features\naccelerates convergence in the learning process during training, despite being\nsolely constructed through link prediction.\n","authors":["Dongsheng Wang","Zhiqiang Ma","Armineh Nourbakhsh","Kang Gu","Sameena Shah"],"pdf_url":"https://arxiv.org/pdf/2401.02823v1.pdf","comment":"Published at SIGIR'23 (repost for easier access)"},{"id":"http://arxiv.org/abs/2401.02797v1","updated":"2024-01-05T13:22:12Z","published":"2024-01-05T13:22:12Z","title":"PeFoMed: Parameter Efficient Fine-tuning on Multimodal Large Language\n  Models for Medical Visual Question Answering","summary":"  Multimodal large language models (MLLMs) represent an evolutionary expansion\nin the capabilities of traditional large language models, enabling them to\ntackle challenges that surpass the scope of purely text-based applications. It\nleverages the knowledge previously encoded within these language models,\nthereby enhancing their applicability and functionality in the reign of\nmultimodal contexts. Recent works investigate the adaptation of MLLMs to\npredict free-form answers as a generative task to solve medical visual question\nanswering (Med-VQA) tasks. In this paper, we propose a parameter efficient\nframework for fine-tuning MLLM specifically tailored to Med-VQA applications,\nand empirically validate it on a public benchmark dataset. To accurately\nmeasure the performance, we employ human evaluation and the results reveal that\nour model achieves an overall accuracy of 81.9%, and outperforms the GPT-4v\nmodel by a significant margin of 26% absolute accuracy on closed-ended\nquestions. The code will be available here: https://github.com/jinlHe/PeFoMed.\n","authors":["Jinlong He","Pengfei Li","Gang Liu","Zixu Zhao","Shenjun Zhong"],"pdf_url":"https://arxiv.org/pdf/2401.02797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02789v1","updated":"2024-01-05T12:59:20Z","published":"2024-01-05T12:59:20Z","title":"Large Language Models in Plant Biology","summary":"  Large Language Models (LLMs), such as ChatGPT, have taken the world by storm\nand have passed certain forms of the Turing test. However, LLMs are not limited\nto human language and analyze sequential data, such as DNA, protein, and gene\nexpression. The resulting foundation models can be repurposed to identify the\ncomplex patterns within the data, resulting in powerful, multi-purpose\nprediction tools able to explain cellular systems. This review outlines the\ndifferent types of LLMs and showcases their recent uses in biology. Since LLMs\nhave not yet been embraced by the plant community, we also cover how these\nmodels can be deployed for the plant kingdom.\n","authors":["Hilbert Yuen In Lam","Xing Er Ong","Marek Mutwil"],"pdf_url":"https://arxiv.org/pdf/2401.02789v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.01282v4","updated":"2024-01-05T12:41:13Z","published":"2023-11-02T14:57:03Z","title":"FlashDecoding++: Faster Large Language Model Inference on GPUs","summary":"  As the Large Language Model (LLM) becomes increasingly important in various\ndomains. However, the following challenges still remain unsolved in\naccelerating LLM inference: (1) Synchronized partial softmax update. The\nsoftmax operation requires a synchronized update operation among each partial\nsoftmax result, leading to ~20% overheads for the attention computation in\nLLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices\nperforming GEMM in LLM inference is flat, leading to under-utilized computation\nand >50% performance loss after padding zeros in previous designs. (3)\nPerformance loss due to static dataflow. Kernel performance in LLM depends on\nvaried input data features, hardware configurations, etc. A single and static\ndataflow may lead to a 50.25% performance loss for GEMMs of different shapes in\nLLM inference.\n  We present FlashDecoding++, a fast LLM inference engine supporting mainstream\nLLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++\ncreatively proposes: (1) Asynchronized softmax with unified max value.\nFlashDecoding++ introduces a unified max value technique for different partial\nsoftmax computations to avoid synchronization. (2) Flat GEMM optimization with\ndouble buffering. FlashDecoding++ points out that flat GEMMs with different\nshapes face varied bottlenecks. Then, techniques like double buffering are\nintroduced. (3) Heuristic dataflow with hardware resource adaptation.\nFlashDecoding++ heuristically optimizes dataflow using different hardware\nresource considering input dynamics. Due to the versatility of optimizations in\nFlashDecoding++, FlashDecoding++ can achieve up to 4.86x and 2.18x speedup on\nboth NVIDIA and AMD GPUs compared to Hugging Face implementations.\nFlashDecoding++ also achieves an average speedup of 1.37x compared to\nstate-of-the-art LLM inference engines on mainstream LLMs.\n","authors":["Ke Hong","Guohao Dai","Jiaming Xu","Qiuli Mao","Xiuhong Li","Jun Liu","Kangdi Chen","Yuhan Dong","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2311.01282v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02777v1","updated":"2024-01-05T12:26:46Z","published":"2024-01-05T12:26:46Z","title":"From LLM to Conversational Agent: A Memory Enhanced Architecture with\n  Fine-Tuning of Large Language Models","summary":"  This paper introduces RAISE (Reasoning and Acting through Scratchpad and\nExamples), an advanced architecture enhancing the integration of Large Language\nModels (LLMs) like GPT-4 into conversational agents. RAISE, an enhancement of\nthe ReAct framework, incorporates a dual-component memory system, mirroring\nhuman short-term and long-term memory, to maintain context and continuity in\nconversations. It entails a comprehensive agent construction scenario,\nincluding phases like Conversation Selection, Scene Extraction, CoT Completion,\nand Scene Augmentation, leading to the LLMs Training phase. This approach\nappears to enhance agent controllability and adaptability in complex,\nmulti-turn dialogues. Our preliminary evaluations in a real estate sales\ncontext suggest that RAISE has some advantages over traditional agents,\nindicating its potential for broader applications. This work contributes to the\nAI field by providing a robust framework for developing more context-aware and\nversatile conversational agents.\n","authors":["Na Liu","Liangyu Chen","Xiaoyu Tian","Wei Zou","Kaijiang Chen","Ming Cui"],"pdf_url":"https://arxiv.org/pdf/2401.02777v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.10622v2","updated":"2024-01-05T12:13:55Z","published":"2022-12-20T19:52:41Z","title":"mFACE: Multilingual Summarization with Factual Consistency Evaluation","summary":"  Abstractive summarization has enjoyed renewed interest in recent years,\nthanks to pre-trained language models and the availability of large-scale\ndatasets. Despite promising results, current models still suffer from\ngenerating factually inconsistent summaries, reducing their utility for\nreal-world application. Several recent efforts attempt to address this by\ndevising models that automatically detect factual inconsistencies in machine\ngenerated summaries. However, they focus exclusively on English, a language\nwith abundant resources. In this work, we leverage factual consistency\nevaluation models to improve multilingual summarization. We explore two\nintuitive approaches to mitigate hallucinations based on the signal provided by\na multilingual NLI model, namely data filtering and controlled generation.\nExperimental results in the 45 languages from the XLSum dataset show gains over\nstrong baselines in both automatic and human evaluation.\n","authors":["Roee Aharoni","Shashi Narayan","Joshua Maynez","Jonathan Herzig","Elizabeth Clark","Mirella Lapata"],"pdf_url":"https://arxiv.org/pdf/2212.10622v2.pdf","comment":"28 pages with links to released data"},{"id":"http://arxiv.org/abs/2401.02772v1","updated":"2024-01-05T12:01:26Z","published":"2024-01-05T12:01:26Z","title":"Complex systems approach to natural language","summary":"  The review summarizes the main methodological concepts used in studying\nnatural language from the perspective of complexity science and documents their\napplicability in identifying both universal and system-specific features of\nlanguage in its written representation. Three main complexity-related research\ntrends in quantitative linguistics are covered. The first part addresses the\nissue of word frequencies in texts and demonstrates that taking punctuation\ninto consideration restores scaling whose violation in the Zipf's law is often\nobserved for the most frequent words. The second part introduces methods\ninspired by time series analysis, used in studying various kinds of\ncorrelations in written texts. The related time series are generated on the\nbasis of text partition into sentences or into phrases between consecutive\npunctuation marks. It turns out that these series develop features often found\nin signals generated by complex systems, like long-range correlations or\n(multi)fractal structures. Moreover, it appears that the distances between\npunctuation marks comply with the discrete variant of the Weibull distribution.\nIn the third part, the application of the network formalism to natural language\nis reviewed, particularly in the context of the so-called word-adjacency\nnetworks. Parameters characterizing topology of such networks can be used for\nclassification of texts, for example, from a stylometric perspective. Network\napproach can also be applied to represent the organization of word\nassociations. Structure of word-association networks turns out to be\nsignificantly different from that observed in random networks, revealing\ngenuine properties of language. Finally, punctuation seems to have a\nsignificant impact not only on the language's information-carrying ability but\nalso on its key statistical properties, hence it is recommended to consider\npunctuation marks on a par with words.\n","authors":["Tomasz Stanisz","Stanisław Drożdż","Jarosław Kwapień"],"pdf_url":"https://arxiv.org/pdf/2401.02772v1.pdf","comment":"113 pages, 49 figures"},{"id":"http://arxiv.org/abs/2401.02749v1","updated":"2024-01-05T11:02:08Z","published":"2024-01-05T11:02:08Z","title":"Hyperparameter-Free Approach for Faster Minimum Bayes Risk Decoding","summary":"  Minimum Bayes-Risk (MBR) decoding is shown to be a powerful alternative to\nbeam search decoding for a wide range of text generation tasks. However, MBR\nrequires a huge amount of time for inference to compute the MBR objective,\nwhich makes the method infeasible in many situations where response time is\ncritical. Confidence-based pruning (CBP) (Cheng and Vlachos, 2023) has recently\nbeen proposed to reduce the inference time in machine translation tasks.\nAlthough it is shown to significantly reduce the amount of computation, it\nrequires hyperparameter tuning using a development set to be effective. To this\nend, we propose Approximate Minimum Bayes-Risk (AMBR) decoding, a\nhyperparameter-free method to run MBR decoding approximately. AMBR is derived\nfrom the observation that the problem of computing the sample-based MBR\nobjective is the medoid identification problem. AMBR uses the Correlated\nSequential Halving (CSH) algorithm (Baharav and Tse, 2019), the best\napproximation algorithm to date for the medoid identification problem, to\ncompute the sample-based MBR objective. We evaluate AMBR on machine\ntranslation, text summarization, and image captioning tasks. The results show\nthat AMBR achieves on par with CBP, with CBP selecting hyperparameters through\nan Oracle for each given computation budget.\n","authors":["Yuu Jinnai","Kaito Ariu"],"pdf_url":"https://arxiv.org/pdf/2401.02749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02744v1","updated":"2024-01-05T10:41:55Z","published":"2024-01-05T10:41:55Z","title":"MAMI: Multi-Attentional Mutual-Information for Long Sequence Neuron\n  Captioning","summary":"  Neuron labeling is an approach to visualize the behaviour and respond of a\ncertain neuron to a certain pattern that activates the neuron. Neuron labeling\nextract information about the features captured by certain neurons in a deep\nneural network, one of which uses the encoder-decoder image captioning\napproach. The encoder used can be a pretrained CNN-based model and the decoder\nis an RNN-based model for text generation. Previous work, namely MILAN (Mutual\nInformation-guided Linguistic Annotation of Neuron), has tried to visualize the\nneuron behaviour using modified Show, Attend, and Tell (SAT) model in the\nencoder, and LSTM added with Bahdanau attention in the decoder. MILAN can show\ngreat result on short sequence neuron captioning, but it does not show great\nresult on long sequence neuron captioning, so in this work, we would like to\nimprove the performance of MILAN even more by utilizing different kind of\nattention mechanism and additionally adding several attention result into one,\nin order to combine all the advantages from several attention mechanism. Using\nour compound dataset, we obtained higher BLEU and F1-Score on our proposed\nmodel, achieving 17.742 and 0.4811 respectively. At some point where the model\nconverges at the peak, our model obtained BLEU of 21.2262 and BERTScore\nF1-Score of 0.4870.\n","authors":["Alfirsa Damasyifa Fauzulhaq","Wahyu Parwitayasa","Joseph Ananda Sugihdharma","M. Fadli Ridhani","Novanto Yudistira"],"pdf_url":"https://arxiv.org/pdf/2401.02744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.04695v2","updated":"2024-01-05T09:54:03Z","published":"2023-09-09T06:27:00Z","title":"Code-Style In-Context Learning for Knowledge-Based Question Answering","summary":"  Current methods for Knowledge-Based Question Answering (KBQA) usually rely on\ncomplex training techniques and model frameworks, leading to many limitations\nin practical applications. Recently, the emergence of In-Context Learning (ICL)\ncapabilities in Large Language Models (LLMs) provides a simple and\ntraining-free semantic parsing paradigm for KBQA: Given a small number of\nquestions and their labeled logical forms as demo examples, LLMs can understand\nthe task intent and generate the logic form for a new question. However,\ncurrent powerful LLMs have little exposure to logic forms during pre-training,\nresulting in a high format error rate. To solve this problem, we propose a\ncode-style in-context learning method for KBQA, which converts the generation\nprocess of unfamiliar logical form into the more familiar code generation\nprocess for LLMs. Experimental results on three mainstream datasets show that\nour method dramatically mitigated the formatting error problem in generating\nlogic forms while realizing a new SOTA on WebQSP, GrailQA, and GraphQ under the\nfew-shot setting. The code and supplementary files are released at\nhttps://github.com/Arthurizijar/KB-Coder .\n","authors":["Zhijie Nie","Richong Zhang","Zhongyuan Wang","Xudong Liu"],"pdf_url":"https://arxiv.org/pdf/2309.04695v2.pdf","comment":"AAAI2024 Camera Ready"},{"id":"http://arxiv.org/abs/2401.02709v1","updated":"2024-01-05T08:42:45Z","published":"2024-01-05T08:42:45Z","title":"German Text Embedding Clustering Benchmark","summary":"  This work introduces a benchmark assessing the performance of clustering\nGerman text embeddings in different domains. This benchmark is driven by the\nincreasing use of clustering neural text embeddings in tasks that require the\ngrouping of texts (such as topic modeling) and the need for German resources in\nexisting benchmarks. We provide an initial analysis for a range of pre-trained\nmono- and multilingual models evaluated on the outcome of different clustering\nalgorithms. Results include strong performing mono- and multilingual models.\nReducing the dimensions of embeddings can further improve clustering.\nAdditionally, we conduct experiments with continued pre-training for German\nBERT models to estimate the benefits of this additional training. Our\nexperiments suggest that significant performance improvements are possible for\nshort text. All code and datasets are publicly available.\n","authors":["Silvan Wehrli","Bert Arnrich","Christopher Irrgang"],"pdf_url":"https://arxiv.org/pdf/2401.02709v1.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2401.01623v2","updated":"2024-01-05T08:21:36Z","published":"2024-01-03T08:49:12Z","title":"Can AI Be as Creative as Humans?","summary":"  Creativity serves as a cornerstone for societal progress and innovation, but\nits assessment remains a complex and often subjective endeavor. With the rise\nof advanced generative AI models capable of tasks once reserved for human\ncreativity, the study of AI's creative potential becomes imperative for its\nresponsible development and application. This paper addresses the complexities\nin defining and evaluating creativity by introducing a new concept called\nRelative Creativity. Instead of trying to define creativity universally, we\nshift the focus to whether AI can match the creative abilities of a\nhypothetical human. This perspective draws inspiration from the Turing Test,\nexpanding upon it to address the challenges and subjectivities inherent in\nevaluating creativity. This methodological shift facilitates a statistically\nquantifiable evaluation of AI's creativity, which we term Statistical\nCreativity. This approach allows for direct comparisons of AI's creative\nabilities with those of specific human groups. Building on this foundation, we\ndiscuss the application of statistical creativity in contemporary\nprompt-conditioned autoregressive models. In addition to defining and analyzing\na measure of creativity, we introduce an actionable training guideline,\neffectively bridging the gap between theoretical quantification of creativity\nand practical model training. Through these multifaceted contributions, the\npaper establishes a cohesive, continuously evolving, and transformative\nframework for assessing and fostering statistical creativity in AI models.\n","authors":["Haonan Wang","James Zou","Michael Mozer","Anirudh Goyal","Alex Lamb","Linjun Zhang","Weijie J Su","Zhun Deng","Michael Qizhe Xie","Hannah Brown","Kenji Kawaguchi"],"pdf_url":"https://arxiv.org/pdf/2401.01623v2.pdf","comment":"The paper introduce the notion of \"Relative Creativity\", presents\n  measurable assessment, and provides AI training guidelines to foster AI's\n  creative capabilities Project Page: https://ai-relative-creativity.github.io/"},{"id":"http://arxiv.org/abs/2401.01916v2","updated":"2024-01-05T07:46:32Z","published":"2024-01-03T04:47:02Z","title":"AstroLLaMA-Chat: Scaling AstroLLaMA with Conversational and Diverse\n  Datasets","summary":"  We explore the potential of enhancing LLM performance in astronomy-focused\nquestion-answering through targeted, continual pre-training. By employing a\ncompact 7B-parameter LLaMA-2 model and focusing exclusively on a curated set of\nastronomy corpora -- comprising abstracts, introductions, and conclusions -- we\nachieve notable improvements in specialized topic comprehension. While general\nLLMs like GPT-4 excel in broader question-answering scenarios due to superior\nreasoning capabilities, our findings suggest that continual pre-training with\nlimited resources can still enhance model performance on specialized topics.\nAdditionally, we present an extension of AstroLLaMA: the fine-tuning of the 7B\nLLaMA model on a domain-specific conversational dataset, culminating in the\nrelease of the chat-enabled AstroLLaMA for community use. Comprehensive\nquantitative benchmarking is currently in progress and will be detailed in an\nupcoming full paper. The model, AstroLLaMA-Chat, is now available at\nhttps://huggingface.co/universeTBD, providing the first open-source\nconversational AI tool tailored for the astronomy community.\n","authors":["Ernest Perkowski","Rui Pan","Tuan Dung Nguyen","Yuan-Sen Ting","Sandor Kruk","Tong Zhang","Charlie O'Neill","Maja Jablonska","Zechang Sun","Michael J. Smith","Huiling Liu","Kevin Schawinski","Kartheik Iyer","Ioana Ciucă for UniverseTBD"],"pdf_url":"https://arxiv.org/pdf/2401.01916v2.pdf","comment":"4 pages, 1 figure, model is available at\n  https://huggingface.co/universeTBD, published in RNAAS"},{"id":"http://arxiv.org/abs/2306.11698v4","updated":"2024-01-05T07:01:05Z","published":"2023-06-20T17:24:23Z","title":"DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT\n  Models","summary":"  Generative Pre-trained Transformer (GPT) models have exhibited exciting\nprogress in their capabilities, capturing the interest of practitioners and the\npublic alike. Yet, while the literature on the trustworthiness of GPT models\nremains limited, practitioners have proposed employing capable GPT models for\nsensitive applications such as healthcare and finance -- where mistakes can be\ncostly. To this end, this work proposes a comprehensive trustworthiness\nevaluation for large language models with a focus on GPT-4 and GPT-3.5,\nconsidering diverse perspectives -- including toxicity, stereotype bias,\nadversarial robustness, out-of-distribution robustness, robustness on\nadversarial demonstrations, privacy, machine ethics, and fairness. Based on our\nevaluations, we discover previously unpublished vulnerabilities to\ntrustworthiness threats. For instance, we find that GPT models can be easily\nmisled to generate toxic and biased outputs and leak private information in\nboth training data and conversation history. We also find that although GPT-4\nis usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is more\nvulnerable given jailbreaking system or user prompts, potentially because GPT-4\nfollows (misleading) instructions more precisely. Our work illustrates a\ncomprehensive trustworthiness evaluation of GPT models and sheds light on the\ntrustworthiness gaps. Our benchmark is publicly available at\nhttps://decodingtrust.github.io/; our dataset can be previewed at\nhttps://huggingface.co/datasets/AI-Secure/DecodingTrust; a concise version of\nthis work is at https://openreview.net/pdf?id=kaHpo8OZw2.\n","authors":["Boxin Wang","Weixin Chen","Hengzhi Pei","Chulin Xie","Mintong Kang","Chenhui Zhang","Chejian Xu","Zidi Xiong","Ritik Dutta","Rylan Schaeffer","Sang T. Truong","Simran Arora","Mantas Mazeika","Dan Hendrycks","Zinan Lin","Yu Cheng","Sanmi Koyejo","Dawn Song","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2306.11698v4.pdf","comment":"NeurIPS 2023 Outstanding Paper (Datasets and Benchmarks Track)"},{"id":"http://arxiv.org/abs/2312.04889v2","updated":"2024-01-05T02:07:48Z","published":"2023-12-08T08:11:11Z","title":"KwaiAgents: Generalized Information-seeking Agent System with Large\n  Language Models","summary":"  Driven by curiosity, humans have continually sought to explore and understand\nthe world around them, leading to the invention of various tools to satiate\nthis inquisitiveness. Despite not having the capacity to process and memorize\nvast amounts of information in their brains, humans excel in critical thinking,\nplanning, reflection, and harnessing available tools to interact with and\ninterpret the world, enabling them to find answers efficiently. The recent\nadvancements in large language models (LLMs) suggest that machines might also\npossess the aforementioned human-like capabilities, allowing them to exhibit\npowerful abilities even with a constrained parameter count. In this paper, we\nintroduce KwaiAgents, a generalized information-seeking agent system based on\nLLMs. Within KwaiAgents, we propose an agent system that employs LLMs as its\ncognitive core, which is capable of understanding a user's query, behavior\nguidelines, and referencing external documents. The agent can also update and\nretrieve information from its internal memory, plan and execute actions using a\ntime-aware search-browse toolkit, and ultimately provide a comprehensive\nresponse. We further investigate the system's performance when powered by LLMs\nless advanced than GPT-4, and introduce the Meta-Agent Tuning (MAT) framework,\ndesigned to ensure even an open-sourced 7B or 13B model performs well among\nmany agent systems. We exploit both benchmark and human evaluations to\nsystematically validate these capabilities. Extensive experiments show the\nsuperiority of our agent system compared to other autonomous agents and\nhighlight the enhanced generalized agent-abilities of our fine-tuned LLMs.\n","authors":["Haojie Pan","Zepeng Zhai","Hao Yuan","Yaojia Lv","Ruiji Fu","Ming Liu","Zhongyuan Wang","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2312.04889v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02594v1","updated":"2024-01-05T01:31:14Z","published":"2024-01-05T01:31:14Z","title":"Unsupervised hard Negative Augmentation for contrastive learning","summary":"  We present Unsupervised hard Negative Augmentation (UNA), a method that\ngenerates synthetic negative instances based on the term frequency-inverse\ndocument frequency (TF-IDF) retrieval model. UNA uses TF-IDF scores to\nascertain the perceived importance of terms in a sentence and then produces\nnegative samples by replacing terms with respect to that. Our experiments\ndemonstrate that models trained with UNA improve the overall performance in\nsemantic textual similarity tasks. Additional performance gains are obtained\nwhen combining UNA with the paraphrasing augmentation. Further results show\nthat our method is compatible with different backbone models. Ablation studies\nalso support the choice of having a TF-IDF-driven control on negative\naugmentation.\n","authors":["Yuxuan Shu","Vasileios Lampos"],"pdf_url":"https://arxiv.org/pdf/2401.02594v1.pdf","comment":"The code and pre-trained models are available at\n  https://github.com/ClaudiaShu/UNA"},{"id":"http://arxiv.org/abs/2312.10997v4","updated":"2024-01-05T01:18:27Z","published":"2023-12-18T07:47:33Z","title":"Retrieval-Augmented Generation for Large Language Models: A Survey","summary":"  Large Language Models (LLMs) demonstrate significant capabilities but face\nchallenges such as hallucination, outdated knowledge, and non-transparent,\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\nemerged as a promising solution by incorporating knowledge from external\ndatabases. This enhances the accuracy and credibility of the models,\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\nupdates and integration of domain-specific information. RAG synergistically\nmerges LLMs' intrinsic knowledge with the vast, dynamic repositories of\nexternal databases. This comprehensive review paper offers a detailed\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\ntripartite foundation of RAG frameworks, which includes the retrieval , the\ngeneration and the augmentation techniques. The paper highlights the\nstate-of-the-art technologies embedded in each of these critical components,\nproviding a profound understanding of the advancements in RAG systems.\nFurthermore, this paper introduces the metrics and benchmarks for assessing RAG\nmodels, along with the most up-to-date evaluation framework. In conclusion, the\npaper delineates prospective avenues for research, including the identification\nof challenges, the expansion of multi-modalities, and the progression of the\nRAG infrastructure and its ecosystem.\n","authors":["Yunfan Gao","Yun Xiong","Xinyu Gao","Kangxiang Jia","Jinliu Pan","Yuxi Bi","Yi Dai","Jiawei Sun","Qianyu Guo","Meng Wang","Haofen Wang"],"pdf_url":"https://arxiv.org/pdf/2312.10997v4.pdf","comment":"Ongoing Work"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2401.02957v1","updated":"2024-01-05T18:59:52Z","published":"2024-01-05T18:59:52Z","title":"Denoising Vision Transformers","summary":"  We delve into a nuanced but significant challenge inherent to Vision\nTransformers (ViTs): feature maps of these models exhibit grid-like artifacts,\nwhich detrimentally hurt the performance of ViTs in downstream tasks. Our\ninvestigations trace this fundamental issue down to the positional embeddings\nat the input stage. To address this, we propose a novel noise model, which is\nuniversally applicable to all ViTs. Specifically, the noise model dissects ViT\noutputs into three components: a semantics term free from noise artifacts and\ntwo artifact-related terms that are conditioned on pixel locations. Such a\ndecomposition is achieved by enforcing cross-view feature consistency with\nneural fields in a per-image basis. This per-image optimization process\nextracts artifact-free features from raw ViT outputs, providing clean features\nfor offline applications. Expanding the scope of our solution to support online\nfunctionality, we introduce a learnable denoiser to predict artifact-free\nfeatures directly from unprocessed ViT outputs, which shows remarkable\ngeneralization capabilities to novel data without the need for per-image\noptimization. Our two-stage approach, termed Denoising Vision Transformers\n(DVT), does not require re-training existing pre-trained ViTs and is\nimmediately applicable to any Transformer-based architecture. We evaluate our\nmethod on a variety of representative ViTs (DINO, MAE, DeiT-III, EVA02, CLIP,\nDINOv2, DINOv2-reg). Extensive evaluations demonstrate that our DVT\nconsistently and significantly improves existing state-of-the-art\ngeneral-purpose models in semantic and geometric tasks across multiple datasets\n(e.g., +3.84 mIoU). We hope our study will encourage a re-evaluation of ViT\ndesign, especially regarding the naive use of positional embeddings.\n","authors":["Jiawei Yang","Katie Z Luo","Jiefeng Li","Kilian Q Weinberger","Yonglong Tian","Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2401.02957v1.pdf","comment":"Project website: https://jiawei-yang.github.io/DenoisingViT/"},{"id":"http://arxiv.org/abs/2304.05292v4","updated":"2024-01-05T18:59:41Z","published":"2023-04-11T15:42:20Z","title":"MC-ViViT: Multi-branch Classifier-ViViT to detect Mild Cognitive\n  Impairment in older adults using facial videos","summary":"  Deep machine learning models including Convolutional Neural Networks (CNN)\nhave been successful in the detection of Mild Cognitive Impairment (MCI) using\nmedical images, questionnaires, and videos. This paper proposes a novel\nMulti-branch Classifier-Video Vision Transformer (MC-ViViT) model to\ndistinguish MCI from those with normal cognition by analyzing facial features.\nThe data comes from the I-CONECT, a behavioral intervention trial aimed at\nimproving cognitive function by providing frequent video chats. MC-ViViT\nextracts spatiotemporal features of videos in one branch and augments\nrepresentations by the MC module. The I-CONECT dataset is challenging as the\ndataset is imbalanced containing Hard-Easy and Positive-Negative samples, which\nimpedes the performance of MC-ViViT. We propose a loss function for Hard-Easy\nand Positive-Negative Samples (HP Loss) by combining Focal loss and AD-CORRE\nloss to address the imbalanced problem. Our experimental results on the\nI-CONECT dataset show the great potential of MC-ViViT in predicting MCI with a\nhigh accuracy of 90.63% accuracy on some of the interview videos.\n","authors":["Jian Sun","Hiroko H. Dodge","Mohammad H. Mahoor"],"pdf_url":"https://arxiv.org/pdf/2304.05292v4.pdf","comment":"13 pages, 7 tables, 7 figures, 9 equations"},{"id":"http://arxiv.org/abs/2401.02955v1","updated":"2024-01-05T18:59:22Z","published":"2024-01-05T18:59:22Z","title":"Open-Vocabulary SAM: Segment and Recognize Twenty-thousand Classes\n  Interactively","summary":"  The CLIP and Segment Anything Model (SAM) are remarkable vision foundation\nmodels (VFMs). SAM excels in segmentation tasks across diverse domains, while\nCLIP is renowned for its zero-shot recognition capabilities. This paper\npresents an in-depth exploration of integrating these two models into a unified\nframework. Specifically, we introduce the Open-Vocabulary SAM, a SAM-inspired\nmodel designed for simultaneous interactive segmentation and recognition,\nleveraging two unique knowledge transfer modules: SAM2CLIP and CLIP2SAM. The\nformer adapts SAM's knowledge into the CLIP via distillation and learnable\ntransformer adapters, while the latter transfers CLIP knowledge into SAM,\nenhancing its recognition capabilities. Extensive experiments on various\ndatasets and detectors show the effectiveness of Open-Vocabulary SAM in both\nsegmentation and recognition tasks, significantly outperforming the naive\nbaselines of simply combining SAM and CLIP. Furthermore, aided with image\nclassification data training, our method can segment and recognize\napproximately 22,000 classes.\n","authors":["Haobo Yuan","Xiangtai Li","Chong Zhou","Yining Li","Kai Chen","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2401.02955v1.pdf","comment":"Project page: https://www.mmlab-ntu.com/project/ovsam"},{"id":"http://arxiv.org/abs/2309.08471v2","updated":"2024-01-05T18:57:34Z","published":"2023-09-15T15:20:16Z","title":"TreeLearn: A Comprehensive Deep Learning Method for Segmenting\n  Individual Trees from Ground-Based LiDAR Forest Point Clouds","summary":"  Laser-scanned point clouds of forests make it possible to extract valuable\ninformation for forest management. To consider single trees, a forest point\ncloud needs to be segmented into individual tree point clouds. Existing\nsegmentation methods are usually based on hand-crafted algorithms, such as\nidentifying trunks and growing trees from them, and face difficulties in dense\nforests with overlapping tree crowns. In this study, we propose TreeLearn, a\ndeep learning-based approach for tree instance segmentation of forest point\nclouds. Unlike previous methods, TreeLearn is trained on already segmented\npoint clouds in a data-driven manner, making it less reliant on predefined\nfeatures and algorithms. Furthermore, TreeLearn is implemented as a fully\nautomatic pipeline and does not rely on extensive hyperparameter tuning, which\nmakes it easy to use. Additionally, we introduce a new manually segmented\nbenchmark forest dataset containing 156 full trees, and 79 partial trees, that\nhave been cleanly segmented by hand. The data is generated by mobile laser\nscanning and contributes to create a larger and more diverse data basis for\nmodel development and fine-grained instance segmentation evaluation. We trained\nTreeLearn on forest point clouds of 6665 trees, labeled using the Lidar360\nsoftware. An evaluation on the benchmark dataset shows that TreeLearn performs\nequally well or better than the algorithm used to generate its training data.\nFurthermore, the method's performance can be vastly improved by fine-tuning on\nthe cleanly labeled benchmark dataset. The TreeLearn code is available from\nhttps://github.com/ecker-lab/TreeLearn. The data as well as trained models can\nbe found at https://doi.org/10.25625/VPMPID.\n","authors":["Jonathan Henrich","Jan van Delden","Dominik Seidel","Thomas Kneib","Alexander Ecker"],"pdf_url":"https://arxiv.org/pdf/2309.08471v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02937v1","updated":"2024-01-05T18:28:51Z","published":"2024-01-05T18:28:51Z","title":"Locally Adaptive Neural 3D Morphable Models","summary":"  We present the Locally Adaptive Morphable Model (LAMM), a highly flexible\nAuto-Encoder (AE) framework for learning to generate and manipulate 3D meshes.\nWe train our architecture following a simple self-supervised training scheme in\nwhich input displacements over a set of sparse control vertices are used to\noverwrite the encoded geometry in order to transform one training sample into\nanother. During inference, our model produces a dense output that adheres\nlocally to the specified sparse geometry while maintaining the overall\nappearance of the encoded object. This approach results in state-of-the-art\nperformance in both disentangling manipulated geometry and 3D mesh\nreconstruction. To the best of our knowledge LAMM is the first end-to-end\nframework that enables direct local control of 3D vertex geometry in a single\nforward pass. A very efficient computational graph allows our network to train\nwith only a fraction of the memory required by previous methods and run faster\nduring inference, generating 12k vertex meshes at $>$60fps on a single CPU\nthread. We further leverage local geometry control as a primitive for higher\nlevel editing operations and present a set of derivative capabilities such as\nswapping and sampling object parts. Code and pretrained models can be found at\nhttps://github.com/michaeltrs/LAMM.\n","authors":["Michail Tarasiou","Rolandos Alexandros Potamias","Eimear O'Sullivan","Stylianos Ploumpis","Stefanos Zafeiriou"],"pdf_url":"https://arxiv.org/pdf/2401.02937v1.pdf","comment":"10 pages, 9 figures, 2 tables"},{"id":"http://arxiv.org/abs/2308.07688v4","updated":"2024-01-05T18:25:42Z","published":"2023-08-15T10:37:13Z","title":"Enhancing Network Initialization for Medical AI Models Using\n  Large-Scale, Unlabeled Natural Images","summary":"  Pre-training datasets, like ImageNet, have become the gold standard in\nmedical image analysis. However, the emergence of self-supervised learning\n(SSL), which leverages unlabeled data to learn robust features, presents an\nopportunity to bypass the intensive labeling process. In this study, we\nexplored if SSL for pre-training on non-medical images can be applied to chest\nradiographs and how it compares to supervised pre-training on non-medical\nimages and on medical images. We utilized a vision transformer and initialized\nits weights based on (i) SSL pre-training on natural images (DINOv2), (ii) SL\npre-training on natural images (ImageNet dataset), and (iii) SL pre-training on\nchest radiographs from the MIMIC-CXR database. We tested our approach on over\n800,000 chest radiographs from six large global datasets, diagnosing more than\n20 different imaging findings. Our SSL pre-training on curated images not only\noutperformed ImageNet-based pre-training (P<0.001 for all datasets) but, in\ncertain cases, also exceeded SL on the MIMIC-CXR dataset. Our findings suggest\nthat selecting the right pre-training strategy, especially with SSL, can be\npivotal for improving artificial intelligence (AI)'s diagnostic accuracy in\nmedical imaging. By demonstrating the promise of SSL in chest radiograph\nanalysis, we underline a transformative shift towards more efficient and\naccurate AI models in medical imaging.\n","authors":["Soroosh Tayebi Arasteh","Leo Misera","Jakob Nikolas Kather","Daniel Truhn","Sven Nebelung"],"pdf_url":"https://arxiv.org/pdf/2308.07688v4.pdf","comment":"Published in European Radiology Experimental"},{"id":"http://arxiv.org/abs/2401.02931v1","updated":"2024-01-05T18:15:26Z","published":"2024-01-05T18:15:26Z","title":"SPFormer: Enhancing Vision Transformer with Superpixel Representation","summary":"  In this work, we introduce SPFormer, a novel Vision Transformer enhanced by\nsuperpixel representation. Addressing the limitations of traditional Vision\nTransformers' fixed-size, non-adaptive patch partitioning, SPFormer employs\nsuperpixels that adapt to the image's content. This approach divides the image\ninto irregular, semantically coherent regions, effectively capturing intricate\ndetails and applicable at both initial and intermediate feature levels.\n  SPFormer, trainable end-to-end, exhibits superior performance across various\nbenchmarks. Notably, it exhibits significant improvements on the challenging\nImageNet benchmark, achieving a 1.4% increase over DeiT-T and 1.1% over DeiT-S\nrespectively. A standout feature of SPFormer is its inherent explainability.\nThe superpixel structure offers a window into the model's internal processes,\nproviding valuable insights that enhance the model's interpretability. This\nlevel of clarity significantly improves SPFormer's robustness, particularly in\nchallenging scenarios such as image rotations and occlusions, demonstrating its\nadaptability and resilience.\n","authors":["Jieru Mei","Liang-Chieh Chen","Alan Yuille","Cihang Xie"],"pdf_url":"https://arxiv.org/pdf/2401.02931v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.03538v2","updated":"2024-01-05T17:56:56Z","published":"2023-07-07T12:00:38Z","title":"Language-free Compositional Action Generation via Decoupling Refinement","summary":"  Composing simple elements into complex concepts is crucial yet challenging,\nespecially for 3D action generation. Existing methods largely rely on extensive\nneural language annotations to discern composable latent semantics, a process\nthat is often costly and labor-intensive. In this study, we introduce a novel\nframework to generate compositional actions without reliance on language\nauxiliaries. Our approach consists of three main components: Action Coupling,\nConditional Action Generation, and Decoupling Refinement. Action Coupling\nutilizes an energy model to extract the attention masks of each sub-action,\nsubsequently integrating two actions using these attentions to generate\npseudo-training examples. Then, we employ a conditional generative model, CVAE,\nto learn a latent space, facilitating the diverse generation. Finally, we\npropose Decoupling Refinement, which leverages a self-supervised pre-trained\nmodel MAE to ensure semantic consistency between the sub-actions and\ncompositional actions. This refinement process involves rendering generated 3D\nactions into 2D space, decoupling these images into two sub-segments, using the\nMAE model to restore the complete image from sub-segments, and constraining the\nrecovered images to match images rendered from raw sub-actions. Due to the lack\nof existing datasets containing both sub-actions and compositional actions, we\ncreated two new datasets, named HumanAct-C and UESTC-C, and present a\ncorresponding evaluation metric. Both qualitative and quantitative assessments\nare conducted to show our efficacy.\n","authors":["Xiao Liu","Guangyi Chen","Yansong Tang","Guangrun Wang","Ser-Nam Lim"],"pdf_url":"https://arxiv.org/pdf/2307.03538v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2401.02916v1","updated":"2024-01-05T17:39:52Z","published":"2024-01-05T17:39:52Z","title":"Uncovering the human motion pattern: Pattern Memory-based Diffusion\n  Model for Trajectory Prediction","summary":"  Human trajectory forecasting is a critical challenge in fields such as\nrobotics and autonomous driving. Due to the inherent uncertainty of human\nactions and intentions in real-world scenarios, various unexpected occurrences\nmay arise. To uncover latent motion patterns in human behavior, we introduce a\nnovel memory-based method, named Motion Pattern Priors Memory Network. Our\nmethod involves constructing a memory bank derived from clustered prior\nknowledge of motion patterns observed in the training set trajectories. We\nintroduce an addressing mechanism to retrieve the matched pattern and the\npotential target distributions for each prediction from the memory bank, which\nenables the identification and retrieval of natural motion patterns exhibited\nby agents, subsequently using the target priors memory token to guide the\ndiffusion model to generate predictions. Extensive experiments validate the\neffectiveness of our approach, achieving state-of-the-art trajectory prediction\naccuracy. The code will be made publicly available.\n","authors":["Yuxin Yang","Pengfei Zhu","Mengshi Qi","Huadong Ma"],"pdf_url":"https://arxiv.org/pdf/2401.02916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.06683v5","updated":"2024-01-05T17:18:56Z","published":"2023-01-17T03:53:29Z","title":"Surgical Aggregation: Federated Class-Heterogeneous Learning","summary":"  The release of numerous chest x-ray datasets has spearheaded the development\nof deep learning models with expert-level performance. However, they have\nlimited interoperability due to class-heterogeneity -- a result of inconsistent\nlabeling schemes and partial annotations. Therefore, it is challenging to\nleverage these datasets in aggregate to train models with a complete\nrepresentation of abnormalities that may occur within the thorax. In this work,\nwe propose surgical aggregation, a federated learning framework for aggregating\nknowledge from class-heterogeneous datasets and learn a model that can\nsimultaneously predict the presence of all disease labels present across the\ndatasets. We evaluate our method using simulated and real-world\nclass-heterogeneous datasets across both independent and identically\ndistributed (iid) and non-iid settings. Our results show that surgical\naggregation outperforms current methods, has better generalizability, and is a\ncrucial first step towards tackling class-heterogeneity in federated learning\nto facilitate the development of clinically-useful models using previously\nnon-interoperable chest x-ray datasets.\n","authors":["Pranav Kulkarni","Adway Kanhere","Paul H. Yi","Vishwa S. Parekh"],"pdf_url":"https://arxiv.org/pdf/2301.06683v5.pdf","comment":"9 pages, 7 figures, 4 tables"},{"id":"http://arxiv.org/abs/2401.02906v1","updated":"2024-01-05T17:05:42Z","published":"2024-01-05T17:05:42Z","title":"MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance","summary":"  The deployment of multimodal large language models (MLLMs) has brought forth\na unique vulnerability: susceptibility to malicious attacks through visual\ninputs. We delve into the novel challenge of defending MLLMs against such\nattacks. We discovered that images act as a \"foreign language\" that is not\nconsidered during alignment, which can make MLLMs prone to producing harmful\nresponses. Unfortunately, unlike the discrete tokens considered in text-based\nLLMs, the continuous nature of image signals presents significant alignment\nchallenges, which poses difficulty to thoroughly cover the possible scenarios.\nThis vulnerability is exacerbated by the fact that open-source MLLMs are\npredominantly fine-tuned on limited image-text pairs that is much less than the\nextensive text-based pretraining corpus, which makes the MLLMs more prone to\ncatastrophic forgetting of their original abilities during explicit alignment\ntuning. To tackle these challenges, we introduce MLLM-Protector, a\nplug-and-play strategy combining a lightweight harm detector and a response\ndetoxifier. The harm detector's role is to identify potentially harmful outputs\nfrom the MLLM, while the detoxifier corrects these outputs to ensure the\nresponse stipulates to the safety standards. This approach effectively\nmitigates the risks posed by malicious visual inputs without compromising the\nmodel's overall performance. Our results demonstrate that MLLM-Protector offers\na robust solution to a previously unaddressed aspect of MLLM security.\n","authors":["Renjie Pi","Tianyang Han","Yueqi Xie","Rui Pan","Qing Lian","Hanze Dong","Jipeng Zhang","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.02906v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.13775v2","updated":"2024-01-05T16:13:31Z","published":"2023-04-26T18:46:26Z","title":"Advancing Ischemic Stroke Diagnosis: A Novel Two-Stage Approach for\n  Blood Clot Origin Identification","summary":"  An innovative two-stage methodology for categorizing blood clot origins is\npresented in this paper, which is important for the diagnosis and treatment of\nischemic stroke. First, a background classifier based on MobileNetV3 segments\nbig whole-slide digital pathology images into numerous tiles to detect the\npresence of cellular material. After that, different pre-trained image\nclassification algorithms are fine-tuned to determine the origin of blood\nclots. Due to complex blood flow dynamics and limitations in conventional\nimaging methods such as computed tomography (CT), magnetic resonance imaging\n(MRI), and ultrasound, identifying the sources of blood clots is a challenging\ntask. Although these techniques are useful for identifying blood clots, they\nare not very good at determining how they originated. To address these\nchallenges, our method makes use of robust computer vision models that have\nbeen refined using information from whole-slide digital pathology images. Out\nof all the models tested, the PoolFormer \\cite{yu2022metaformer} performs\nbetter than the others, with 93.4\\% accuracy, 93.4\\% precision, 93.4\\% recall,\nand 93.4\\% F1-score. Moreover, it achieves the good weighted multi-class\nlogarithmic loss (WMCLL) of 0.4361, which emphasizes how effective it is in\nthis particular application. These encouraging findings suggest that our\napproach can successfully identify the origin of blood clots in a variety of\nvascular locations, potentially advancing ischemic stroke diagnosis and\ntreatment approaches.\n","authors":["Koushik Sivarama Krishnan","P. J. Joe Nikesh","Swathi Gnanasekar","Karthik Sivarama Krishnan"],"pdf_url":"https://arxiv.org/pdf/2304.13775v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.16221v3","updated":"2024-01-05T15:43:12Z","published":"2023-10-24T22:24:44Z","title":"Hierarchical Randomized Smoothing","summary":"  Real-world data is complex and often consists of objects that can be\ndecomposed into multiple entities (e.g. images into pixels, graphs into\ninterconnected nodes). Randomized smoothing is a powerful framework for making\nmodels provably robust against small changes to their inputs - by guaranteeing\nrobustness of the majority vote when randomly adding noise before\nclassification. Yet, certifying robustness on such complex data via randomized\nsmoothing is challenging when adversaries do not arbitrarily perturb entire\nobjects (e.g. images) but only a subset of their entities (e.g. pixels). As a\nsolution, we introduce hierarchical randomized smoothing: We partially smooth\nobjects by adding random noise only on a randomly selected subset of their\nentities. By adding noise in a more targeted manner than existing methods we\nobtain stronger robustness guarantees while maintaining high accuracy. We\ninitialize hierarchical smoothing using different noising distributions,\nyielding novel robustness certificates for discrete and continuous domains. We\nexperimentally demonstrate the importance of hierarchical smoothing in image\nand node classification, where it yields superior robustness-accuracy\ntrade-offs. Overall, hierarchical smoothing is an important contribution\ntowards models that are both - certifiably robust to perturbations and\naccurate.\n","authors":["Yan Scholten","Jan Schuchardt","Aleksandar Bojchevski","Stephan Günnemann"],"pdf_url":"https://arxiv.org/pdf/2310.16221v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02861v1","updated":"2024-01-05T15:32:40Z","published":"2024-01-05T15:32:40Z","title":"Reversing the Irreversible: A Survey on Inverse Biometrics","summary":"  With the widespread use of biometric recognition, several issues related to\nthe privacy and security provided by this technology have been recently raised\nand analysed. As a result, the early common belief among the biometrics\ncommunity of templates irreversibility has been proven wrong. It is now an\naccepted fact that it is possible to reconstruct from an unprotected template a\nsynthetic sample that matches the bona fide one. This reverse engineering\nprocess, commonly referred to as \\textit{inverse biometrics}, constitutes a\nsevere threat for biometric systems from two different angles: on the one hand,\nsensitive personal data (i.e., biometric data) can be derived from compromised\nunprotected templates; on the other hand, other powerful attacks can be\nlaunched building upon these reconstructed samples. Given its important\nimplications, biometric stakeholders have produced over the last fifteen years\nnumerous works analysing the different aspects related to inverse biometrics:\ndevelopment of reconstruction algorithms for different characteristics;\nproposal of methodologies to assess the vulnerabilities of biometric systems to\nthe aforementioned algorithms; development of countermeasures to reduce the\npossible effects of attacks. The present article is an effort to condense all\nthis information in one comprehensive review of: the problem itself, the\nevaluation of the problem, and the mitigation of the problem. The present\narticle is an effort to condense all this information in one comprehensive\nreview of: the problem itself, the evaluation of the problem, and the\nmitigation of the problem.\n","authors":["Marta Gomez-Barrero","Javier Galbally"],"pdf_url":"https://arxiv.org/pdf/2401.02861v1.pdf","comment":"18 pages, journal, survey"},{"id":"http://arxiv.org/abs/2401.02847v1","updated":"2024-01-05T15:07:05Z","published":"2024-01-05T15:07:05Z","title":"Generating Non-Stationary Textures using Self-Rectification","summary":"  This paper addresses the challenge of example-based non-stationary texture\nsynthesis. We introduce a novel twostep approach wherein users first modify a\nreference texture using standard image editing tools, yielding an initial rough\ntarget for the synthesis. Subsequently, our proposed method, termed\n\"self-rectification\", automatically refines this target into a coherent,\nseamless texture, while faithfully preserving the distinct visual\ncharacteristics of the reference exemplar. Our method leverages a pre-trained\ndiffusion network, and uses self-attention mechanisms, to gradually align the\nsynthesized texture with the reference, ensuring the retention of the\nstructures in the provided target. Through experimental validation, our\napproach exhibits exceptional proficiency in handling non-stationary textures,\ndemonstrating significant advancements in texture synthesis when compared to\nexisting state-of-the-art techniques. Code is available at\nhttps://github.com/xiaorongjun000/Self-Rectification\n","authors":["Yang Zhou","Rongjun Xiao","Dani Lischinski","Daniel Cohen-Or","Hui Huang"],"pdf_url":"https://arxiv.org/pdf/2401.02847v1.pdf","comment":"Project page: https://github.com/xiaorongjun000/Self-Rectification"},{"id":"http://arxiv.org/abs/2401.02841v1","updated":"2024-01-05T14:48:19Z","published":"2024-01-05T14:48:19Z","title":"Multi-Stage Contrastive Regression for Action Quality Assessment","summary":"  In recent years, there has been growing interest in the video-based action\nquality assessment (AQA). Most existing methods typically solve AQA problem by\nconsidering the entire video yet overlooking the inherent stage-level\ncharacteristics of actions. To address this issue, we design a novel\nMulti-stage Contrastive Regression (MCoRe) framework for the AQA task. This\napproach allows us to efficiently extract spatial-temporal information, while\nsimultaneously reducing computational costs by segmenting the input video into\nmultiple stages or procedures. Inspired by the graph contrastive learning, we\npropose a new stage-wise contrastive learning loss function to enhance\nperformance. As a result, MCoRe demonstrates the state-of-the-art result so far\non the widely-adopted fine-grained AQA dataset.\n","authors":["Qi An","Mengshi Qi","Huadong Ma"],"pdf_url":"https://arxiv.org/pdf/2401.02841v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02838v1","updated":"2024-01-05T14:45:45Z","published":"2024-01-05T14:45:45Z","title":"CrisisViT: A Robust Vision Transformer for Crisis Image Classification","summary":"  In times of emergency, crisis response agencies need to quickly and\naccurately assess the situation on the ground in order to deploy relevant\nservices and resources. However, authorities often have to make decisions based\non limited information, as data on affected regions can be scarce until local\nresponse services can provide first-hand reports. Fortunately, the widespread\navailability of smartphones with high-quality cameras has made citizen\njournalism through social media a valuable source of information for crisis\nresponders. However, analyzing the large volume of images posted by citizens\nrequires more time and effort than is typically available. To address this\nissue, this paper proposes the use of state-of-the-art deep neural models for\nautomatic image classification/tagging, specifically by adapting\ntransformer-based architectures for crisis image classification (CrisisViT). We\nleverage the new Incidents1M crisis image dataset to develop a range of new\ntransformer-based image classification models. Through experimentation over the\nstandard Crisis image benchmark dataset, we demonstrate that the CrisisViT\nmodels significantly outperform previous approaches in emergency type, image\nrelevance, humanitarian category, and damage severity classification.\nAdditionally, we show that the new Incidents1M dataset can further augment the\nCrisisViT models resulting in an additional 1.25% absolute accuracy gain.\n","authors":["Zijun Long","Richard McCreadie","Muhammad Imran"],"pdf_url":"https://arxiv.org/pdf/2401.02838v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02831v1","updated":"2024-01-05T14:31:20Z","published":"2024-01-05T14:31:20Z","title":"Two-stage Progressive Residual Dense Attention Network for Image\n  Denoising","summary":"  Deep convolutional neural networks (CNNs) for image denoising can effectively\nexploit rich hierarchical features and have achieved great success. However,\nmany deep CNN-based denoising models equally utilize the hierarchical features\nof noisy images without paying attention to the more important and useful\nfeatures, leading to relatively low performance. To address the issue, we\ndesign a new Two-stage Progressive Residual Dense Attention Network\n(TSP-RDANet) for image denoising, which divides the whole process of denoising\ninto two sub-tasks to remove noise progressively. Two different attention\nmechanism-based denoising networks are designed for the two sequential\nsub-tasks: the residual dense attention module (RDAM) is designed for the first\nstage, and the hybrid dilated residual dense attention module (HDRDAM) is\nproposed for the second stage. The proposed attention modules are able to learn\nappropriate local features through dense connection between different\nconvolutional layers, and the irrelevant features can also be suppressed. The\ntwo sub-networks are then connected by a long skip connection to retain the\nshallow feature to enhance the denoising performance. The experiments on seven\nbenchmark datasets have verified that compared with many state-of-the-art\nmethods, the proposed TSP-RDANet can obtain favorable results both on synthetic\nand real noisy image denoising. The code of our TSP-RDANet is available at\nhttps://github.com/WenCongWu/TSP-RDANet.\n","authors":["Wencong Wu","An Ge","Guannan Lv","Yuelong Xia","Yungang Zhang","Wen Xiong"],"pdf_url":"https://arxiv.org/pdf/2401.02831v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02826v1","updated":"2024-01-05T14:20:22Z","published":"2024-01-05T14:20:22Z","title":"CRSOT: Cross-Resolution Object Tracking using Unaligned Frame and Event\n  Cameras","summary":"  Existing datasets for RGB-DVS tracking are collected with DVS346 camera and\ntheir resolution ($346 \\times 260$) is low for practical applications.\nActually, only visible cameras are deployed in many practical systems, and the\nnewly designed neuromorphic cameras may have different resolutions. The latest\nneuromorphic sensors can output high-definition event streams, but it is very\ndifficult to achieve strict alignment between events and frames on both spatial\nand temporal views. Therefore, how to achieve accurate tracking with unaligned\nneuromorphic and visible sensors is a valuable but unresearched problem. In\nthis work, we formally propose the task of object tracking using unaligned\nneuromorphic and visible cameras. We build the first unaligned frame-event\ndataset CRSOT collected with a specially built data acquisition system, which\ncontains 1,030 high-definition RGB-Event video pairs, 304,974 video frames. In\naddition, we propose a novel unaligned object tracking framework that can\nrealize robust tracking even using the loosely aligned RGB-Event data.\nSpecifically, we extract the template and search regions of RGB and Event data\nand feed them into a unified ViT backbone for feature embedding. Then, we\npropose uncertainty perception modules to encode the RGB and Event features,\nrespectively, then, we propose a modality uncertainty fusion module to\naggregate the two modalities. These three branches are jointly optimized in the\ntraining phase. Extensive experiments demonstrate that our tracker can\ncollaborate the dual modalities for high-performance tracking even without\nstrictly temporal and spatial alignment. The source code, dataset, and\npre-trained models will be released at\nhttps://github.com/Event-AHU/Cross_Resolution_SOT.\n","authors":["Yabin Zhu","Xiao Wang","Chenglong Li","Bo Jiang","Lin Zhu","Zhixiang Huang","Yonghong Tian","Jin Tang"],"pdf_url":"https://arxiv.org/pdf/2401.02826v1.pdf","comment":"In Peer Review"},{"id":"http://arxiv.org/abs/2401.02814v1","updated":"2024-01-05T13:54:45Z","published":"2024-01-05T13:54:45Z","title":"Object-Centric Instruction Augmentation for Robotic Manipulation","summary":"  Humans interpret scenes by recognizing both the identities and positions of\nobjects in their observations. For a robot to perform tasks such as\n\\enquote{pick and place}, understanding both what the objects are and where\nthey are located is crucial. While the former has been extensively discussed in\nthe literature that uses the large language model to enrich the text\ndescriptions, the latter remains underexplored. In this work, we introduce the\n\\textit{Object-Centric Instruction Augmentation (OCI)} framework to augment\nhighly semantic and information-dense language instruction with position cues.\nWe utilize a Multi-modal Large Language Model (MLLM) to weave knowledge of\nobject locations into natural language instruction, thus aiding the policy\nnetwork in mastering actions for versatile manipulation. Additionally, we\npresent a feature reuse mechanism to integrate the vision-language features\nfrom off-the-shelf pre-trained MLLM into policy networks. Through a series of\nsimulated and real-world robotic tasks, we demonstrate that robotic manipulator\nimitation policies trained with our enhanced instructions outperform those\nrelying solely on traditional language instructions.\n","authors":["Junjie Wen","Yichen Zhu","Minjie Zhu","Jinming Li","Zhiyuan Xu","Zhengping Che","Chaomin Shen","Yaxin Peng","Dong Liu","Feifei Feng","Jian Tang"],"pdf_url":"https://arxiv.org/pdf/2401.02814v1.pdf","comment":"Submitted to ICRA2024"},{"id":"http://arxiv.org/abs/2401.02804v1","updated":"2024-01-05T13:36:19Z","published":"2024-01-05T13:36:19Z","title":"Diffbody: Diffusion-based Pose and Shape Editing of Human Images","summary":"  Pose and body shape editing in a human image has received increasing\nattention. However, current methods often struggle with dataset biases and\ndeteriorate realism and the person's identity when users make large edits. We\npropose a one-shot approach that enables large edits with identity\npreservation. To enable large edits, we fit a 3D body model, project the input\nimage onto the 3D model, and change the body's pose and shape. Because this\ninitial textured body model has artifacts due to occlusion and the inaccurate\nbody shape, the rendered image undergoes a diffusion-based refinement, in which\nstrong noise destroys body structure and identity whereas insufficient noise\ndoes not help. We thus propose an iterative refinement with weak noise, applied\nfirst for the whole body and then for the face. We further enhance the realism\nby fine-tuning text embeddings via self-supervised learning. Our quantitative\nand qualitative evaluations demonstrate that our method outperforms other\nexisting methods across various datasets.\n","authors":["Yuta Okuyama","Yuki Endo","Yoshihiro Kanamori"],"pdf_url":"https://arxiv.org/pdf/2401.02804v1.pdf","comment":"Accepted to WACV 2024, project page:\n  https://www.cgg.cs.tsukuba.ac.jp/~okuyama/pub/diffbody/"},{"id":"http://arxiv.org/abs/2311.09215v2","updated":"2024-01-05T13:16:25Z","published":"2023-11-15T18:56:51Z","title":"ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy","summary":"  Modern computer vision offers a great variety of models to practitioners, and\nselecting a model from multiple options for specific applications can be\nchallenging. Conventionally, competing model architectures and training\nprotocols are compared by their classification accuracy on ImageNet. However,\nthis single metric does not fully capture performance nuances critical for\nspecialized tasks. In this work, we conduct an in-depth comparative analysis of\nmodel behaviors beyond ImageNet accuracy, for both ConvNet and Vision\nTransformer architectures, each across supervised and CLIP training paradigms.\nAlthough our selected models have similar ImageNet accuracies and compute\nrequirements, we find that they differ in many other aspects: types of\nmistakes, output calibration, transferability, and feature invariance, among\nothers. This diversity in model characteristics, not captured by traditional\nmetrics, highlights the need for more nuanced analysis when choosing among\ndifferent models. Our code is available at\nhttps://github.com/kirill-vish/Beyond-INet.\n","authors":["Kirill Vishniakov","Zhiqiang Shen","Zhuang Liu"],"pdf_url":"https://arxiv.org/pdf/2311.09215v2.pdf","comment":"Project page: https://kirill-vish.github.io/beyond-imagenet-accuracy/"},{"id":"http://arxiv.org/abs/2401.02794v1","updated":"2024-01-05T13:13:09Z","published":"2024-01-05T13:13:09Z","title":"Subjective and Objective Analysis of Indian Social Media Video Quality","summary":"  We conducted a large-scale subjective study of the perceptual quality of\nUser-Generated Mobile Video Content on a set of mobile-originated videos\nobtained from the Indian social media platform ShareChat. The content viewed by\nvolunteer human subjects under controlled laboratory conditions has the benefit\nof culturally diversifying the existing corpus of User-Generated Content (UGC)\nvideo quality datasets. There is a great need for large and diverse UGC-VQA\ndatasets, given the explosive global growth of the visual internet and social\nmedia platforms. This is particularly true in regard to videos obtained by\nsmartphones, especially in rapidly emerging economies like India. ShareChat\nprovides a safe and cultural community oriented space for users to generate and\nshare content in their preferred Indian languages and dialects. Our subjective\nquality study, which is based on this data, offers a boost of cultural, visual,\nand language diversification to the video quality research community. We expect\nthat this new data resource will also allow for the development of systems that\ncan predict the perceived visual quality of Indian social media videos, to\ncontrol scaling and compression protocols for streaming, provide better user\nrecommendations, and guide content analysis and processing. We demonstrate the\nvalue of the new data resource by conducting a study of leading blind video\nquality models on it, including a new model, called MoEVA, which deploys a\nmixture of experts to predict video quality. Both the new LIVE-ShareChat\ndataset and sample source code for MoEVA are being made freely available to the\nresearch community at https://github.com/sandeep-sm/LIVE-SC\n","authors":["Sandeep Mishra","Mukul Jha","Alan C. Bovik"],"pdf_url":"https://arxiv.org/pdf/2401.02794v1.pdf","comment":"Submitted to the IEEE Transactions on Image Processing"},{"id":"http://arxiv.org/abs/2401.02791v1","updated":"2024-01-05T13:05:02Z","published":"2024-01-05T13:05:02Z","title":"Weakly Semi-supervised Tool Detection in Minimally Invasive Surgery\n  Videos","summary":"  Surgical tool detection is essential for analyzing and evaluating minimally\ninvasive surgery videos. Current approaches are mostly based on supervised\nmethods that require large, fully instance-level labels (i.e., bounding boxes).\nHowever, large image datasets with instance-level labels are often limited\nbecause of the burden of annotation. Thus, surgical tool detection is important\nwhen providing image-level labels instead of instance-level labels since\nimage-level annotations are considerably more time-efficient than\ninstance-level annotations. In this work, we propose to strike a balance\nbetween the extremely costly annotation burden and detection performance. We\nfurther propose a co-occurrence loss, which considers a characteristic that\nsome tool pairs often co-occur together in an image to leverage image-level\nlabels. Encapsulating the knowledge of co-occurrence using the co-occurrence\nloss helps to overcome the difficulty in classification that originates from\nthe fact that some tools have similar shapes and textures. Extensive\nexperiments conducted on the Endovis2018 dataset in various data settings show\nthe effectiveness of our method.\n","authors":["Ryo Fujii","Ryo Hachiuma","Hideo Saito"],"pdf_url":"https://arxiv.org/pdf/2401.02791v1.pdf","comment":"ICASSP 2024"},{"id":"http://arxiv.org/abs/2306.02986v2","updated":"2024-01-05T12:48:31Z","published":"2023-06-05T15:56:30Z","title":"Brain tumor segmentation using synthetic MR images -- A comparison of\n  GANs and diffusion models","summary":"  Large annotated datasets are required for training deep learning models, but\nin medical imaging data sharing is often complicated due to ethics,\nanonymization and data protection legislation. Generative AI models, such as\ngenerative adversarial networks (GANs) and diffusion models, can today produce\nvery realistic synthetic images, and can potentially facilitate data sharing.\nHowever, in order to share synthetic medical images it must first be\ndemonstrated that they can be used for training different networks with\nacceptable performance. Here, we therefore comprehensively evaluate four GANs\n(progressive GAN, StyleGAN 1-3) and a diffusion model for the task of brain\ntumor segmentation (using two segmentation networks, U-Net and a Swin\ntransformer). Our results show that segmentation networks trained on synthetic\nimages reach Dice scores that are 80% - 90% of Dice scores when training with\nreal images, but that memorization of the training images can be a problem for\ndiffusion models if the original dataset is too small. Our conclusion is that\nsharing synthetic medical images is a viable option to sharing real images, but\nthat further work is required. The trained generative models and the generated\nsynthetic images are shared on AIDA data hub\n","authors":["Muhammad Usman Akbar","Måns Larsson","Anders Eklund"],"pdf_url":"https://arxiv.org/pdf/2306.02986v2.pdf","comment":"28 Pages. 5 figures"},{"id":"http://arxiv.org/abs/2311.05197v4","updated":"2024-01-05T12:09:38Z","published":"2023-11-09T08:23:44Z","title":"Deep learning in computed tomography pulmonary angiography imaging: a\n  dual-pronged approach for pulmonary embolism detection","summary":"  The increasing reliance on Computed Tomography Pulmonary Angiography (CTPA)\nfor Pulmonary Embolism (PE) diagnosis presents challenges and a pressing need\nfor improved diagnostic solutions. The primary objective of this study is to\nleverage deep learning techniques to enhance the Computer Assisted Diagnosis\n(CAD) of PE. With this aim, we propose a classifier-guided detection approach\nthat effectively leverages the classifier's probabilistic inference to direct\nthe detection predictions, marking a novel contribution in the domain of\nautomated PE diagnosis. Our classification system includes an Attention-Guided\nConvolutional Neural Network (AG-CNN) that uses local context by employing an\nattention mechanism. This approach emulates a human expert's attention by\nlooking at both global appearances and local lesion regions before making a\ndecision. The classifier demonstrates robust performance on the FUMPE dataset,\nachieving an AUROC of 0.927, sensitivity of 0.862, specificity of 0.879, and an\nF1-score of 0.805 with the Inception-v3 backbone architecture. Moreover, AG-CNN\noutperforms the baseline DenseNet-121 model, achieving an 8.1% AUROC gain.\nWhile previous research has mostly focused on finding PE in the main arteries,\nour use of cutting-edge object detection models and ensembling techniques\ngreatly improves the accuracy of detecting small embolisms in the peripheral\narteries. Finally, our proposed classifier-guided detection approach further\nrefines the detection metrics, contributing new state-of-the-art to the\ncommunity: mAP$_{50}$, sensitivity, and F1-score of 0.846, 0.901, and 0.779,\nrespectively, outperforming the former benchmark with a significant 3.7%\nimprovement in mAP$_{50}$. Our research aims to elevate PE patient care by\nintegrating AI solutions into clinical workflows, highlighting the potential of\nhuman-AI collaboration in medical diagnostics.\n","authors":["Fabiha Bushra","Muhammad E. H. Chowdhury","Rusab Sarmun","Saidul Kabir","Menatalla Said","Sohaib Bassam Zoghoul","Adam Mushtak","Israa Al-Hashimi","Abdulrahman Alqahtani","Anwarul Hasan"],"pdf_url":"https://arxiv.org/pdf/2311.05197v4.pdf","comment":"Published in Expert Systems With Applications"},{"id":"http://arxiv.org/abs/2305.12711v3","updated":"2024-01-05T12:03:37Z","published":"2023-05-22T04:40:30Z","title":"Unsupervised Visible-Infrared Person ReID by Collaborative Learning with\n  Neighbor-Guided Label Refinement","summary":"  Unsupervised learning visible-infrared person re-identification (USL-VI-ReID)\naims at learning modality-invariant features from unlabeled cross-modality\ndataset, which is crucial for practical applications in video surveillance\nsystems. The key to essentially address the USL-VI-ReID task is to solve the\ncross-modality data association problem for further heterogeneous joint\nlearning. To address this issue, we propose a Dual Optimal Transport Label\nAssignment (DOTLA) framework to simultaneously assign the generated labels from\none modality to its counterpart modality. The proposed DOTLA mechanism\nformulates a mutual reinforcement and efficient solution to cross-modality data\nassociation, which could effectively reduce the side-effects of some\ninsufficient and noisy label associations. Besides, we further propose a\ncross-modality neighbor consistency guided label refinement and regularization\nmodule, to eliminate the negative effects brought by the inaccurate supervised\nsignals, under the assumption that the prediction or label distribution of each\nexample should be similar to its nearest neighbors. Extensive experimental\nresults on the public SYSU-MM01 and RegDB datasets demonstrate the\neffectiveness of the proposed method, surpassing existing state-of-the-art\napproach by a large margin of 7.76% mAP on average, which even surpasses some\nsupervised VI-ReID methods.\n","authors":["De Cheng","Xiaojian Huang","Nannan Wang","Lingfeng He","Zhihui Li","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2305.12711v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.10907v3","updated":"2024-01-05T12:03:22Z","published":"2022-09-22T10:29:17Z","title":"DRKF: Distilled Rotated Kernel Fusion for Efficient Rotation Invariant\n  Descriptors in Local Feature Matching","summary":"  The performance of local feature descriptors degrades in the presence of\nlarge rotation variations. To address this issue, we present an efficient\napproach to learning rotation invariant descriptors. Specifically, we propose\nRotated Kernel Fusion (RKF) which imposes rotations on the convolution kernel\nto improve the inherent nature of CNN. Since RKF can be processed by the\nsubsequent re-parameterization, no extra computational costs will be introduced\nin the inference stage. Moreover, we present Multi-oriented Feature Aggregation\n(MOFA) which aggregates features extracted from multiple rotated versions of\nthe input image and can provide auxiliary knowledge for the training of RKF by\nleveraging the distillation strategy. We refer to the distilled RKF model as\nDRKF. Besides the evaluation on a rotation-augmented version of the public\ndataset HPatches, we also contribute a new dataset named DiverseBEV which is\ncollected during the drone's flight and consists of bird's eye view images with\nlarge viewpoint changes and camera rotations. Extensive experiments show that\nour method can outperform other state-of-the-art techniques when exposed to\nlarge rotation variations.\n","authors":["Ranran Huang","Jiancheng Cai","Chao Li","Zhuoyuan Wu","Xinmin Liu","Zhenhua Chai"],"pdf_url":"https://arxiv.org/pdf/2209.10907v3.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2312.08785v2","updated":"2024-01-05T11:42:22Z","published":"2023-12-14T10:08:12Z","title":"Managing the unknown: a survey on Open Set Recognition and tangential\n  areas","summary":"  In real-world scenarios classification models are often required to perform\nrobustly when predicting samples belonging to classes that have not appeared\nduring its training stage. Open Set Recognition addresses this issue by\ndevising models capable of detecting unknown classes from samples arriving\nduring the testing phase, while maintaining a good level of performance in the\nclassification of samples belonging to known classes. This review\ncomprehensively overviews the recent literature related to Open Set\nRecognition, identifying common practices, limitations, and connections of this\nfield with other machine learning research areas, such as continual learning,\nout-of-distribution detection, novelty detection, and uncertainty estimation.\nOur work also uncovers open problems and suggests several research directions\nthat may motivate and articulate future efforts towards more safe Artificial\nIntelligence methods.\n","authors":["Marcos Barcina-Blanco","Jesus L. Lobo","Pablo Garcia-Bringas","Javier Del Ser"],"pdf_url":"https://arxiv.org/pdf/2312.08785v2.pdf","comment":"35 pages, 1 figure, 1 table"},{"id":"http://arxiv.org/abs/2401.02764v1","updated":"2024-01-05T11:36:21Z","published":"2024-01-05T11:36:21Z","title":"Fus-MAE: A cross-attention-based data fusion approach for Masked\n  Autoencoders in remote sensing","summary":"  Self-supervised frameworks for representation learning have recently stirred\nup interest among the remote sensing community, given their potential to\nmitigate the high labeling costs associated with curating large satellite image\ndatasets. In the realm of multimodal data fusion, while the often used\ncontrastive learning methods can help bridging the domain gap between different\nsensor types, they rely on data augmentations techniques that require expertise\nand careful design, especially for multispectral remote sensing data. A\npossible but rather scarcely studied way to circumvent these limitations is to\nuse a masked image modelling based pretraining strategy. In this paper, we\nintroduce Fus-MAE, a self-supervised learning framework based on masked\nautoencoders that uses cross-attention to perform early and feature-level data\nfusion between synthetic aperture radar and multispectral optical data - two\nmodalities with a significant domain gap. Our empirical findings demonstrate\nthat Fus-MAE can effectively compete with contrastive learning strategies\ntailored for SAR-optical data fusion and outperforms other masked-autoencoders\nframeworks trained on a larger corpus.\n","authors":["Hugo Chan-To-Hing","Bharadwaj Veeravalli"],"pdf_url":"https://arxiv.org/pdf/2401.02764v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02759v1","updated":"2024-01-05T11:19:24Z","published":"2024-01-05T11:19:24Z","title":"Detection and Classification of Diabetic Retinopathy using Deep Learning\n  Algorithms for Segmentation to Facilitate Referral Recommendation for Test\n  and Treatment Prediction","summary":"  This research paper addresses the critical challenge of diabetic retinopathy\n(DR), a severe complication of diabetes leading to potential blindness. The\nproposed methodology leverages transfer learning with convolutional neural\nnetworks (CNNs) for automatic DR detection using a single fundus photograph,\ndemonstrating high effectiveness with a quadratic weighted kappa score of\n0.92546 in the APTOS 2019 Blindness Detection Competition. The paper reviews\nexisting literature on DR detection, spanning classical computer vision methods\nto deep learning approaches, particularly focusing on CNNs. It identifies gaps\nin the research, emphasizing the lack of exploration in integrating pretrained\nlarge language models with segmented image inputs for generating\nrecommendations and understanding dynamic interactions within a web application\ncontext.Objectives include developing a comprehensive DR detection methodology,\nexploring model integration, evaluating performance through competition\nranking, contributing significantly to DR detection methodologies, and\nidentifying research gaps.The methodology involves data preprocessing, data\naugmentation, and the use of a U-Net neural network architecture for\nsegmentation. The U-Net model efficiently segments retinal structures,\nincluding blood vessels, hard and soft exudates, haemorrhages, microaneurysms,\nand the optical disc. High evaluation scores in Jaccard, F1, recall, precision,\nand accuracy underscore the model's potential for enhancing diagnostic\ncapabilities in retinal pathology assessment.The outcomes of this research hold\npromise for improving patient outcomes through timely diagnosis and\nintervention in the fight against diabetic retinopathy, marking a significant\ncontribution to the field of medical image analysis.\n","authors":["Manoj S H","Arya A Bosale"],"pdf_url":"https://arxiv.org/pdf/2401.02759v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14404v2","updated":"2024-01-05T11:14:58Z","published":"2023-12-22T03:09:11Z","title":"Cross-Covariate Gait Recognition: A Benchmark","summary":"  Gait datasets are essential for gait research. However, this paper observes\nthat present benchmarks, whether conventional constrained or emerging\nreal-world datasets, fall short regarding covariate diversity. To bridge this\ngap, we undertake an arduous 20-month effort to collect a cross-covariate gait\nrecognition (CCGR) dataset. The CCGR dataset has 970 subjects and about 1.6\nmillion sequences; almost every subject has 33 views and 53 different\ncovariates. Compared to existing datasets, CCGR has both population and\nindividual-level diversity. In addition, the views and covariates are well\nlabeled, enabling the analysis of the effects of different factors. CCGR\nprovides multiple types of gait data, including RGB, parsing, silhouette, and\npose, offering researchers a comprehensive resource for exploration. In order\nto delve deeper into addressing cross-covariate gait recognition, we propose\nparsing-based gait recognition (ParsingGait) by utilizing the newly proposed\nparsing data. We have conducted extensive experiments. Our main results show:\n1) Cross-covariate emerges as a pivotal challenge for practical applications of\ngait recognition. 2) ParsingGait demonstrates remarkable potential for further\nadvancement. 3) Alarmingly, existing SOTA methods achieve less than 43%\naccuracy on the CCGR, highlighting the urgency of exploring cross-covariate\ngait recognition. Link: https://github.com/ShinanZou/CCGR.\n","authors":["Shinan Zou","Chao Fan","Jianbo Xiong","Chuanfu Shen","Shiqi Yu","Jin Tang"],"pdf_url":"https://arxiv.org/pdf/2312.14404v2.pdf","comment":"This paper has been accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2401.02758v1","updated":"2024-01-05T11:14:07Z","published":"2024-01-05T11:14:07Z","title":"Systematic review of image segmentation using complex networks","summary":"  This review presents various image segmentation methods using complex\nnetworks.\n  Image segmentation is one of the important steps in image analysis as it\nhelps analyze and understand complex images. At first, it has been tried to\nclassify complex networks based on how it being used in image segmentation.\n  In computer vision and image processing applications, image segmentation is\nessential for analyzing complex images with irregular shapes, textures, or\noverlapping boundaries. Advanced algorithms make use of machine learning,\nclustering, edge detection, and region-growing techniques. Graph theory\nprinciples combined with community detection-based methods allow for more\nprecise analysis and interpretation of complex images. Hybrid approaches\ncombine multiple techniques for comprehensive, robust segmentation, improving\nresults in computer vision and image processing tasks.\n","authors":["Amin Rezaei","Fatemeh Asadi"],"pdf_url":"https://arxiv.org/pdf/2401.02758v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07180v4","updated":"2024-01-05T11:10:24Z","published":"2023-12-12T11:27:13Z","title":"Context-Aware Iteration Policy Network for Efficient Optical Flow\n  Estimation","summary":"  Existing recurrent optical flow estimation networks are computationally\nexpensive since they use a fixed large number of iterations to update the flow\nfield for each sample. An efficient network should skip iterations when the\nflow improvement is limited. In this paper, we develop a Context-Aware\nIteration Policy Network for efficient optical flow estimation, which\ndetermines the optimal number of iterations per sample. The policy network\nachieves this by learning contextual information to realize whether flow\nimprovement is bottlenecked or minimal. On the one hand, we use iteration\nembedding and historical hidden cell, which include previous iterations\ninformation, to convey how flow has changed from previous iterations. On the\nother hand, we use the incremental loss to make the policy network implicitly\nperceive the magnitude of optical flow improvement in the subsequent iteration.\nFurthermore, the computational complexity in our dynamic network is\ncontrollable, allowing us to satisfy various resource preferences with a single\ntrained model. Our policy network can be easily integrated into\nstate-of-the-art optical flow networks. Extensive experiments show that our\nmethod maintains performance while reducing FLOPs by about 40%/20% for the\nSintel/KITTI datasets.\n","authors":["Ri Cheng","Ruian He","Xuhao Jiang","Shili Zhou","Weimin Tan","Bo Yan"],"pdf_url":"https://arxiv.org/pdf/2312.07180v4.pdf","comment":"2024, Association for the Advancement of Artificial Intelligence"},{"id":"http://arxiv.org/abs/2401.02746v1","updated":"2024-01-05T10:47:42Z","published":"2024-01-05T10:47:42Z","title":"Reading Between the Frames: Multi-Modal Depression Detection in Videos\n  from Non-Verbal Cues","summary":"  Depression, a prominent contributor to global disability, affects a\nsubstantial portion of the population. Efforts to detect depression from social\nmedia texts have been prevalent, yet only a few works explored depression\ndetection from user-generated video content. In this work, we address this\nresearch gap by proposing a simple and flexible multi-modal temporal model\ncapable of discerning non-verbal depression cues from diverse modalities in\nnoisy, real-world videos. We show that, for in-the-wild videos, using\nadditional high-level non-verbal cues is crucial to achieving good performance,\nand we extracted and processed audio speech embeddings, face emotion\nembeddings, face, body and hand landmarks, and gaze and blinking information.\nThrough extensive experiments, we show that our model achieves state-of-the-art\nresults on three key benchmark datasets for depression detection from video by\na substantial margin. Our code is publicly available on GitHub.\n","authors":["David Gimeno-Gómez","Ana-Maria Bucur","Adrian Cosma","Carlos-David Martínez-Hinarejos","Paolo Rosso"],"pdf_url":"https://arxiv.org/pdf/2401.02746v1.pdf","comment":"Accepted at 46th European Conference on Information Retrieval (ECIR\n  2024)"},{"id":"http://arxiv.org/abs/2401.02744v1","updated":"2024-01-05T10:41:55Z","published":"2024-01-05T10:41:55Z","title":"MAMI: Multi-Attentional Mutual-Information for Long Sequence Neuron\n  Captioning","summary":"  Neuron labeling is an approach to visualize the behaviour and respond of a\ncertain neuron to a certain pattern that activates the neuron. Neuron labeling\nextract information about the features captured by certain neurons in a deep\nneural network, one of which uses the encoder-decoder image captioning\napproach. The encoder used can be a pretrained CNN-based model and the decoder\nis an RNN-based model for text generation. Previous work, namely MILAN (Mutual\nInformation-guided Linguistic Annotation of Neuron), has tried to visualize the\nneuron behaviour using modified Show, Attend, and Tell (SAT) model in the\nencoder, and LSTM added with Bahdanau attention in the decoder. MILAN can show\ngreat result on short sequence neuron captioning, but it does not show great\nresult on long sequence neuron captioning, so in this work, we would like to\nimprove the performance of MILAN even more by utilizing different kind of\nattention mechanism and additionally adding several attention result into one,\nin order to combine all the advantages from several attention mechanism. Using\nour compound dataset, we obtained higher BLEU and F1-Score on our proposed\nmodel, achieving 17.742 and 0.4811 respectively. At some point where the model\nconverges at the peak, our model obtained BLEU of 21.2262 and BERTScore\nF1-Score of 0.4870.\n","authors":["Alfirsa Damasyifa Fauzulhaq","Wahyu Parwitayasa","Joseph Ananda Sugihdharma","M. Fadli Ridhani","Novanto Yudistira"],"pdf_url":"https://arxiv.org/pdf/2401.02744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.19787v2","updated":"2024-01-05T10:29:59Z","published":"2023-05-31T12:27:58Z","title":"DeepMerge: Deep-Learning-Based Region-Merging for Image Segmentation","summary":"  Image segmentation aims to partition an image according to the objects in the\nscene and is a fundamental step in analysing very high spatial-resolution (VHR)\nremote sensing imagery. Current methods struggle to effectively consider land\nobjects with diverse shapes and sizes. Additionally, the determination of\nsegmentation scale parameters frequently adheres to a static and empirical\ndoctrine, posing limitations on the segmentation of large-scale remote sensing\nimages and yielding algorithms with limited interpretability. To address the\nabove challenges, we propose a deep-learning-based region merging method dubbed\nDeepMerge to handle the segmentation of complete objects in large VHR images by\nintegrating deep learning and region adjacency graph (RAG). This is the first\nmethod to use deep learning to learn the similarity and merge similar adjacent\nsuper-pixels in RAG. We propose a modified binary tree sampling method to\ngenerate shift-scale data, serving as inputs for transformer-based deep\nlearning networks, a shift-scale attention with 3-Dimension relative position\nembedding to learn features across scales, and an embedding to fuse learned\nfeatures with hand-crafted features. DeepMerge can achieve high segmentation\naccuracy in a supervised manner from large-scale remotely sensed images and\nprovides an interpretable optimal scale parameter, which is validated using a\nremote sensing image of 0.55 m resolution covering an area of 5,660 km^2. The\nexperimental results show that DeepMerge achieves the highest F value (0.9550)\nand the lowest total error TE (0.0895), correctly segmenting objects of\ndifferent sizes and outperforming all competing segmentation methods.\n","authors":["Xianwei Lv","Claudio Persello","Wangbin Li","Xiao Huang","Dongping Ming","Alfred Stein"],"pdf_url":"https://arxiv.org/pdf/2305.19787v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.17163v3","updated":"2024-01-05T10:01:14Z","published":"2023-12-28T17:52:09Z","title":"FENet: Focusing Enhanced Network for Lane Detection","summary":"  Inspired by human driving focus, this research pioneers networks augmented\nwith Focusing Sampling, Partial Field of View Evaluation, Enhanced FPN\narchitecture and Directional IoU Loss - targeted innovations addressing\nobstacles to precise lane detection for autonomous driving. Experiments\ndemonstrate our Focusing Sampling strategy, emphasizing vital distant details\nunlike uniform approaches, significantly boosts both benchmark and practical\ncurved/distant lane recognition accuracy essential for safety. While FENetV1\nachieves state-of-the-art conventional metric performance via enhancements\nisolating perspective-aware contexts mimicking driver vision, FENetV2 proves\nmost reliable on the proposed Partial Field analysis. Hence we specifically\nrecommend V2 for practical lane navigation despite fractional degradation on\nstandard entire-image measures. Future directions include collecting on-road\ndata and integrating complementary dual frameworks to further breakthroughs\nguided by human perception principles. The Code is available at\nhttps://github.com/HanyangZhong/FENet.\n","authors":["Liman Wang","Hanyang Zhong"],"pdf_url":"https://arxiv.org/pdf/2312.17163v3.pdf","comment":"12 pages including appendix. The Code is available at\n  https://github.com/HanyangZhong/FENet"},{"id":"http://arxiv.org/abs/2401.02727v1","updated":"2024-01-05T09:46:42Z","published":"2024-01-05T09:46:42Z","title":"Enhancing targeted transferability via feature space fine-tuning","summary":"  Adversarial examples (AEs) have been extensively studied due to their\npotential for privacy protection and inspiring robust neural networks. However,\nmaking a targeted AE transferable across unknown models remains challenging. In\nthis paper, to alleviate the overfitting dilemma common in an AE crafted by\nexisting simple iterative attacks, we propose fine-tuning it in the feature\nspace. Specifically, starting with an AE generated by a baseline attack, we\nencourage the features that contribute to the target class and discourage the\nfeatures that contribute to the original class in a middle layer of the source\nmodel. Extensive experiments demonstrate that only a few iterations of\nfine-tuning can boost existing attacks in terms of targeted transferability\nnontrivially and universally. Our results also verify that the simple iterative\nattacks can yield comparable or even better transferability than the\nresource-intensive methods, which rely on training target-specific classifiers\nor generators with additional data. The code is available at:\ngithub.com/zengh5/TA_feature_FT.\n","authors":["Hui Zeng","Biwei Chen","Anjie Peng"],"pdf_url":"https://arxiv.org/pdf/2401.02727v1.pdf","comment":"9 pages, 10 figures, accepted by 2024ICASSP"},{"id":"http://arxiv.org/abs/2401.02723v1","updated":"2024-01-05T09:36:42Z","published":"2024-01-05T09:36:42Z","title":"Predicting Traffic Flow with Federated Learning and Graph Neural with\n  Asynchronous Computations Network","summary":"  Real-time traffic flow prediction holds significant importance within the\ndomain of Intelligent Transportation Systems (ITS). The task of achieving a\nbalance between prediction precision and computational efficiency presents a\nsignificant challenge. In this article, we present a novel deep-learning method\ncalled Federated Learning and Asynchronous Graph Convolutional Network\n(FLAGCN). Our framework incorporates the principles of asynchronous graph\nconvolutional networks with federated learning to enhance the accuracy and\nefficiency of real-time traffic flow prediction. The FLAGCN model employs a\nspatial-temporal graph convolution technique to asynchronously address\nspatio-temporal dependencies within traffic data effectively. To efficiently\nhandle the computational requirements associated with this deep learning model,\nthis study used a graph federated learning technique known as GraphFL. This\napproach is designed to facilitate the training process. The experimental\nresults obtained from conducting tests on two distinct traffic datasets\ndemonstrate that the utilization of FLAGCN leads to the optimization of both\ntraining and inference durations while maintaining a high level of prediction\naccuracy. FLAGCN outperforms existing models with significant improvements by\nachieving up to approximately 6.85% reduction in RMSE, 20.45% reduction in\nMAPE, compared to the best-performing existing models.\n","authors":["Muhammad Yaqub","Shahzad Ahmad","Malik Abdul Manan","Imran Shabir Chuhan"],"pdf_url":"https://arxiv.org/pdf/2401.02723v1.pdf","comment":"15 pages, 7 Figures, 1 Table"},{"id":"http://arxiv.org/abs/2312.17290v2","updated":"2024-01-05T09:36:36Z","published":"2023-12-28T10:30:54Z","title":"Predicting Parkinson's disease evolution using deep learning","summary":"  Parkinson's disease is a neurological condition that occurs in nearly 1% of\nthe world's population. The disease is manifested by a drop in dopamine\nproduction, symptoms are cognitive and behavioural and include a wide range of\npersonality changes, depressive disorders, memory problems, and emotional\ndysregulation, which can occur as the disease progresses. Early diagnosis and\naccurate staging of the disease are essential to apply the appropriate\ntherapeutic approaches to slow cognitive and motor decline.\n  Currently, there is not a single blood test or biomarker available to\ndiagnose Parkinson's disease. Magnetic resonance imaging has been used for the\npast three decades to diagnose and distinguish between PD and other\nneurological conditions. However, in recent years new possibilities have\narisen: several AI algorithms have been developed to increase the precision and\naccuracy of differential diagnosis of PD at an early stage.\n  To our knowledge, no AI tools have been designed to identify the stage of\nprogression. This paper aims to fill this gap. Using the \"Parkinson's\nProgression Markers Initiative\" dataset, which reports the patient's MRI and an\nindication of the disease stage, we developed a model to identify the level of\nprogression. The images and the associated scores were used for training and\nassessing different deep-learning models. Our analysis distinguished four\ndistinct disease progression levels based on a standard scale (Hoehn and Yah\nscale). The final architecture consists of the cascading of a 3DCNN network,\nadopted to reduce and extract the spatial characteristics of the RMI for\nefficient training of the successive LSTM layers, aiming at modelling the\ntemporal dependencies among the data.\n  Our results show that the proposed 3DCNN + LSTM model achieves\nstate-of-the-art results by classifying the elements with 91.90\\% as macro\naveraged OVR AUC on four classes\n","authors":["Maria Frasca","Davide La Torre","Gabriella Pravettoni","Ilaria Cutica"],"pdf_url":"https://arxiv.org/pdf/2312.17290v2.pdf","comment":"27 pages, 11 figures"},{"id":"http://arxiv.org/abs/2401.02719v1","updated":"2024-01-05T09:26:35Z","published":"2024-01-05T09:26:35Z","title":"Learning Image Demoireing from Unpaired Real Data","summary":"  This paper focuses on addressing the issue of image demoireing. Unlike the\nlarge volume of existing studies that rely on learning from paired real data,\nwe attempt to learn a demoireing model from unpaired real data, i.e., moire\nimages associated with irrelevant clean images. The proposed method, referred\nto as Unpaired Demoireing (UnDeM), synthesizes pseudo moire images from\nunpaired datasets, generating pairs with clean images for training demoireing\nmodels. To achieve this, we divide real moire images into patches and group\nthem in compliance with their moire complexity. We introduce a novel moire\ngeneration framework to synthesize moire images with diverse moire features,\nresembling real moire patches, and details akin to real moire-free images.\nAdditionally, we introduce an adaptive denoise method to eliminate the\nlow-quality pseudo moire images that adversely impact the learning of\ndemoireing models. We conduct extensive experiments on the commonly-used FHDMi\nand UHDM datasets. Results manifest that our UnDeM performs better than\nexisting methods when using existing demoireing models such as MBCNN and\nESDNet-L. Code: https://github.com/zysxmu/UnDeM\n","authors":["Yunshan Zhong","Yuyao Zhou","Yuxin Zhang","Fei Chao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2401.02719v1.pdf","comment":"AAAI2024"},{"id":"http://arxiv.org/abs/2401.02717v1","updated":"2024-01-05T09:21:45Z","published":"2024-01-05T09:21:45Z","title":"Complementary Information Mutual Learning for Multimodality Medical\n  Image Segmentation","summary":"  Radiologists must utilize multiple modal images for tumor segmentation and\ndiagnosis due to the limitations of medical imaging and the diversity of tumor\nsignals. This leads to the development of multimodal learning in segmentation.\nHowever, the redundancy among modalities creates challenges for existing\nsubtraction-based joint learning methods, such as misjudging the importance of\nmodalities, ignoring specific modal information, and increasing cognitive load.\nThese thorny issues ultimately decrease segmentation accuracy and increase the\nrisk of overfitting. This paper presents the complementary information mutual\nlearning (CIML) framework, which can mathematically model and address the\nnegative impact of inter-modal redundant information. CIML adopts the idea of\naddition and removes inter-modal redundant information through inductive\nbias-driven task decomposition and message passing-based redundancy filtering.\nCIML first decomposes the multimodal segmentation task into multiple subtasks\nbased on expert prior knowledge, minimizing the information dependence between\nmodalities. Furthermore, CIML introduces a scheme in which each modality can\nextract information from other modalities additively through message passing.\nTo achieve non-redundancy of extracted information, the redundant filtering is\ntransformed into complementary information learning inspired by the variational\ninformation bottleneck. The complementary information learning procedure can be\nefficiently solved by variational inference and cross-modal spatial attention.\nNumerical results from the verification task and standard benchmarks indicate\nthat CIML efficiently removes redundant information between modalities,\noutperforming SOTA methods regarding validation accuracy and segmentation\neffect.\n","authors":["Chuyun Shen","Wenhao Li","Haoqing Chen","Xiaoling Wang","Fengping Zhu","Yuxin Li","Xiangfeng Wang","Bo Jin"],"pdf_url":"https://arxiv.org/pdf/2401.02717v1.pdf","comment":"35 pages, 18 figures"},{"id":"http://arxiv.org/abs/2312.11153v2","updated":"2024-01-05T08:41:06Z","published":"2023-12-18T12:46:35Z","title":"Research on Multilingual Natural Scene Text Detection Algorithm","summary":"  Natural scene text detection is a significant challenge in computer vision,\nwith tremendous potential applications in multilingual, diverse, and complex\ntext scenarios. We propose a multilingual text detection model to address the\nissues of low accuracy and high difficulty in detecting multilingual text in\nnatural scenes. In response to the challenges posed by multilingual text images\nwith multiple character sets and various font styles, we introduce the SFM Swin\nTransformer feature extraction network to enhance the model's robustness in\ndetecting characters and fonts across different languages. Dealing with the\nconsiderable variation in text scales and complex arrangements in natural scene\ntext images, we present the AS-HRFPN feature fusion network by incorporating an\nAdaptive Spatial Feature Fusion module and a Spatial Pyramid Pooling module.\nThe feature fusion network improvements enhance the model's ability to detect\ntext sizes and orientations. Addressing diverse backgrounds and font variations\nin multilingual scene text images is a challenge for existing methods. Limited\nlocal receptive fields hinder detection performance. To overcome this, we\npropose a Global Semantic Segmentation Branch, extracting and preserving global\nfeatures for more effective text detection, aligning with the need for\ncomprehensive information. In this study, we collected and built a real-world\nmultilingual natural scene text image dataset and conducted comprehensive\nexperiments and analyses. The experimental results demonstrate that the\nproposed algorithm achieves an F-measure of 85.02\\%, which is 4.71\\% higher\nthan the baseline model. We also conducted extensive cross-dataset validation\non MSRA-TD500, ICDAR2017MLT, and ICDAR2015 datasets to verify the generality of\nour approach. The code and dataset can be found at\nhttps://github.com/wangmelon/CEMLT.\n","authors":["Tao Wang"],"pdf_url":"https://arxiv.org/pdf/2312.11153v2.pdf","comment":"Sorry, we discovered certain mistake and asked that the current\n  version be removed in order to perform a thorough reanalysis"},{"id":"http://arxiv.org/abs/2401.02702v1","updated":"2024-01-05T08:10:49Z","published":"2024-01-05T08:10:49Z","title":"VoxelNextFusion: A Simple, Unified and Effective Voxel Fusion Framework\n  for Multi-Modal 3D Object Detection","summary":"  LiDAR-camera fusion can enhance the performance of 3D object detection by\nutilizing complementary information between depth-aware LiDAR points and\nsemantically rich images. Existing voxel-based methods face significant\nchallenges when fusing sparse voxel features with dense image features in a\none-to-one manner, resulting in the loss of the advantages of images, including\nsemantic and continuity information, leading to sub-optimal detection\nperformance, especially at long distances. In this paper, we present\nVoxelNextFusion, a multi-modal 3D object detection framework specifically\ndesigned for voxel-based methods, which effectively bridges the gap between\nsparse point clouds and dense images. In particular, we propose a voxel-based\nimage pipeline that involves projecting point clouds onto images to obtain both\npixel- and patch-level features. These features are then fused using a\nself-attention to obtain a combined representation. Moreover, to address the\nissue of background features present in patches, we propose a feature\nimportance module that effectively distinguishes between foreground and\nbackground features, thus minimizing the impact of the background features.\nExtensive experiments were conducted on the widely used KITTI and nuScenes 3D\nobject detection benchmarks. Notably, our VoxelNextFusion achieved around\n+3.20% in AP@0.7 improvement for car detection in hard level compared to the\nVoxel R-CNN baseline on the KITTI test dataset\n","authors":["Ziying Song","Guoxin Zhang","Jun Xie","Lin Liu","Caiyan Jia","Shaoqing Xu","Zhepeng Wang"],"pdf_url":"https://arxiv.org/pdf/2401.02702v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02695v1","updated":"2024-01-05T08:05:07Z","published":"2024-01-05T08:05:07Z","title":"VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language\n  Model","summary":"  In the realm of household robotics, the Zero-Shot Object Navigation (ZSON)\ntask empowers agents to adeptly traverse unfamiliar environments and locate\nobjects from novel categories without prior explicit training. This paper\nintroduces VoroNav, a novel semantic exploration framework that proposes the\nReduced Voronoi Graph to extract exploratory paths and planning nodes from a\nsemantic map constructed in real time. By harnessing topological and semantic\ninformation, VoroNav designs text-based descriptions of paths and images that\nare readily interpretable by a large language model (LLM). Our approach\npresents a synergy of path and farsight descriptions to represent the\nenvironmental context, enabling the LLM to apply commonsense reasoning to\nascertain the optimal waypoints for navigation. Extensive evaluation on the\nHM3D and HSSD datasets validates that VoroNav surpasses existing ZSON\nbenchmarks in both success rates and exploration efficiency (+2.8% Success and\n+3.7% SPL on HM3D, +2.6% Success and +3.8% SPL on HSSD). Additionally\nintroduced metrics that evaluate obstacle avoidance proficiency and perceptual\nefficiency further corroborate the enhancements achieved by our method in ZSON\nplanning.\n","authors":["Pengying Wu","Yao Mu","Bingxian Wu","Yi Hou","Ji Ma","Shanghang Zhang","Chang Liu"],"pdf_url":"https://arxiv.org/pdf/2401.02695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02687v1","updated":"2024-01-05T07:37:51Z","published":"2024-01-05T07:37:51Z","title":"PAHD: Perception-Action based Human Decision Making using Explainable\n  Graph Neural Networks on SAR Images","summary":"  Synthetic Aperture Radar (SAR) images are commonly utilized in military\napplications for automatic target recognition (ATR). Machine learning (ML)\nmethods, such as Convolutional Neural Networks (CNN) and Graph Neural Networks\n(GNN), are frequently used to identify ground-based objects, including battle\ntanks, personnel carriers, and missile launchers. Determining the vehicle\nclass, such as the BRDM2 tank, BMP2 tank, BTR60 tank, and BTR70 tank, is\ncrucial, as it can help determine whether the target object is an ally or an\nenemy. While the ML algorithm provides feedback on the recognized target, the\nfinal decision is left to the commanding officers. Therefore, providing\ndetailed information alongside the identified target can significantly impact\ntheir actions. This detailed information includes the SAR image features that\ncontributed to the classification, the classification confidence, and the\nprobability of the identified object being classified as a different object\ntype or class. We propose a GNN-based ATR framework that provides the final\nclassified class and outputs the detailed information mentioned above. This is\nthe first study to provide a detailed analysis of the classification class,\nmaking final decisions more straightforward. Moreover, our GNN framework\nachieves an overall accuracy of 99.2\\% when evaluated on the MSTAR dataset,\nimproving over previous state-of-the-art GNN methods.\n","authors":["Sasindu Wijeratne","Bingyi Zhang","Rajgopal Kannan","Viktor Prasanna","Carl Busart"],"pdf_url":"https://arxiv.org/pdf/2401.02687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02677v1","updated":"2024-01-05T07:21:46Z","published":"2024-01-05T07:21:46Z","title":"Progressive Knowledge Distillation Of Stable Diffusion XL Using Layer\n  Level Loss","summary":"  Stable Diffusion XL (SDXL) has become the best open source text-to-image\nmodel (T2I) for its versatility and top-notch image quality. Efficiently\naddressing the computational demands of SDXL models is crucial for wider reach\nand applicability. In this work, we introduce two scaled-down variants, Segmind\nStable Diffusion (SSD-1B) and Segmind-Vega, with 1.3B and 0.74B parameter\nUNets, respectively, achieved through progressive removal using layer-level\nlosses focusing on reducing the model size while preserving generative quality.\nWe release these models weights at https://hf.co/Segmind. Our methodology\ninvolves the elimination of residual networks and transformer blocks from the\nU-Net structure of SDXL, resulting in significant reductions in parameters, and\nlatency. Our compact models effectively emulate the original SDXL by\ncapitalizing on transferred knowledge, achieving competitive results against\nlarger multi-billion parameter SDXL. Our work underscores the efficacy of\nknowledge distillation coupled with layer-level losses in reducing model size\nwhile preserving the high-quality generative capabilities of SDXL, thus\nfacilitating more accessible deployment in resource-constrained environments.\n","authors":["Yatharth Gupta","Vishnu V. Jaddipal","Harish Prabhala","Sayak Paul","Patrick Von Platen"],"pdf_url":"https://arxiv.org/pdf/2401.02677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01386v2","updated":"2024-01-05T07:12:41Z","published":"2024-01-01T19:58:36Z","title":"Tissue Artifact Segmentation and Severity Analysis for Automated\n  Diagnosis Using Whole Slide Images","summary":"  Traditionally, pathological analysis and diagnosis are performed by manually\neyeballing glass slide specimens under a microscope by an expert. The whole\nslide image is the digital specimen produced from the glass slide. Whole slide\nimage enabled specimens to be observed on a computer screen and led to\ncomputational pathology where computer vision and artificial intelligence are\nutilized for automated analysis and diagnosis. With the current computational\nadvancement, the entire whole slide image can be analyzed autonomously without\nhuman supervision. However, the analysis could fail or lead to wrong diagnosis\nif the whole slide image is affected by tissue artifacts such as tissue fold or\nair bubbles depending on the severity. Existing artifact detection methods rely\non experts for severity assessment to eliminate artifact affected regions from\nthe analysis. This process is time consuming, exhausting and undermines the\ngoal of automated analysis or removal of artifacts without evaluating their\nseverity, which could result in the loss of diagnostically important data.\nTherefore, it is necessary to detect artifacts and then assess their severity\nautomatically. In this paper, we propose a system that incorporates severity\nevaluation with artifact detection utilizing convolutional neural networks. The\nproposed system uses DoubleUNet to segment artifacts and an ensemble network of\nsix fine tuned convolutional neural network models to determine severity. This\nmethod outperformed current state of the art in accuracy by 9 percent for\nartifact segmentation and achieved a strong correlation of 97 percent with the\nevaluation of pathologists for severity assessment. The robustness of the\nsystem was demonstrated using our proposed heterogeneous dataset and practical\nusability was ensured by integrating it with an automated analysis system.\n","authors":["Galib Muhammad Shahriar Himel"],"pdf_url":"https://arxiv.org/pdf/2401.01386v2.pdf","comment":"60 pages, 21 figures, 16 tables"},{"id":"http://arxiv.org/abs/2303.04940v4","updated":"2024-01-05T07:04:09Z","published":"2023-03-08T23:23:44Z","title":"Non-aligned supervision for Real Image Dehazing","summary":"  Removing haze from real-world images is challenging due to unpredictable\nweather conditions, resulting in the misalignment of hazy and clear image\npairs. In this paper, we propose an innovative dehazing framework that operates\nunder non-aligned supervision. This framework is grounded in the atmospheric\nscattering model, and consists of three interconnected networks: dehazing,\nairlight, and transmission networks. In particular, we explore a non-alignment\nscenario that a clear reference image, unaligned with the input hazy image, is\nutilized to supervise the dehazing network. To implement this, we present a\nmulti-scale reference loss that compares the feature representations between\nthe referred image and the dehazed output. Our scenario makes it easier to\ncollect hazy/clear image pairs in real-world environments, even under\nconditions of misalignment and shift views. To showcase the effectiveness of\nour scenario, we have collected a new hazy dataset including 415 image pairs\ncaptured by mobile Phone in both rural and urban areas, called \"Phone-Hazy\".\nFurthermore, we introduce a self-attention network based on mean and variance\nfor modeling real infinite airlight, using the dark channel prior as positional\nguidance. Additionally, a channel attention network is employed to estimate the\nthree-channel transmission. Experimental results demonstrate the superior\nperformance of our framework over existing state-of-the-art techniques in the\nreal-world image dehazing task. Phone-Hazy and code will be available at\nhttps://fanjunkai1.github.io/projectpage/NSDNet/index.html.\n","authors":["Junkai Fan","Fei Guo","Jianjun Qian","Xiang Li","Jun Li","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2303.04940v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.04512v2","updated":"2024-01-05T06:51:15Z","published":"2023-06-07T15:25:27Z","title":"Cross-attention learning enables real-time nonuniform rotational\n  distortion correction in OCT","summary":"  Nonuniform rotational distortion (NURD) correction is vital for endoscopic\noptical coherence tomography (OCT) imaging and its functional extensions, such\nas angiography and elastography. Current NURD correction methods require\ntime-consuming feature tracking or cross-correlation calculations and thus\nsacrifice temporal resolution. Here we propose a cross-attention learning\nmethod for the NURD correction in OCT. Our method is inspired by the recent\nsuccess of the self-attention mechanism in natural language processing and\ncomputer vision. By leveraging its ability to model long-range dependencies, we\ncan directly obtain the correlation between OCT A-lines at any distance, thus\naccelerating the NURD correction. We develop an end-to-end stacked\ncross-attention network and design three types of optimization constraints. We\ncompare our method with two traditional feature-based methods and a CNN-based\nmethod, on two publicly-available endoscopic OCT datasets and a private dataset\ncollected on our home-built endoscopic OCT system. Our method achieved a\n$\\sim3\\times$ speedup to real time ($26\\pm 3$ fps), and superior correction\nperformance.\n","authors":["Haoran Zhang","Jianlong Yang","Jingqian Zhang","Shiqing Zhao","Aili Zhang"],"pdf_url":"https://arxiv.org/pdf/2306.04512v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.16742v2","updated":"2024-01-05T06:47:45Z","published":"2023-08-31T14:00:47Z","title":"Unsupervised CT Metal Artifact Reduction by Plugging Diffusion Priors in\n  Dual Domains","summary":"  During the process of computed tomography (CT), metallic implants often cause\ndisruptive artifacts in the reconstructed images, impeding accurate diagnosis.\nSeveral supervised deep learning-based approaches have been proposed for\nreducing metal artifacts (MAR). However, these methods heavily rely on training\nwith simulated data, as obtaining paired metal artifact CT and clean CT data in\nclinical settings is challenging. This limitation can lead to decreased\nperformance when applying these methods in clinical practice. Existing\nunsupervised MAR methods, whether based on learning or not, typically operate\nwithin a single domain, either in the image domain or the sinogram domain. In\nthis paper, we propose an unsupervised MAR method based on the diffusion model,\na generative model with a high capacity to represent data distributions.\nSpecifically, we first train a diffusion model using CT images without metal\nartifacts. Subsequently, we iteratively utilize the priors embedded within the\npre-trained diffusion model in both the sinogram and image domains to restore\nthe degraded portions caused by metal artifacts. This dual-domain processing\nempowers our approach to outperform existing unsupervised MAR methods,\nincluding another MAR method based on the diffusion model, which we have\nqualitatively and quantitatively validated using synthetic datasets. Moreover,\nour method demonstrates superior visual results compared to both supervised and\nunsupervised methods on clinical datasets.\n","authors":["Xuan Liu","Yaoqin Xie","Songhui Diao","Shan Tan","Xiaokun Liang"],"pdf_url":"https://arxiv.org/pdf/2308.16742v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02656v1","updated":"2024-01-05T06:24:41Z","published":"2024-01-05T06:24:41Z","title":"GTA: Guided Transfer of Spatial Attention from Object-Centric\n  Representations","summary":"  Utilizing well-trained representations in transfer learning often results in\nsuperior performance and faster convergence compared to training from scratch.\nHowever, even if such good representations are transferred, a model can easily\noverfit the limited training dataset and lose the valuable properties of the\ntransferred representations. This phenomenon is more severe in ViT due to its\nlow inductive bias. Through experimental analysis using attention maps in ViT,\nwe observe that the rich representations deteriorate when trained on a small\ndataset. Motivated by this finding, we propose a novel and simple\nregularization method for ViT called Guided Transfer of spatial Attention\n(GTA). Our proposed method regularizes the self-attention maps between the\nsource and target models. A target model can fully exploit the knowledge\nrelated to object localization properties through this explicit regularization.\nOur experimental results show that the proposed GTA consistently improves the\naccuracy across five benchmark datasets especially when the number of training\ndata is small.\n","authors":["SeokHyun Seo","Jinwoo Hong","JungWoo Chae","Kyungyul Kim","Sangheum Hwang"],"pdf_url":"https://arxiv.org/pdf/2401.02656v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.07728v3","updated":"2024-01-05T06:22:32Z","published":"2023-08-15T12:08:43Z","title":"Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability","summary":"  Fine-tuning pre-trained neural network models has become a widely adopted\napproach across various domains. However, it can lead to the distortion of\npre-trained feature extractors that already possess strong generalization\ncapabilities. Mitigating feature distortion during adaptation to new target\ndomains is crucial. Recent studies have shown promising results in handling\nfeature distortion by aligning the head layer on in-distribution datasets\nbefore performing fine-tuning. Nonetheless, a significant limitation arises\nfrom the treatment of batch normalization layers during fine-tuning, leading to\nsuboptimal performance. In this paper, we propose Domain-Aware Fine-Tuning\n(DAFT), a novel approach that incorporates batch normalization conversion and\nthe integration of linear probing and fine-tuning. Our batch normalization\nconversion method effectively mitigates feature distortion by reducing\nmodifications to the neural network during fine-tuning. Additionally, we\nintroduce the integration of linear probing and fine-tuning to optimize the\nhead layer with gradual adaptation of the feature extractor. By leveraging\nbatch normalization layers and integrating linear probing and fine-tuning, our\nDAFT significantly mitigates feature distortion and achieves improved model\nperformance on both in-distribution and out-of-distribution datasets. Extensive\nexperiments demonstrate that our method outperforms other baseline methods,\ndemonstrating its effectiveness in not only improving performance but also\nmitigating feature distortion.\n","authors":["Seokhyeon Ha","Sunbeom Jung","Jungwoo Lee"],"pdf_url":"https://arxiv.org/pdf/2308.07728v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02361v2","updated":"2024-01-05T06:21:19Z","published":"2024-01-04T17:00:49Z","title":"An Open and Comprehensive Pipeline for Unified Object Grounding and\n  Detection","summary":"  Grounding-DINO is a state-of-the-art open-set detection model that tackles\nmultiple vision tasks including Open-Vocabulary Detection (OVD), Phrase\nGrounding (PG), and Referring Expression Comprehension (REC). Its effectiveness\nhas led to its widespread adoption as a mainstream architecture for various\ndownstream applications. However, despite its significance, the original\nGrounding-DINO model lacks comprehensive public technical details due to the\nunavailability of its training code. To bridge this gap, we present\nMM-Grounding-DINO, an open-source, comprehensive, and user-friendly baseline,\nwhich is built with the MMDetection toolbox. It adopts abundant vision datasets\nfor pre-training and various detection and grounding datasets for fine-tuning.\nWe give a comprehensive analysis of each reported result and detailed settings\nfor reproduction. The extensive experiments on the benchmarks mentioned\ndemonstrate that our MM-Grounding-DINO-Tiny outperforms the Grounding-DINO-Tiny\nbaseline. We release all our models to the research community. Codes and\ntrained models are released at\nhttps://github.com/open-mmlab/mmdetection/tree/main/configs/mm_grounding_dino.\n","authors":["Xiangyu Zhao","Yicheng Chen","Shilin Xu","Xiangtai Li","Xinjiang Wang","Yining Li","Haian Huang"],"pdf_url":"https://arxiv.org/pdf/2401.02361v2.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2311.11700v3","updated":"2024-01-05T06:18:04Z","published":"2023-11-20T12:08:23Z","title":"GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting","summary":"  In this paper, we introduce $\\textbf{GS-SLAM}$ that first utilizes 3D\nGaussian representation in the Simultaneous Localization and Mapping (SLAM)\nsystem. It facilitates a better balance between efficiency and accuracy.\nCompared to recent SLAM methods employing neural implicit representations, our\nmethod utilizes a real-time differentiable splatting rendering pipeline that\noffers significant speedup to map optimization and RGB-D re-rendering.\nSpecifically, we propose an adaptive expansion strategy that adds new or\ndeletes noisy 3D Gaussian in order to efficiently reconstruct new observed\nscene geometry and improve the mapping of previously observed areas. This\nstrategy is essential to extend 3D Gaussian representation to reconstruct the\nwhole scene rather than synthesize a static object in existing methods.\nMoreover, in the pose tracking process, an effective coarse-to-fine technique\nis designed to select reliable 3D Gaussian representations to optimize camera\npose, resulting in runtime reduction and robust estimation. Our method achieves\ncompetitive performance compared with existing state-of-the-art real-time\nmethods on the Replica, TUM-RGBD datasets. The source code will be released\nsoon.\n","authors":["Chi Yan","Delin Qu","Dong Wang","Dan Xu","Zhigang Wang","Bin Zhao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2311.11700v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02651v1","updated":"2024-01-05T05:58:22Z","published":"2024-01-05T05:58:22Z","title":"Benchmarking PathCLIP for Pathology Image Analysis","summary":"  Accurate image classification and retrieval are of importance for clinical\ndiagnosis and treatment decision-making. The recent contrastive language-image\npretraining (CLIP) model has shown remarkable proficiency in understanding\nnatural images. Drawing inspiration from CLIP, PathCLIP is specifically\ndesigned for pathology image analysis, utilizing over 200,000 image and text\npairs in training. While the performance the PathCLIP is impressive, its\nrobustness under a wide range of image corruptions remains unknown. Therefore,\nwe conduct an extensive evaluation to analyze the performance of PathCLIP on\nvarious corrupted images from the datasets of Osteosarcoma and WSSS4LUAD. In\nour experiments, we introduce seven corruption types including brightness,\ncontrast, Gaussian blur, resolution, saturation, hue, and markup at four\nseverity levels. Through experiments, we find that PathCLIP is relatively\nrobustness to image corruptions and surpasses OpenAI-CLIP and PLIP in zero-shot\nclassification. Among the seven corruptions, blur and resolution can cause\nserver performance degradation of the PathCLIP. This indicates that ensuring\nthe quality of images is crucial before conducting a clinical test.\nAdditionally, we assess the robustness of PathCLIP in the task of image-image\nretrieval, revealing that PathCLIP performs less effectively than PLIP on\nOsteosarcoma but performs better on WSSS4LUAD under diverse corruptions.\nOverall, PathCLIP presents impressive zero-shot classification and retrieval\nperformance for pathology images, but appropriate care needs to be taken when\nusing it. We hope this study provides a qualitative impression of PathCLIP and\nhelps understand its differences from other CLIP models.\n","authors":["Sunyi Zheng","Xiaonan Cui","Yuxuan Sun","Jingxiong Li","Honglin Li","Yunlong Zhang","Pingyi Chen","Xueping Jing","Zhaoxiang Ye","Lin Yang"],"pdf_url":"https://arxiv.org/pdf/2401.02651v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02649v1","updated":"2024-01-05T05:40:59Z","published":"2024-01-05T05:40:59Z","title":"Enhancing 3D-Air Signature by Pen Tip Tail Trajectory Awareness: Dataset\n  and Featuring by Novel Spatio-temporal CNN","summary":"  This work proposes a novel process of using pen tip and tail 3D trajectory\nfor air signature. To acquire the trajectories we developed a new pen tool and\na stereo camera was used. We proposed SliT-CNN, a novel 2D spatial-temporal\nconvolutional neural network (CNN) for better featuring of the air signature.\nIn addition, we also collected an air signature dataset from $45$ signers.\nSkilled forgery signatures per user are also collected. A detailed benchmarking\nof the proposed dataset using existing techniques and proposed CNN on existing\nand proposed dataset exhibit the effectiveness of our methodology.\n","authors":["Saurabh Atreya","Maheswar Bora","Aritra Mukherjee","Abhijit Das"],"pdf_url":"https://arxiv.org/pdf/2401.02649v1.pdf","comment":"Accepted and presented in IJCB 2023"},{"id":"http://arxiv.org/abs/2401.02646v1","updated":"2024-01-05T05:32:37Z","published":"2024-01-05T05:32:37Z","title":"Recent Advancement in 3D Biometrics using Monocular Camera","summary":"  Recent literature has witnessed significant interest towards 3D biometrics\nemploying monocular vision for robust authentication methods. Motivated by\nthis, in this work we seek to provide insight on recent development in the area\nof 3D biometrics employing monocular vision. We present the similarity and\ndissimilarity of 3D monocular biometrics and classical biometrics, listing the\nstrengths and challenges. Further, we provide an overview of recent techniques\nin 3D biometrics with monocular vision, as well as application systems adopted\nby the industry. Finally, we discuss open research problems in this area of\nresearch\n","authors":["Aritra Mukherjee","Abhijit Das"],"pdf_url":"https://arxiv.org/pdf/2401.02646v1.pdf","comment":"Accepted and presented in IJCB 2023"},{"id":"http://arxiv.org/abs/2312.01129v2","updated":"2024-01-05T05:03:27Z","published":"2023-12-02T13:04:54Z","title":"ControlDreamer: Stylized 3D Generation with Multi-View ControlNet","summary":"  Recent advancements in text-to-3D generation have significantly contributed\nto the automation and democratization of 3D content creation. Building upon\nthese developments, we aim to address the limitations of current methods in\ngenerating 3D models with creative geometry and styles. We introduce multi-view\nControlNet, a novel depth-aware multi-view diffusion model trained on generated\ndatasets from a carefully curated text corpus. Our multi-view ControlNet is\nthen integrated into our two-stage pipeline, ControlDreamer, enabling\ntext-guided generation of stylized 3D models. Additionally, we present a\ncomprehensive benchmark for 3D style editing, encompassing a broad range of\nsubjects, including objects, animals, and characters, to further facilitate\nresearch on diverse 3D generation. Our comparative analysis reveals that this\nnew pipeline outperforms existing text-to-3D methods as evidenced by human\nevaluations and CLIP score metrics.\n","authors":["Yeongtak Oh","Jooyoung Choi","Yongsung Kim","Minjun Park","Chaehun Shin","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2312.01129v2.pdf","comment":"Project page: https://controldreamer.github.io/"},{"id":"http://arxiv.org/abs/2401.02634v1","updated":"2024-01-05T04:53:33Z","published":"2024-01-05T04:53:33Z","title":"AG-ReID.v2: Bridging Aerial and Ground Views for Person\n  Re-identification","summary":"  Aerial-ground person re-identification (Re-ID) presents unique challenges in\ncomputer vision, stemming from the distinct differences in viewpoints, poses,\nand resolutions between high-altitude aerial and ground-based cameras. Existing\nresearch predominantly focuses on ground-to-ground matching, with aerial\nmatching less explored due to a dearth of comprehensive datasets. To address\nthis, we introduce AG-ReID.v2, a dataset specifically designed for person Re-ID\nin mixed aerial and ground scenarios. This dataset comprises 100,502 images of\n1,615 unique individuals, each annotated with matching IDs and 15 soft\nattribute labels. Data were collected from diverse perspectives using a UAV,\nstationary CCTV, and smart glasses-integrated camera, providing a rich variety\nof intra-identity variations. Additionally, we have developed an explainable\nattention network tailored for this dataset. This network features a\nthree-stream architecture that efficiently processes pairwise image distances,\nemphasizes key top-down features, and adapts to variations in appearance due to\naltitude differences. Comparative evaluations demonstrate the superiority of\nour approach over existing baselines. We plan to release the dataset and\nalgorithm source code publicly, aiming to advance research in this specialized\nfield of computer vision. For access, please visit\nhttps://github.com/huynguyen792/AG-ReID.v2.\n","authors":["Huy Nguyen","Kien Nguyen","Sridha Sridharan","Clinton Fookes"],"pdf_url":"https://arxiv.org/pdf/2401.02634v1.pdf","comment":"13 pages, Accepted by TIFS 2023"},{"id":"http://arxiv.org/abs/2401.02633v1","updated":"2024-01-05T04:43:14Z","published":"2024-01-05T04:43:14Z","title":"A Random Ensemble of Encrypted models for Enhancing Robustness against\n  Adversarial Examples","summary":"  Deep neural networks (DNNs) are well known to be vulnerable to adversarial\nexamples (AEs). In addition, AEs have adversarial transferability, which means\nAEs generated for a source model can fool another black-box model (target\nmodel) with a non-trivial probability. In previous studies, it was confirmed\nthat the vision transformer (ViT) is more robust against the property of\nadversarial transferability than convolutional neural network (CNN) models such\nas ConvMixer, and moreover encrypted ViT is more robust than ViT without any\nencryption. In this article, we propose a random ensemble of encrypted ViT\nmodels to achieve much more robust models. In experiments, the proposed scheme\nis verified to be more robust against not only black-box attacks but also\nwhite-box ones than convention methods.\n","authors":["Ryota Iijima","Sayaka Shiota","Hitoshi Kiya"],"pdf_url":"https://arxiv.org/pdf/2401.02633v1.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2312.10983v2","updated":"2024-01-05T04:36:43Z","published":"2023-12-18T07:11:45Z","title":"MatchDet: A Collaborative Framework for Image Matching and Object\n  Detection","summary":"  Image matching and object detection are two fundamental and challenging\ntasks, while many related applications consider them two individual tasks (i.e.\ntask-individual). In this paper, a collaborative framework called MatchDet\n(i.e. task-collaborative) is proposed for image matching and object detection\nto obtain mutual improvements. To achieve the collaborative learning of the two\ntasks, we propose three novel modules, including a Weighted Spatial Attention\nModule (WSAM) for Detector, and Weighted Attention Module (WAM) and Box Filter\nfor Matcher. Specifically, the WSAM highlights the foreground regions of target\nimage to benefit the subsequent detector, the WAM enhances the connection\nbetween the foreground regions of pair images to ensure high-quality matches,\nand Box Filter mitigates the impact of false matches. We evaluate the\napproaches on a new benchmark with two datasets called Warp-COCO and\nminiScanNet. Experimental results show our approaches are effective and achieve\ncompetitive improvements.\n","authors":["Jinxiang Lai","Wenlong Wu","Bin-Bin Gao","Jun Liu","Jiawei Zhan","Congchong Nie","Yi Zeng","Chengjie Wang"],"pdf_url":"https://arxiv.org/pdf/2312.10983v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02616v1","updated":"2024-01-05T03:23:38Z","published":"2024-01-05T03:23:38Z","title":"FED-NeRF: Achieve High 3D Consistency and Temporal Coherence for Face\n  Video Editing on Dynamic NeRF","summary":"  The success of the GAN-NeRF structure has enabled face editing on NeRF to\nmaintain 3D view consistency. However, achieving simultaneously multi-view\nconsistency and temporal coherence while editing video sequences remains a\nformidable challenge. This paper proposes a novel face video editing\narchitecture built upon the dynamic face GAN-NeRF structure, which effectively\nutilizes video sequences to restore the latent code and 3D face geometry. By\nediting the latent code, multi-view consistent editing on the face can be\nensured, as validated by multiview stereo reconstruction on the resulting\nedited images in our dynamic NeRF. As the estimation of face geometries occurs\non a frame-by-frame basis, this may introduce a jittering issue. We propose a\nstabilizer that maintains temporal coherence by preserving smooth changes of\nface expressions in consecutive frames. Quantitative and qualitative analyses\nreveal that our method, as the pioneering 4D face video editor, achieves\nstate-of-the-art performance in comparison to existing 2D or 3D-based\napproaches independently addressing identity and motion. Codes will be\nreleased.\n","authors":["Hao Zhang","Yu-Wing Tai","Chi-Keung Tang"],"pdf_url":"https://arxiv.org/pdf/2401.02616v1.pdf","comment":"Our code will be available at: https://github.com/ZHANG1023/FED-NeRF"},{"id":"http://arxiv.org/abs/2401.02614v1","updated":"2024-01-05T03:12:03Z","published":"2024-01-05T03:12:03Z","title":"Scaling and Masking: A New Paradigm of Data Sampling for Image and Video\n  Quality Assessment","summary":"  Quality assessment of images and videos emphasizes both local details and\nglobal semantics, whereas general data sampling methods (e.g., resizing,\ncropping or grid-based fragment) fail to catch them simultaneously. To address\nthe deficiency, current approaches have to adopt multi-branch models and take\nas input the multi-resolution data, which burdens the model complexity. In this\nwork, instead of stacking up models, a more elegant data sampling method (named\nas SAMA, scaling and masking) is explored, which compacts both the local and\nglobal content in a regular input size. The basic idea is to scale the data\ninto a pyramid first, and reduce the pyramid into a regular data dimension with\na masking strategy. Benefiting from the spatial and temporal redundancy in\nimages and videos, the processed data maintains the multi-scale characteristics\nwith a regular input size, thus can be processed by a single-branch model. We\nverify the sampling method in image and video quality assessment. Experiments\nshow that our sampling method can improve the performance of current\nsingle-branch models significantly, and achieves competitive performance to the\nmulti-branch models without extra model complexity. The source code will be\navailable at https://github.com/Sissuire/SAMA.\n","authors":["Yongxu Liu","Yinghui Quan","Guoyao Xiao","Aobo Li","Jinjian Wu"],"pdf_url":"https://arxiv.org/pdf/2401.02614v1.pdf","comment":"Accepted by AAAI2024. Code has been released at\n  https://github.com/Sissuire/SAMA"},{"id":"http://arxiv.org/abs/2401.02309v2","updated":"2024-01-05T03:11:28Z","published":"2024-01-04T14:55:57Z","title":"TR-DETR: Task-Reciprocal Transformer for Joint Moment Retrieval and\n  Highlight Detection","summary":"  Video moment retrieval (MR) and highlight detection (HD) based on natural\nlanguage queries are two highly related tasks, which aim to obtain relevant\nmoments within videos and highlight scores of each video clip. Recently,\nseveral methods have been devoted to building DETR-based networks to solve both\nMR and HD jointly. These methods simply add two separate task heads after\nmulti-modal feature extraction and feature interaction, achieving good\nperformance. Nevertheless, these approaches underutilize the reciprocal\nrelationship between two tasks. In this paper, we propose a task-reciprocal\ntransformer based on DETR (TR-DETR) that focuses on exploring the inherent\nreciprocity between MR and HD. Specifically, a local-global multi-modal\nalignment module is first built to align features from diverse modalities into\na shared latent space. Subsequently, a visual feature refinement is designed to\neliminate query-irrelevant information from visual features for modal\ninteraction. Finally, a task cooperation module is constructed to refine the\nretrieval pipeline and the highlight score prediction process by utilizing the\nreciprocity between MR and HD. Comprehensive experiments on QVHighlights,\nCharades-STA and TVSum datasets demonstrate that TR-DETR outperforms existing\nstate-of-the-art methods. Codes are available at\n\\url{https://github.com/mingyao1120/TR-DETR}.\n","authors":["Hao Sun","Mingyao Zhou","Wenjing Chen","Wei Xie"],"pdf_url":"https://arxiv.org/pdf/2401.02309v2.pdf","comment":"Accepted by AAAI-24"},{"id":"http://arxiv.org/abs/2401.02611v1","updated":"2024-01-05T02:57:58Z","published":"2024-01-05T02:57:58Z","title":"MOODv2: Masked Image Modeling for Out-of-Distribution Detection","summary":"  The crux of effective out-of-distribution (OOD) detection lies in acquiring a\nrobust in-distribution (ID) representation, distinct from OOD samples. While\nprevious methods predominantly leaned on recognition-based techniques for this\npurpose, they often resulted in shortcut learning, lacking comprehensive\nrepresentations. In our study, we conducted a comprehensive analysis, exploring\ndistinct pretraining tasks and employing various OOD score functions. The\nresults highlight that the feature representations pre-trained through\nreconstruction yield a notable enhancement and narrow the performance gap among\nvarious score functions. This suggests that even simple score functions can\nrival complex ones when leveraging reconstruction-based pretext tasks.\nReconstruction-based pretext tasks adapt well to various score functions. As\nsuch, it holds promising potential for further expansion. Our OOD detection\nframework, MOODv2, employs the masked image modeling pretext task. Without\nbells and whistles, MOODv2 impressively enhances 14.30% AUROC to 95.68% on\nImageNet and achieves 99.98% on CIFAR-10.\n","authors":["Jingyao Li","Pengguang Chen","Shaozuo Yu","Shu Liu","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2401.02611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02610v1","updated":"2024-01-05T02:54:23Z","published":"2024-01-05T02:54:23Z","title":"DHGCN: Dynamic Hop Graph Convolution Network for Self-supervised Point\n  Cloud Learning","summary":"  Recent works attempt to extend Graph Convolution Networks (GCNs) to point\nclouds for classification and segmentation tasks. These works tend to sample\nand group points to create smaller point sets locally and mainly focus on\nextracting local features through GCNs, while ignoring the relationship between\npoint sets. In this paper, we propose the Dynamic Hop Graph Convolution Network\n(DHGCN) for explicitly learning the contextual relationships between the\nvoxelized point parts, which are treated as graph nodes. Motivated by the\nintuition that the contextual information between point parts lies in the\npairwise adjacent relationship, which can be depicted by the hop distance of\nthe graph quantitatively, we devise a novel self-supervised part-level hop\ndistance reconstruction task and design a novel loss function accordingly to\nfacilitate training. In addition, we propose the Hop Graph Attention (HGA),\nwhich takes the learned hop distance as input for producing attention weights\nto allow edge features to contribute distinctively in aggregation. Eventually,\nthe proposed DHGCN is a plug-and-play module that is compatible with\npoint-based backbone networks. Comprehensive experiments on different backbones\nand tasks demonstrate that our self-supervised method achieves state-of-the-art\nperformance. Our source code is available at: https://github.com/Jinec98/DHGCN.\n","authors":["Jincen Jiang","Lizhi Zhao","Xuequan Lu","Wei Hu","Imran Razzak","Meili Wang"],"pdf_url":"https://arxiv.org/pdf/2401.02610v1.pdf","comment":"Accepted to AAAI 2024"},{"id":"http://arxiv.org/abs/2309.16924v3","updated":"2024-01-05T02:49:43Z","published":"2023-09-29T01:51:04Z","title":"Incremental Rotation Averaging Revisited and More: A New Rotation\n  Averaging Benchmark","summary":"  In order to further advance the accuracy and robustness of the incremental\nparameter estimation-based rotation averaging methods, in this paper, a new\nmember of the Incremental Rotation Averaging (IRA) family is introduced, which\nis termed as IRAv4. As the most significant feature of the IRAv4, a\ntask-specific connected dominating set is extracted to serve as a more reliable\nand accurate reference for rotation global alignment. In addition, to further\naddress the limitations of the existing rotation averaging benchmark of relying\non the slightly outdated Bundler camera calibration results as ground truths\nand focusing solely on rotation estimation accuracy, this paper presents a new\nCOLMAP-based rotation averaging benchmark that incorporates a cross check\nbetween COLMAP and Bundler, and employ the accuracy of both rotation and\ndownstream location estimation as evaluation metrics, which is desired to\nprovide a more reliable and comprehensive evaluation tool for the rotation\naveraging research. Comprehensive comparisons between the proposed IRAv4 and\nother mainstream rotation averaging methods on this new benchmark demonstrate\nthe effectiveness of our proposed approach.\n","authors":["Xiang Gao","Hainan Cui","Shuhan Shen"],"pdf_url":"https://arxiv.org/pdf/2309.16924v3.pdf","comment":"Submitted to IEEE Transactions"},{"id":"http://arxiv.org/abs/2401.02607v1","updated":"2024-01-05T02:46:08Z","published":"2024-01-05T02:46:08Z","title":"Partition-based Nonrigid Registration for 3D Face Model","summary":"  This paper presents a partition-based surface registration for 3D morphable\nmodel(3DMM). In the 3DMM, it often requires to warp a handcrafted template\nmodel into different captured models. The proposed method first utilizes the\nlandmarks to partition the template model then scale each part and finally\nsmooth the boundaries. This method is especially effective when the disparity\nbetween the template model and the target model is huge. The experiment result\nshows the method perform well than the traditional warp method and robust to\nthe local minima.\n","authors":["Yuping Ye","Zhan Song","Juan Zhao"],"pdf_url":"https://arxiv.org/pdf/2401.02607v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02606v1","updated":"2024-01-05T02:34:53Z","published":"2024-01-05T02:34:53Z","title":"Exploiting Polarized Material Cues for Robust Car Detection","summary":"  Car detection is an important task that serves as a crucial prerequisite for\nmany automated driving functions. The large variations in lighting/weather\nconditions and vehicle densities of the scenes pose significant challenges to\nexisting car detection algorithms to meet the highly accurate perception demand\nfor safety, due to the unstable/limited color information, which impedes the\nextraction of meaningful/discriminative features of cars. In this work, we\npresent a novel learning-based car detection method that leverages trichromatic\nlinear polarization as an additional cue to disambiguate such challenging\ncases. A key observation is that polarization, characteristic of the light\nwave, can robustly describe intrinsic physical properties of the scene objects\nin various imaging conditions and is strongly linked to the nature of materials\nfor cars (e.g., metal and glass) and their surrounding environment (e.g., soil\nand trees), thereby providing reliable and discriminative features for robust\ncar detection in challenging scenes. To exploit polarization cues, we first\nconstruct a pixel-aligned RGB-Polarization car detection dataset, which we\nsubsequently employ to train a novel multimodal fusion network. Our car\ndetection network dynamically integrates RGB and polarization features in a\nrequest-and-complement manner and can explore the intrinsic material properties\nof cars across all learning samples. We extensively validate our method and\ndemonstrate that it outperforms state-of-the-art detection methods.\nExperimental results show that polarization is a powerful cue for car\ndetection.\n","authors":["Wen Dong","Haiyang Mei","Ziqi Wei","Ao Jin","Sen Qiu","Qiang Zhang","Xin Yang"],"pdf_url":"https://arxiv.org/pdf/2401.02606v1.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2211.12735v2","updated":"2024-01-05T02:05:52Z","published":"2022-11-23T06:56:12Z","title":"Fast-iTPN: Integrally Pre-Trained Transformer Pyramid Network with Token\n  Migration","summary":"  We propose integrally pre-trained transformer pyramid network (iTPN), towards\njointly optimizing the network backbone and the neck, so that transfer gap\nbetween representation models and downstream tasks is minimal. iTPN is born\nwith two elaborated designs: 1) The first pre-trained feature pyramid upon\nvision transformer (ViT). 2) Multi-stage supervision to the feature pyramid\nusing masked feature modeling (MFM). iTPN is updated to Fast-iTPN, reducing\ncomputational memory overhead and accelerating inference through two flexible\ndesigns. 1) Token migration: dropping redundant tokens of the backbone while\nreplenishing them in the feature pyramid without attention operations. 2) Token\ngathering: reducing computation cost caused by global attention by introducing\nfew gathering tokens. The base/large-level Fast-iTPN achieve 88.75%/89.5% top-1\naccuracy on ImageNet-1K. With 1x training schedule using DINO, the\nbase/large-level Fast-iTPN achieves 58.4%/58.8% box AP on COCO object\ndetection, and a 57.5%/58.7% mIoU on ADE20K semantic segmentation using\nMaskDINO. Fast-iTPN can accelerate the inference procedure by up to 70%, with\nnegligible performance loss, demonstrating the potential to be a powerful\nbackbone for downstream vision tasks. The code is available at:\ngithub.com/sunsmarterjie/iTPN.\n","authors":["Yunjie Tian","Lingxi Xie","Jihao Qiu","Jianbin Jiao","Yaowei Wang","Qi Tian","Qixiang Ye"],"pdf_url":"https://arxiv.org/pdf/2211.12735v2.pdf","comment":"The tiny/small/base-level models report new records on ImageNet-1K.\n  Code: github.com/sunsmarterjie/iTPN"},{"id":"http://arxiv.org/abs/2309.06023v3","updated":"2024-01-05T02:04:04Z","published":"2023-09-12T07:50:54Z","title":"Learning from History: Task-agnostic Model Contrastive Learning for\n  Image Restoration","summary":"  Contrastive learning has emerged as a prevailing paradigm for high-level\nvision tasks, which, by introducing properly negative samples, has also been\nexploited for low-level vision tasks to achieve a compact optimization space to\naccount for their ill-posed nature. However, existing methods rely on manually\npredefined and task-oriented negatives, which often exhibit pronounced\ntask-specific biases. To address this challenge, our paper introduces an\ninnovative method termed 'learning from history', which dynamically generates\nnegative samples from the target model itself. Our approach, named Model\nContrastive paradigm for Image Restoration (MCIR), rejuvenates latency models\nas negative models, making it compatible with diverse image restoration tasks.\nWe propose the Self-Prior guided Negative loss (SPN) to enable it. This\napproach significantly enhances existing models when retrained with the\nproposed model contrastive paradigm. The results show significant improvements\nin image restoration across various tasks and architectures. For example,\nmodels retrained with SPN outperform the original FFANet and DehazeFormer by\n3.41 dB and 0.57 dB on the RESIDE indoor dataset for image dehazing. Similarly,\nthey achieve notable improvements of 0.47 dB on SPA-Data over IDT for image\nderaining and 0.12 dB on Manga109 for a 4x scale super-resolution over\nlightweight SwinIR, respectively. Code and retrained models are available at\nhttps://github.com/Aitical/MCIR.\n","authors":["Gang Wu","Junjun Jiang","Kui Jiang","Xianming Liu"],"pdf_url":"https://arxiv.org/pdf/2309.06023v3.pdf","comment":"Camera Ready Version. Accepted to The 38th Annual AAAI Conference on\n  Artificial Intelligence (AAAI 2024)"},{"id":"http://arxiv.org/abs/2308.04074v3","updated":"2024-01-05T02:03:52Z","published":"2023-08-08T06:16:37Z","title":"Exploiting Spatial-Temporal Context for Interacting Hand Reconstruction\n  on Monocular RGB Video","summary":"  Reconstructing interacting hands from monocular RGB data is a challenging\ntask, as it involves many interfering factors, e.g. self- and mutual occlusion\nand similar textures. Previous works only leverage information from a single\nRGB image without modeling their physically plausible relation, which leads to\ninferior reconstruction results. In this work, we are dedicated to explicitly\nexploiting spatial-temporal information to achieve better interacting hand\nreconstruction. On one hand, we leverage temporal context to complement\ninsufficient information provided by the single frame, and design a novel\ntemporal framework with a temporal constraint for interacting hand motion\nsmoothness. On the other hand, we further propose an interpenetration detection\nmodule to produce kinetically plausible interacting hands without physical\ncollisions. Extensive experiments are performed to validate the effectiveness\nof our proposed framework, which achieves new state-of-the-art performance on\npublic benchmarks.\n","authors":["Weichao Zhao","Hezhen Hu","Wengang Zhou","Li li","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2308.04074v3.pdf","comment":"Accepted by TOMM 2023"},{"id":"http://arxiv.org/abs/2311.12144v7","updated":"2024-01-05T01:58:58Z","published":"2023-11-20T19:45:27Z","title":"Applications of Large Scale Foundation Models for Autonomous Driving","summary":"  Since DARPA Grand Challenges (rural) in 2004/05 and Urban Challenges in 2007,\nautonomous driving has been the most active field of AI applications. Recently\npowered by large language models (LLMs), chat systems, such as chatGPT and\nPaLM, emerge and rapidly become a promising direction to achieve artificial\ngeneral intelligence (AGI) in natural language processing (NLP). There comes a\nnatural thinking that we could employ these abilities to reformulate autonomous\ndriving. By combining LLM with foundation models, it is possible to utilize the\nhuman knowledge, commonsense and reasoning to rebuild autonomous driving\nsystems from the current long-tailed AI dilemma. In this paper, we investigate\nthe techniques of foundation models and LLMs applied for autonomous driving,\ncategorized as simulation, world model, data annotation and planning or E2E\nsolutions etc.\n","authors":["Yu Huang","Yue Chen","Zhu Li"],"pdf_url":"https://arxiv.org/pdf/2311.12144v7.pdf","comment":"23 pages. A survey paper"},{"id":"http://arxiv.org/abs/2401.02600v1","updated":"2024-01-05T01:52:13Z","published":"2024-01-05T01:52:13Z","title":"Object-oriented backdoor attack against image captioning","summary":"  Backdoor attack against image classification task has been widely studied and\nproven to be successful, while there exist little research on the backdoor\nattack against vision-language models. In this paper, we explore backdoor\nattack towards image captioning models by poisoning training data. Assuming the\nattacker has total access to the training dataset, and cannot intervene in\nmodel construction or training process. Specifically, a portion of benign\ntraining samples is randomly selected to be poisoned. Afterwards, considering\nthat the captions are usually unfolded around objects in an image, we design an\nobject-oriented method to craft poisons, which aims to modify pixel values by a\nslight range with the modification number proportional to the scale of the\ncurrent detected object region. After training with the poisoned data, the\nattacked model behaves normally on benign images, but for poisoned images, the\nmodel will generate some sentences irrelevant to the given image. The attack\ncontrols the model behavior on specific test images without sacrificing the\ngeneration performance on benign test images. Our method proves the weakness of\nimage captioning models to backdoor attack and we hope this work can raise the\nawareness of defending against backdoor attack in the image captioning field.\n","authors":["Meiling Li","Nan Zhong","Xinpeng Zhang","Zhenxing Qian","Sheng Li"],"pdf_url":"https://arxiv.org/pdf/2401.02600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.13018v3","updated":"2024-01-05T01:50:52Z","published":"2023-11-21T21:48:51Z","title":"GeoLocator: a location-integrated large multimodal model for inferring\n  geo-privacy","summary":"  Geographic privacy or geo-privacy refers to the keeping private of one's\ngeographic location, especially the restriction of geographical data maintained\nby personal electronic devices. Geo-privacy is a crucial aspect of personal\nsecurity; however, it often goes unnoticed in daily activities. With the surge\nin the use of Large Multimodal Models (LMMs), such as GPT-4, for Open Source\nIntelligence (OSINT), the potential risks associated with geo-privacy breaches\nhave intensified. This study develops a location-integrated GPT-4 based model\nnamed GeoLocator and designs four-dimensional experiments to demonstrate its\ncapability in inferring the locational information of input imageries and/or\nsocial media contents. Our experiments reveal that GeoLocator generates\nspecific geographic details with high accuracy and consequently embeds the risk\nof the model users exposing geospatial information to the public\nunintentionally, highlighting the thread of online data sharing, information\ngathering technologies and LLMs on geo-privacy. We conclude with the broader\nimplications of GeoLocator and our findings for individuals and the community\nat large, by emphasizing the urgency for enhanced awareness and protective\nmeasures against geo-privacy leakage in the era of advanced AI and widespread\nsocial media usage.\n","authors":["Yifan Yang","Siqin Wang","Daoyang Li","Yixian Zhang","Shuju Sun","Junzhou He"],"pdf_url":"https://arxiv.org/pdf/2311.13018v3.pdf","comment":"16pages, 2 figures"},{"id":"http://arxiv.org/abs/2308.15752v3","updated":"2024-01-05T01:50:38Z","published":"2023-08-30T04:29:48Z","title":"Large-scale data extraction from the UNOS organ donor documents","summary":"  In this paper we focus on three major task: 1) discussing our methods: Our\nmethod captures a portion of the data in DCD flowsheets, kidney perfusion data,\nand Flowsheet data captured peri-organ recovery surgery. 2) demonstrating the\nresult: We built a comprehensive, analyzable database from 2022 OPTN data. This\ndataset is by far larger than any previously available even in this preliminary\nphase; and 3) proving that our methods can be extended to all the past OPTN\ndata and future data.\n  The scope of our study is all Organ Procurement and Transplantation Network\n(OPTN) data of the USA organ donors since 2008. The data was not analyzable in\na large scale in the past because it was captured in PDF documents known as\n``Attachments'', whereby every donor's information was recorded into dozens of\nPDF documents in heterogeneous formats. To make the data analyzable, one needs\nto convert the content inside these PDFs to an analyzable data format, such as\na standard SQL database. In this paper we will focus on 2022 OPTN data, which\nconsists of $\\approx 400,000$ PDF documents spanning millions of pages. The\nentire OPTN data covers 15 years (2008--20022). This paper assumes that readers\nare familiar with the content of the OPTN data.\n","authors":["Marek Rychlik","Bekir Tanriover","Yan Han"],"pdf_url":"https://arxiv.org/pdf/2308.15752v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02588v1","updated":"2024-01-05T00:49:56Z","published":"2024-01-05T00:49:56Z","title":"Characterizing Satellite Geometry via Accelerated 3D Gaussian Splatting","summary":"  The accelerating deployment of spacecraft in orbit have generated interest in\non-orbit servicing (OOS), inspection of spacecraft, and active debris removal\n(ADR). Such missions require precise rendezvous and proximity operations in the\nvicinity of non-cooperative, possible unknown, resident space objects. Safety\nconcerns with manned missions and lag times with ground-based control\nnecessitate complete autonomy. This requires robust characterization of the\ntarget's geometry. In this article, we present an approach for mapping\ngeometries of satellites on orbit based on 3D Gaussian Splatting that can run\non computing resources available on current spaceflight hardware. We\ndemonstrate model training and 3D rendering performance on a\nhardware-in-the-loop satellite mock-up under several realistic lighting and\nmotion conditions. Our model is shown to be capable of training on-board and\nrendering higher quality novel views of an unknown satellite nearly 2 orders of\nmagnitude faster than previous NeRF-based algorithms. Such on-board\ncapabilities are critical to enable downstream machine intelligence tasks\nnecessary for autonomous guidance, navigation, and control tasks.\n","authors":["Van Minh Nguyen","Emma Sandidge","Trupti Mahendrakar","Ryan T. White"],"pdf_url":"https://arxiv.org/pdf/2401.02588v1.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.15065v3","updated":"2024-01-05T00:48:20Z","published":"2023-03-27T10:18:42Z","title":"Single-subject Multi-contrast MRI Super-resolution via Implicit Neural\n  Representations","summary":"  Clinical routine and retrospective cohorts commonly include multi-parametric\nMagnetic Resonance Imaging; however, they are mostly acquired in different\nanisotropic 2D views due to signal-to-noise-ratio and scan-time constraints.\nThus acquired views suffer from poor out-of-plane resolution and affect\ndownstream volumetric image analysis that typically requires isotropic 3D\nscans. Combining different views of multi-contrast scans into high-resolution\nisotropic 3D scans is challenging due to the lack of a large training cohort,\nwhich calls for a subject-specific framework. This work proposes a novel\nsolution to this problem leveraging Implicit Neural Representations (INR). Our\nproposed INR jointly learns two different contrasts of complementary views in a\ncontinuous spatial function and benefits from exchanging anatomical information\nbetween them. Trained within minutes on a single commodity GPU, our model\nprovides realistic super-resolution across different pairs of contrasts in our\nexperiments with three datasets. Using Mutual Information (MI) as a metric, we\nfind that our model converges to an optimum MI amongst sequences, achieving\nanatomically faithful reconstruction. Code is available at:\nhttps://github.com/jqmcginnis/multi_contrast_inr/\n","authors":["Julian McGinnis","Suprosanna Shit","Hongwei Bran Li","Vasiliki Sideri-Lampretsa","Robert Graf","Maik Dannecker","Jiazhen Pan","Nil Stolt Ansó","Mark Mühlau","Jan S. Kirschke","Daniel Rueckert","Benedikt Wiestler"],"pdf_url":"https://arxiv.org/pdf/2303.15065v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12910v2","updated":"2024-01-05T00:44:21Z","published":"2023-08-24T16:35:35Z","title":"SCoRD: Subject-Conditional Relation Detection with Text-Augmented Data","summary":"  We propose Subject-Conditional Relation Detection SCoRD, where conditioned on\nan input subject, the goal is to predict all its relations to other objects in\na scene along with their locations. Based on the Open Images dataset, we\npropose a challenging OIv6-SCoRD benchmark such that the training and testing\nsplits have a distribution shift in terms of the occurrence statistics of\n$\\langle$subject, relation, object$\\rangle$ triplets. To solve this problem, we\npropose an auto-regressive model that given a subject, it predicts its\nrelations, objects, and object locations by casting this output as a sequence\nof tokens. First, we show that previous scene-graph prediction methods fail to\nproduce as exhaustive an enumeration of relation-object pairs when conditioned\non a subject on this benchmark. Particularly, we obtain a recall@3 of 83.8% for\nour relation-object predictions compared to the 49.75% obtained by a recent\nscene graph detector. Then, we show improved generalization on both\nrelation-object and object-box predictions by leveraging during training\nrelation-object pairs obtained automatically from textual captions and for\nwhich no object-box annotations are available. Particularly, for\n$\\langle$subject, relation, object$\\rangle$ triplets for which no object\nlocations are available during training, we are able to obtain a recall@3 of\n33.80% for relation-object pairs and 26.75% for their box locations.\n","authors":["Ziyan Yang","Kushal Kafle","Zhe Lin","Scott Cohen","Zhihong Ding","Vicente Ordonez"],"pdf_url":"https://arxiv.org/pdf/2308.12910v2.pdf","comment":"WACV 2024"},{"id":"http://arxiv.org/abs/2401.02582v1","updated":"2024-01-05T00:26:07Z","published":"2024-01-05T00:26:07Z","title":"CoCoT: Contrastive Chain-of-Thought Prompting for Large Multimodal\n  Models with Multiple Image Inputs","summary":"  When exploring the development of Artificial General Intelligence (AGI), a\ncritical task for these models involves interpreting and processing information\nfrom multiple image inputs. However, Large Multimodal Models (LMMs) encounter\ntwo issues in such scenarios: (1) a lack of fine-grained perception, and (2) a\ntendency to blend information across multiple images. We first extensively\ninvestigate the capability of LMMs to perceive fine-grained visual details when\ndealing with multiple input images. The research focuses on two aspects: first,\nimage-to-image matching (to evaluate whether LMMs can effectively reason and\npair relevant images), and second, multi-image-to-text matching (to assess\nwhether LMMs can accurately capture and summarize detailed image information).\nWe conduct evaluations on a range of both open-source and closed-source large\nmodels, including GPT-4V, Gemini, OpenFlamingo, and MMICL. To enhance model\nperformance, we further develop a Contrastive Chain-of-Thought (CoCoT)\nprompting approach based on multi-input multimodal models. This method requires\nLMMs to compare the similarities and differences among multiple image inputs,\nand then guide the models to answer detailed questions about multi-image inputs\nbased on the identified similarities and differences. Our experimental results\nshowcase CoCoT's proficiency in enhancing the multi-image comprehension\ncapabilities of large multimodal models.\n","authors":["Daoan Zhang","Junming Yang","Hanjia Lyu","Zijian Jin","Yuan Yao","Mingkai Chen","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2401.02582v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2401.02913v1","updated":"2024-01-05T17:38:40Z","published":"2024-01-05T17:38:40Z","title":"Plug-in Diffusion Model for Sequential Recommendation","summary":"  Pioneering efforts have verified the effectiveness of the diffusion models in\nexploring the informative uncertainty for recommendation. Considering the\ndifference between recommendation and image synthesis tasks, existing methods\nhave undertaken tailored refinements to the diffusion and reverse process.\nHowever, these approaches typically use the highest-score item in corpus for\nuser interest prediction, leading to the ignorance of the user's generalized\npreference contained within other items, thereby remaining constrained by the\ndata sparsity issue. To address this issue, this paper presents a novel Plug-in\nDiffusion Model for Recommendation (PDRec) framework, which employs the\ndiffusion model as a flexible plugin to jointly take full advantage of the\ndiffusion-generating user preferences on all items. Specifically, PDRec first\ninfers the users' dynamic preferences on all items via a time-interval\ndiffusion model and proposes a Historical Behavior Reweighting (HBR) mechanism\nto identify the high-quality behaviors and suppress noisy behaviors. In\naddition to the observed items, PDRec proposes a Diffusion-based Positive\nAugmentation (DPA) strategy to leverage the top-ranked unobserved items as the\npotential positive samples, bringing in informative and diverse soft signals to\nalleviate data sparsity. To alleviate the false negative sampling issue, PDRec\nemploys Noise-free Negative Sampling (NNS) to select stable negative samples\nfor ensuring effective model optimization. Extensive experiments and analyses\non four datasets have verified the superiority of the proposed PDRec over the\nstate-of-the-art baselines and showcased the universality of PDRec as a\nflexible plugin for commonly-used sequential encoders in different\nrecommendation scenarios. The code is available in\nhttps://github.com/hulkima/PDRec.\n","authors":["Haokai Ma","Ruobing Xie","Lei Meng","Xin Chen","Xu Zhang","Leyu Lin","Zhanhui Kang"],"pdf_url":"https://arxiv.org/pdf/2401.02913v1.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2401.02827v1","updated":"2024-01-05T14:21:10Z","published":"2024-01-05T14:21:10Z","title":"Let's Get It Started: Fostering the Discoverability of New Releases on\n  Deezer","summary":"  This paper presents our recent initiatives to foster the discoverability of\nnew releases on the music streaming service Deezer. After introducing our\nsearch and recommendation features dedicated to new releases, we outline our\nshift from editorial to personalized release suggestions using cold start\nembeddings and contextual bandits. Backed by online experiments, we discuss the\nadvantages of this shift in terms of recommendation quality and exposure of new\nreleases on the service.\n","authors":["Léa Briand","Théo Bontempelli","Walid Bendada","Mathieu Morlon","François Rigaud","Benjamin Chapus","Thomas Bouabça","Guillaume Salha-Galvan"],"pdf_url":"https://arxiv.org/pdf/2401.02827v1.pdf","comment":"Accepted for presentation as an \"Industry Talk\" at the 46th European\n  Conference on Information Retrieval (ECIR 2024)"},{"id":"http://arxiv.org/abs/2401.02823v1","updated":"2024-01-05T14:15:36Z","published":"2024-01-05T14:15:36Z","title":"DocGraphLM: Documental Graph Language Model for Information Extraction","summary":"  Advances in Visually Rich Document Understanding (VrDU) have enabled\ninformation extraction and question answering over documents with complex\nlayouts. Two tropes of architectures have emerged -- transformer-based models\ninspired by LLMs, and Graph Neural Networks. In this paper, we introduce\nDocGraphLM, a novel framework that combines pre-trained language models with\ngraph semantics. To achieve this, we propose 1) a joint encoder architecture to\nrepresent documents, and 2) a novel link prediction approach to reconstruct\ndocument graphs. DocGraphLM predicts both directions and distances between\nnodes using a convergent joint loss function that prioritizes neighborhood\nrestoration and downweighs distant node detection. Our experiments on three\nSotA datasets show consistent improvement on IE and QA tasks with the adoption\nof graph features. Moreover, we report that adopting the graph features\naccelerates convergence in the learning process during training, despite being\nsolely constructed through link prediction.\n","authors":["Dongsheng Wang","Zhiqiang Ma","Armineh Nourbakhsh","Kang Gu","Sameena Shah"],"pdf_url":"https://arxiv.org/pdf/2401.02823v1.pdf","comment":"Published at SIGIR'23 (repost for easier access)"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2304.05292v4","updated":"2024-01-05T18:59:41Z","published":"2023-04-11T15:42:20Z","title":"MC-ViViT: Multi-branch Classifier-ViViT to detect Mild Cognitive\n  Impairment in older adults using facial videos","summary":"  Deep machine learning models including Convolutional Neural Networks (CNN)\nhave been successful in the detection of Mild Cognitive Impairment (MCI) using\nmedical images, questionnaires, and videos. This paper proposes a novel\nMulti-branch Classifier-Video Vision Transformer (MC-ViViT) model to\ndistinguish MCI from those with normal cognition by analyzing facial features.\nThe data comes from the I-CONECT, a behavioral intervention trial aimed at\nimproving cognitive function by providing frequent video chats. MC-ViViT\nextracts spatiotemporal features of videos in one branch and augments\nrepresentations by the MC module. The I-CONECT dataset is challenging as the\ndataset is imbalanced containing Hard-Easy and Positive-Negative samples, which\nimpedes the performance of MC-ViViT. We propose a loss function for Hard-Easy\nand Positive-Negative Samples (HP Loss) by combining Focal loss and AD-CORRE\nloss to address the imbalanced problem. Our experimental results on the\nI-CONECT dataset show the great potential of MC-ViViT in predicting MCI with a\nhigh accuracy of 90.63% accuracy on some of the interview videos.\n","authors":["Jian Sun","Hiroko H. Dodge","Mohammad H. Mahoor"],"pdf_url":"https://arxiv.org/pdf/2304.05292v4.pdf","comment":"13 pages, 7 tables, 7 figures, 9 equations"},{"id":"http://arxiv.org/abs/2401.02954v1","updated":"2024-01-05T18:59:13Z","published":"2024-01-05T18:59:13Z","title":"DeepSeek LLM: Scaling Open-Source Language Models with Longtermism","summary":"  The rapid development of open-source large language models (LLMs) has been\ntruly remarkable. However, the scaling law described in previous literature\npresents varying conclusions, which casts a dark cloud over scaling LLMs. We\ndelve into the study of scaling laws and present our distinctive findings that\nfacilitate scaling of large scale models in two commonly used open-source\nconfigurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek\nLLM, a project dedicated to advancing open-source language models with a\nlong-term perspective. To support the pre-training phase, we have developed a\ndataset that currently consists of 2 trillion tokens and is continuously\nexpanding. We further conduct supervised fine-tuning (SFT) and Direct\nPreference Optimization (DPO) on DeepSeek LLM Base models, resulting in the\ncreation of DeepSeek Chat models. Our evaluation results demonstrate that\nDeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in\nthe domains of code, mathematics, and reasoning. Furthermore, open-ended\nevaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance\ncompared to GPT-3.5.\n","authors":[" DeepSeek-AI"," :","Xiao Bi","Deli Chen","Guanting Chen","Shanhuang Chen","Damai Dai","Chengqi Deng","Honghui Ding","Kai Dong","Qiushi Du","Zhe Fu","Huazuo Gao","Kaige Gao","Wenjun Gao","Ruiqi Ge","Kang Guan","Daya Guo","Jianzhong Guo","Guangbo Hao","Zhewen Hao","Ying He","Wenjie Hu","Panpan Huang","Erhang Li","Guowei Li","Jiashi Li","Yao Li","Y. K. Li","Wenfeng Liang","Fangyun Lin","A. X. Liu","Bo Liu","Wen Liu","Xiaodong Liu","Xin Liu","Yiyuan Liu","Haoyu Lu","Shanghao Lu","Fuli Luo","Shirong Ma","Xiaotao Nie","Tian Pei","Yishi Piao","Junjie Qiu","Hui Qu","Tongzheng Ren","Zehui Ren","Chong Ruan","Zhangli Sha","Zhihong Shao","Junxiao Song","Xuecheng Su","Jingxiang Sun","Yaofeng Sun","Minghui Tang","Bingxuan Wang","Peiyi Wang","Shiyu Wang","Yaohui Wang","Yongji Wang","Tong Wu","Y. Wu","Xin Xie","Zhenda Xie","Ziwei Xie","Yiliang Xiong","Hanwei Xu","R. X. Xu","Yanhong Xu","Dejian Yang","Yuxiang You","Shuiping Yu","Xingkai Yu","B. Zhang","Haowei Zhang","Lecong Zhang","Liyue Zhang","Mingchuan Zhang","Minghua Zhang","Wentao Zhang","Yichao Zhang","Chenggang Zhao","Yao Zhao","Shangyan Zhou","Shunfeng Zhou","Qihao Zhu","Yuheng Zou"],"pdf_url":"https://arxiv.org/pdf/2401.02954v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02950v1","updated":"2024-01-05T18:52:35Z","published":"2024-01-05T18:52:35Z","title":"The Tactician's Web of Large-Scale Formal Knowledge","summary":"  The Tactician's Web is a platform offering a large web of strongly\ninterconnected, machine-checked, formal mathematical knowledge conveniently\npackaged for machine learning, analytics, and proof engineering. Built on top\nof the Coq proof assistant, the platform exports a dataset containing a wide\nvariety of formal theories, presented as a web of definitions, theorems, proof\nterms, tactics, and proof states. Theories are encoded both as a semantic graph\n(rendered below) and as human-readable text, each with a unique set of\nadvantages and disadvantages. Proving agents may interact with Coq through the\nsame rich data representation and can be automatically benchmarked on a set of\ntheorems. Tight integration with Coq provides the unique possibility to make\nagents available to proof engineers as practical tools.\n","authors":["Lasse Blaauwbroek"],"pdf_url":"https://arxiv.org/pdf/2401.02950v1.pdf","comment":"47 pages"},{"id":"http://arxiv.org/abs/2401.02949v1","updated":"2024-01-05T18:52:09Z","published":"2024-01-05T18:52:09Z","title":"Graph2Tac: Learning Hierarchical Representations of Math Concepts in\n  Theorem proving","summary":"  Concepts abound in mathematics and its applications. They vary greatly\nbetween subject areas, and new ones are introduced in each mathematical paper\nor application. A formal theory builds a hierarchy of definitions, theorems and\nproofs that reference each other. When an AI agent is proving a new theorem,\nmost of the mathematical concepts and lemmas relevant to that theorem may have\nnever been seen during training. This is especially true in the Coq proof\nassistant, which has a diverse library of Coq projects, each with its own\ndefinitions, lemmas, and even custom tactic procedures used to prove those\nlemmas. It is essential for agents to incorporate such new information into\ntheir knowledge base on the fly. We work towards this goal by utilizing a new,\nlarge-scale, graph-based dataset for machine learning in Coq. We leverage a\nfaithful graph-representation of Coq terms that induces a directed graph of\ndependencies between definitions to create a novel graph neural network,\nGraph2Tac (G2T), that takes into account not only the current goal, but also\nthe entire hierarchy of definitions that led to the current goal. G2T is an\nonline model that is deeply integrated into the users' workflow and can adapt\nin real time to new Coq projects and their definitions. It complements well\nwith other online models that learn in real time from new proof scripts. Our\nnovel definition embedding task, which is trained to compute representations of\nmathematical concepts not seen during training, boosts the performance of the\nneural network to rival state-of-the-art k-nearest neighbor predictors.\n","authors":["Jason Rute","Miroslav Olšák","Lasse Blaauwbroek","Fidel Ivan Schaposnik Massolo","Jelle Piepenbrock","Vasily Pestun"],"pdf_url":"https://arxiv.org/pdf/2401.02949v1.pdf","comment":"32 pages"},{"id":"http://arxiv.org/abs/2401.02940v1","updated":"2024-01-05T18:31:00Z","published":"2024-01-05T18:31:00Z","title":"Digital-analog quantum learning on Rydberg atom arrays","summary":"  We propose hybrid digital-analog learning algorithms on Rydberg atom arrays,\ncombining the potentially practical utility and near-term realizability of\nquantum learning with the rapidly scaling architectures of neutral atoms. Our\nconstruction requires only single-qubit operations in the digital setting and\nglobal driving according to the Rydberg Hamiltonian in the analog setting. We\nperform a comprehensive numerical study of our algorithm on both classical and\nquantum data, given respectively by handwritten digit classification and\nunsupervised quantum phase boundary learning. We show in the two representative\nproblems that digital-analog learning is not only feasible in the near term,\nbut also requires shorter circuit depths and is more robust to realistic error\nmodels as compared to digital learning schemes. Our results suggest that\ndigital-analog learning opens a promising path towards improved variational\nquantum learning experiments in the near term.\n","authors":["Jonathan Z. Lu","Lucy Jiao","Kristina Wolinski","Milan Kornjača","Hong-Ye Hu","Sergio Cantu","Fangli Liu","Susanne F. Yelin","Sheng-Tao Wang"],"pdf_url":"https://arxiv.org/pdf/2401.02940v1.pdf","comment":"22 pages, 20 figures"},{"id":"http://arxiv.org/abs/2308.07688v4","updated":"2024-01-05T18:25:42Z","published":"2023-08-15T10:37:13Z","title":"Enhancing Network Initialization for Medical AI Models Using\n  Large-Scale, Unlabeled Natural Images","summary":"  Pre-training datasets, like ImageNet, have become the gold standard in\nmedical image analysis. However, the emergence of self-supervised learning\n(SSL), which leverages unlabeled data to learn robust features, presents an\nopportunity to bypass the intensive labeling process. In this study, we\nexplored if SSL for pre-training on non-medical images can be applied to chest\nradiographs and how it compares to supervised pre-training on non-medical\nimages and on medical images. We utilized a vision transformer and initialized\nits weights based on (i) SSL pre-training on natural images (DINOv2), (ii) SL\npre-training on natural images (ImageNet dataset), and (iii) SL pre-training on\nchest radiographs from the MIMIC-CXR database. We tested our approach on over\n800,000 chest radiographs from six large global datasets, diagnosing more than\n20 different imaging findings. Our SSL pre-training on curated images not only\noutperformed ImageNet-based pre-training (P<0.001 for all datasets) but, in\ncertain cases, also exceeded SL on the MIMIC-CXR dataset. Our findings suggest\nthat selecting the right pre-training strategy, especially with SSL, can be\npivotal for improving artificial intelligence (AI)'s diagnostic accuracy in\nmedical imaging. By demonstrating the promise of SSL in chest radiograph\nanalysis, we underline a transformative shift towards more efficient and\naccurate AI models in medical imaging.\n","authors":["Soroosh Tayebi Arasteh","Leo Misera","Jakob Nikolas Kather","Daniel Truhn","Sven Nebelung"],"pdf_url":"https://arxiv.org/pdf/2308.07688v4.pdf","comment":"Published in European Radiology Experimental"},{"id":"http://arxiv.org/abs/2401.02930v1","updated":"2024-01-05T18:15:19Z","published":"2024-01-05T18:15:19Z","title":"Dagma-DCE: Interpretable, Non-Parametric Differentiable Causal Discovery","summary":"  We introduce Dagma-DCE, an interpretable and model-agnostic scheme for\ndifferentiable causal discovery. Current non- or over-parametric methods in\ndifferentiable causal discovery use opaque proxies of ``independence'' to\njustify the inclusion or exclusion of a causal relationship. We show\ntheoretically and empirically that these proxies may be arbitrarily different\nthan the actual causal strength. Juxtaposed to existing differentiable causal\ndiscovery algorithms, \\textsc{Dagma-DCE} uses an interpretable measure of\ncausal strength to define weighted adjacency matrices. In a number of simulated\ndatasets, we show our method achieves state-of-the-art level performance. We\nadditionally show that \\textsc{Dagma-DCE} allows for principled thresholding\nand sparsity penalties by domain-experts. The code for our method is available\nopen-source at https://github.com/DanWaxman/DAGMA-DCE, and can easily be\nadapted to arbitrary differentiable models.\n","authors":["Daniel Waxman","Kurt Butler","Petar M. Djuric"],"pdf_url":"https://arxiv.org/pdf/2401.02930v1.pdf","comment":"9 pages, 2 figures. Accepted to the IEEE Open Journal of Signal\n  Processing"},{"id":"http://arxiv.org/abs/2401.02914v1","updated":"2024-01-05T17:39:00Z","published":"2024-01-05T17:39:00Z","title":"A unified uncertainty-aware exploration: Combining epistemic and\n  aleatory uncertainty","summary":"  Exploration is a significant challenge in practical reinforcement learning\n(RL), and uncertainty-aware exploration that incorporates the quantification of\nepistemic and aleatory uncertainty has been recognized as an effective\nexploration strategy. However, capturing the combined effect of aleatory and\nepistemic uncertainty for decision-making is difficult. Existing works estimate\naleatory and epistemic uncertainty separately and consider the composite\nuncertainty as an additive combination of the two. Nevertheless, the additive\nformulation leads to excessive risk-taking behavior, causing instability. In\nthis paper, we propose an algorithm that clarifies the theoretical connection\nbetween aleatory and epistemic uncertainty, unifies aleatory and epistemic\nuncertainty estimation, and quantifies the combined effect of both\nuncertainties for a risk-sensitive exploration. Our method builds on a novel\nextension of distributional RL that estimates a parameterized return\ndistribution whose parameters are random variables encoding epistemic\nuncertainty. Experimental results on tasks with exploration and risk challenges\nshow that our method outperforms alternative approaches.\n","authors":["Parvin Malekzadeh","Ming Hou","Konstantinos N. Plataniotis"],"pdf_url":"https://arxiv.org/pdf/2401.02914v1.pdf","comment":"Accepted by ICASSP2023"},{"id":"http://arxiv.org/abs/2301.06683v5","updated":"2024-01-05T17:18:56Z","published":"2023-01-17T03:53:29Z","title":"Surgical Aggregation: Federated Class-Heterogeneous Learning","summary":"  The release of numerous chest x-ray datasets has spearheaded the development\nof deep learning models with expert-level performance. However, they have\nlimited interoperability due to class-heterogeneity -- a result of inconsistent\nlabeling schemes and partial annotations. Therefore, it is challenging to\nleverage these datasets in aggregate to train models with a complete\nrepresentation of abnormalities that may occur within the thorax. In this work,\nwe propose surgical aggregation, a federated learning framework for aggregating\nknowledge from class-heterogeneous datasets and learn a model that can\nsimultaneously predict the presence of all disease labels present across the\ndatasets. We evaluate our method using simulated and real-world\nclass-heterogeneous datasets across both independent and identically\ndistributed (iid) and non-iid settings. Our results show that surgical\naggregation outperforms current methods, has better generalizability, and is a\ncrucial first step towards tackling class-heterogeneity in federated learning\nto facilitate the development of clinically-useful models using previously\nnon-interoperable chest x-ray datasets.\n","authors":["Pranav Kulkarni","Adway Kanhere","Paul H. Yi","Vishwa S. Parekh"],"pdf_url":"https://arxiv.org/pdf/2301.06683v5.pdf","comment":"9 pages, 7 figures, 4 tables"},{"id":"http://arxiv.org/abs/2401.02905v1","updated":"2024-01-05T17:05:33Z","published":"2024-01-05T17:05:33Z","title":"H2G2-Net: A Hierarchical Heterogeneous Graph Generative Network\n  Framework for Discovery of Multi-Modal Physiological Responses","summary":"  Discovering human cognitive and emotional states using multi-modal\nphysiological signals draws attention across various research applications.\nPhysiological responses of the human body are influenced by human cognition and\ncommonly used to analyze cognitive states. From a network science perspective,\nthe interactions of these heterogeneous physiological modalities in a graph\nstructure may provide insightful information to support prediction of cognitive\nstates. However, there is no clue to derive exact connectivity between\nheterogeneous modalities and there exists a hierarchical structure of\nsub-modalities. Existing graph neural networks are designed to learn on\nnon-hierarchical homogeneous graphs with pre-defined graph structures; they\nfailed to learn from hierarchical, multi-modal physiological data without a\npre-defined graph structure. To this end, we propose a hierarchical\nheterogeneous graph generative network (H2G2-Net) that automatically learns a\ngraph structure without domain knowledge, as well as a powerful representation\non the hierarchical heterogeneous graph in an end-to-end fashion. We validate\nthe proposed method on the CogPilot dataset that consists of multi-modal\nphysiological signals. Extensive experiments demonstrate that our proposed\nmethod outperforms the state-of-the-art GNNs by 5%-20% in prediction accuracy.\n","authors":["Haidong Gu","Nathan Gaw","Yinan Wang","Chancellor Johnstone","Christine Beauchene","Sophia Yuditskaya","Hrishikesh Rao","Chun-An Chou"],"pdf_url":"https://arxiv.org/pdf/2401.02905v1.pdf","comment":"Paper accepted in Human-Centric Representation Learning workshop at\n  AAAI 2024 (https://hcrl-workshop.github.io/2024/)"},{"id":"http://arxiv.org/abs/2401.02904v1","updated":"2024-01-05T17:05:14Z","published":"2024-01-05T17:05:14Z","title":"Class-wise Generalization Error: an Information-Theoretic Analysis","summary":"  Existing generalization theories of supervised learning typically take a\nholistic approach and provide bounds for the expected generalization over the\nwhole data distribution, which implicitly assumes that the model generalizes\nsimilarly for all the classes. In practice, however, there are significant\nvariations in generalization performance among different classes, which cannot\nbe captured by the existing generalization bounds. In this work, we tackle this\nproblem by theoretically studying the class-generalization error, which\nquantifies the generalization performance of each individual class. We derive a\nnovel information-theoretic bound for class-generalization error using the KL\ndivergence, and we further obtain several tighter bounds using the conditional\nmutual information (CMI), which are significantly easier to estimate in\npractice. We empirically validate our proposed bounds in different neural\nnetworks and show that they accurately capture the complex class-generalization\nerror behavior. Moreover, we show that the theoretical tools developed in this\npaper can be applied in several applications beyond this context.\n","authors":["Firas Laakom","Yuheng Bu","Moncef Gabbouj"],"pdf_url":"https://arxiv.org/pdf/2401.02904v1.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2401.02903v1","updated":"2024-01-05T17:04:43Z","published":"2024-01-05T17:04:43Z","title":"Deep Reinforcement Learning for Local Path Following of an Autonomous\n  Formula SAE Vehicle","summary":"  With the continued introduction of driverless events to Formula:Society of\nAutomotive Engineers (F:SAE) competitions around the world, teams are\ninvestigating all aspects of the autonomous vehicle stack. This paper presents\nthe use of Deep Reinforcement Learning (DRL) and Inverse Reinforcement Learning\n(IRL) to map locally-observed cone positions to a desired steering angle for\nrace track following. Two state-of-the-art algorithms not previously tested in\nthis context: soft actor critic (SAC) and adversarial inverse reinforcement\nlearning (AIRL), are used to train models in a representative simulation. Three\nnovel reward functions for use by RL algorithms in an autonomous racing context\nare also discussed. Tests performed in simulation and the real world suggest\nthat both algorithms can successfully train models for local path following.\nSuggestions for future work are presented to allow these models to scale to a\nfull F:SAE vehicle.\n","authors":["Harvey Merton","Thomas Delamore","Karl Stol","Henry Williams"],"pdf_url":"https://arxiv.org/pdf/2401.02903v1.pdf","comment":"As presented at the Australasian Conference on Robotics and\n  Automation (ACRA 2023)"},{"id":"http://arxiv.org/abs/2401.02902v1","updated":"2024-01-05T17:04:33Z","published":"2024-01-05T17:04:33Z","title":"State Derivative Normalization for Continuous-Time Deep Neural Networks","summary":"  The importance of proper data normalization for deep neural networks is well\nknown. However, in continuous-time state-space model estimation, it has been\nobserved that improper normalization of either the hidden state or hidden state\nderivative of the model estimate, or even of the time interval can lead to\nnumerical and optimization challenges with deep learning based methods. This\nresults in a reduced model quality. In this contribution, we show that these\nthree normalization tasks are inherently coupled. Due to the existence of this\ncoupling, we propose a solution to all three normalization challenges by\nintroducing a normalization constant at the state derivative level. We show\nthat the appropriate choice of the normalization constant is related to the\ndynamics of the to-be-identified system and we derive multiple methods of\nobtaining an effective normalization constant. We compare and discuss all the\nnormalization strategies on a benchmark problem based on experimental data from\na cascaded tanks system and compare our results with other methods of the\nidentification literature.\n","authors":["Jonas Weigand","Gerben I. Beintema","Jonas Ulmen","Daniel Görges","Roland Tóth","Maarten Schoukens","Martin Ruskowski"],"pdf_url":"https://arxiv.org/pdf/2401.02902v1.pdf","comment":"This work has been submitted to the 20th IFAC Symposium on System\n  Identification (SYSID2024) for possible publication"},{"id":"http://arxiv.org/abs/2401.02890v1","updated":"2024-01-05T16:43:39Z","published":"2024-01-05T16:43:39Z","title":"Nonlinear functional regression by functional deep neural network with\n  kernel embedding","summary":"  With the rapid development of deep learning in various fields of science and\ntechnology, such as speech recognition, image classification, and natural\nlanguage processing, recently it is also widely applied in the functional data\nanalysis (FDA) with some empirical success. However, due to the infinite\ndimensional input, we need a powerful dimension reduction method for functional\nlearning tasks, especially for the nonlinear functional regression. In this\npaper, based on the idea of smooth kernel integral transformation, we propose a\nfunctional deep neural network with an efficient and fully data-dependent\ndimension reduction method. The architecture of our functional net consists of\na kernel embedding step: an integral transformation with a data-dependent\nsmooth kernel; a projection step: a dimension reduction by projection with\neigenfunction basis based on the embedding kernel; and finally an expressive\ndeep ReLU neural network for the prediction. The utilization of smooth kernel\nembedding enables our functional net to be discretization invariant, efficient,\nand robust to noisy observations, capable of utilizing information in both\ninput functions and responses data, and have a low requirement on the number of\ndiscrete points for an unimpaired generalization performance. We conduct\ntheoretical analysis including approximation error and generalization error\nanalysis, and numerical simulations to verify these advantages of our\nfunctional net.\n","authors":["Zhongjie Shi","Jun Fan","Linhao Song","Ding-Xuan Zhou","Johan A. K. Suykens"],"pdf_url":"https://arxiv.org/pdf/2401.02890v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.04443v2","updated":"2024-01-05T16:40:44Z","published":"2022-12-08T18:06:13Z","title":"A Distributed Block Chebyshev-Davidson Algorithm for Parallel Spectral\n  Clustering","summary":"  We develop a distributed Block Chebyshev-Davidson algorithm to solve\nlarge-scale leading eigenvalue problems for spectral analysis in spectral\nclustering. First, the efficiency of the Chebyshev-Davidson algorithm relies on\nthe prior knowledge of the eigenvalue spectrum, which could be expensive to\nestimate. This issue can be lessened by the analytic spectrum estimation of the\nLaplacian or normalized Laplacian matrices in spectral clustering, making the\nproposed algorithm very efficient for spectral clustering. Second, to make the\nproposed algorithm capable of analyzing big data, a distributed and parallel\nversion has been developed with attractive scalability. The speedup by parallel\ncomputing is approximately equivalent to $\\sqrt{p}$, where $p$ denotes the\nnumber of processes. {Numerical results will be provided to demonstrate its\nefficiency in spectral clustering and scalability advantage over existing\neigensolvers used for spectral clustering in parallel computing environments.}\n","authors":["Qiyuan Pang","Haizhao Yang"],"pdf_url":"https://arxiv.org/pdf/2212.04443v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02889v1","updated":"2024-01-05T16:39:48Z","published":"2024-01-05T16:39:48Z","title":"Energy-Preserving Reduced Operator Inference for Efficient Design and\n  Control","summary":"  Many-query computations, in which a computational model for an engineering\nsystem must be evaluated many times, are crucial in design and control. For\nsystems governed by partial differential equations (PDEs), typical\nhigh-fidelity numerical models are high-dimensional and too computationally\nexpensive for the many-query setting. Thus, efficient surrogate models are\nrequired to enable low-cost computations in design and control. This work\npresents a physics-preserving reduced model learning approach that targets PDEs\nwhose quadratic operators preserve energy, such as those arising in governing\nequations in many fluids problems. The approach is based on the Operator\nInference method, which fits reduced model operators to state snapshot and time\nderivative data in a least-squares sense. However, Operator Inference does not\ngenerally learn a reduced quadratic operator with the energy-preserving\nproperty of the original PDE. Thus, we propose a new energy-preserving Operator\nInference (EP-OpInf) approach, which imposes this structure on the learned\nreduced model via constrained optimization. Numerical results using the viscous\nBurgers' and Kuramoto-Sivashinksy equation (KSE) demonstrate that EP-OpInf\nlearns efficient and accurate reduced models that retain this energy-preserving\nstructure.\n","authors":["Tomoki Koike","Elizabeth Qian"],"pdf_url":"https://arxiv.org/pdf/2401.02889v1.pdf","comment":"17 pages, AIAA SciTech Forum 2024"},{"id":"http://arxiv.org/abs/2208.10993v3","updated":"2024-01-05T16:32:10Z","published":"2022-08-23T14:21:16Z","title":"Application of federated learning techniques for arrhythmia\n  classification using 12-lead ECG signals","summary":"  Artificial Intelligence-based (AI) analysis of large, curated medical\ndatasets is promising for providing early detection, faster diagnosis, and more\neffective treatment using low-power Electrocardiography (ECG) monitoring\ndevices information. However, accessing sensitive medical data from diverse\nsources is highly restricted since improper use, unsafe storage, or data\nleakage could violate a person's privacy. This work uses a Federated Learning\n(FL) privacy-preserving methodology to train AI models over heterogeneous sets\nof high-definition ECG from 12-lead sensor arrays collected from six\nheterogeneous sources. We evaluated the capacity of the resulting models to\nachieve equivalent performance compared to state-of-the-art models trained in a\nCentralized Learning (CL) fashion. Moreover, we assessed the performance of our\nsolution over Independent and Identical distributed (IID) and non-IID federated\ndata. Our methodology involves machine learning techniques based on Deep Neural\nNetworks and Long-Short-Term Memory models. It has a robust data preprocessing\npipeline with feature engineering, selection, and data balancing techniques.\nOur AI models demonstrated comparable performance to models trained using CL,\nIID, and non-IID approaches. They showcased advantages in reduced complexity\nand faster training time, making them well-suited for cloud-edge architectures.\n","authors":["Daniel Mauricio Jimenez Gutierrez","Hafiz Muuhammad Hassan","Lorella Landi","Andrea Vitaletti","Ioannis Chatzigiannakis"],"pdf_url":"https://arxiv.org/pdf/2208.10993v3.pdf","comment":"Preprint of International Symposium on Algorithmic Aspects of Cloud\n  Computing (ALGOCLOUD) 2023"},{"id":"http://arxiv.org/abs/2104.04987v3","updated":"2024-01-05T16:15:40Z","published":"2021-04-11T10:49:23Z","title":"AutoGL: A Library for Automated Graph Learning","summary":"  Recent years have witnessed an upsurge in research interests and applications\nof machine learning on graphs. However, manually designing the optimal machine\nlearning algorithms for different graph datasets and tasks is inflexible,\nlabor-intensive, and requires expert knowledge, limiting its adaptivity and\napplicability. Automated machine learning (AutoML) on graphs, aiming to\nautomatically design the optimal machine learning algorithm for a given graph\ndataset and task, has received considerable attention. However, none of the\nexisting libraries can fully support AutoML on graphs. To fill this gap, we\npresent Automated Graph Learning (AutoGL), the first dedicated library for\nautomated machine learning on graphs. AutoGL is open-source, easy to use, and\nflexible to be extended. Specifically, we propose a three-layer architecture,\nconsisting of backends to interface with devices, a complete automated graph\nlearning pipeline, and supported graph applications. The automated machine\nlearning pipeline further contains five functional modules: auto feature\nengineering, neural architecture search, hyper-parameter optimization, model\ntraining, and auto ensemble, covering the majority of existing AutoML methods\non graphs. For each module, we provide numerous state-of-the-art methods and\nflexible base classes and APIs, which allow easy usage and customization. We\nfurther provide experimental results to showcase the usage of our AutoGL\nlibrary. We also present AutoGL-light, a lightweight version of AutoGL to\nfacilitate customizing pipelines and enriching applications, as well as\nbenchmarks for graph neural architecture search. The codes of AutoGL are\npublicly available at https://github.com/THUMNLab/AutoGL.\n","authors":["Ziwei Zhang","Yijian Qin","Zeyang Zhang","Chaoyu Guan","Jie Cai","Heng Chang","Jiyan Jiang","Haoyang Li","Zixin Sun","Beini Xie","Yang Yao","Yipeng Zhang","Xin Wang","Wenwu Zhu"],"pdf_url":"https://arxiv.org/pdf/2104.04987v3.pdf","comment":"Extended version; initial version published at ICLR 2021 Workshop on\n  Geometrical and Topological Representation Learning"},{"id":"http://arxiv.org/abs/2401.02879v1","updated":"2024-01-05T16:11:34Z","published":"2024-01-05T16:11:34Z","title":"Efficient Parameter Optimisation for Quantum Kernel Alignment: A\n  Sub-sampling Approach in Variational Training","summary":"  Quantum machine learning with quantum kernels for classification problems is\na growing area of research. Recently, quantum kernel alignment techniques that\nparameterise the kernel have been developed, allowing the kernel to be trained\nand therefore aligned with a specific dataset. While quantum kernel alignment\nis a promising technique, it has been hampered by considerable training costs\nbecause the full kernel matrix must be constructed at every training iteration.\nAddressing this challenge, we introduce a novel method that seeks to balance\nefficiency and performance. We present a sub-sampling training approach that\nuses a subset of the kernel matrix at each training step, thereby reducing the\noverall computational cost of the training. In this work, we apply the\nsub-sampling method to synthetic datasets and a real-world breast cancer\ndataset and demonstrate considerable reductions in the number of circuits\nrequired to train the quantum kernel while maintaining classification accuracy.\n","authors":["M. Emre Sahin","Benjamin C. B. Symons","Pushpak Pati","Fayyaz Minhas","Declan Millar","Maria Gabrani","Jan Lukas Robertus","Stefano Mensa"],"pdf_url":"https://arxiv.org/pdf/2401.02879v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.16221v3","updated":"2024-01-05T15:43:12Z","published":"2023-10-24T22:24:44Z","title":"Hierarchical Randomized Smoothing","summary":"  Real-world data is complex and often consists of objects that can be\ndecomposed into multiple entities (e.g. images into pixels, graphs into\ninterconnected nodes). Randomized smoothing is a powerful framework for making\nmodels provably robust against small changes to their inputs - by guaranteeing\nrobustness of the majority vote when randomly adding noise before\nclassification. Yet, certifying robustness on such complex data via randomized\nsmoothing is challenging when adversaries do not arbitrarily perturb entire\nobjects (e.g. images) but only a subset of their entities (e.g. pixels). As a\nsolution, we introduce hierarchical randomized smoothing: We partially smooth\nobjects by adding random noise only on a randomly selected subset of their\nentities. By adding noise in a more targeted manner than existing methods we\nobtain stronger robustness guarantees while maintaining high accuracy. We\ninitialize hierarchical smoothing using different noising distributions,\nyielding novel robustness certificates for discrete and continuous domains. We\nexperimentally demonstrate the importance of hierarchical smoothing in image\nand node classification, where it yields superior robustness-accuracy\ntrade-offs. Overall, hierarchical smoothing is an important contribution\ntowards models that are both - certifiably robust to perturbations and\naccurate.\n","authors":["Yan Scholten","Jan Schuchardt","Aleksandar Bojchevski","Stephan Günnemann"],"pdf_url":"https://arxiv.org/pdf/2310.16221v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02860v1","updated":"2024-01-05T15:32:24Z","published":"2024-01-05T15:32:24Z","title":"Framework for Variable-lag Motif Following Relation Inference In Time\n  Series using Matrix Profile analysis","summary":"  Knowing who follows whom and what patterns they are following are crucial\nsteps to understand collective behaviors (e.g. a group of human, a school of\nfish, or a stock market). Time series is one of resources that can be used to\nget insight regarding following relations. However, the concept of following\npatterns or motifs and the solution to find them in time series are not\nobvious. In this work, we formalize a concept of following motifs between two\ntime series and present a framework to infer following patterns between two\ntime series. The framework utilizes one of efficient and scalable methods to\nretrieve motifs from time series called the Matrix Profile Method. We compare\nour proposed framework with several baselines. The framework performs better\nthan baselines in the simulation datasets. In the dataset of sound recording,\nthe framework is able to retrieve the following motifs within a pair of time\nseries that two singers sing following each other. In the cryptocurrency\ndataset, the framework is capable of capturing the following motifs within a\npair of time series from two digital currencies, which implies that the values\nof one currency follow the values of another currency patterns. Our framework\ncan be utilized in any field of time series to get insight regarding following\npatterns between time series.\n","authors":["Naaek Chinpattanakarn","Chainarong Amornbunchornvej"],"pdf_url":"https://arxiv.org/pdf/2401.02860v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02847v1","updated":"2024-01-05T15:07:05Z","published":"2024-01-05T15:07:05Z","title":"Generating Non-Stationary Textures using Self-Rectification","summary":"  This paper addresses the challenge of example-based non-stationary texture\nsynthesis. We introduce a novel twostep approach wherein users first modify a\nreference texture using standard image editing tools, yielding an initial rough\ntarget for the synthesis. Subsequently, our proposed method, termed\n\"self-rectification\", automatically refines this target into a coherent,\nseamless texture, while faithfully preserving the distinct visual\ncharacteristics of the reference exemplar. Our method leverages a pre-trained\ndiffusion network, and uses self-attention mechanisms, to gradually align the\nsynthesized texture with the reference, ensuring the retention of the\nstructures in the provided target. Through experimental validation, our\napproach exhibits exceptional proficiency in handling non-stationary textures,\ndemonstrating significant advancements in texture synthesis when compared to\nexisting state-of-the-art techniques. Code is available at\nhttps://github.com/xiaorongjun000/Self-Rectification\n","authors":["Yang Zhou","Rongjun Xiao","Dani Lischinski","Daniel Cohen-Or","Hui Huang"],"pdf_url":"https://arxiv.org/pdf/2401.02847v1.pdf","comment":"Project page: https://github.com/xiaorongjun000/Self-Rectification"},{"id":"http://arxiv.org/abs/2401.02843v1","updated":"2024-01-05T14:53:09Z","published":"2024-01-05T14:53:09Z","title":"Thousands of AI Authors on the Future of AI","summary":"  In the largest survey of its kind, 2,778 researchers who had published in\ntop-tier artificial intelligence (AI) venues gave predictions on the pace of AI\nprogress and the nature and impacts of advanced AI systems The aggregate\nforecasts give at least a 50% chance of AI systems achieving several milestones\nby 2028, including autonomously constructing a payment processing site from\nscratch, creating a song indistinguishable from a new song by a popular\nmusician, and autonomously downloading and fine-tuning a large language model.\nIf science continues undisrupted, the chance of unaided machines outperforming\nhumans in every possible task was estimated at 10% by 2027, and 50% by 2047.\nThe latter estimate is 13 years earlier than that reached in a similar survey\nwe conducted only one year earlier [Grace et al., 2022]. However, the chance of\nall human occupations becoming fully automatable was forecast to reach 10% by\n2037, and 50% as late as 2116 (compared to 2164 in the 2022 survey).\n  Most respondents expressed substantial uncertainty about the long-term value\nof AI progress: While 68.3% thought good outcomes from superhuman AI are more\nlikely than bad, of these net optimists 48% gave at least a 5% chance of\nextremely bad outcomes such as human extinction, and 59% of net pessimists gave\n5% or more to extremely good outcomes. Between 38% and 51% of respondents gave\nat least a 10% chance to advanced AI leading to outcomes as bad as human\nextinction. More than half suggested that \"substantial\" or \"extreme\" concern is\nwarranted about six different AI-related scenarios, including misinformation,\nauthoritarian control, and inequality. There was disagreement about whether\nfaster or slower AI progress would be better for the future of humanity.\nHowever, there was broad agreement that research aimed at minimizing potential\nrisks from AI systems ought to be prioritized more.\n","authors":["Katja Grace","Harlan Stewart","Julia Fabienne Sandkühler","Stephen Thomas","Ben Weinstein-Raun","Jan Brauner"],"pdf_url":"https://arxiv.org/pdf/2401.02843v1.pdf","comment":"The asterisk indicates the corresponding author. The dagger indicates\n  equal contribution"},{"id":"http://arxiv.org/abs/2312.07910v2","updated":"2024-01-05T14:45:00Z","published":"2023-12-13T05:58:34Z","title":"PromptBench: A Unified Library for Evaluation of Large Language Models","summary":"  The evaluation of large language models (LLMs) is crucial to assess their\nperformance and mitigate potential security risks. In this paper, we introduce\nPromptBench, a unified library to evaluate LLMs. It consists of several key\ncomponents that are easily used and extended by researchers: prompt\nconstruction, prompt engineering, dataset and model loading, adversarial prompt\nattack, dynamic evaluation protocols, and analysis tools. PromptBench is\ndesigned to be an open, general, and flexible codebase for research purposes\nthat can facilitate original study in creating new benchmarks, deploying\ndownstream applications, and designing new evaluation protocols. The code is\navailable at: https://github.com/microsoft/promptbench and will be continuously\nsupported.\n","authors":["Kaijie Zhu","Qinlin Zhao","Hao Chen","Jindong Wang","Xing Xie"],"pdf_url":"https://arxiv.org/pdf/2312.07910v2.pdf","comment":"An extension to PromptBench (arXiv:2306.04528) for unified evaluation\n  of LLMs using the same name; code: https://github.com/microsoft/promptbench"},{"id":"http://arxiv.org/abs/2401.01148v2","updated":"2024-01-05T14:28:19Z","published":"2024-01-02T10:58:54Z","title":"PAC-Bayes-Chernoff bounds for unbounded losses","summary":"  We present a new high-probability PAC-Bayes oracle bound for unbounded\nlosses. This result can be understood as a PAC-Bayes version of the Chernoff\nbound. The proof technique relies on uniformly bounding the tail of certain\nrandom variable based on the Cram\\'er transform of the loss. We highlight two\napplications of our main result. First, we show that our bound solves the open\nproblem of optimizing the free parameter on many PAC-Bayes bounds. Finally, we\nshow that our approach allows working with flexible assumptions on the loss\nfunction, resulting in novel bounds that generalize previous ones and can be\nminimized to obtain Gibbs-like posteriors.\n","authors":["Ioar Casado","Luis A. Ortega","Andrés R. Masegosa","Aritz Pérez"],"pdf_url":"https://arxiv.org/pdf/2401.01148v2.pdf","comment":"11 pages, minor typos"},{"id":"http://arxiv.org/abs/2401.02827v1","updated":"2024-01-05T14:21:10Z","published":"2024-01-05T14:21:10Z","title":"Let's Get It Started: Fostering the Discoverability of New Releases on\n  Deezer","summary":"  This paper presents our recent initiatives to foster the discoverability of\nnew releases on the music streaming service Deezer. After introducing our\nsearch and recommendation features dedicated to new releases, we outline our\nshift from editorial to personalized release suggestions using cold start\nembeddings and contextual bandits. Backed by online experiments, we discuss the\nadvantages of this shift in terms of recommendation quality and exposure of new\nreleases on the service.\n","authors":["Léa Briand","Théo Bontempelli","Walid Bendada","Mathieu Morlon","François Rigaud","Benjamin Chapus","Thomas Bouabça","Guillaume Salha-Galvan"],"pdf_url":"https://arxiv.org/pdf/2401.02827v1.pdf","comment":"Accepted for presentation as an \"Industry Talk\" at the 46th European\n  Conference on Information Retrieval (ECIR 2024)"},{"id":"http://arxiv.org/abs/2311.14212v2","updated":"2024-01-05T14:18:35Z","published":"2023-11-23T21:54:22Z","title":"Annotation Sensitivity: Training Data Collection Methods Affect Model\n  Performance","summary":"  When training data are collected from human annotators, the design of the\nannotation instrument, the instructions given to annotators, the\ncharacteristics of the annotators, and their interactions can impact training\ndata. This study demonstrates that design choices made when creating an\nannotation instrument also impact the models trained on the resulting\nannotations. We introduce the term annotation sensitivity to refer to the\nimpact of annotation data collection methods on the annotations themselves and\non downstream model performance and predictions. We collect annotations of hate\nspeech and offensive language in five experimental conditions of an annotation\ninstrument, randomly assigning annotators to conditions. We then fine-tune BERT\nmodels on each of the five resulting datasets and evaluate model performance on\na holdout portion of each condition. We find considerable differences between\nthe conditions for 1) the share of hate speech/offensive language annotations,\n2) model performance, 3) model predictions, and 4) model learning curves. Our\nresults emphasize the crucial role played by the annotation instrument which\nhas received little attention in the machine learning literature. We call for\nadditional research into how and why the instrument impacts the annotations to\ninform the development of best practices in instrument design.\n","authors":["Christoph Kern","Stephanie Eckman","Jacob Beck","Rob Chew","Bolei Ma","Frauke Kreuter"],"pdf_url":"https://arxiv.org/pdf/2311.14212v2.pdf","comment":"EMNLP 2023 Findings:\n  https://aclanthology.org/2023.findings-emnlp.992/"},{"id":"http://arxiv.org/abs/2401.02810v1","updated":"2024-01-05T13:45:08Z","published":"2024-01-05T13:45:08Z","title":"Physics-Informed Neural Networks for High-Frequency and Multi-Scale\n  Problems using Transfer Learning","summary":"  Physics-informed neural network (PINN) is a data-driven solver for partial\nand ordinary differential equations(ODEs/PDEs). It provides a unified framework\nto address both forward and inverse problems. However, the complexity of the\nobjective function often leads to training failures. This issue is particularly\nprominent when solving high-frequency and multi-scale problems. We proposed\nusing transfer learning to boost the robustness and convergence of training\nPINN, starting training from low-frequency problems and gradually approaching\nhigh-frequency problems. Through two case studies, we discovered that transfer\nlearning can effectively train PINN to approximate solutions from low-frequency\nproblems to high-frequency problems without increasing network parameters.\nFurthermore, it requires fewer data points and less training time. We\nelaborately described our training strategy, including optimizer selection, and\nsuggested guidelines for using transfer learning to train neural networks for\nsolving more complex problems.\n","authors":["Abdul Hannan Mustajab","Hao Lyu","Zarghaam Rizvi","Frank Wuttke"],"pdf_url":"https://arxiv.org/pdf/2401.02810v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2401.02801v1","updated":"2024-01-05T13:29:59Z","published":"2024-01-05T13:29:59Z","title":"Credence: Augmenting Datacenter Switch Buffer Sharing with ML\n  Predictions","summary":"  Packet buffers in datacenter switches are shared across all the switch ports\nin order to improve the overall throughput. The trend of shrinking buffer sizes\nin datacenter switches makes buffer sharing extremely challenging and a\ncritical performance issue. Literature suggests that push-out buffer sharing\nalgorithms have significantly better performance guarantees compared to\ndrop-tail algorithms. Unfortunately, switches are unable to benefit from these\nalgorithms due to lack of support for push-out operations in hardware. Our key\nobservation is that drop-tail buffers can emulate push-out buffers if the\nfuture packet arrivals are known ahead of time. This suggests that augmenting\ndrop-tail algorithms with predictions about the future arrivals has the\npotential to significantly improve performance.\n  This paper is the first research attempt in this direction. We propose\nCredence, a drop-tail buffer sharing algorithm augmented with machine-learned\npredictions. Credence can unlock the performance only attainable by push-out\nalgorithms so far. Its performance hinges on the accuracy of predictions.\nSpecifically, Credence achieves near-optimal performance of the best known\npush-out algorithm LQD (Longest Queue Drop) with perfect predictions, but\ngracefully degrades to the performance of the simplest drop-tail algorithm\nComplete Sharing when the prediction error gets arbitrarily worse. Our\nevaluations show that Credence improves throughput by $1.5$x compared to\ntraditional approaches. In terms of flow completion times, we show that\nCredence improves upon the state-of-the-art approaches by up to $95\\%$ using\noff-the-shelf machine learning techniques that are also practical in today's\nhardware. We believe this work opens several interesting future work\nopportunities both in systems and theory that we discuss at the end of this\npaper.\n","authors":["Vamsi Addanki","Maciej Pacut","Stefan Schmid"],"pdf_url":"https://arxiv.org/pdf/2401.02801v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09215v2","updated":"2024-01-05T13:16:25Z","published":"2023-11-15T18:56:51Z","title":"ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy","summary":"  Modern computer vision offers a great variety of models to practitioners, and\nselecting a model from multiple options for specific applications can be\nchallenging. Conventionally, competing model architectures and training\nprotocols are compared by their classification accuracy on ImageNet. However,\nthis single metric does not fully capture performance nuances critical for\nspecialized tasks. In this work, we conduct an in-depth comparative analysis of\nmodel behaviors beyond ImageNet accuracy, for both ConvNet and Vision\nTransformer architectures, each across supervised and CLIP training paradigms.\nAlthough our selected models have similar ImageNet accuracies and compute\nrequirements, we find that they differ in many other aspects: types of\nmistakes, output calibration, transferability, and feature invariance, among\nothers. This diversity in model characteristics, not captured by traditional\nmetrics, highlights the need for more nuanced analysis when choosing among\ndifferent models. Our code is available at\nhttps://github.com/kirill-vish/Beyond-INet.\n","authors":["Kirill Vishniakov","Zhiqiang Shen","Zhuang Liu"],"pdf_url":"https://arxiv.org/pdf/2311.09215v2.pdf","comment":"Project page: https://kirill-vish.github.io/beyond-imagenet-accuracy/"},{"id":"http://arxiv.org/abs/2401.02106v2","updated":"2024-01-05T13:05:16Z","published":"2024-01-04T07:28:56Z","title":"Cadmium Zinc Telluride (CZT) photon counting detector Characterisation\n  for soft tissue imaging","summary":"  The use of photon counting detection technology has resulted in significant\nX-ray imaging research interest in recent years. Computed Tomography (CT)\nscanners can benefit from photon-counting detectors, which are new technology\nwith the potential to overcome key limitations of conventional CT detectors.\nResearchers are still studying the effectiveness and sensitivity of\nsemiconductor detector materials in photon counting detectors for detecting\nsoft tissue contrasts. This study aimed to characterize the performance of the\nCadmium Zinc Telluride photon counting detector in identifying various tissues.\nAn optimal frame rate per second (FPS) of CZT detector was evaluated by setting\nthe X-ray tube voltage and current at 25 keV, 35 keV and 0.5 mA, 1.0 mA\nrespectively by keeping the optimum FPS fixed, the detector energy thresholds\nwere set in small steps from 15 keV to 35 keV and the Currents were set for\nX-ray tubes in ranges of 0.1 mA to 1.0 mA to find the relationship between\nvoltage and current of the X-ray source and counts per second (CPS). The\nsamples i.e., fat, liver, muscles, paraffin wax, and contrast media were\nstacked at six different thickness levels in a stair-step chamber made from\nPlexi-glass. X-ray transmission at six different thicknesses of tissue samples\nwas also examined for five different energy (regions) thresholds (21 keV, 25\nkeV, 29 keV, 31 keV, and 45 keV) to determine the effect on count per second\n(CPS). In this study, 12 frames per second is found to be the optimum frame\nrate per second (FPS) based on the spectral response of an X-ray source and CPS\nhas a linear relationship with X-ray tube current as well. It was also noted\nthat A sample's thickness also affects its X-ray transmission at different\nenergy thresholds. A high sensitivity and linearity of the detectors make them\nsuitable for use in both preclinical and medical applications.\n","authors":["K. Hameed","Rafidah Zainon","Mahbubunnabi Tamal"],"pdf_url":"https://arxiv.org/pdf/2401.02106v2.pdf","comment":"29 pages and 11 figures"},{"id":"http://arxiv.org/abs/2401.02791v1","updated":"2024-01-05T13:05:02Z","published":"2024-01-05T13:05:02Z","title":"Weakly Semi-supervised Tool Detection in Minimally Invasive Surgery\n  Videos","summary":"  Surgical tool detection is essential for analyzing and evaluating minimally\ninvasive surgery videos. Current approaches are mostly based on supervised\nmethods that require large, fully instance-level labels (i.e., bounding boxes).\nHowever, large image datasets with instance-level labels are often limited\nbecause of the burden of annotation. Thus, surgical tool detection is important\nwhen providing image-level labels instead of instance-level labels since\nimage-level annotations are considerably more time-efficient than\ninstance-level annotations. In this work, we propose to strike a balance\nbetween the extremely costly annotation burden and detection performance. We\nfurther propose a co-occurrence loss, which considers a characteristic that\nsome tool pairs often co-occur together in an image to leverage image-level\nlabels. Encapsulating the knowledge of co-occurrence using the co-occurrence\nloss helps to overcome the difficulty in classification that originates from\nthe fact that some tools have similar shapes and textures. Extensive\nexperiments conducted on the Endovis2018 dataset in various data settings show\nthe effectiveness of our method.\n","authors":["Ryo Fujii","Ryo Hachiuma","Hideo Saito"],"pdf_url":"https://arxiv.org/pdf/2401.02791v1.pdf","comment":"ICASSP 2024"},{"id":"http://arxiv.org/abs/2306.02986v2","updated":"2024-01-05T12:48:31Z","published":"2023-06-05T15:56:30Z","title":"Brain tumor segmentation using synthetic MR images -- A comparison of\n  GANs and diffusion models","summary":"  Large annotated datasets are required for training deep learning models, but\nin medical imaging data sharing is often complicated due to ethics,\nanonymization and data protection legislation. Generative AI models, such as\ngenerative adversarial networks (GANs) and diffusion models, can today produce\nvery realistic synthetic images, and can potentially facilitate data sharing.\nHowever, in order to share synthetic medical images it must first be\ndemonstrated that they can be used for training different networks with\nacceptable performance. Here, we therefore comprehensively evaluate four GANs\n(progressive GAN, StyleGAN 1-3) and a diffusion model for the task of brain\ntumor segmentation (using two segmentation networks, U-Net and a Swin\ntransformer). Our results show that segmentation networks trained on synthetic\nimages reach Dice scores that are 80% - 90% of Dice scores when training with\nreal images, but that memorization of the training images can be a problem for\ndiffusion models if the original dataset is too small. Our conclusion is that\nsharing synthetic medical images is a viable option to sharing real images, but\nthat further work is required. The trained generative models and the generated\nsynthetic images are shared on AIDA data hub\n","authors":["Muhammad Usman Akbar","Måns Larsson","Anders Eklund"],"pdf_url":"https://arxiv.org/pdf/2306.02986v2.pdf","comment":"28 Pages. 5 figures"},{"id":"http://arxiv.org/abs/2312.06942v3","updated":"2024-01-05T12:41:27Z","published":"2023-12-12T02:34:06Z","title":"AI Control: Improving Safety Despite Intentional Subversion","summary":"  As large language models (LLMs) become more powerful and are deployed more\nautonomously, it will be increasingly important to prevent them from causing\nharmful outcomes. Researchers have investigated a variety of safety techniques\nfor this purpose, e.g. using models to review the outputs of other models, or\nred-teaming techniques to surface subtle failure modes. However, researchers\nhave not evaluated whether such techniques still ensure safety if the model is\nitself intentionally trying to subvert them. In this paper, we develop and\nevaluate pipelines of safety techniques (\"protocols\") that are robust to\nintentional subversion.\n  We investigate a scenario in which we want to solve a sequence of programming\nproblems, using access to a powerful but untrusted model (in our case, GPT-4),\naccess to a less powerful trusted model (in our case, GPT-3.5), and limited\naccess to high-quality trusted labor. We investigate protocols that aim to\nnever submit solutions containing backdoors, which we operationalize here as\nlogical errors that are not caught by test cases.\n  We investigate a range of protocols and test each against strategies that the\nuntrusted model could use to subvert them. One protocol is what we call trusted\nediting. This protocol first asks GPT-4 to write code, and then asks GPT-3.5 to\nrate the suspiciousness of that code. If the code is below some suspiciousness\nthreshold, it is submitted. Otherwise, GPT-3.5 edits the solution to remove\nparts that seem suspicious and then submits the edited code. Another protocol\nis untrusted monitoring. This protocol asks GPT-4 to write code, and then asks\nanother instance of GPT-4 whether the code is backdoored, using various\ntechniques to prevent the GPT-4 instances from colluding. These protocols\nimprove substantially on simple baselines.\n","authors":["Ryan Greenblatt","Buck Shlegeris","Kshitij Sachan","Fabien Roger"],"pdf_url":"https://arxiv.org/pdf/2312.06942v3.pdf","comment":"Edit: Fix minor typos and clarify abstract"},{"id":"http://arxiv.org/abs/2311.01282v4","updated":"2024-01-05T12:41:13Z","published":"2023-11-02T14:57:03Z","title":"FlashDecoding++: Faster Large Language Model Inference on GPUs","summary":"  As the Large Language Model (LLM) becomes increasingly important in various\ndomains. However, the following challenges still remain unsolved in\naccelerating LLM inference: (1) Synchronized partial softmax update. The\nsoftmax operation requires a synchronized update operation among each partial\nsoftmax result, leading to ~20% overheads for the attention computation in\nLLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices\nperforming GEMM in LLM inference is flat, leading to under-utilized computation\nand >50% performance loss after padding zeros in previous designs. (3)\nPerformance loss due to static dataflow. Kernel performance in LLM depends on\nvaried input data features, hardware configurations, etc. A single and static\ndataflow may lead to a 50.25% performance loss for GEMMs of different shapes in\nLLM inference.\n  We present FlashDecoding++, a fast LLM inference engine supporting mainstream\nLLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++\ncreatively proposes: (1) Asynchronized softmax with unified max value.\nFlashDecoding++ introduces a unified max value technique for different partial\nsoftmax computations to avoid synchronization. (2) Flat GEMM optimization with\ndouble buffering. FlashDecoding++ points out that flat GEMMs with different\nshapes face varied bottlenecks. Then, techniques like double buffering are\nintroduced. (3) Heuristic dataflow with hardware resource adaptation.\nFlashDecoding++ heuristically optimizes dataflow using different hardware\nresource considering input dynamics. Due to the versatility of optimizations in\nFlashDecoding++, FlashDecoding++ can achieve up to 4.86x and 2.18x speedup on\nboth NVIDIA and AMD GPUs compared to Hugging Face implementations.\nFlashDecoding++ also achieves an average speedup of 1.37x compared to\nstate-of-the-art LLM inference engines on mainstream LLMs.\n","authors":["Ke Hong","Guohao Dai","Jiaming Xu","Qiuli Mao","Xiuhong Li","Jun Liu","Kangdi Chen","Yuhan Dong","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2311.01282v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02773v1","updated":"2024-01-05T12:13:00Z","published":"2024-01-05T12:13:00Z","title":"Tackling Electrode Shift In Gesture Recognition with HD-EMG Electrode\n  Subsets","summary":"  sEMG pattern recognition algorithms have been explored extensively in\ndecoding movement intent, yet are known to be vulnerable to changing recording\nconditions, exhibiting significant drops in performance across subjects, and\neven across sessions. Multi-channel surface EMG, also referred to as\nhigh-density sEMG (HD-sEMG) systems, have been used to improve performance with\nthe information collected through the use of additional electrodes. However, a\nlack of robustness is ever present due to limited datasets and the difficulties\nin addressing sources of variability, such as electrode placement. In this\nstudy, we propose training on a collection of input channel subsets and\naugmenting our training distribution with data from different electrode\nlocations, simultaneously targeting electrode shift and reducing input\ndimensionality. Our method increases robustness against electrode shift and\nresults in significantly higher intersession performance across subjects and\nclassification algorithms.\n","authors":["Joao Pereira","Dimitrios Chalatsis","Balint Hodossy","Dario Farina"],"pdf_url":"https://arxiv.org/pdf/2401.02773v1.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2401.02771v1","updated":"2024-01-05T12:01:19Z","published":"2024-01-05T12:01:19Z","title":"Powerformer: A Section-adaptive Transformer for Power Flow Adjustment","summary":"  In this paper, we present a novel transformer architecture tailored for\nlearning robust power system state representations, which strives to optimize\npower dispatch for the power flow adjustment across different transmission\nsections. Specifically, our proposed approach, named Powerformer, develops a\ndedicated section-adaptive attention mechanism, separating itself from the\nself-attention used in conventional transformers. This mechanism effectively\nintegrates power system states with transmission section information, which\nfacilitates the development of robust state representations. Furthermore, by\nconsidering the graph topology of power system and the electrical attributes of\nbus nodes, we introduce two customized strategies to further enhance the\nexpressiveness: graph neural network propagation and multi-factor attention\nmechanism. Extensive evaluations are conducted on three power system scenarios,\nincluding the IEEE 118-bus system, a realistic 300-bus system in China, and a\nlarge-scale European system with 9241 buses, where Powerformer demonstrates its\nsuperior performance over several baseline methods.\n","authors":["Kaixuan Chen","Wei Luo","Shunyu Liu","Yaoquan Wei","Yihe Zhou","Yunpeng Qing","Quan Zhang","Jie Song","Mingli Song"],"pdf_url":"https://arxiv.org/pdf/2401.02771v1.pdf","comment":"8 figures"},{"id":"http://arxiv.org/abs/2208.04988v2","updated":"2024-01-05T12:01:04Z","published":"2022-08-09T18:30:23Z","title":"Quantum artificial vision for defect detection in manufacturing","summary":"  In this paper we consider several algorithms for quantum computer vision\nusing Noisy Intermediate-Scale Quantum (NISQ) devices, and benchmark them for a\nreal problem against their classical counterparts. Specifically, we consider\ntwo approaches: a quantum Support Vector Machine (QSVM) on a universal\ngate-based quantum computer, and QBoost on a quantum annealer. The quantum\nvision systems are benchmarked for an unbalanced dataset of images where the\naim is to detect defects in manufactured car pieces. We see that the quantum\nalgorithms outperform their classical counterparts in several ways, with QBoost\nallowing for larger problems to be analyzed with present-day quantum annealers.\nData preprocessing, including dimensionality reduction and contrast\nenhancement, is also discussed, as well as hyperparameter tuning in QBoost. To\nthe best of our knowledge, this is the first implementation of quantum computer\nvision systems for a problem of industrial relevance in a manufacturing\nproduction line.\n","authors":["Daniel Guijo","Victor Onofre","Gianni Del Bimbo","Samuel Mugel","Daniel Estepa","Xabier De Carlos","Ana Adell","Aizea Lojo","Josu Bilbao","Roman Orus"],"pdf_url":"https://arxiv.org/pdf/2208.04988v2.pdf","comment":"11 pages, 7 figures, 16 tables, revised version"},{"id":"http://arxiv.org/abs/2312.08785v2","updated":"2024-01-05T11:42:22Z","published":"2023-12-14T10:08:12Z","title":"Managing the unknown: a survey on Open Set Recognition and tangential\n  areas","summary":"  In real-world scenarios classification models are often required to perform\nrobustly when predicting samples belonging to classes that have not appeared\nduring its training stage. Open Set Recognition addresses this issue by\ndevising models capable of detecting unknown classes from samples arriving\nduring the testing phase, while maintaining a good level of performance in the\nclassification of samples belonging to known classes. This review\ncomprehensively overviews the recent literature related to Open Set\nRecognition, identifying common practices, limitations, and connections of this\nfield with other machine learning research areas, such as continual learning,\nout-of-distribution detection, novelty detection, and uncertainty estimation.\nOur work also uncovers open problems and suggests several research directions\nthat may motivate and articulate future efforts towards more safe Artificial\nIntelligence methods.\n","authors":["Marcos Barcina-Blanco","Jesus L. Lobo","Pablo Garcia-Bringas","Javier Del Ser"],"pdf_url":"https://arxiv.org/pdf/2312.08785v2.pdf","comment":"35 pages, 1 figure, 1 table"},{"id":"http://arxiv.org/abs/2401.00867v2","updated":"2024-01-05T11:40:56Z","published":"2023-12-29T22:35:45Z","title":"Tensor Networks for Explainable Machine Learning in Cybersecurity","summary":"  In this paper we show how tensor networks help in developing explainability\nof machine learning algorithms. Specifically, we develop an unsupervised\nclustering algorithm based on Matrix Product States (MPS) and apply it in the\ncontext of a real use-case of adversary-generated threat intelligence. Our\ninvestigation proves that MPS rival traditional deep learning models such as\nautoencoders and GANs in terms of performance, while providing much richer\nmodel interpretability. Our approach naturally facilitates the extraction of\nfeature-wise probabilities, Von Neumann Entropy, and mutual information,\noffering a compelling narrative for classification of anomalies and fostering\nan unprecedented level of transparency and interpretability, something\nfundamental to understand the rationale behind artificial intelligence\ndecisions.\n","authors":["Borja Aizpurua","Roman Orus"],"pdf_url":"https://arxiv.org/pdf/2401.00867v2.pdf","comment":"9 pages, 9 figures, 2 table, minor typos corrected"},{"id":"http://arxiv.org/abs/2208.12511v3","updated":"2024-01-05T11:38:20Z","published":"2022-08-26T09:09:14Z","title":"Lower Difficulty and Better Robustness: A Bregman Divergence Perspective\n  for Adversarial Training","summary":"  In this paper, we investigate on improving the adversarial robustness\nobtained in adversarial training (AT) via reducing the difficulty of\noptimization. To better study this problem, we build a novel Bregman divergence\nperspective for AT, in which AT can be viewed as the sliding process of the\ntraining data points on the negative entropy curve. Based on this perspective,\nwe analyze the learning objectives of two typical AT methods, i.e., PGD-AT and\nTRADES, and we find that the optimization process of TRADES is easier than\nPGD-AT for that TRADES separates PGD-AT. In addition, we discuss the function\nof entropy in TRADES, and we find that models with high entropy can be better\nrobustness learners. Inspired by the above findings, we propose two methods,\ni.e., FAIT and MER, which can both not only reduce the difficulty of\noptimization under the 10-step PGD adversaries, but also provide better\nrobustness. Our work suggests that reducing the difficulty of optimization\nunder the 10-step PGD adversaries is a promising approach for enhancing the\nadversarial robustness in AT.\n","authors":["Zihui Wu","Haichang Gao","Bingqian Zhou","Xiaoyan Guo","Shudong Zhang"],"pdf_url":"https://arxiv.org/pdf/2208.12511v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.15960v2","updated":"2024-01-05T10:33:32Z","published":"2023-12-26T08:49:57Z","title":"MoTCoder: Elevating Large Language Models with Modular of Thought for\n  Challenging Programming Tasks","summary":"  Large Language Models (LLMs) have showcased impressive capabilities in\nhandling straightforward programming tasks. However, their performance tends to\nfalter when confronted with more challenging programming problems. We observe\nthat conventional models often generate solutions as monolithic code blocks,\nrestricting their effectiveness in tackling intricate questions. To overcome\nthis limitation, we present Modular-of-Thought Coder (MoTCoder). We introduce a\npioneering framework for MoT instruction tuning, designed to promote the\ndecomposition of tasks into logical sub-tasks and sub-modules. Our\ninvestigations reveal that, through the cultivation and utilization of\nsub-modules, MoTCoder significantly improves both the modularity and\ncorrectness of the generated solutions, leading to substantial relative pass@1\nimprovements of 12.9% on APPS and 9.43% on CodeContests. Our codes are\navailable at https://github.com/dvlab-research/MoTCoder.\n","authors":["Jingyao Li","Pengguang Chen","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2312.15960v2.pdf","comment":"Model: https://huggingface.co/JingyaoLi/MoTCoder-15B-v1.0. Code:\n  https://github.com/dvlab-research/MoTCoder"},{"id":"http://arxiv.org/abs/2401.02740v1","updated":"2024-01-05T10:29:08Z","published":"2024-01-05T10:29:08Z","title":"Fairness-Aware Job Scheduling for Multi-Job Federated Learning","summary":"  Federated learning (FL) enables multiple data owners (a.k.a. FL clients) to\ncollaboratively train machine learning models without disclosing sensitive\nprivate data. Existing FL research mostly focuses on the monopoly scenario in\nwhich a single FL server selects a subset of FL clients to update their local\nmodels in each round of training. In practice, there can be multiple FL servers\nsimultaneously trying to select clients from the same pool. In this paper, we\npropose a first-of-its-kind Fairness-aware Federated Job Scheduling (FairFedJS)\napproach to bridge this gap. Based on Lyapunov optimization, it ensures fair\nallocation of high-demand FL client datasets to FL jobs in need of them, by\njointly considering the current demand and the job payment bids, in order to\nprevent prolonged waiting. Extensive experiments comparing FairFedJS against\nfour state-of-the-art approaches on two datasets demonstrate its significant\nadvantages. It outperforms the best baseline by 31.9% and 1.0% on average in\nterms of scheduling fairness and convergence time, respectively, while\nachieving comparable test accuracy.\n","authors":["Yuxin Shi","Han Yu"],"pdf_url":"https://arxiv.org/pdf/2401.02740v1.pdf","comment":"accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2401.02739v1","updated":"2024-01-05T10:27:44Z","published":"2024-01-05T10:27:44Z","title":"Diffusion Variational Inference: Diffusion Models as Expressive\n  Variational Posteriors","summary":"  We propose denoising diffusion variational inference (DDVI), an approximate\ninference algorithm for latent variable models which relies on diffusion models\nas expressive variational posteriors. Our method augments variational\nposteriors with auxiliary latents, which yields an expressive class of models\nthat perform diffusion in latent space by reversing a user-specified noising\nprocess. We fit these models by optimizing a novel lower bound on the marginal\nlikelihood inspired by the wake-sleep algorithm. Our method is easy to\nimplement (it fits a regularized extension of the ELBO), is compatible with\nblack-box variational inference, and outperforms alternative classes of\napproximate posteriors based on normalizing flows or adversarial networks. When\napplied to deep latent variable models, our method yields the denoising\ndiffusion VAE (DD-VAE) algorithm. We use this algorithm on a motivating task in\nbiology -- inferring latent ancestry from human genomes -- outperforming strong\nbaselines on the Thousand Genomes dataset.\n","authors":["Top Piriyakulkij","Yingheng Wang","Volodymyr Kuleshov"],"pdf_url":"https://arxiv.org/pdf/2401.02739v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02736v1","updated":"2024-01-05T10:14:39Z","published":"2024-01-05T10:14:39Z","title":"On the numerical reliability of nonsmooth autodiff: a MaxPool case study","summary":"  This paper considers the reliability of automatic differentiation (AD) for\nneural networks involving the nonsmooth MaxPool operation. We investigate the\nbehavior of AD across different precision levels (16, 32, 64 bits) and\nconvolutional architectures (LeNet, VGG, and ResNet) on various datasets\n(MNIST, CIFAR10, SVHN, and ImageNet). Although AD can be incorrect, recent\nresearch has shown that it coincides with the derivative almost everywhere,\neven in the presence of nonsmooth operations (such as MaxPool and ReLU). On the\nother hand, in practice, AD operates with floating-point numbers (not real\nnumbers), and there is, therefore, a need to explore subsets on which AD can be\nnumerically incorrect. These subsets include a bifurcation zone (where AD is\nincorrect over reals) and a compensation zone (where AD is incorrect over\nfloating-point numbers but correct over reals). Using SGD for the training\nprocess, we study the impact of different choices of the nonsmooth Jacobian for\nthe MaxPool function on the precision of 16 and 32 bits. These findings suggest\nthat nonsmooth MaxPool Jacobians with lower norms help maintain stable and\nefficient test accuracy, whereas those with higher norms can result in\ninstability and decreased performance. We also observe that the influence of\nMaxPool's nonsmooth Jacobians on learning can be reduced by using batch\nnormalization, Adam-like optimizers, or increasing the precision level.\n","authors":["Ryan Boustany"],"pdf_url":"https://arxiv.org/pdf/2401.02736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02735v1","updated":"2024-01-05T10:08:38Z","published":"2024-01-05T10:08:38Z","title":"Shared active subspace for multivariate vector-valued functions","summary":"  This paper proposes several approaches as baselines to compute a shared\nactive subspace for multivariate vector-valued functions. The goal is to\nminimize the deviation between the function evaluations on the original space\nand those on the reconstructed one. This is done either by manipulating the\ngradients or the symmetric positive (semi-)definite (SPD) matrices computed\nfrom the gradients of each component function so as to get a single structure\ncommon to all component functions. These approaches can be applied to any data\nirrespective of the underlying distribution unlike the existing vector-valued\napproach that is constrained to a normal distribution. We test the\neffectiveness of these methods on five optimization problems. The experiments\nshow that, in general, the SPD-level methods are superior to the gradient-level\nones, and are close to the vector-valued approach in the case of a normal\ndistribution. Interestingly, in most cases it suffices to take the sum of the\nSPD matrices to identify the best shared active subspace.\n","authors":["Khadija Musayeva","Mickael Binois"],"pdf_url":"https://arxiv.org/pdf/2401.02735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02734v1","updated":"2024-01-05T10:06:41Z","published":"2024-01-05T10:06:41Z","title":"FedNS: A Fast Sketching Newton-Type Algorithm for Federated Learning","summary":"  Recent Newton-type federated learning algorithms have demonstrated linear\nconvergence with respect to the communication rounds. However, communicating\nHessian matrices is often unfeasible due to their quadratic communication\ncomplexity. In this paper, we introduce a novel approach to tackle this issue\nwhile still achieving fast convergence rates. Our proposed method, named as\nFederated Newton Sketch methods (FedNS), approximates the centralized Newton's\nmethod by communicating the sketched square-root Hessian instead of the exact\nHessian. To enhance communication efficiency, we reduce the sketch size to\nmatch the effective dimension of the Hessian matrix. We provide convergence\nanalysis based on statistical learning for the federated Newton sketch\napproaches. Specifically, our approaches reach super-linear convergence rates\nw.r.t. the communication rounds for the first time. We validate the\neffectiveness of our algorithms through various experiments, which coincide\nwith our theoretical findings.\n","authors":["Jian Li","Yong Liu","Wei Wang","Haoran Wu","Weiping Wang"],"pdf_url":"https://arxiv.org/pdf/2401.02734v1.pdf","comment":"Accepted at AAAI 2024"},{"id":"http://arxiv.org/abs/2401.02723v1","updated":"2024-01-05T09:36:42Z","published":"2024-01-05T09:36:42Z","title":"Predicting Traffic Flow with Federated Learning and Graph Neural with\n  Asynchronous Computations Network","summary":"  Real-time traffic flow prediction holds significant importance within the\ndomain of Intelligent Transportation Systems (ITS). The task of achieving a\nbalance between prediction precision and computational efficiency presents a\nsignificant challenge. In this article, we present a novel deep-learning method\ncalled Federated Learning and Asynchronous Graph Convolutional Network\n(FLAGCN). Our framework incorporates the principles of asynchronous graph\nconvolutional networks with federated learning to enhance the accuracy and\nefficiency of real-time traffic flow prediction. The FLAGCN model employs a\nspatial-temporal graph convolution technique to asynchronously address\nspatio-temporal dependencies within traffic data effectively. To efficiently\nhandle the computational requirements associated with this deep learning model,\nthis study used a graph federated learning technique known as GraphFL. This\napproach is designed to facilitate the training process. The experimental\nresults obtained from conducting tests on two distinct traffic datasets\ndemonstrate that the utilization of FLAGCN leads to the optimization of both\ntraining and inference durations while maintaining a high level of prediction\naccuracy. FLAGCN outperforms existing models with significant improvements by\nachieving up to approximately 6.85% reduction in RMSE, 20.45% reduction in\nMAPE, compared to the best-performing existing models.\n","authors":["Muhammad Yaqub","Shahzad Ahmad","Malik Abdul Manan","Imran Shabir Chuhan"],"pdf_url":"https://arxiv.org/pdf/2401.02723v1.pdf","comment":"15 pages, 7 Figures, 1 Table"},{"id":"http://arxiv.org/abs/2312.01878v5","updated":"2024-01-05T09:35:52Z","published":"2023-12-04T13:20:15Z","title":"HGPROMPT: Bridging Homogeneous and Heterogeneous Graphs for Few-shot\n  Prompt Learning","summary":"  Graph neural networks (GNNs) and heterogeneous graph neural networks (HGNNs)\nare prominent techniques for homogeneous and heterogeneous graph representation\nlearning, yet their performance in an end-to-end supervised framework greatly\ndepends on the availability of task-specific supervision. To reduce the\nlabeling cost, pre-training on self-supervised pretext tasks has become a\npopular paradigm,but there is often a gap between the pre-trained model and\ndownstream tasks, stemming from the divergence in their objectives. To bridge\nthe gap, prompt learning has risen as a promising direction especially in\nfew-shot settings, without the need to fully fine-tune the pre-trained model.\nWhile there has been some early exploration of prompt-based learning on graphs,\nthey primarily deal with homogeneous graphs, ignoring the heterogeneous graphs\nthat are prevalent in downstream applications. In this paper, we propose\nHGPROMPT, a novel pre-training and prompting framework to unify not only\npre-training and downstream tasks but also homogeneous and heterogeneous graphs\nvia a dual-template design. Moreover, we propose dual-prompt in HGPROMPT to\nassist a downstream task in locating the most relevant prior to bridge the gaps\ncaused by not only feature variations but also heterogeneity differences across\ntasks. Finally, we thoroughly evaluate and analyze HGPROMPT through extensive\nexperiments on three public datasets.\n","authors":["Xingtong Yu","Yuan Fang","Zemin Liu","Xinming Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.01878v5.pdf","comment":"Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2401.02721v1","updated":"2024-01-05T09:32:39Z","published":"2024-01-05T09:32:39Z","title":"A Cost-Efficient FPGA Implementation of Tiny Transformer Model using\n  Neural ODE","summary":"  Transformer is an emerging neural network model with attention mechanism. It\nhas been adopted to various tasks and achieved a favorable accuracy compared to\nCNNs and RNNs. While the attention mechanism is recognized as a general-purpose\ncomponent, many of the Transformer models require a significant number of\nparameters compared to the CNN-based ones. To mitigate the computational\ncomplexity, recently, a hybrid approach has been proposed, which uses ResNet as\na backbone architecture and replaces a part of its convolution layers with an\nMHSA (Multi-Head Self-Attention) mechanism. In this paper, we significantly\nreduce the parameter size of such models by using Neural ODE (Ordinary\nDifferential Equation) as a backbone architecture instead of ResNet. The\nproposed hybrid model reduces the parameter size by 94.6% compared to the\nCNN-based ones without degrading the accuracy. We then deploy the proposed\nmodel on a modest-sized FPGA device for edge computing. To further reduce FPGA\nresource utilization, we quantize the model following QAT (Quantization Aware\nTraining) scheme instead of PTQ (Post Training Quantization) to suppress the\naccuracy loss. As a result, an extremely lightweight Transformer-based model\ncan be implemented on resource-limited FPGAs. The weights of the feature\nextraction network are stored on-chip to minimize the memory transfer overhead,\nallowing faster inference. By eliminating the overhead of memory transfers,\ninference can be executed seamlessly, leading to accelerated inference. The\nproposed FPGA implementation achieves 12.8x speedup and 9.21x energy efficiency\ncompared to ARM Cortex-A53 CPU.\n","authors":["Ikumi Okubo","Keisuke Sugiura","Hiroki Matsutani"],"pdf_url":"https://arxiv.org/pdf/2401.02721v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02718v1","updated":"2024-01-05T09:21:54Z","published":"2024-01-05T09:21:54Z","title":"Calibration Attack: A Framework For Adversarial Attacks Targeting\n  Calibration","summary":"  We introduce a new framework of adversarial attacks, named calibration\nattacks, in which the attacks are generated and organized to trap victim models\nto be miscalibrated without altering their original accuracy, hence seriously\nendangering the trustworthiness of the models and any decision-making based on\ntheir confidence scores. Specifically, we identify four novel forms of\ncalibration attacks: underconfidence attacks, overconfidence attacks, maximum\nmiscalibration attacks, and random confidence attacks, in both the black-box\nand white-box setups. We then test these new attacks on typical victim models\nwith comprehensive datasets, demonstrating that even with a relatively low\nnumber of queries, the attacks can create significant calibration mistakes. We\nfurther provide detailed analyses to understand different aspects of\ncalibration attacks. Building on that, we investigate the effectiveness of\nwidely used adversarial defences and calibration methods against these types of\nattacks, which then inspires us to devise two novel defences against such\ncalibration attacks.\n","authors":["Stephen Obadinma","Xiaodan Zhu","Hongyu Guo"],"pdf_url":"https://arxiv.org/pdf/2401.02718v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.06166v4","updated":"2024-01-05T09:09:12Z","published":"2021-10-08T07:38:33Z","title":"Game Theory for Adversarial Attacks and Defenses","summary":"  Adversarial attacks can generate adversarial inputs by applying small but\nintentionally worst-case perturbations to samples from the dataset, which leads\nto even state-of-the-art deep neural networks outputting incorrect answers with\nhigh confidence. Hence, some adversarial defense techniques are developed to\nimprove the security and robustness of the models and avoid them being\nattacked. Gradually, a game-like competition between attackers and defenders\nformed, in which both players would attempt to play their best strategies\nagainst each other while maximizing their own payoffs. To solve the game, each\nplayer would choose an optimal strategy against the opponent based on the\nprediction of the opponent's strategy choice. In this work, we are on the\ndefensive side to apply game-theoretic approaches on defending against attacks.\nWe use two randomization methods, random initialization and stochastic\nactivation pruning, to create diversity of networks. Furthermore, we use one\ndenoising technique, super resolution, to improve models' robustness by\npreprocessing images before attacks. Our experimental results indicate that\nthose three methods can effectively improve the robustness of deep-learning\nneural networks.\n","authors":["Shorya Sharma"],"pdf_url":"https://arxiv.org/pdf/2110.06166v4.pdf","comment":"With the agreement of my coauthors, I would like to withdraw the\n  manuscript \"Game Theory for Adversarial Attacks and Defenses\". Some\n  experimental procedures were not included in the manuscript, which makes a\n  part of important claims not meaningful"},{"id":"http://arxiv.org/abs/2401.02713v1","updated":"2024-01-05T09:05:33Z","published":"2024-01-05T09:05:33Z","title":"Graph-level Protein Representation Learning by Structure Knowledge\n  Refinement","summary":"  This paper focuses on learning representation on the whole graph level in an\nunsupervised manner. Learning graph-level representation plays an important\nrole in a variety of real-world issues such as molecule property prediction,\nprotein structure feature extraction, and social network analysis. The\nmainstream method is utilizing contrastive learning to facilitate graph feature\nextraction, known as Graph Contrastive Learning (GCL). GCL, although effective,\nsuffers from some complications in contrastive learning, such as the effect of\nfalse negative pairs. Moreover, augmentation strategies in GCL are weakly\nadaptive to diverse graph datasets. Motivated by these problems, we propose a\nnovel framework called Structure Knowledge Refinement (SKR) which uses data\nstructure to determine the probability of whether a pair is positive or\nnegative. Meanwhile, we propose an augmentation strategy that naturally\npreserves the semantic meaning of the original data and is compatible with our\nSKR framework. Furthermore, we illustrate the effectiveness of our SKR\nframework through intuition and experiments. The experimental results on the\ntasks of graph-level classification demonstrate that our SKR framework is\nsuperior to most state-of-the-art baselines.\n","authors":["Ge Wang","Zelin Zang","Jiangbin Zheng","Jun Xia","Stan Z. Li"],"pdf_url":"https://arxiv.org/pdf/2401.02713v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02708v1","updated":"2024-01-05T08:37:57Z","published":"2024-01-05T08:37:57Z","title":"TripleSurv: Triplet Time-adaptive Coordinate Loss for Survival Analysis","summary":"  A core challenge in survival analysis is to model the distribution of\ncensored time-to-event data, where the event of interest may be a death,\nfailure, or occurrence of a specific event. Previous studies have showed that\nranking and maximum likelihood estimation (MLE)loss functions are widely-used\nfor survival analysis. However, ranking loss only focus on the ranking of\nsurvival time and does not consider potential effect of samples for exact\nsurvival time values. Furthermore, the MLE is unbounded and easily subject to\noutliers (e.g., censored data), which may cause poor performance of modeling.\nTo handle the complexities of learning process and exploit valuable survival\ntime values, we propose a time-adaptive coordinate loss function, TripleSurv,\nto achieve adaptive adjustments by introducing the differences in the survival\ntime between sample pairs into the ranking, which can encourage the model to\nquantitatively rank relative risk of pairs, ultimately enhancing the accuracy\nof predictions. Most importantly, the TripleSurv is proficient in quantifying\nthe relative risk between samples by ranking ordering of pairs, and consider\nthe time interval as a trade-off to calibrate the robustness of model over\nsample distribution. Our TripleSurv is evaluated on three real-world survival\ndatasets and a public synthetic dataset. The results show that our method\noutperforms the state-of-the-art methods and exhibits good model performance\nand robustness on modeling various sophisticated data distributions with\ndifferent censor rates. Our code will be available upon acceptance.\n","authors":["Liwen Zhang","Lianzhen Zhong","Fan Yang","Di Dong","Hui Hui","Jie Tian"],"pdf_url":"https://arxiv.org/pdf/2401.02708v1.pdf","comment":"9 pages,6 figures"},{"id":"http://arxiv.org/abs/2401.01100v2","updated":"2024-01-05T08:09:14Z","published":"2024-01-02T08:43:06Z","title":"Scalable manifold learning by uniform landmark sampling and constrained\n  locally linear embedding","summary":"  As a pivotal approach in machine learning and data science, manifold learning\naims to uncover the intrinsic low-dimensional structure within complex\nnonlinear manifolds in high-dimensional space. By exploiting the manifold\nhypothesis, various techniques for nonlinear dimension reduction have been\ndeveloped to facilitate visualization, classification, clustering, and gaining\nkey insights. Although existing manifold learning methods have achieved\nremarkable successes, they still suffer from extensive distortions incurred in\nthe global structure, which hinders the understanding of underlying patterns.\nScalability issues also limit their applicability for handling large-scale\ndata. Here, we propose a scalable manifold learning (scML) method that can\nmanipulate large-scale and high-dimensional data in an efficient manner. It\nstarts by seeking a set of landmarks to construct the low-dimensional skeleton\nof the entire data, and then incorporates the non-landmarks into the learned\nspace based on the constrained locally linear embedding (CLLE). We empirically\nvalidated the effectiveness of scML on synthetic datasets and real-world\nbenchmarks of different types, and applied it to analyze the single-cell\ntranscriptomics and detect anomalies in electrocardiogram (ECG) signals. scML\nscales well with increasing data sizes and embedding dimensions, and exhibits\npromising performance in preserving the global structure. The experiments\ndemonstrate notable robustness in embedding quality as the sample rate\ndecreases.\n","authors":["Dehua Peng","Zhipeng Gui","Wenzhang Wei","Huayi Wu"],"pdf_url":"https://arxiv.org/pdf/2401.01100v2.pdf","comment":"33 pages, 10 figures"},{"id":"http://arxiv.org/abs/2311.09441v2","updated":"2024-01-05T08:07:20Z","published":"2023-11-15T23:23:42Z","title":"Exploring the Privacy-Energy Consumption Tradeoff for Split Federated\n  Learning","summary":"  Split Federated Learning (SFL) has recently emerged as a promising\ndistributed learning technology, leveraging the strengths of both federated\nlearning and split learning. It emphasizes the advantages of rapid convergence\nwhile addressing privacy concerns. As a result, this innovation has received\nsignificant attention from both industry and academia. However, since the model\nis split at a specific layer, known as a cut layer, into both client-side and\nserver-side models for the SFL, the choice of the cut layer in SFL can have a\nsubstantial impact on the energy consumption of clients and their privacy, as\nit influences the training burden and the output of the client-side models.\nMoreover, the design challenge of determining the cut layer is highly\nintricate, primarily due to the inherent heterogeneity in the computing and\nnetworking capabilities of clients. In this article, we provide a comprehensive\noverview of the SFL process and conduct a thorough analysis of energy\nconsumption and privacy. This analysis takes into account the influence of\nvarious system parameters on the cut layer selection strategy. Additionally, we\nprovide an illustrative example of the cut layer selection, aiming to minimize\nthe risk of clients from reconstructing the raw data at the server while\nsustaining energy consumption within the required energy budget, which involve\ntrade-offs. Finally, we address open challenges in this field. These directions\nrepresent promising avenues for future research and development.\n","authors":["Joohyung Lee","Mohamed Seif","Jungchan Cho","H. Vincent Poor"],"pdf_url":"https://arxiv.org/pdf/2311.09441v2.pdf","comment":"7 pages, 5 figures"},{"id":"http://arxiv.org/abs/2401.01916v2","updated":"2024-01-05T07:46:32Z","published":"2024-01-03T04:47:02Z","title":"AstroLLaMA-Chat: Scaling AstroLLaMA with Conversational and Diverse\n  Datasets","summary":"  We explore the potential of enhancing LLM performance in astronomy-focused\nquestion-answering through targeted, continual pre-training. By employing a\ncompact 7B-parameter LLaMA-2 model and focusing exclusively on a curated set of\nastronomy corpora -- comprising abstracts, introductions, and conclusions -- we\nachieve notable improvements in specialized topic comprehension. While general\nLLMs like GPT-4 excel in broader question-answering scenarios due to superior\nreasoning capabilities, our findings suggest that continual pre-training with\nlimited resources can still enhance model performance on specialized topics.\nAdditionally, we present an extension of AstroLLaMA: the fine-tuning of the 7B\nLLaMA model on a domain-specific conversational dataset, culminating in the\nrelease of the chat-enabled AstroLLaMA for community use. Comprehensive\nquantitative benchmarking is currently in progress and will be detailed in an\nupcoming full paper. The model, AstroLLaMA-Chat, is now available at\nhttps://huggingface.co/universeTBD, providing the first open-source\nconversational AI tool tailored for the astronomy community.\n","authors":["Ernest Perkowski","Rui Pan","Tuan Dung Nguyen","Yuan-Sen Ting","Sandor Kruk","Tong Zhang","Charlie O'Neill","Maja Jablonska","Zechang Sun","Michael J. Smith","Huiling Liu","Kevin Schawinski","Kartheik Iyer","Ioana Ciucă for UniverseTBD"],"pdf_url":"https://arxiv.org/pdf/2401.01916v2.pdf","comment":"4 pages, 1 figure, model is available at\n  https://huggingface.co/universeTBD, published in RNAAS"},{"id":"http://arxiv.org/abs/2401.02687v1","updated":"2024-01-05T07:37:51Z","published":"2024-01-05T07:37:51Z","title":"PAHD: Perception-Action based Human Decision Making using Explainable\n  Graph Neural Networks on SAR Images","summary":"  Synthetic Aperture Radar (SAR) images are commonly utilized in military\napplications for automatic target recognition (ATR). Machine learning (ML)\nmethods, such as Convolutional Neural Networks (CNN) and Graph Neural Networks\n(GNN), are frequently used to identify ground-based objects, including battle\ntanks, personnel carriers, and missile launchers. Determining the vehicle\nclass, such as the BRDM2 tank, BMP2 tank, BTR60 tank, and BTR70 tank, is\ncrucial, as it can help determine whether the target object is an ally or an\nenemy. While the ML algorithm provides feedback on the recognized target, the\nfinal decision is left to the commanding officers. Therefore, providing\ndetailed information alongside the identified target can significantly impact\ntheir actions. This detailed information includes the SAR image features that\ncontributed to the classification, the classification confidence, and the\nprobability of the identified object being classified as a different object\ntype or class. We propose a GNN-based ATR framework that provides the final\nclassified class and outputs the detailed information mentioned above. This is\nthe first study to provide a detailed analysis of the classification class,\nmaking final decisions more straightforward. Moreover, our GNN framework\nachieves an overall accuracy of 99.2\\% when evaluated on the MSTAR dataset,\nimproving over previous state-of-the-art GNN methods.\n","authors":["Sasindu Wijeratne","Bingyi Zhang","Rajgopal Kannan","Viktor Prasanna","Carl Busart"],"pdf_url":"https://arxiv.org/pdf/2401.02687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02686v1","updated":"2024-01-05T07:37:35Z","published":"2024-01-05T07:37:35Z","title":"Beyond Fidelity: Explaining Vulnerability Localization of Learning-based\n  Detectors","summary":"  Vulnerability detectors based on deep learning (DL) models have proven their\neffectiveness in recent years. However, the shroud of opacity surrounding the\ndecision-making process of these detectors makes it difficult for security\nanalysts to comprehend. To address this, various explanation approaches have\nbeen proposed to explain the predictions by highlighting important features,\nwhich have been demonstrated effective in other domains such as computer vision\nand natural language processing. Unfortunately, an in-depth evaluation of\nvulnerability-critical features, such as fine-grained vulnerability-related\ncode lines, learned and understood by these explanation approaches remains\nlacking. In this study, we first evaluate the performance of ten explanation\napproaches for vulnerability detectors based on graph and sequence\nrepresentations, measured by two quantitative metrics including fidelity and\nvulnerability line coverage rate. Our results show that fidelity alone is not\nsufficient for evaluating these approaches, as fidelity incurs significant\nfluctuations across different datasets and detectors. We subsequently check the\nprecision of the vulnerability-related code lines reported by the explanation\napproaches, and find poor accuracy in this task among all of them. This can be\nattributed to the inefficiency of explainers in selecting important features\nand the presence of irrelevant artifacts learned by DL-based detectors.\n","authors":["Baijun Cheng","Shengming Zhao","Kailong Wang","Meizhen Wang","Guangdong Bai","Ruitao Feng","Yao Guo","Lei Ma","Haoyu Wang"],"pdf_url":"https://arxiv.org/pdf/2401.02686v1.pdf","comment":"Accepted by Tosem"},{"id":"http://arxiv.org/abs/2401.02683v1","updated":"2024-01-05T07:29:21Z","published":"2024-01-05T07:29:21Z","title":"Geometric-Facilitated Denoising Diffusion Model for 3D Molecule\n  Generation","summary":"  Denoising diffusion models have shown great potential in multiple research\nareas. Existing diffusion-based generative methods on de novo 3D molecule\ngeneration face two major challenges. Since majority heavy atoms in molecules\nallow connections to multiple atoms through single bonds, solely using\npair-wise distance to model molecule geometries is insufficient. Therefore, the\nfirst one involves proposing an effective neural network as the denoising\nkernel that is capable to capture complex multi-body interatomic relationships\nand learn high-quality features. Due to the discrete nature of graphs,\nmainstream diffusion-based methods for molecules heavily rely on predefined\nrules and generate edges in an indirect manner. The second challenge involves\naccommodating molecule generation to diffusion and accurately predicting the\nexistence of bonds. In our research, we view the iterative way of updating\nmolecule conformations in diffusion process is consistent with molecular\ndynamics and introduce a novel molecule generation method named\nGeometric-Facilitated Molecular Diffusion (GFMDiff). For the first challenge,\nwe introduce a Dual-Track Transformer Network (DTN) to fully excevate global\nspatial relationships and learn high quality representations which contribute\nto accurate predictions of features and geometries. As for the second\nchallenge, we design Geometric-Facilitated Loss (GFLoss) which intervenes the\nformation of bonds during the training period, instead of directly embedding\nedges into the latent space. Comprehensive experiments on current benchmarks\ndemonstrate the superiority of GFMDiff.\n","authors":["Can Xu","Haosen Wang","Weigang Wang","Pengfei Zheng","Hongyang Chen"],"pdf_url":"https://arxiv.org/pdf/2401.02683v1.pdf","comment":"9 pages, 6 figures, AAAI-24 Main Track"},{"id":"http://arxiv.org/abs/2401.02682v1","updated":"2024-01-05T07:27:29Z","published":"2024-01-05T07:27:29Z","title":"Homophily-Related: Adaptive Hybrid Graph Filter for Multi-View Graph\n  Clustering","summary":"  Recently there is a growing focus on graph data, and multi-view graph\nclustering has become a popular area of research interest. Most of the existing\nmethods are only applicable to homophilous graphs, yet the extensive real-world\ngraph data can hardly fulfill the homophily assumption, where the connected\nnodes tend to belong to the same class. Several studies have pointed out that\nthe poor performance on heterophilous graphs is actually due to the fact that\nconventional graph neural networks (GNNs), which are essentially low-pass\nfilters, discard information other than the low-frequency information on the\ngraph. Nevertheless, on certain graphs, particularly heterophilous ones,\nneglecting high-frequency information and focusing solely on low-frequency\ninformation impedes the learning of node representations. To break this\nlimitation, our motivation is to perform graph filtering that is closely\nrelated to the homophily degree of the given graph, with the aim of fully\nleveraging both low-frequency and high-frequency signals to learn\ndistinguishable node embedding. In this work, we propose Adaptive Hybrid Graph\nFilter for Multi-View Graph Clustering (AHGFC). Specifically, a graph joint\nprocess and graph joint aggregation matrix are first designed by using the\nintrinsic node features and adjacency relationship, which makes the low and\nhigh-frequency signals on the graph more distinguishable. Then we design an\nadaptive hybrid graph filter that is related to the homophily degree, which\nlearns the node embedding based on the graph joint aggregation matrix. After\nthat, the node embedding of each view is weighted and fused into a consensus\nembedding for the downstream task. Experimental results show that our proposed\nmodel performs well on six datasets containing homophilous and heterophilous\ngraphs.\n","authors":["Zichen Wen","Yawen Ling","Yazhou Ren","Tianyi Wu","Jianpeng Chen","Xiaorong Pu","Zhifeng Hao","Lifang He"],"pdf_url":"https://arxiv.org/pdf/2401.02682v1.pdf","comment":"Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2401.00031v2","updated":"2024-01-05T07:21:06Z","published":"2023-12-29T08:18:52Z","title":"Self-supervised Pretraining for Decision Foundation Model: Formulation,\n  Pipeline and Challenges","summary":"  Decision-making is a dynamic process requiring perception, memory, and\nreasoning to make choices and find optimal policies. Traditional approaches to\ndecision-making suffer from sample efficiency and generalization, while\nlarge-scale self-supervised pretraining has enabled fast adaptation with\nfine-tuning or few-shot learning in language and vision. We thus argue to\nintegrate knowledge acquired from generic large-scale self-supervised\npretraining into downstream decision-making problems. We propose\nPretrain-Then-Adapt pipeline and survey recent work on data collection,\npretraining objectives and adaptation strategies for decision-making\npretraining and downstream inference. Finally, we identify critical challenges\nand future directions for developing decision foundation model with the help of\ngeneric and flexible self-supervised pretraining.\n","authors":["Xiaoqian Liu","Jianbin Jiao","Junge Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.00031v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02675v1","updated":"2024-01-05T07:19:19Z","published":"2024-01-05T07:19:19Z","title":"LMaaS: Exploring Pricing Strategy of Large Model as a Service for\n  Communication","summary":"  The next generation of communication is envisioned to be intelligent\ncommunication, that can replace traditional symbolic communication, where\nhighly condensed semantic information considering both source and channel will\nbe extracted and transmitted with high efficiency. The recent popular large\nmodels such as GPT4 and the boosting learning techniques lay a solid foundation\nfor the intelligent communication, and prompt the practical deployment of it in\nthe near future. Given the characteristics of \"training once and widely use\" of\nthose multimodal large language models, we argue that a pay-as-you-go service\nmode will be suitable in this context, referred to as Large Model as a Service\n(LMaaS). However, the trading and pricing problem is quite complex with\nheterogeneous and dynamic customer environments, making the pricing\noptimization problem challenging in seeking on-hand solutions. In this paper,\nwe aim to fill this gap and formulate the LMaaS market trading as a Stackelberg\ngame with two steps. In the first step, we optimize the seller's pricing\ndecision and propose an Iterative Model Pricing (IMP) algorithm that optimizes\nthe prices of large models iteratively by reasoning customers' future rental\ndecisions, which is able to achieve a near-optimal pricing solution. In the\nsecond step, we optimize customers' selection decisions by designing a robust\nselecting and renting (RSR) algorithm, which is guaranteed to be optimal with\nrigorous theoretical proof. Extensive experiments confirm the effectiveness and\nrobustness of our algorithms.\n","authors":["Panlong Wu","Qi Liu","Yanjie Dong","Fangxin Wang"],"pdf_url":"https://arxiv.org/pdf/2401.02675v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01386v2","updated":"2024-01-05T07:12:41Z","published":"2024-01-01T19:58:36Z","title":"Tissue Artifact Segmentation and Severity Analysis for Automated\n  Diagnosis Using Whole Slide Images","summary":"  Traditionally, pathological analysis and diagnosis are performed by manually\neyeballing glass slide specimens under a microscope by an expert. The whole\nslide image is the digital specimen produced from the glass slide. Whole slide\nimage enabled specimens to be observed on a computer screen and led to\ncomputational pathology where computer vision and artificial intelligence are\nutilized for automated analysis and diagnosis. With the current computational\nadvancement, the entire whole slide image can be analyzed autonomously without\nhuman supervision. However, the analysis could fail or lead to wrong diagnosis\nif the whole slide image is affected by tissue artifacts such as tissue fold or\nair bubbles depending on the severity. Existing artifact detection methods rely\non experts for severity assessment to eliminate artifact affected regions from\nthe analysis. This process is time consuming, exhausting and undermines the\ngoal of automated analysis or removal of artifacts without evaluating their\nseverity, which could result in the loss of diagnostically important data.\nTherefore, it is necessary to detect artifacts and then assess their severity\nautomatically. In this paper, we propose a system that incorporates severity\nevaluation with artifact detection utilizing convolutional neural networks. The\nproposed system uses DoubleUNet to segment artifacts and an ensemble network of\nsix fine tuned convolutional neural network models to determine severity. This\nmethod outperformed current state of the art in accuracy by 9 percent for\nartifact segmentation and achieved a strong correlation of 97 percent with the\nevaluation of pathologists for severity assessment. The robustness of the\nsystem was demonstrated using our proposed heterogeneous dataset and practical\nusability was ensured by integrating it with an automated analysis system.\n","authors":["Galib Muhammad Shahriar Himel"],"pdf_url":"https://arxiv.org/pdf/2401.01386v2.pdf","comment":"60 pages, 21 figures, 16 tables"},{"id":"http://arxiv.org/abs/2401.02668v1","updated":"2024-01-05T06:52:55Z","published":"2024-01-05T06:52:55Z","title":"Towards Integrated Fine-tuning and Inference when Generative AI meets\n  Edge Intelligence","summary":"  The high-performance generative artificial intelligence (GAI) represents the\nlatest evolution of computational intelligence, while the blessing of future 6G\nnetworks also makes edge intelligence (EI) full of development potential. The\ninevitable encounter between GAI and EI can unleash new opportunities, where\nGAI's pre-training based on massive computing resources and large-scale\nunlabeled corpora can provide strong foundational knowledge for EI, while EI\ncan harness fragmented computing resources to aggregate personalized knowledge\nfor GAI. However, the natural contradictory features pose significant\nchallenges to direct knowledge sharing. To address this, in this paper, we\npropose the GAI-oriented synthetical network (GaisNet), a collaborative\ncloud-edge-end intelligence framework that buffers contradiction leveraging\ndata-free knowledge relay, where the bidirectional knowledge flow enables GAI's\nvirtuous-cycle model fine-tuning and task inference, achieving mutualism\nbetween GAI and EI with seamless fusion and collaborative evolution.\nExperimental results demonstrate the effectiveness of the proposed mechanisms.\nFinally, we discuss the future challenges and directions in the interplay\nbetween GAI and EI.\n","authors":["Ning Chen","Zhipeng Cheng","Xuwei Fan","Xiaoyu Xia","Lianfen Huang"],"pdf_url":"https://arxiv.org/pdf/2401.02668v1.pdf","comment":"11 pages, 8 figures, and 5 tables"},{"id":"http://arxiv.org/abs/2307.03756v3","updated":"2024-01-05T06:49:04Z","published":"2023-07-06T15:01:58Z","title":"FITS: Modeling Time Series with $10k$ Parameters","summary":"  In this paper, we introduce FITS, a lightweight yet powerful model for time\nseries analysis. Unlike existing models that directly process raw time-domain\ndata, FITS operates on the principle that time series can be manipulated\nthrough interpolation in the complex frequency domain. By discarding\nhigh-frequency components with negligible impact on time series data, FITS\nachieves performance comparable to state-of-the-art models for time series\nforecasting and anomaly detection tasks, while having a remarkably compact size\nof only approximately $10k$ parameters. Such a lightweight model can be easily\ntrained and deployed in edge devices, creating opportunities for various\napplications. The code is available in: \\url{https://github.com/VEWOXIC/FITS}\n","authors":["Zhijian Xu","Ailing Zeng","Qiang Xu"],"pdf_url":"https://arxiv.org/pdf/2307.03756v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02665v1","updated":"2024-01-05T06:46:56Z","published":"2024-01-05T06:46:56Z","title":"Zero-shot Microclimate Prediction with Deep Learning","summary":"  Weather station data is a valuable resource for climate prediction, however,\nits reliability can be limited in remote locations. To compound the issue,\nmaking local predictions often relies on sensor data that may not be accessible\nfor a new, previously unmonitored location. In response to these challenges, we\npropose a novel zero-shot learning approach designed to forecast various\nclimate measurements at new and unmonitored locations. Our method surpasses\nconventional weather forecasting techniques in predicting microclimate\nvariables by leveraging knowledge extracted from other geographic locations.\n","authors":["Iman Deznabi","Peeyush Kumar","Madalina Fiterau"],"pdf_url":"https://arxiv.org/pdf/2401.02665v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02663v1","updated":"2024-01-05T06:45:48Z","published":"2024-01-05T06:45:48Z","title":"A backdoor attack against link prediction tasks with graph neural\n  networks","summary":"  Graph Neural Networks (GNNs) are a class of deep learning models capable of\nprocessing graph-structured data, and they have demonstrated significant\nperformance in a variety of real-world applications. Recent studies have found\nthat GNN models are vulnerable to backdoor attacks. When specific patterns\n(called backdoor triggers, e.g., subgraphs, nodes, etc.) appear in the input\ndata, the backdoor embedded in the GNN models is activated, which misclassifies\nthe input data into the target class label specified by the attacker, whereas\nwhen there are no backdoor triggers in the input, the backdoor embedded in the\nGNN models is not activated, and the models work normally. Backdoor attacks are\nhighly stealthy and expose GNN models to serious security risks. Currently,\nresearch on backdoor attacks against GNNs mainly focus on tasks such as graph\nclassification and node classification, and backdoor attacks against link\nprediction tasks are rarely studied. In this paper, we propose a backdoor\nattack against the link prediction tasks based on GNNs and reveal the existence\nof such security vulnerability in GNN models, which make the backdoored GNN\nmodels to incorrectly predict unlinked two nodes as having a link relationship\nwhen a trigger appear. The method uses a single node as the trigger and poison\nselected node pairs in the training graph, and then the backdoor will be\nembedded in the GNN models through the training process. In the inference\nstage, the backdoor in the GNN models can be activated by simply linking the\ntrigger node to the two end nodes of the unlinked node pairs in the input data,\ncausing the GNN models to produce incorrect link prediction results for the\ntarget node pairs.\n","authors":["Jiazhu Dai","Haoyu Sun"],"pdf_url":"https://arxiv.org/pdf/2401.02663v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.05305v2","updated":"2024-01-05T06:45:02Z","published":"2023-09-11T08:44:07Z","title":"Fully-Connected Spatial-Temporal Graph for Multivariate Time-Series Data","summary":"  Multivariate Time-Series (MTS) data is crucial in various application fields.\nWith its sequential and multi-source (multiple sensors) properties, MTS data\ninherently exhibits Spatial-Temporal (ST) dependencies, involving temporal\ncorrelations between timestamps and spatial correlations between sensors in\neach timestamp. To effectively leverage this information, Graph Neural\nNetwork-based methods (GNNs) have been widely adopted. However, existing\napproaches separately capture spatial dependency and temporal dependency and\nfail to capture the correlations between Different sEnsors at Different\nTimestamps (DEDT). Overlooking such correlations hinders the comprehensive\nmodelling of ST dependencies within MTS data, thus restricting existing GNNs\nfrom learning effective representations. To address this limitation, we propose\na novel method called Fully-Connected Spatial-Temporal Graph Neural Network\n(FC-STGNN), including two key components namely FC graph construction and FC\ngraph convolution. For graph construction, we design a decay graph to connect\nsensors across all timestamps based on their temporal distances, enabling us to\nfully model the ST dependencies by considering the correlations between DEDT.\nFurther, we devise FC graph convolution with a moving-pooling GNN layer to\neffectively capture the ST dependencies for learning effective representations.\nExtensive experiments show the effectiveness of FC-STGNN on multiple MTS\ndatasets compared to SOTA methods.\n","authors":["Yucheng Wang","Yuecong Xu","Jianfei Yang","Min Wu","Xiaoli Li","Lihua Xie","Zhenghua Chen"],"pdf_url":"https://arxiv.org/pdf/2309.05305v2.pdf","comment":"10 pages, 8 figures, Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2309.05202v2","updated":"2024-01-05T06:40:02Z","published":"2023-09-11T02:35:22Z","title":"Graph-Aware Contrasting for Multivariate Time-Series Classification","summary":"  Contrastive learning, as a self-supervised learning paradigm, becomes popular\nfor Multivariate Time-Series (MTS) classification. It ensures the consistency\nacross different views of unlabeled samples and then learns effective\nrepresentations for these samples. Existing contrastive learning methods mainly\nfocus on achieving temporal consistency with temporal augmentation and\ncontrasting techniques, aiming to preserve temporal patterns against\nperturbations for MTS data. However, they overlook spatial consistency that\nrequires the stability of individual sensors and their correlations. As MTS\ndata typically originate from multiple sensors, ensuring spatial consistency\nbecomes essential for the overall performance of contrastive learning on MTS\ndata. Thus, we propose Graph-Aware Contrasting for spatial consistency across\nMTS data. Specifically, we propose graph augmentations including node and edge\naugmentations to preserve the stability of sensors and their correlations,\nfollowed by graph contrasting with both node- and graph-level contrasting to\nextract robust sensor- and global-level features. We further introduce\nmulti-window temporal contrasting to ensure temporal consistency in the data\nfor each sensor. Extensive experiments demonstrate that our proposed method\nachieves state-of-the-art performance on various MTS classification tasks.\n","authors":["Yucheng Wang","Yuecong Xu","Jianfei Yang","Min Wu","Xiaoli Li","Lihua Xie","Zhenghua Chen"],"pdf_url":"https://arxiv.org/pdf/2309.05202v2.pdf","comment":"10 pages, 7 figures, Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2401.02661v1","updated":"2024-01-05T06:38:50Z","published":"2024-01-05T06:38:50Z","title":"Nurse-in-the-Loop Artificial Intelligence for Precision Management of\n  Type 2 Diabetes in a Clinical Trial Utilizing Transfer-Learned Predictive\n  Digital Twin","summary":"  Background: Type 2 diabetes (T2D) is a prevalent chronic disease with a\nsignificant risk of serious health complications and negative impacts on the\nquality of life. Given the impact of individual characteristics and lifestyle\non the treatment plan and patient outcomes, it is crucial to develop precise\nand personalized management strategies. Artificial intelligence (AI) provides\ngreat promise in combining patterns from various data sources with nurses'\nexpertise to achieve optimal care. Methods: This is a 6-month ancillary study\namong T2D patients (n = 20, age = 57 +- 10). Participants were randomly\nassigned to an intervention (AI, n=10) group to receive daily AI-generated\nindividualized feedback or a control group without receiving the daily feedback\n(non-AI, n=10) in the last three months. The study developed an online\nnurse-in-the-loop predictive control (ONLC) model that utilizes a predictive\ndigital twin (PDT). The PDT was developed using a transfer-learning-based\nArtificial Neural Network. The PDT was trained on participants self-monitoring\ndata (weight, food logs, physical activity, glucose) from the first three\nmonths, and the online control algorithm applied particle swarm optimization to\nidentify impactful behavioral changes for maintaining the patient's glucose and\nweight levels for the next three months. The ONLC provided the intervention\ngroup with individualized feedback and recommendations via text messages. The\nPDT was re-trained weekly to improve its performance. Findings: The trained\nONLC model achieved >=80% prediction accuracy across all patients while the\nmodel was tuned online. Participants in the intervention group exhibited a\ntrend of improved daily steps and stable or improved total caloric and total\ncarb intake as recommended.\n","authors":["Syed Hasib Akhter Faruqui","Adel Alaeddini","Yan Du","Shiyu Li","Kumar Sharma","Jing Wang"],"pdf_url":"https://arxiv.org/pdf/2401.02661v1.pdf","comment":"Submitted for review"},{"id":"http://arxiv.org/abs/2111.06318v2","updated":"2024-01-05T06:37:09Z","published":"2021-11-11T17:17:24Z","title":"Multi-agent Reinforcement Learning for Cooperative Lane Changing of\n  Connected and Autonomous Vehicles in Mixed Traffic","summary":"  Autonomous driving has attracted significant research interests in the past\ntwo decades as it offers many potential benefits, including releasing drivers\nfrom exhausting driving and mitigating traffic congestion, among others.\nDespite promising progress, lane-changing remains a great challenge for\nautonomous vehicles (AV), especially in mixed and dynamic traffic scenarios.\nRecently, reinforcement learning (RL), a powerful data-driven control method,\nhas been widely explored for lane-changing decision makings in AVs with\nencouraging results demonstrated. However, the majority of those studies are\nfocused on a single-vehicle setting, and lane-changing in the context of\nmultiple AVs coexisting with human-driven vehicles (HDVs) have received scarce\nattention. In this paper, we formulate the lane-changing decision making of\nmultiple AVs in a mixed-traffic highway environment as a multi-agent\nreinforcement learning (MARL) problem, where each AV makes lane-changing\ndecisions based on the motions of both neighboring AVs and HDVs. Specifically,\na multi-agent advantage actor-critic network (MA2C) is developed with a novel\nlocal reward design and a parameter sharing scheme. In particular, a\nmulti-objective reward function is proposed to incorporate fuel efficiency,\ndriving comfort, and safety of autonomous driving. Comprehensive experimental\nresults, conducted under three different traffic densities and various levels\nof human driver aggressiveness, show that our proposed MARL framework\nconsistently outperforms several state-of-the-art benchmarks in terms of\nefficiency, safety and driver comfort.\n","authors":["Wei Zhou","Dong Chen","Jun Yan","Zhaojian Li","Huilin Yin","Wanchen Ge"],"pdf_url":"https://arxiv.org/pdf/2111.06318v2.pdf","comment":"This paper was published on Autonomous Intelligent Systems (Volume 2,\n  article number 5, 2022)"},{"id":"http://arxiv.org/abs/2401.02656v1","updated":"2024-01-05T06:24:41Z","published":"2024-01-05T06:24:41Z","title":"GTA: Guided Transfer of Spatial Attention from Object-Centric\n  Representations","summary":"  Utilizing well-trained representations in transfer learning often results in\nsuperior performance and faster convergence compared to training from scratch.\nHowever, even if such good representations are transferred, a model can easily\noverfit the limited training dataset and lose the valuable properties of the\ntransferred representations. This phenomenon is more severe in ViT due to its\nlow inductive bias. Through experimental analysis using attention maps in ViT,\nwe observe that the rich representations deteriorate when trained on a small\ndataset. Motivated by this finding, we propose a novel and simple\nregularization method for ViT called Guided Transfer of spatial Attention\n(GTA). Our proposed method regularizes the self-attention maps between the\nsource and target models. A target model can fully exploit the knowledge\nrelated to object localization properties through this explicit regularization.\nOur experimental results show that the proposed GTA consistently improves the\naccuracy across five benchmark datasets especially when the number of training\ndata is small.\n","authors":["SeokHyun Seo","Jinwoo Hong","JungWoo Chae","Kyungyul Kim","Sangheum Hwang"],"pdf_url":"https://arxiv.org/pdf/2401.02656v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.07728v3","updated":"2024-01-05T06:22:32Z","published":"2023-08-15T12:08:43Z","title":"Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability","summary":"  Fine-tuning pre-trained neural network models has become a widely adopted\napproach across various domains. However, it can lead to the distortion of\npre-trained feature extractors that already possess strong generalization\ncapabilities. Mitigating feature distortion during adaptation to new target\ndomains is crucial. Recent studies have shown promising results in handling\nfeature distortion by aligning the head layer on in-distribution datasets\nbefore performing fine-tuning. Nonetheless, a significant limitation arises\nfrom the treatment of batch normalization layers during fine-tuning, leading to\nsuboptimal performance. In this paper, we propose Domain-Aware Fine-Tuning\n(DAFT), a novel approach that incorporates batch normalization conversion and\nthe integration of linear probing and fine-tuning. Our batch normalization\nconversion method effectively mitigates feature distortion by reducing\nmodifications to the neural network during fine-tuning. Additionally, we\nintroduce the integration of linear probing and fine-tuning to optimize the\nhead layer with gradual adaptation of the feature extractor. By leveraging\nbatch normalization layers and integrating linear probing and fine-tuning, our\nDAFT significantly mitigates feature distortion and achieves improved model\nperformance on both in-distribution and out-of-distribution datasets. Extensive\nexperiments demonstrate that our method outperforms other baseline methods,\ndemonstrating its effectiveness in not only improving performance but also\nmitigating feature distortion.\n","authors":["Seokhyeon Ha","Sunbeom Jung","Jungwoo Lee"],"pdf_url":"https://arxiv.org/pdf/2308.07728v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02653v1","updated":"2024-01-05T06:04:46Z","published":"2024-01-05T06:04:46Z","title":"A Deep Q-Learning based Smart Scheduling of EVs for Demand Response in\n  Smart Grids","summary":"  Economic and policy factors are driving the continuous increase in the\nadoption and usage of electrical vehicles (EVs). However, despite being a\ncleaner alternative to combustion engine vehicles, EVs have negative impacts on\nthe lifespan of microgrid equipment and energy balance due to increased power\ndemand and the timing of their usage. In our view grid management should\nleverage on EVs scheduling flexibility to support local network balancing\nthrough active participation in demand response programs. In this paper, we\npropose a model-free solution, leveraging Deep Q-Learning to schedule the\ncharging and discharging activities of EVs within a microgrid to align with a\ntarget energy profile provided by the distribution system operator. We adapted\nthe Bellman Equation to assess the value of a state based on specific rewards\nfor EV scheduling actions and used a neural network to estimate Q-values for\navailable actions and the epsilon-greedy algorithm to balance exploitation and\nexploration to meet the target energy profile. The results are promising\nshowing that the proposed solution can effectively schedule the EVs charging\nand discharging actions to align with the target profile with a Person\ncoefficient of 0.99, handling effective EVs scheduling situations that involve\ndynamicity given by the e-mobility features, relying only on data with no\nknowledge of EVs and microgrid dynamics.\n","authors":["Viorica Rozina Chifu","Tudor Cioara","Cristina Bianca Pop","Horia Rusu","Ionut Anghel"],"pdf_url":"https://arxiv.org/pdf/2401.02653v1.pdf","comment":"Submitted to journal"},{"id":"http://arxiv.org/abs/2401.02652v1","updated":"2024-01-05T06:03:14Z","published":"2024-01-05T06:03:14Z","title":"Adaptive Discounting of Training Time Attacks","summary":"  Among the most insidious attacks on Reinforcement Learning (RL) solutions are\ntraining-time attacks (TTAs) that create loopholes and backdoors in the learned\nbehaviour. Not limited to a simple disruption, constructive TTAs (C-TTAs) are\nnow available, where the attacker forces a specific, target behaviour upon a\ntraining RL agent (victim). However, even state-of-the-art C-TTAs focus on\ntarget behaviours that could be naturally adopted by the victim if not for a\nparticular feature of the environment dynamics, which C-TTAs exploit. In this\nwork, we show that a C-TTA is possible even when the target behaviour is\nun-adoptable due to both environment dynamics as well as non-optimality with\nrespect to the victim objective(s). To find efficient attacks in this context,\nwe develop a specialised flavour of the DDPG algorithm, which we term\ngammaDDPG, that learns this stronger version of C-TTA. gammaDDPG dynamically\nalters the attack policy planning horizon based on the victim's current\nbehaviour. This improves effort distribution throughout the attack timeline and\nreduces the effect of uncertainty the attacker has about the victim. To\ndemonstrate the features of our method and better relate the results to prior\nresearch, we borrow a 3D grid domain from a state-of-the-art C-TTA for our\nexperiments. Code is available at \"bit.ly/github-rb-gDDPG\".\n","authors":["Ridhima Bector","Abhay Aradhya","Chai Quek","Zinovi Rabinovich"],"pdf_url":"https://arxiv.org/pdf/2401.02652v1.pdf","comment":"19 pages, 7 figures"},{"id":"http://arxiv.org/abs/2401.02650v1","updated":"2024-01-05T05:56:42Z","published":"2024-01-05T05:56:42Z","title":"Improving sample efficiency of high dimensional Bayesian optimization\n  with MCMC","summary":"  Sequential optimization methods are often confronted with the curse of\ndimensionality in high-dimensional spaces. Current approaches under the\nGaussian process framework are still burdened by the computational complexity\nof tracking Gaussian process posteriors and need to partition the optimization\nproblem into small regions to ensure exploration or assume an underlying\nlow-dimensional structure. With the idea of transiting the candidate points\ntowards more promising positions, we propose a new method based on Markov Chain\nMonte Carlo to efficiently sample from an approximated posterior. We provide\ntheoretical guarantees of its convergence in the Gaussian process Thompson\nsampling setting. We also show experimentally that both the Metropolis-Hastings\nand the Langevin Dynamics version of our algorithm outperform state-of-the-art\nmethods in high-dimensional sequential optimization and reinforcement learning\nbenchmarks.\n","authors":["Zeji Yi","Yunyue Wei","Chu Xin Cheng","Kaibo He","Yanan Sui"],"pdf_url":"https://arxiv.org/pdf/2401.02650v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.00201v2","updated":"2024-01-05T05:54:58Z","published":"2023-09-01T01:40:58Z","title":"Subjectivity in Unsupervised Machine Learning Model Selection","summary":"  Model selection is a necessary step in unsupervised machine learning. Despite\nnumerous criteria and metrics, model selection remains subjective. A high\ndegree of subjectivity may lead to questions about repeatability and\nreproducibility of various machine learning studies and doubts about the\nrobustness of models deployed in the real world. Yet, the impact of modelers'\npreferences on model selection outcomes remains largely unexplored. This study\nuses the Hidden Markov Model as an example to investigate the subjectivity\ninvolved in model selection. We asked 33 participants and three Large Language\nModels (LLMs) to make model selections in three scenarios. Results revealed\nvariability and inconsistencies in both the participants' and the LLMs'\nchoices, especially when different criteria and metrics disagree. Sources of\nsubjectivity include varying opinions on the importance of different criteria\nand metrics, differing views on how parsimonious a model should be, and how the\nsize of a dataset should influence model selection. The results underscore the\nimportance of developing a more standardized way to document subjective choices\nmade in model selection processes.\n","authors":["Wanyi Chen","Mary L. Cummings"],"pdf_url":"https://arxiv.org/pdf/2309.00201v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02644v1","updated":"2024-01-05T05:28:40Z","published":"2024-01-05T05:28:40Z","title":"Simple Hierarchical Planning with Diffusion","summary":"  Diffusion-based generative methods have proven effective in modeling\ntrajectories with offline datasets. However, they often face computational\nchallenges and can falter in generalization, especially in capturing temporal\nabstractions for long-horizon tasks. To overcome this, we introduce the\nHierarchical Diffuser, a simple, fast, yet surprisingly effective planning\nmethod combining the advantages of hierarchical and diffusion-based planning.\nOur model adopts a \"jumpy\" planning strategy at the higher level, which allows\nit to have a larger receptive field but at a lower computational cost -- a\ncrucial factor for diffusion-based planning methods, as we have empirically\nverified. Additionally, the jumpy sub-goals guide our low-level planner,\nfacilitating a fine-tuning stage and further improving our approach's\neffectiveness. We conducted empirical evaluations on standard offline\nreinforcement learning benchmarks, demonstrating our method's superior\nperformance and efficiency in terms of training and planning speed compared to\nthe non-hierarchical Diffuser as well as other hierarchical planning methods.\nMoreover, we explore our model's generalization capability, particularly on how\nour method improves generalization capabilities on compositional\nout-of-distribution tasks.\n","authors":["Chang Chen","Fei Deng","Kenji Kawaguchi","Caglar Gulcehre","Sungjin Ahn"],"pdf_url":"https://arxiv.org/pdf/2401.02644v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.03890v2","updated":"2024-01-05T05:14:13Z","published":"2023-05-06T01:33:26Z","title":"Approximation by non-symmetric networks for cross-domain learning","summary":"  For the past 30 years or so, machine learning has stimulated a great deal of\nresearch in the study of approximation capabilities (expressive power) of a\nmultitude of processes, such as approximation by shallow or deep neural\nnetworks, radial basis function networks, and a variety of kernel based\nmethods. Motivated by applications such as invariant learning, transfer\nlearning, and synthetic aperture radar imaging, we initiate in this paper a\ngeneral approach to study the approximation capabilities of kernel based\nnetworks using non-symmetric kernels. While singular value decomposition is a\nnatural instinct to study such kernels, we consider a more general approach to\ninclude the use of a family of kernels, such as generalized translation\nnetworks (which include neural networks and translation invariant kernels as\nspecial cases) and rotated zonal function kernels. Naturally, unlike\ntraditional kernel based approximation, we cannot require the kernels to be\npositive definite. In particular, we obtain estimates on the accuracy of\nuniform approximation of functions in a ($L^2$)-Sobolev class by ReLU$^r$\nnetworks when $r$ is not necessarily an integer. Our general results apply to\nthe approximation of functions with small smoothness compared to the dimension\nof the input space.\n","authors":["Hrushikesh Mhaskar"],"pdf_url":"https://arxiv.org/pdf/2305.03890v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02630v1","updated":"2024-01-05T04:25:21Z","published":"2024-01-05T04:25:21Z","title":"Model-Agnostic Interpretation Framework in Machine Learning: A\n  Comparative Study in NBA Sports","summary":"  The field of machine learning has seen tremendous progress in recent years,\nwith deep learning models delivering exceptional performance across a range of\ntasks. However, these models often come at the cost of interpretability, as\nthey operate as opaque \"black boxes\" that obscure the rationale behind their\ndecisions. This lack of transparency can limit understanding of the models'\nunderlying principles and impede their deployment in sensitive domains, such as\nhealthcare or finance. To address this challenge, our research team has\nproposed an innovative framework designed to reconcile the trade-off between\nmodel performance and interpretability. Our approach is centered around modular\noperations on high-dimensional data, which enable end-to-end processing while\npreserving interpretability. By fusing diverse interpretability techniques and\nmodularized data processing, our framework sheds light on the decision-making\nprocesses of complex models without compromising their performance. We have\nextensively tested our framework and validated its superior efficacy in\nachieving a harmonious balance between computational efficiency and\ninterpretability. Our approach addresses a critical need in contemporary\nmachine learning applications by providing unprecedented insights into the\ninner workings of complex models, fostering trust, transparency, and\naccountability in their deployment across diverse domains.\n","authors":["Shun Liu"],"pdf_url":"https://arxiv.org/pdf/2401.02630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.08109v2","updated":"2024-01-05T04:03:51Z","published":"2023-06-13T19:55:46Z","title":"Provable Accelerated Convergence of Nesterov's Momentum for Deep ReLU\n  Neural Networks","summary":"  Current state-of-the-art analyses on the convergence of gradient descent for\ntraining neural networks focus on characterizing properties of the loss\nlandscape, such as the Polyak-Lojaciewicz (PL) condition and the restricted\nstrong convexity. While gradient descent converges linearly under such\nconditions, it remains an open question whether Nesterov's momentum enjoys\naccelerated convergence under similar settings and assumptions. In this work,\nwe consider a new class of objective functions, where only a subset of the\nparameters satisfies strong convexity, and show Nesterov's momentum achieves\nacceleration in theory for this objective class. We provide two realizations of\nthe problem class, one of which is deep ReLU networks, which --to the best of\nour knowledge--constitutes this work the first that proves accelerated\nconvergence rate for non-trivial neural network architectures.\n","authors":["Fangshuo Liao","Anastasios Kyrillidis"],"pdf_url":"https://arxiv.org/pdf/2306.08109v2.pdf","comment":"Accepted by ALT 2024"},{"id":"http://arxiv.org/abs/2312.13143v3","updated":"2024-01-05T02:14:13Z","published":"2023-12-20T16:04:02Z","title":"Underwater Acoustic Signal Recognition Based on Salient Feature","summary":"  With the rapid advancement of technology, the recognition of underwater\nacoustic signals in complex environments has become increasingly crucial.\nCurrently, mainstream underwater acoustic signal recognition relies primarily\non time-frequency analysis to extract spectral features, finding widespread\napplications in the field. However, existing recognition methods heavily depend\non expert systems, facing limitations such as restricted knowledge bases and\nchallenges in handling complex relationships. These limitations stem from the\ncomplexity and maintenance difficulties associated with rules or inference\nengines. Recognizing the potential advantages of deep learning in handling\nintricate relationships, this paper proposes a method utilizing neural networks\nfor underwater acoustic signal recognition. The proposed approach involves\ncontinual learning of features extracted from spectra for the classification of\nunderwater acoustic signals. Deep learning models can automatically learn\nabstract features from data and continually adjust weights during training to\nenhance classification performance.\n","authors":["Minghao Chen"],"pdf_url":"https://arxiv.org/pdf/2312.13143v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04889v2","updated":"2024-01-05T02:07:48Z","published":"2023-12-08T08:11:11Z","title":"KwaiAgents: Generalized Information-seeking Agent System with Large\n  Language Models","summary":"  Driven by curiosity, humans have continually sought to explore and understand\nthe world around them, leading to the invention of various tools to satiate\nthis inquisitiveness. Despite not having the capacity to process and memorize\nvast amounts of information in their brains, humans excel in critical thinking,\nplanning, reflection, and harnessing available tools to interact with and\ninterpret the world, enabling them to find answers efficiently. The recent\nadvancements in large language models (LLMs) suggest that machines might also\npossess the aforementioned human-like capabilities, allowing them to exhibit\npowerful abilities even with a constrained parameter count. In this paper, we\nintroduce KwaiAgents, a generalized information-seeking agent system based on\nLLMs. Within KwaiAgents, we propose an agent system that employs LLMs as its\ncognitive core, which is capable of understanding a user's query, behavior\nguidelines, and referencing external documents. The agent can also update and\nretrieve information from its internal memory, plan and execute actions using a\ntime-aware search-browse toolkit, and ultimately provide a comprehensive\nresponse. We further investigate the system's performance when powered by LLMs\nless advanced than GPT-4, and introduce the Meta-Agent Tuning (MAT) framework,\ndesigned to ensure even an open-sourced 7B or 13B model performs well among\nmany agent systems. We exploit both benchmark and human evaluations to\nsystematically validate these capabilities. Extensive experiments show the\nsuperiority of our agent system compared to other autonomous agents and\nhighlight the enhanced generalized agent-abilities of our fine-tuned LLMs.\n","authors":["Haojie Pan","Zepeng Zhai","Hao Yuan","Yaojia Lv","Ruiji Fu","Ming Liu","Zhongyuan Wang","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2312.04889v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02602v1","updated":"2024-01-05T02:00:27Z","published":"2024-01-05T02:00:27Z","title":"Neural Causal Abstractions","summary":"  The abilities of humans to understand the world in terms of cause and effect\nrelationships, as well as to compress information into abstract concepts, are\ntwo hallmark features of human intelligence. These two topics have been studied\nin tandem in the literature under the rubric of causal abstractions theory. In\npractice, it remains an open problem how to best leverage abstraction theory in\nreal-world causal inference tasks, where the true mechanisms are unknown and\nonly limited data is available. In this paper, we develop a new family of\ncausal abstractions by clustering variables and their domains. This approach\nrefines and generalizes previous notions of abstractions to better accommodate\nindividual causal distributions that are spawned by Pearl's causal hierarchy.\nWe show that such abstractions are learnable in practical settings through\nNeural Causal Models (Xia et al., 2021), enabling the use of the deep learning\ntoolkit to solve various challenging causal inference tasks -- identification,\nestimation, sampling -- at different levels of granularity. Finally, we\nintegrate these results with representation learning to create more flexible\nabstractions, moving these results closer to practical applications. Our\nexperiments support the theory and illustrate how to scale causal inferences to\nhigh-dimensional settings involving image data.\n","authors":["Kevin Xia","Elias Bareinboim"],"pdf_url":"https://arxiv.org/pdf/2401.02602v1.pdf","comment":"47 total pages, 20 figures, short version accepted to AAAI-24"},{"id":"http://arxiv.org/abs/2211.07866v4","updated":"2024-01-05T02:00:10Z","published":"2022-11-15T03:17:11Z","title":"Efficient Estimation for Longitudinal Networks via Adaptive Merging","summary":"  Longitudinal network consists of a sequence of temporal edges among multiple\nnodes, where the temporal edges are observed in real time. It has become\nubiquitous with the rise of online social platform and e-commerce, but largely\nunder-investigated in literature. In this paper, we propose an efficient\nestimation framework for longitudinal network, leveraging strengths of adaptive\nnetwork merging, tensor decomposition and point process. It merges neighboring\nsparse networks so as to enlarge the number of observed edges and reduce\nestimation variance, whereas the estimation bias introduced by network merging\nis controlled by exploiting local temporal structures for adaptive network\nneighborhood. A projected gradient descent algorithm is proposed to facilitate\nestimation, where the upper bound of the estimation error in each iteration is\nestablished. A thorough analysis is conducted to quantify the asymptotic\nbehavior of the proposed method, which shows that it can significantly reduce\nthe estimation error and also provides guideline for network merging under\nvarious scenarios. We further demonstrate the advantage of the proposed method\nthrough extensive numerical experiments on synthetic datasets and a militarized\ninterstate dispute dataset.\n","authors":["Haoran Zhang","Junhui Wang"],"pdf_url":"https://arxiv.org/pdf/2211.07866v4.pdf","comment":"30 pages and 4 figures; appendix including technical proof will be\n  uploaded later"},{"id":"http://arxiv.org/abs/2308.12325v2","updated":"2024-01-05T01:28:36Z","published":"2023-08-23T15:35:20Z","title":"Predicting Drug Solubility Using Different Machine Learning Methods --\n  Linear Regression Model with Extracted Chemical Features vs Graph\n  Convolutional Neural Network","summary":"  Predicting the solubility of given molecules remains crucial in the\npharmaceutical industry. In this study, we revisited this extensively studied\ntopic, leveraging the capabilities of contemporary computing resources. We\nemployed two machine learning models: a linear regression model and a graph\nconvolutional neural network (GCNN) model, using various experimental datasets.\nBoth methods yielded reasonable predictions, with the GCNN model exhibiting the\nhighest level of performance. However, the present GCNN model has limited\ninterpretability while the linear regression model allows scientists for a\ngreater in-depth analysis of the underlying factors through feature importance\nanalysis, although more human inputs and evaluations on the overall dataset is\nrequired. From the perspective of chemistry, using the linear regression model,\nwe elucidated the impact of individual atom species and functional groups on\noverall solubility, highlighting the significance of comprehending how chemical\nstructure influences chemical properties in the drug development process. It is\nlearned that introducing oxygen atoms can increase the solubility of organic\nmolecules, while almost all other hetero atoms except oxygen and nitrogen tend\nto decrease solubility.\n","authors":["John Ho","Zhao-Heng Yin","Colin Zhang","Nicole Guo","Yang Ha"],"pdf_url":"https://arxiv.org/pdf/2308.12325v2.pdf","comment":"7 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2401.00023v2","updated":"2024-01-05T01:23:28Z","published":"2023-12-28T22:54:15Z","title":"CycleGAN Models for MRI Image Translation","summary":"  Image-to-image translation has gained popularity in the medical field to\ntransform images from one domain to another. Medical image synthesis via domain\ntransformation is advantageous in its ability to augment an image dataset where\nimages for a given class is limited. From the learning perspective, this\nprocess contributes to data-oriented robustness of the model by inherently\nbroadening the model's exposure to more diverse visual data and enabling it to\nlearn more generalized features. In the case of generating additional\nneuroimages, it is advantageous to obtain unidentifiable medical data and\naugment smaller annotated datasets. This study proposes the development of a\nCycleGAN model for translating neuroimages from one field strength to another\n(e.g., 3 Tesla to 1.5). This model was compared to a model based on DCGAN\narchitecture. CycleGAN was able to generate the synthetic and reconstructed\nimages with reasonable accuracy. The mapping function from the source (3 Tesla)\nto target domain (1.5 Tesla) performed optimally with an average PSNR value of\n25.69 $\\pm$ 2.49 dB and an MAE value of 2106.27 $\\pm$ 1218.37.\n","authors":["Cassandra Czobit","Reza Samavi"],"pdf_url":"https://arxiv.org/pdf/2401.00023v2.pdf","comment":"Accepted and presented in ACML PRHA 2023 workshop"},{"id":"http://arxiv.org/abs/2401.02592v1","updated":"2024-01-05T01:17:16Z","published":"2024-01-05T01:17:16Z","title":"Guaranteed Nonconvex Factorization Approach for Tensor Train Recovery","summary":"  In this paper, we provide the first convergence guarantee for the\nfactorization approach. Specifically, to avoid the scaling ambiguity and to\nfacilitate theoretical analysis, we optimize over the so-called left-orthogonal\nTT format which enforces orthonormality among most of the factors. To ensure\nthe orthonormal structure, we utilize the Riemannian gradient descent (RGD) for\noptimizing those factors over the Stiefel manifold. We first delve into the TT\nfactorization problem and establish the local linear convergence of RGD.\nNotably, the rate of convergence only experiences a linear decline as the\ntensor order increases. We then study the sensing problem that aims to recover\na TT format tensor from linear measurements. Assuming the sensing operator\nsatisfies the restricted isometry property (RIP), we show that with a proper\ninitialization, which could be obtained through spectral initialization, RGD\nalso converges to the ground-truth tensor at a linear rate. Furthermore, we\nexpand our analysis to encompass scenarios involving Gaussian noise in the\nmeasurements. We prove that RGD can reliably recover the ground truth at a\nlinear rate, with the recovery error exhibiting only polynomial growth in\nrelation to the tensor order. We conduct various experiments to validate our\ntheoretical findings.\n","authors":["Zhen Qin","Michael B. Wakin","Zhihui Zhu"],"pdf_url":"https://arxiv.org/pdf/2401.02592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02591v1","updated":"2024-01-05T01:08:26Z","published":"2024-01-05T01:08:26Z","title":"Synthetic Information towards Maximum Posterior Ratio for deep learning\n  on Imbalanced Data","summary":"  This study examines the impact of class-imbalanced data on deep learning\nmodels and proposes a technique for data balancing by generating synthetic data\nfor the minority class. Unlike random-based oversampling, our method\nprioritizes balancing the informative regions by identifying high entropy\nsamples. Generating well-placed synthetic data can enhance machine learning\nalgorithms accuracy and efficiency, whereas poorly-placed ones may lead to\nhigher misclassification rates. We introduce an algorithm that maximizes the\nprobability of generating a synthetic sample in the correct region of its class\nby optimizing the class posterior ratio. Additionally, to maintain data\ntopology, synthetic data are generated within each minority sample's\nneighborhood. Our experimental results on forty-one datasets demonstrate the\nsuperior performance of our technique in enhancing deep-learning models.\n","authors":["Hung Nguyen","Morris Chang"],"pdf_url":"https://arxiv.org/pdf/2401.02591v1.pdf","comment":"Accepted to IEEE Transaction on Artificial Intelligence"},{"id":"http://arxiv.org/abs/2308.12075v2","updated":"2024-01-05T00:56:41Z","published":"2023-08-23T11:48:35Z","title":"Stabilizing RNN Gradients through Pre-training","summary":"  Numerous theories of learning propose to prevent the gradient from\nexponential growth with depth or time, to stabilize and improve training.\nTypically, these analyses are conducted on feed-forward fully-connected neural\nnetworks or simple single-layer recurrent neural networks, given their\nmathematical tractability. In contrast, this study demonstrates that\npre-training the network to local stability can be effective whenever the\narchitectures are too complex for an analytical initialization. Furthermore, we\nextend known stability theories to encompass a broader family of deep recurrent\nnetworks, requiring minimal assumptions on data and parameter distribution, a\ntheory we call the Local Stability Condition (LSC). Our investigation reveals\nthat the classical Glorot, He, and Orthogonal initialization schemes satisfy\nthe LSC when applied to feed-forward fully-connected neural networks. However,\nanalysing deep recurrent networks, we identify a new additive source of\nexponential explosion that emerges from counting gradient paths in a\nrectangular grid in depth and time. We propose a new approach to mitigate this\nissue, that consists on giving a weight of a half to the time and depth\ncontributions to the gradient, instead of the classical weight of one. Our\nempirical results confirm that pre-training both feed-forward and recurrent\nnetworks, for differentiable, neuromorphic and state-space models to fulfill\nthe LSC, often results in improved final performance. This study contributes to\nthe field by providing a means to stabilize networks of any complexity. Our\napproach can be implemented as an additional step before pre-training on large\naugmented datasets, and as an alternative to finding stable initializations\nanalytically.\n","authors":["Luca Herranz-Celotti","Jean Rouat"],"pdf_url":"https://arxiv.org/pdf/2308.12075v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02586v1","updated":"2024-01-05T00:46:11Z","published":"2024-01-05T00:46:11Z","title":"Federated Learning for distribution skewed data using sample weights","summary":"  One of the most challenging issues in federated learning is that the data is\noften not independent and identically distributed (nonIID). Clients are\nexpected to contribute the same type of data and drawn from one global\ndistribution. However, data are often collected in different ways from\ndifferent resources. Thus, the data distributions among clients might be\ndifferent from the underlying global distribution. This creates a weight\ndivergence issue and reduces federated learning performance. This work focuses\non improving federated learning performance for skewed data distribution across\nclients. The main idea is to adjust the client distribution closer to the\nglobal distribution using sample weights. Thus, the machine learning model\nconverges faster with higher accuracy. We start from the fundamental concept of\nempirical risk minimization and theoretically derive a solution for adjusting\nthe distribution skewness using sample weights. To determine sample weights, we\nimplicitly exchange density information by leveraging a neural network-based\ndensity estimation model, MADE. The clients data distribution can then be\nadjusted without exposing their raw data. Our experiment results on three\nreal-world datasets show that the proposed method not only improves federated\nlearning accuracy but also significantly reduces communication costs compared\nto the other experimental methods.\n","authors":["Hung Nguyen","Peiyuan Wu","Morris Chang"],"pdf_url":"https://arxiv.org/pdf/2401.02586v1.pdf","comment":"Accepted to IEEE Transaction on Artificial Intelligence"}],"Multimedia":[{"id":"http://arxiv.org/abs/2401.02838v1","updated":"2024-01-05T14:45:45Z","published":"2024-01-05T14:45:45Z","title":"CrisisViT: A Robust Vision Transformer for Crisis Image Classification","summary":"  In times of emergency, crisis response agencies need to quickly and\naccurately assess the situation on the ground in order to deploy relevant\nservices and resources. However, authorities often have to make decisions based\non limited information, as data on affected regions can be scarce until local\nresponse services can provide first-hand reports. Fortunately, the widespread\navailability of smartphones with high-quality cameras has made citizen\njournalism through social media a valuable source of information for crisis\nresponders. However, analyzing the large volume of images posted by citizens\nrequires more time and effort than is typically available. To address this\nissue, this paper proposes the use of state-of-the-art deep neural models for\nautomatic image classification/tagging, specifically by adapting\ntransformer-based architectures for crisis image classification (CrisisViT). We\nleverage the new Incidents1M crisis image dataset to develop a range of new\ntransformer-based image classification models. Through experimentation over the\nstandard Crisis image benchmark dataset, we demonstrate that the CrisisViT\nmodels significantly outperform previous approaches in emergency type, image\nrelevance, humanitarian category, and damage severity classification.\nAdditionally, we show that the new Incidents1M dataset can further augment the\nCrisisViT models resulting in an additional 1.25% absolute accuracy gain.\n","authors":["Zijun Long","Richard McCreadie","Muhammad Imran"],"pdf_url":"https://arxiv.org/pdf/2401.02838v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.08051v2","updated":"2024-01-05T14:10:51Z","published":"2023-09-14T22:35:39Z","title":"Retrieval-Augmented Text-to-Audio Generation","summary":"  Despite recent progress in text-to-audio (TTA) generation, we show that the\nstate-of-the-art models, such as AudioLDM, trained on datasets with an\nimbalanced class distribution, such as AudioCaps, are biased in their\ngeneration performance. Specifically, they excel in generating common audio\nclasses while underperforming in the rare ones, thus degrading the overall\ngeneration performance. We refer to this problem as long-tailed text-to-audio\ngeneration. To address this issue, we propose a simple retrieval-augmented\napproach for TTA models. Specifically, given an input text prompt, we first\nleverage a Contrastive Language Audio Pretraining (CLAP) model to retrieve\nrelevant text-audio pairs. The features of the retrieved audio-text data are\nthen used as additional conditions to guide the learning of TTA models. We\nenhance AudioLDM with our proposed approach and denote the resulting augmented\nsystem as Re-AudioLDM. On the AudioCaps dataset, Re-AudioLDM achieves a\nstate-of-the-art Frechet Audio Distance (FAD) of 1.37, outperforming the\nexisting approaches by a large margin. Furthermore, we show that Re-AudioLDM\ncan generate realistic audio for complex scenes, rare audio classes, and even\nunseen audio types, indicating its potential in TTA tasks.\n","authors":["Yi Yuan","Haohe Liu","Xubo Liu","Qiushi Huang","Mark D. Plumbley","Wenwu Wang"],"pdf_url":"https://arxiv.org/pdf/2309.08051v2.pdf","comment":"Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2401.02678v1","updated":"2024-01-05T07:24:07Z","published":"2024-01-05T07:24:07Z","title":"MusicAOG: an Energy-Based Model for Learning and Sampling a Hierarchical\n  Representation of Symbolic Music","summary":"  In addressing the challenge of interpretability and generalizability of\nartificial music intelligence, this paper introduces a novel symbolic\nrepresentation that amalgamates both explicit and implicit musical information\nacross diverse traditions and granularities. Utilizing a hierarchical and-or\ngraph representation, the model employs nodes and edges to encapsulate a broad\nspectrum of musical elements, including structures, textures, rhythms, and\nharmonies. This hierarchical approach expands the representability across\nvarious scales of music. This representation serves as the foundation for an\nenergy-based model, uniquely tailored to learn musical concepts through a\nflexible algorithm framework relying on the minimax entropy principle.\nUtilizing an adapted Metropolis-Hastings sampling technique, the model enables\nfine-grained control over music generation. A comprehensive empirical\nevaluation, contrasting this novel approach with existing methodologies,\nmanifests considerable advancements in interpretability and controllability.\nThis study marks a substantial contribution to the fields of music analysis,\ncomposition, and computational musicology.\n","authors":["Yikai Qian","Tianle Wang","Xinyi Tong","Xin Jin","Duo Xu","Bo Zheng","Tiezheng Ge","Feng Yu","Song-Chun Zhu"],"pdf_url":"https://arxiv.org/pdf/2401.02678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02614v1","updated":"2024-01-05T03:12:03Z","published":"2024-01-05T03:12:03Z","title":"Scaling and Masking: A New Paradigm of Data Sampling for Image and Video\n  Quality Assessment","summary":"  Quality assessment of images and videos emphasizes both local details and\nglobal semantics, whereas general data sampling methods (e.g., resizing,\ncropping or grid-based fragment) fail to catch them simultaneously. To address\nthe deficiency, current approaches have to adopt multi-branch models and take\nas input the multi-resolution data, which burdens the model complexity. In this\nwork, instead of stacking up models, a more elegant data sampling method (named\nas SAMA, scaling and masking) is explored, which compacts both the local and\nglobal content in a regular input size. The basic idea is to scale the data\ninto a pyramid first, and reduce the pyramid into a regular data dimension with\na masking strategy. Benefiting from the spatial and temporal redundancy in\nimages and videos, the processed data maintains the multi-scale characteristics\nwith a regular input size, thus can be processed by a single-branch model. We\nverify the sampling method in image and video quality assessment. Experiments\nshow that our sampling method can improve the performance of current\nsingle-branch models significantly, and achieves competitive performance to the\nmulti-branch models without extra model complexity. The source code will be\navailable at https://github.com/Sissuire/SAMA.\n","authors":["Yongxu Liu","Yinghui Quan","Guoyao Xiao","Aobo Li","Jinjian Wu"],"pdf_url":"https://arxiv.org/pdf/2401.02614v1.pdf","comment":"Accepted by AAAI2024. Code has been released at\n  https://github.com/Sissuire/SAMA"},{"id":"http://arxiv.org/abs/2401.02309v2","updated":"2024-01-05T03:11:28Z","published":"2024-01-04T14:55:57Z","title":"TR-DETR: Task-Reciprocal Transformer for Joint Moment Retrieval and\n  Highlight Detection","summary":"  Video moment retrieval (MR) and highlight detection (HD) based on natural\nlanguage queries are two highly related tasks, which aim to obtain relevant\nmoments within videos and highlight scores of each video clip. Recently,\nseveral methods have been devoted to building DETR-based networks to solve both\nMR and HD jointly. These methods simply add two separate task heads after\nmulti-modal feature extraction and feature interaction, achieving good\nperformance. Nevertheless, these approaches underutilize the reciprocal\nrelationship between two tasks. In this paper, we propose a task-reciprocal\ntransformer based on DETR (TR-DETR) that focuses on exploring the inherent\nreciprocity between MR and HD. Specifically, a local-global multi-modal\nalignment module is first built to align features from diverse modalities into\na shared latent space. Subsequently, a visual feature refinement is designed to\neliminate query-irrelevant information from visual features for modal\ninteraction. Finally, a task cooperation module is constructed to refine the\nretrieval pipeline and the highlight score prediction process by utilizing the\nreciprocity between MR and HD. Comprehensive experiments on QVHighlights,\nCharades-STA and TVSum datasets demonstrate that TR-DETR outperforms existing\nstate-of-the-art methods. Codes are available at\n\\url{https://github.com/mingyao1120/TR-DETR}.\n","authors":["Hao Sun","Mingyao Zhou","Wenjing Chen","Wei Xie"],"pdf_url":"https://arxiv.org/pdf/2401.02309v2.pdf","comment":"Accepted by AAAI-24"},{"id":"http://arxiv.org/abs/2311.11255v3","updated":"2024-01-05T02:17:34Z","published":"2023-11-19T06:50:52Z","title":"M$^{2}$UGen: Multi-modal Music Understanding and Generation with the\n  Power of Large Language Models","summary":"  The current landscape of research leveraging large language models (LLMs) is\nexperiencing a surge. Many works harness the powerful reasoning capabilities of\nthese models to comprehend various modalities, such as text, speech, images,\nvideos, etc. They also utilize LLMs to understand human intention and generate\ndesired outputs like images, videos, and music. However, research that combines\nboth understanding and generation using LLMs is still limited and in its\nnascent stage. To address this gap, we introduce a Multi-modal Music\nUnderstanding and Generation (M$^{2}$UGen) framework that integrates LLM's\nabilities to comprehend and generate music for different modalities. The\nM$^{2}$UGen framework is purpose-built to unlock creative potential from\ndiverse sources of inspiration, encompassing music, image, and video through\nthe use of pretrained MERT, ViT, and ViViT models, respectively. To enable\nmusic generation, we explore the use of AudioLDM 2 and MusicGen. Bridging\nmulti-modal understanding and music generation is accomplished through the\nintegration of the LLaMA 2 model. Furthermore, we make use of the MU-LLaMA\nmodel to generate extensive datasets that support text/image/video-to-music\ngeneration, facilitating the training of our M$^{2}$UGen framework. We conduct\na thorough evaluation of our proposed framework. The experimental results\ndemonstrate that our model achieves or surpasses the performance of the current\nstate-of-the-art models.\n","authors":["Atin Sakkeer Hussain","Shansong Liu","Chenshuo Sun","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2311.11255v3.pdf","comment":null}]}}