<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-01-05T00:00:00Z">2024-01-05</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">28</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepSeek LLM: Scaling Open-Source Language Models with Longtermism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02954v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02954v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         DeepSeek-AI,  :, Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y. K. Li, Wenfeng Liang, Fangyun Lin, A. X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R. X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, Yuheng Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of open-source large language models (LLMs) has been
truly remarkable. However, the scaling law described in previous literature
presents varying conclusions, which casts a dark cloud over scaling LLMs. We
delve into the study of scaling laws and present our distinctive findings that
facilitate scaling of large scale models in two commonly used open-source
configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek
LLM, a project dedicated to advancing open-source language models with a
long-term perspective. To support the pre-training phase, we have developed a
dataset that currently consists of 2 trillion tokens and is continuously
expanding. We further conduct supervised fine-tuning (SFT) and Direct
Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the
creation of DeepSeek Chat models. Our evaluation results demonstrate that
DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in
the domains of code, mathematics, and reasoning. Furthermore, open-ended
evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance
compared to GPT-3.5.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards ASR Robust Spoken Language Understanding Through In-Context
  Learning With Word Confusion Networks <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Everson, Yile Gu, Huck Yang, Prashanth Gurunath Shivakumar, Guan-Ting Lin, Jari Kolehmainen, Ivan Bulyko, Ankur Gandhe, Shalini Ghosh, Wael Hamza, Hung-yi Lee, Ariya Rastrow, Andreas Stolcke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of spoken language understanding (SLU), numerous natural
language understanding (NLU) methodologies have been adapted by supplying large
language models (LLMs) with transcribed speech instead of conventional written
text. In real-world scenarios, prior to input into an LLM, an automated speech
recognition (ASR) system generates an output transcript hypothesis, where
inherent errors can degrade subsequent SLU tasks. Here we introduce a method
that utilizes the ASR system's lattice output instead of relying solely on the
top hypothesis, aiming to encapsulate speech ambiguities and enhance SLU
outcomes. Our in-context learning experiments, covering spoken question
answering and intent classification, underline the LLM's resilience to noisy
speech transcripts with the help of word confusion networks from lattices,
bridging the SLU performance gap between using the top ASR hypothesis and an
oracle upper bound. Additionally, we delve into the LLM's robustness to varying
ASR performance conditions and scrutinize the aspects of in-context learning
which prove the most influential.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Introducing Bode: A Fine-Tuned Large Language Model for Portuguese
  <span class="highlight-title">Prompt</span>-Based Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02909v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02909v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel Lino Garcia, Pedro Henrique Paiola, Luis Henrique Morelli, Giovani Candido, Arnaldo Cândido Júnior, Danilo Samuel Jodas, Luis C. S. Afonso, Ivan Rizzo Guilherme, Bruno Elias Penteado, João Paulo Papa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are increasingly bringing advances to Natural
Language Processing. However, low-resource languages, those lacking extensive
prominence in datasets for various NLP tasks, or where existing datasets are
not as substantial, such as Portuguese, already obtain several benefits from
LLMs, but not to the same extent. LLMs trained on multilingual datasets
normally struggle to respond to prompts in Portuguese satisfactorily,
presenting, for example, code switching in their responses. This work proposes
a fine-tuned LLaMA 2-based model for Portuguese prompts named Bode in two
versions: 7B and 13B. We evaluate the performance of this model in
classification tasks using the zero-shot approach with in-context learning, and
compare it with other LLMs. Our main contribution is to bring an LLM with
satisfactory results in the Portuguese language, as well as to provide a model
that is free for research or commercial purposes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02906v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02906v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renjie Pi, Tianyang Han, Yueqi Xie, Rui Pan, Qing Lian, Hanze Dong, Jipeng Zhang, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The deployment of multimodal large language models (MLLMs) has brought forth
a unique vulnerability: susceptibility to malicious attacks through visual
inputs. We delve into the novel challenge of defending MLLMs against such
attacks. We discovered that images act as a "foreign language" that is not
considered during alignment, which can make MLLMs prone to producing harmful
responses. Unfortunately, unlike the discrete tokens considered in text-based
LLMs, the continuous nature of image signals presents significant alignment
challenges, which poses difficulty to thoroughly cover the possible scenarios.
This vulnerability is exacerbated by the fact that open-source MLLMs are
predominantly fine-tuned on limited image-text pairs that is much less than the
extensive text-based pretraining corpus, which makes the MLLMs more prone to
catastrophic forgetting of their original abilities during explicit alignment
tuning. To tackle these challenges, we introduce MLLM-Protector, a
plug-and-play strategy combining a lightweight harm detector and a response
detoxifier. The harm detector's role is to identify potentially harmful outputs
from the MLLM, while the detoxifier corrects these outputs to ensure the
response stipulates to the safety standards. This approach effectively
mitigates the risks posed by malicious visual inputs without compromising the
model's overall performance. Our results demonstrate that MLLM-Protector offers
a robust solution to a previously unaddressed aspect of MLLM security.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AFSPP: Agent Framework for Shaping Preference and Personality with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihong He, Changwang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution of Large Language Models (LLMs) has introduced a new paradigm
for investigating human behavior emulation. Recent research has employed
LLM-based Agents to create a sociological research environment, in which agents
exhibit behavior based on the unfiltered characteristics of large language
models. However, these studies overlook the iterative development within a
human-like setting - Human preferences and personalities are complex, shaped by
various factors and subject to ongoing change as a result of environmental and
subjective influences. In light of this observation, we propose Agent Framework
for Shaping Preference and Personality (AFSPP), exploring the multifaceted
impact of social networks and subjective consciousness on LLM-based Agents'
preference and personality formation. With AFSPP, we have, for the first time,
successfully replicated several key findings from human personality
experiments. And other AFSPP-based experimental results indicate that plan
making, sensory perceptions and social networking with subjective information,
wield the most pronounced influence on preference shaping. AFSPP can
significantly enhance the efficiency and scope of psychological experiments,
while yielding valuable insights for Trustworthy Artificial Intelligence
research for strategies to prevent undesirable preference and personality
development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pheme: Efficient and Conversational Speech Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02839v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02839v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paweł Budzianowski, Taras Sereda, Tomasz Cichy, Ivan Vulić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, speech generation has seen remarkable progress, now
achieving one-shot generation capability that is often virtually
indistinguishable from real human voice. Integrating such advancements in
speech generation with large language models might revolutionize a wide range
of applications. However, certain applications, such as assistive
conversational systems, require natural and conversational speech generation
tools that also operate efficiently in real time. Current state-of-the-art
models like VALL-E and SoundStorm, powered by hierarchical neural audio codecs,
require large neural components and extensive training data to work well. In
contrast, MQTTS aims to build more compact conversational TTS models while
capitalizing on smaller-scale real-life conversational speech data. However,
its autoregressive nature yields high inference latency and thus limits its
real-time usage. In order to mitigate the current limitations of the
state-of-the-art TTS models while capitalizing on their strengths, in this work
we introduce the Pheme model series that 1) offers compact yet high-performing
models, 2) allows for parallel speech generation of 3) natural conversational
speech, and 4) it can be trained efficiently on smaller-scale conversational
data, cutting data demands by more than 10x but still matching the quality of
the autoregressive TTS models. We also show that through simple teacher-student
distillation we can meet significant improvements in voice quality for
single-speaker setups on top of pretrained Pheme checkpoints, relying solely on
synthetic speech generated by much larger teacher models. Audio samples and
pretrained models are available online.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DocGraphLM: Documental Graph Language Model for Information Extraction <span class="chip">SIGIR'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongsheng Wang, Zhiqiang Ma, Armineh Nourbakhsh, Kang Gu, Sameena Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in Visually Rich Document Understanding (VrDU) have enabled
information extraction and question answering over documents with complex
layouts. Two tropes of architectures have emerged -- transformer-based models
inspired by LLMs, and Graph Neural Networks. In this paper, we introduce
DocGraphLM, a novel framework that combines pre-trained language models with
graph semantics. To achieve this, we propose 1) a joint encoder architecture to
represent documents, and 2) a novel link prediction approach to reconstruct
document graphs. DocGraphLM predicts both directions and distances between
nodes using a convergent joint loss function that prioritizes neighborhood
restoration and downweighs distant node detection. Our experiments on three
SotA datasets show consistent improvement on IE and QA tasks with the adoption
of graph features. Moreover, we report that adopting the graph features
accelerates convergence in the learning process during training, despite being
solely constructed through link prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at SIGIR'23 (repost for easier access)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PeFoMed: Parameter Efficient Fine-tuning on Multimodal Large Language
  Models for Medical Visual Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02797v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02797v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinlong He, Pengfei Li, Gang Liu, Zixu Zhao, Shenjun Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) represent an evolutionary expansion
in the capabilities of traditional large language models, enabling them to
tackle challenges that surpass the scope of purely text-based applications. It
leverages the knowledge previously encoded within these language models,
thereby enhancing their applicability and functionality in the reign of
multimodal contexts. Recent works investigate the adaptation of MLLMs to
predict free-form answers as a generative task to solve medical visual question
answering (Med-VQA) tasks. In this paper, we propose a parameter efficient
framework for fine-tuning MLLM specifically tailored to Med-VQA applications,
and empirically validate it on a public benchmark dataset. To accurately
measure the performance, we employ human evaluation and the results reveal that
our model achieves an overall accuracy of 81.9%, and outperforms the GPT-4v
model by a significant margin of 26% absolute accuracy on closed-ended
questions. The code will be available here: https://github.com/jinlHe/PeFoMed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models in Plant Biology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02789v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02789v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hilbert Yuen In Lam, Xing Er Ong, Marek Mutwil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs), such as ChatGPT, have taken the world by storm
and have passed certain forms of the Turing test. However, LLMs are not limited
to human language and analyze sequential data, such as DNA, protein, and gene
expression. The resulting foundation models can be repurposed to identify the
complex patterns within the data, resulting in powerful, multi-purpose
prediction tools able to explain cellular systems. This review outlines the
different types of LLMs and showcases their recent uses in biology. Since LLMs
have not yet been embraced by the plant community, we also cover how these
models can be deployed for the plant kingdom.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From LLM to Conversational Agent: A Memory Enhanced Architecture with
  Fine-Tuning of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Na Liu, Liangyu Chen, Xiaoyu Tian, Wei Zou, Kaijiang Chen, Ming Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces RAISE (Reasoning and Acting through Scratchpad and
Examples), an advanced architecture enhancing the integration of Large Language
Models (LLMs) like GPT-4 into conversational agents. RAISE, an enhancement of
the ReAct framework, incorporates a dual-component memory system, mirroring
human short-term and long-term memory, to maintain context and continuity in
conversations. It entails a comprehensive agent construction scenario,
including phases like Conversation Selection, Scene Extraction, CoT Completion,
and Scene Augmentation, leading to the LLMs Training phase. This approach
appears to enhance agent controllability and adaptability in complex,
multi-turn dialogues. Our preliminary evaluations in a real estate sales
context suggest that RAISE has some advantages over traditional agents,
indicating its potential for broader applications. This work contributes to the
AI field by providing a robust framework for developing more context-aware and
versatile conversational agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Complex systems approach to natural language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02772v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02772v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomasz Stanisz, Stanisław Drożdż, Jarosław Kwapień
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The review summarizes the main methodological concepts used in studying
natural language from the perspective of complexity science and documents their
applicability in identifying both universal and system-specific features of
language in its written representation. Three main complexity-related research
trends in quantitative linguistics are covered. The first part addresses the
issue of word frequencies in texts and demonstrates that taking punctuation
into consideration restores scaling whose violation in the Zipf's law is often
observed for the most frequent words. The second part introduces methods
inspired by time series analysis, used in studying various kinds of
correlations in written texts. The related time series are generated on the
basis of text partition into sentences or into phrases between consecutive
punctuation marks. It turns out that these series develop features often found
in signals generated by complex systems, like long-range correlations or
(multi)fractal structures. Moreover, it appears that the distances between
punctuation marks comply with the discrete variant of the Weibull distribution.
In the third part, the application of the network formalism to natural language
is reviewed, particularly in the context of the so-called word-adjacency
networks. Parameters characterizing topology of such networks can be used for
classification of texts, for example, from a stylometric perspective. Network
approach can also be applied to represent the organization of word
associations. Structure of word-association networks turns out to be
significantly different from that observed in random networks, revealing
genuine properties of language. Finally, punctuation seems to have a
significant impact not only on the language's information-carrying ability but
also on its key statistical properties, hence it is recommended to consider
punctuation marks on a par with words.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>113 pages, 49 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hyperparameter-Free Approach for Faster Minimum Bayes Risk Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuu Jinnai, Kaito Ariu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Minimum Bayes-Risk (MBR) decoding is shown to be a powerful alternative to
beam search decoding for a wide range of text generation tasks. However, MBR
requires a huge amount of time for inference to compute the MBR objective,
which makes the method infeasible in many situations where response time is
critical. Confidence-based pruning (CBP) (Cheng and Vlachos, 2023) has recently
been proposed to reduce the inference time in machine translation tasks.
Although it is shown to significantly reduce the amount of computation, it
requires hyperparameter tuning using a development set to be effective. To this
end, we propose Approximate Minimum Bayes-Risk (AMBR) decoding, a
hyperparameter-free method to run MBR decoding approximately. AMBR is derived
from the observation that the problem of computing the sample-based MBR
objective is the medoid identification problem. AMBR uses the Correlated
Sequential Halving (CSH) algorithm (Baharav and Tse, 2019), the best
approximation algorithm to date for the medoid identification problem, to
compute the sample-based MBR objective. We evaluate AMBR on machine
translation, text summarization, and image captioning tasks. The results show
that AMBR achieves on par with CBP, with CBP selecting hyperparameters through
an Oracle for each given computation budget.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAMI: Multi-Attentional Mutual-Information for Long Sequence Neuron
  Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alfirsa Damasyifa Fauzulhaq, Wahyu Parwitayasa, Joseph Ananda Sugihdharma, M. Fadli Ridhani, Novanto Yudistira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neuron labeling is an approach to visualize the behaviour and respond of a
certain neuron to a certain pattern that activates the neuron. Neuron labeling
extract information about the features captured by certain neurons in a deep
neural network, one of which uses the encoder-decoder image captioning
approach. The encoder used can be a pretrained CNN-based model and the decoder
is an RNN-based model for text generation. Previous work, namely MILAN (Mutual
Information-guided Linguistic Annotation of Neuron), has tried to visualize the
neuron behaviour using modified Show, Attend, and Tell (SAT) model in the
encoder, and LSTM added with Bahdanau attention in the decoder. MILAN can show
great result on short sequence neuron captioning, but it does not show great
result on long sequence neuron captioning, so in this work, we would like to
improve the performance of MILAN even more by utilizing different kind of
attention mechanism and additionally adding several attention result into one,
in order to combine all the advantages from several attention mechanism. Using
our compound dataset, we obtained higher BLEU and F1-Score on our proposed
model, achieving 17.742 and 0.4811 respectively. At some point where the model
converges at the peak, our model obtained BLEU of 21.2262 and BERTScore
F1-Score of 0.4870.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ German Text Embedding Clustering Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02709v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02709v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Silvan Wehrli, Bert Arnrich, Christopher Irrgang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces a benchmark assessing the performance of clustering
German text embeddings in different domains. This benchmark is driven by the
increasing use of clustering neural text embeddings in tasks that require the
grouping of texts (such as topic modeling) and the need for German resources in
existing benchmarks. We provide an initial analysis for a range of pre-trained
mono- and multilingual models evaluated on the outcome of different clustering
algorithms. Results include strong performing mono- and multilingual models.
Reducing the dimensions of embeddings can further improve clustering.
Additionally, we conduct experiments with continued pre-training for German
BERT models to estimate the benefits of this additional training. Our
experiments suggest that significant performance improvements are possible for
short text. All code and datasets are publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised hard Negative Augmentation for contrastive learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Shu, Vasileios Lampos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Unsupervised hard Negative Augmentation (UNA), a method that
generates synthetic negative instances based on the term frequency-inverse
document frequency (TF-IDF) retrieval model. UNA uses TF-IDF scores to
ascertain the perceived importance of terms in a sentence and then produces
negative samples by replacing terms with respect to that. Our experiments
demonstrate that models trained with UNA improve the overall performance in
semantic textual similarity tasks. Additional performance gains are obtained
when combining UNA with the paraphrasing augmentation. Further results show
that our method is compatible with different backbone models. Ablation studies
also support the choice of having a TF-IDF-driven control on negative
augmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code and pre-trained models are available at
  https://github.com/ClaudiaShu/UNA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Analyzing Wrap-Up Effects through an Information-Theoretic Lens <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.17213v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.17213v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clara Meister, Tiago Pimentel, Thomas Hikaru Clark, Ryan Cotterell, Roger Levy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerous analyses of reading time (RT) data have been implemented -- all in
an effort to better understand the cognitive processes driving reading
comprehension. However, data measured on words at the end of a sentence -- or
even at the end of a clause -- is often omitted due to the confounding factors
introduced by so-called "wrap-up effects," which manifests as a skewed
distribution of RTs for these words. Consequently, the understanding of the
cognitive processes that might be involved in these wrap-up effects is limited.
In this work, we attempt to learn more about these processes by examining the
relationship between wrap-up effects and information-theoretic quantities, such
as word and context surprisals. We find that the distribution of information in
prior contexts is often predictive of sentence- and clause-final RTs (while not
of sentence-medial RTs). This lends support to several prior hypotheses about
the processes involved in wrap-up effects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022 (main conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Efficacy of Sampling Adapters <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03749v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03749v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clara Meister, Tiago Pimentel, Luca Malagutti, Ethan G. Wilcox, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sampling is a common strategy for generating text from probabilistic models,
yet standard ancestral sampling often results in text that is incoherent or
ungrammatical. To alleviate this issue, various modifications to a model's
sampling distribution, such as nucleus or top-k sampling, have been introduced
and are now ubiquitously used in language generation systems. We propose a
unified framework for understanding these techniques, which we term sampling
adapters. Sampling adapters often lead to qualitatively better text, which
raises the question: From a formal perspective, how are they changing the
(sub)word-level distributions of language generation models? And why do these
local changes lead to higher-quality text? We argue that the shift they enforce
can be viewed as a trade-off between precision and recall: while the model
loses its ability to produce certain strings, its precision rate on desirable
text increases. While this trade-off is not reflected in standard metrics of
distribution quality (such as perplexity), we find that several
precision-emphasizing measures indeed indicate that sampling adapters can lead
to probability distributions more aligned with the true distribution. Further,
these measures correlate with higher sequence-level quality scores,
specifically, Mauve.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023 Main Conference Proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Patterns of Persistence and Diffusibility across the World's Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01698v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01698v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyi Chen, Johannes Bjerva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language similarities can be caused by genetic relatedness, areal contact,
universality, or chance. Colexification, i.e. a type of similarity where a
single lexical form is used to convey multiple meanings, is underexplored. In
our work, we shed light on the linguistic causes of cross-lingual similarity in
colexification and phonology, by exploring genealogical stability (persistence)
and contact-induced change (diffusibility). We construct large-scale graphs
incorporating semantic, genealogical, phonological and geographical data for
1,966 languages. We then show the potential of this resource, by investigating
several established hypotheses from previous work in linguistics, while
proposing new ones. Our results strongly support a previously established
hypothesis in the linguistic literature, while offering contradicting evidence
to another. Our large scale resource opens for further research across
disciplines, e.g.~in multilingual NLP and comparative linguistics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span>Bench: A Unified Library for Evaluation of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07910v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07910v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaijie Zhu, Qinlin Zhao, Hao Chen, Jindong Wang, Xing Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evaluation of large language models (LLMs) is crucial to assess their
performance and mitigate potential security risks. In this paper, we introduce
PromptBench, a unified library to evaluate LLMs. It consists of several key
components that are easily used and extended by researchers: prompt
construction, prompt engineering, dataset and model loading, adversarial prompt
attack, dynamic evaluation protocols, and analysis tools. PromptBench is
designed to be an open, general, and flexible codebase for research purposes
that can facilitate original study in creating new benchmarks, deploying
downstream applications, and designing new evaluation protocols. The code is
available at: https://github.com/microsoft/promptbench and will be continuously
supported.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>An extension to PromptBench (arXiv:2306.04528) for unified evaluation
  of LLMs using the same name; code: https://github.com/microsoft/promptbench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Annotation Sensitivity: Training Data Collection Methods Affect Model
  Performance <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.14212v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.14212v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christoph Kern, Stephanie Eckman, Jacob Beck, Rob Chew, Bolei Ma, Frauke Kreuter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When training data are collected from human annotators, the design of the
annotation instrument, the instructions given to annotators, the
characteristics of the annotators, and their interactions can impact training
data. This study demonstrates that design choices made when creating an
annotation instrument also impact the models trained on the resulting
annotations. We introduce the term annotation sensitivity to refer to the
impact of annotation data collection methods on the annotations themselves and
on downstream model performance and predictions. We collect annotations of hate
speech and offensive language in five experimental conditions of an annotation
instrument, randomly assigning annotators to conditions. We then fine-tune BERT
models on each of the five resulting datasets and evaluate model performance on
a holdout portion of each condition. We find considerable differences between
the conditions for 1) the share of hate speech/offensive language annotations,
2) model performance, 3) model predictions, and 4) model learning curves. Our
results emphasize the crucial role played by the annotation instrument which
has received little attention in the machine learning literature. We call for
additional research into how and why the instrument impacts the annotations to
inform the development of best practices in instrument design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2023 Findings:
  https://aclanthology.org/2023.findings-emnlp.992/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FlashDecoding++: Faster Large Language Model Inference on GPUs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01282v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01282v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li, Jun Liu, Kangdi Chen, Yuhan Dong, Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the Large Language Model (LLM) becomes increasingly important in various
domains. However, the following challenges still remain unsolved in
accelerating LLM inference: (1) Synchronized partial softmax update. The
softmax operation requires a synchronized update operation among each partial
softmax result, leading to ~20% overheads for the attention computation in
LLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices
performing GEMM in LLM inference is flat, leading to under-utilized computation
and >50% performance loss after padding zeros in previous designs. (3)
Performance loss due to static dataflow. Kernel performance in LLM depends on
varied input data features, hardware configurations, etc. A single and static
dataflow may lead to a 50.25% performance loss for GEMMs of different shapes in
LLM inference.
  We present FlashDecoding++, a fast LLM inference engine supporting mainstream
LLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++
creatively proposes: (1) Asynchronized softmax with unified max value.
FlashDecoding++ introduces a unified max value technique for different partial
softmax computations to avoid synchronization. (2) Flat GEMM optimization with
double buffering. FlashDecoding++ points out that flat GEMMs with different
shapes face varied bottlenecks. Then, techniques like double buffering are
introduced. (3) Heuristic dataflow with hardware resource adaptation.
FlashDecoding++ heuristically optimizes dataflow using different hardware
resource considering input dynamics. Due to the versatility of optimizations in
FlashDecoding++, FlashDecoding++ can achieve up to 4.86x and 2.18x speedup on
both NVIDIA and AMD GPUs compared to Hugging Face implementations.
FlashDecoding++ also achieves an average speedup of 1.37x compared to
state-of-the-art LLM inference engines on mainstream LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ mFACE: Multilingual Summarization with Factual Consistency Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.10622v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.10622v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roee Aharoni, Shashi Narayan, Joshua Maynez, Jonathan Herzig, Elizabeth Clark, Mirella Lapata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Abstractive summarization has enjoyed renewed interest in recent years,
thanks to pre-trained language models and the availability of large-scale
datasets. Despite promising results, current models still suffer from
generating factually inconsistent summaries, reducing their utility for
real-world application. Several recent efforts attempt to address this by
devising models that automatically detect factual inconsistencies in machine
generated summaries. However, they focus exclusively on English, a language
with abundant resources. In this work, we leverage factual consistency
evaluation models to improve multilingual summarization. We explore two
intuitive approaches to mitigate hallucinations based on the signal provided by
a multilingual NLI model, namely data filtering and controlled generation.
Experimental results in the 45 languages from the XLSum dataset show gains over
strong baselines in both automatic and human evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages with links to released data</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Code-Style In-Context Learning for Knowledge-Based Question Answering <span class="chip">AAAI2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.04695v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.04695v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijie Nie, Richong Zhang, Zhongyuan Wang, Xudong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current methods for Knowledge-Based Question Answering (KBQA) usually rely on
complex training techniques and model frameworks, leading to many limitations
in practical applications. Recently, the emergence of In-Context Learning (ICL)
capabilities in Large Language Models (LLMs) provides a simple and
training-free semantic parsing paradigm for KBQA: Given a small number of
questions and their labeled logical forms as demo examples, LLMs can understand
the task intent and generate the logic form for a new question. However,
current powerful LLMs have little exposure to logic forms during pre-training,
resulting in a high format error rate. To solve this problem, we propose a
code-style in-context learning method for KBQA, which converts the generation
process of unfamiliar logical form into the more familiar code generation
process for LLMs. Experimental results on three mainstream datasets show that
our method dramatically mitigated the formatting error problem in generating
logic forms while realizing a new SOTA on WebQSP, GrailQA, and GraphQ under the
few-shot setting. The code and supplementary files are released at
https://github.com/Arthurizijar/KB-Coder .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI2024 Camera Ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can AI Be as Creative as Humans? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01623v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01623v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan Wang, James Zou, Michael Mozer, Anirudh Goyal, Alex Lamb, Linjun Zhang, Weijie J Su, Zhun Deng, Michael Qizhe Xie, Hannah Brown, Kenji Kawaguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creativity serves as a cornerstone for societal progress and innovation, but
its assessment remains a complex and often subjective endeavor. With the rise
of advanced generative AI models capable of tasks once reserved for human
creativity, the study of AI's creative potential becomes imperative for its
responsible development and application. This paper addresses the complexities
in defining and evaluating creativity by introducing a new concept called
Relative Creativity. Instead of trying to define creativity universally, we
shift the focus to whether AI can match the creative abilities of a
hypothetical human. This perspective draws inspiration from the Turing Test,
expanding upon it to address the challenges and subjectivities inherent in
evaluating creativity. This methodological shift facilitates a statistically
quantifiable evaluation of AI's creativity, which we term Statistical
Creativity. This approach allows for direct comparisons of AI's creative
abilities with those of specific human groups. Building on this foundation, we
discuss the application of statistical creativity in contemporary
prompt-conditioned autoregressive models. In addition to defining and analyzing
a measure of creativity, we introduce an actionable training guideline,
effectively bridging the gap between theoretical quantification of creativity
and practical model training. Through these multifaceted contributions, the
paper establishes a cohesive, continuously evolving, and transformative
framework for assessing and fostering statistical creativity in AI models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper introduce the notion of "Relative Creativity", presents
  measurable assessment, and provides AI training guidelines to foster AI's
  creative capabilities Project Page: https://ai-relative-creativity.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AstroLLaMA-Chat: Scaling AstroLLaMA with Conversational and Diverse
  <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01916v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01916v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ernest Perkowski, Rui Pan, Tuan Dung Nguyen, Yuan-Sen Ting, Sandor Kruk, Tong Zhang, Charlie O'Neill, Maja Jablonska, Zechang Sun, Michael J. Smith, Huiling Liu, Kevin Schawinski, Kartheik Iyer, Ioana Ciucă for UniverseTBD
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore the potential of enhancing LLM performance in astronomy-focused
question-answering through targeted, continual pre-training. By employing a
compact 7B-parameter LLaMA-2 model and focusing exclusively on a curated set of
astronomy corpora -- comprising abstracts, introductions, and conclusions -- we
achieve notable improvements in specialized topic comprehension. While general
LLMs like GPT-4 excel in broader question-answering scenarios due to superior
reasoning capabilities, our findings suggest that continual pre-training with
limited resources can still enhance model performance on specialized topics.
Additionally, we present an extension of AstroLLaMA: the fine-tuning of the 7B
LLaMA model on a domain-specific conversational dataset, culminating in the
release of the chat-enabled AstroLLaMA for community use. Comprehensive
quantitative benchmarking is currently in progress and will be detailed in an
upcoming full paper. The model, AstroLLaMA-Chat, is now available at
https://huggingface.co/universeTBD, providing the first open-source
conversational AI tool tailored for the astronomy community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 1 figure, model is available at
  https://huggingface.co/universeTBD, published in RNAAS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DecodingTrust: A Comprehensive Assessment of Trustworthiness in <span class="highlight-title">GPT</span>
  Models <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.11698v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.11698v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T. Truong, Simran Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng, Sanmi Koyejo, Dawn Song, Bo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Pre-trained Transformer (GPT) models have exhibited exciting
progress in their capabilities, capturing the interest of practitioners and the
public alike. Yet, while the literature on the trustworthiness of GPT models
remains limited, practitioners have proposed employing capable GPT models for
sensitive applications such as healthcare and finance -- where mistakes can be
costly. To this end, this work proposes a comprehensive trustworthiness
evaluation for large language models with a focus on GPT-4 and GPT-3.5,
considering diverse perspectives -- including toxicity, stereotype bias,
adversarial robustness, out-of-distribution robustness, robustness on
adversarial demonstrations, privacy, machine ethics, and fairness. Based on our
evaluations, we discover previously unpublished vulnerabilities to
trustworthiness threats. For instance, we find that GPT models can be easily
misled to generate toxic and biased outputs and leak private information in
both training data and conversation history. We also find that although GPT-4
is usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is more
vulnerable given jailbreaking system or user prompts, potentially because GPT-4
follows (misleading) instructions more precisely. Our work illustrates a
comprehensive trustworthiness evaluation of GPT models and sheds light on the
trustworthiness gaps. Our benchmark is publicly available at
https://decodingtrust.github.io/; our dataset can be previewed at
https://huggingface.co/datasets/AI-Secure/DecodingTrust; a concise version of
this work is at https://openreview.net/pdf?id=kaHpo8OZw2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023 Outstanding Paper (Datasets and Benchmarks Track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KwaiAgents: Generalized Information-seeking Agent System with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04889v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04889v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haojie Pan, Zepeng Zhai, Hao Yuan, Yaojia Lv, Ruiji Fu, Ming Liu, Zhongyuan Wang, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Driven by curiosity, humans have continually sought to explore and understand
the world around them, leading to the invention of various tools to satiate
this inquisitiveness. Despite not having the capacity to process and memorize
vast amounts of information in their brains, humans excel in critical thinking,
planning, reflection, and harnessing available tools to interact with and
interpret the world, enabling them to find answers efficiently. The recent
advancements in large language models (LLMs) suggest that machines might also
possess the aforementioned human-like capabilities, allowing them to exhibit
powerful abilities even with a constrained parameter count. In this paper, we
introduce KwaiAgents, a generalized information-seeking agent system based on
LLMs. Within KwaiAgents, we propose an agent system that employs LLMs as its
cognitive core, which is capable of understanding a user's query, behavior
guidelines, and referencing external documents. The agent can also update and
retrieve information from its internal memory, plan and execute actions using a
time-aware search-browse toolkit, and ultimately provide a comprehensive
response. We further investigate the system's performance when powered by LLMs
less advanced than GPT-4, and introduce the Meta-Agent Tuning (MAT) framework,
designed to ensure even an open-sourced 7B or 13B model performs well among
many agent systems. We exploit both benchmark and human evaluations to
systematically validate these capabilities. Extensive experiments show the
superiority of our agent system compared to other autonomous agents and
highlight the enhanced generalized agent-abilities of our fine-tuned LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieval-Augmented Generation for Large Language Models: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10997v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10997v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, Haofen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) demonstrate significant capabilities but face
challenges such as hallucination, outdated knowledge, and non-transparent,
untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has
emerged as a promising solution by incorporating knowledge from external
databases. This enhances the accuracy and credibility of the models,
particularly for knowledge-intensive tasks, and allows for continuous knowledge
updates and integration of domain-specific information. RAG synergistically
merges LLMs' intrinsic knowledge with the vast, dynamic repositories of
external databases. This comprehensive review paper offers a detailed
examination of the progression of RAG paradigms, encompassing the Naive RAG,
the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the
tripartite foundation of RAG frameworks, which includes the retrieval , the
generation and the augmentation techniques. The paper highlights the
state-of-the-art technologies embedded in each of these critical components,
providing a profound understanding of the advancements in RAG systems.
Furthermore, this paper introduces the metrics and benchmarks for assessing RAG
models, along with the most up-to-date evaluation framework. In conclusion, the
paper delineates prospective avenues for research, including the identification
of challenges, the expansion of multi-modalities, and the progression of the
RAG infrastructure and its ecosystem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing Work</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">82</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Denoising Vision <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02957v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02957v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Yang, Katie Z Luo, Jiefeng Li, Kilian Q Weinberger, Yonglong Tian, Yue Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We delve into a nuanced but significant challenge inherent to Vision
Transformers (ViTs): feature maps of these models exhibit grid-like artifacts,
which detrimentally hurt the performance of ViTs in downstream tasks. Our
investigations trace this fundamental issue down to the positional embeddings
at the input stage. To address this, we propose a novel noise model, which is
universally applicable to all ViTs. Specifically, the noise model dissects ViT
outputs into three components: a semantics term free from noise artifacts and
two artifact-related terms that are conditioned on pixel locations. Such a
decomposition is achieved by enforcing cross-view feature consistency with
neural fields in a per-image basis. This per-image optimization process
extracts artifact-free features from raw ViT outputs, providing clean features
for offline applications. Expanding the scope of our solution to support online
functionality, we introduce a learnable denoiser to predict artifact-free
features directly from unprocessed ViT outputs, which shows remarkable
generalization capabilities to novel data without the need for per-image
optimization. Our two-stage approach, termed Denoising Vision Transformers
(DVT), does not require re-training existing pre-trained ViTs and is
immediately applicable to any Transformer-based architecture. We evaluate our
method on a variety of representative ViTs (DINO, MAE, DeiT-III, EVA02, CLIP,
DINOv2, DINOv2-reg). Extensive evaluations demonstrate that our DVT
consistently and significantly improves existing state-of-the-art
general-purpose models in semantic and geometric tasks across multiple datasets
(e.g., +3.84 mIoU). We hope our study will encourage a re-evaluation of ViT
design, especially regarding the naive use of positional embeddings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://jiawei-yang.github.io/DenoisingViT/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open-Vocabulary SAM: Segment and Recognize Twenty-thousand Classes
  Interactively 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02955v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02955v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haobo Yuan, Xiangtai Li, Chong Zhou, Yining Li, Kai Chen, Chen Change Loy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The CLIP and Segment Anything Model (SAM) are remarkable vision foundation
models (VFMs). SAM excels in segmentation tasks across diverse domains, while
CLIP is renowned for its zero-shot recognition capabilities. This paper
presents an in-depth exploration of integrating these two models into a unified
framework. Specifically, we introduce the Open-Vocabulary SAM, a SAM-inspired
model designed for simultaneous interactive segmentation and recognition,
leveraging two unique knowledge transfer modules: SAM2CLIP and CLIP2SAM. The
former adapts SAM's knowledge into the CLIP via distillation and learnable
transformer adapters, while the latter transfers CLIP knowledge into SAM,
enhancing its recognition capabilities. Extensive experiments on various
datasets and detectors show the effectiveness of Open-Vocabulary SAM in both
segmentation and recognition tasks, significantly outperforming the naive
baselines of simply combining SAM and CLIP. Furthermore, aided with image
classification data training, our method can segment and recognize
approximately 22,000 classes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://www.mmlab-ntu.com/project/ovsam</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Locally Adaptive Neural 3D Morphable Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michail Tarasiou, Rolandos Alexandros Potamias, Eimear O'Sullivan, Stylianos Ploumpis, Stefanos Zafeiriou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the Locally Adaptive Morphable Model (LAMM), a highly flexible
Auto-Encoder (AE) framework for learning to generate and manipulate 3D meshes.
We train our architecture following a simple self-supervised training scheme in
which input displacements over a set of sparse control vertices are used to
overwrite the encoded geometry in order to transform one training sample into
another. During inference, our model produces a dense output that adheres
locally to the specified sparse geometry while maintaining the overall
appearance of the encoded object. This approach results in state-of-the-art
performance in both disentangling manipulated geometry and 3D mesh
reconstruction. To the best of our knowledge LAMM is the first end-to-end
framework that enables direct local control of 3D vertex geometry in a single
forward pass. A very efficient computational graph allows our network to train
with only a fraction of the memory required by previous methods and run faster
during inference, generating 12k vertex meshes at $>$60fps on a single CPU
thread. We further leverage local geometry control as a primitive for higher
level editing operations and present a set of derivative capabilities such as
swapping and sampling object parts. Code and pretrained models can be found at
https://github.com/michaeltrs/LAMM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPFormer: Enhancing Vision <span class="highlight-title">Transformer</span> with Superpixel Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02931v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02931v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jieru Mei, Liang-Chieh Chen, Alan Yuille, Cihang Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce SPFormer, a novel Vision Transformer enhanced by
superpixel representation. Addressing the limitations of traditional Vision
Transformers' fixed-size, non-adaptive patch partitioning, SPFormer employs
superpixels that adapt to the image's content. This approach divides the image
into irregular, semantically coherent regions, effectively capturing intricate
details and applicable at both initial and intermediate feature levels.
  SPFormer, trainable end-to-end, exhibits superior performance across various
benchmarks. Notably, it exhibits significant improvements on the challenging
ImageNet benchmark, achieving a 1.4% increase over DeiT-T and 1.1% over DeiT-S
respectively. A standout feature of SPFormer is its inherent explainability.
The superpixel structure offers a window into the model's internal processes,
providing valuable insights that enhance the model's interpretability. This
level of clarity significantly improves SPFormer's robustness, particularly in
challenging scenarios such as image rotations and occlusions, demonstrating its
adaptability and resilience.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncovering the human motion pattern: Pattern Memory-based Diffusion
  Model for Trajectory Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Yang, Pengfei Zhu, Mengshi Qi, Huadong Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human trajectory forecasting is a critical challenge in fields such as
robotics and autonomous driving. Due to the inherent uncertainty of human
actions and intentions in real-world scenarios, various unexpected occurrences
may arise. To uncover latent motion patterns in human behavior, we introduce a
novel memory-based method, named Motion Pattern Priors Memory Network. Our
method involves constructing a memory bank derived from clustered prior
knowledge of motion patterns observed in the training set trajectories. We
introduce an addressing mechanism to retrieve the matched pattern and the
potential target distributions for each prediction from the memory bank, which
enables the identification and retrieval of natural motion patterns exhibited
by agents, subsequently using the target priors memory token to guide the
diffusion model to generate predictions. Extensive experiments validate the
effectiveness of our approach, achieving state-of-the-art trajectory prediction
accuracy. The code will be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02906v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02906v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renjie Pi, Tianyang Han, Yueqi Xie, Rui Pan, Qing Lian, Hanze Dong, Jipeng Zhang, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The deployment of multimodal large language models (MLLMs) has brought forth
a unique vulnerability: susceptibility to malicious attacks through visual
inputs. We delve into the novel challenge of defending MLLMs against such
attacks. We discovered that images act as a "foreign language" that is not
considered during alignment, which can make MLLMs prone to producing harmful
responses. Unfortunately, unlike the discrete tokens considered in text-based
LLMs, the continuous nature of image signals presents significant alignment
challenges, which poses difficulty to thoroughly cover the possible scenarios.
This vulnerability is exacerbated by the fact that open-source MLLMs are
predominantly fine-tuned on limited image-text pairs that is much less than the
extensive text-based pretraining corpus, which makes the MLLMs more prone to
catastrophic forgetting of their original abilities during explicit alignment
tuning. To tackle these challenges, we introduce MLLM-Protector, a
plug-and-play strategy combining a lightweight harm detector and a response
detoxifier. The harm detector's role is to identify potentially harmful outputs
from the MLLM, while the detoxifier corrects these outputs to ensure the
response stipulates to the safety standards. This approach effectively
mitigates the risks posed by malicious visual inputs without compromising the
model's overall performance. Our results demonstrate that MLLM-Protector offers
a robust solution to a previously unaddressed aspect of MLLM security.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reversing the Irreversible: A <span class="highlight-title">Survey</span> on Inverse Biometrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02861v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02861v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marta Gomez-Barrero, Javier Galbally
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the widespread use of biometric recognition, several issues related to
the privacy and security provided by this technology have been recently raised
and analysed. As a result, the early common belief among the biometrics
community of templates irreversibility has been proven wrong. It is now an
accepted fact that it is possible to reconstruct from an unprotected template a
synthetic sample that matches the bona fide one. This reverse engineering
process, commonly referred to as \textit{inverse biometrics}, constitutes a
severe threat for biometric systems from two different angles: on the one hand,
sensitive personal data (i.e., biometric data) can be derived from compromised
unprotected templates; on the other hand, other powerful attacks can be
launched building upon these reconstructed samples. Given its important
implications, biometric stakeholders have produced over the last fifteen years
numerous works analysing the different aspects related to inverse biometrics:
development of reconstruction algorithms for different characteristics;
proposal of methodologies to assess the vulnerabilities of biometric systems to
the aforementioned algorithms; development of countermeasures to reduce the
possible effects of attacks. The present article is an effort to condense all
this information in one comprehensive review of: the problem itself, the
evaluation of the problem, and the mitigation of the problem. The present
article is an effort to condense all this information in one comprehensive
review of: the problem itself, the evaluation of the problem, and the
mitigation of the problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, journal, survey</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Non-Stationary Textures using Self-Rectification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02847v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02847v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Zhou, Rongjun Xiao, Dani Lischinski, Daniel Cohen-Or, Hui Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the challenge of example-based non-stationary texture
synthesis. We introduce a novel twostep approach wherein users first modify a
reference texture using standard image editing tools, yielding an initial rough
target for the synthesis. Subsequently, our proposed method, termed
"self-rectification", automatically refines this target into a coherent,
seamless texture, while faithfully preserving the distinct visual
characteristics of the reference exemplar. Our method leverages a pre-trained
diffusion network, and uses self-attention mechanisms, to gradually align the
synthesized texture with the reference, ensuring the retention of the
structures in the provided target. Through experimental validation, our
approach exhibits exceptional proficiency in handling non-stationary textures,
demonstrating significant advancements in texture synthesis when compared to
existing state-of-the-art techniques. Code is available at
https://github.com/xiaorongjun000/Self-Rectification
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/xiaorongjun000/Self-Rectification</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Stage Contrastive Regression for Action Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi An, Mengshi Qi, Huadong Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, there has been growing interest in the video-based action
quality assessment (AQA). Most existing methods typically solve AQA problem by
considering the entire video yet overlooking the inherent stage-level
characteristics of actions. To address this issue, we design a novel
Multi-stage Contrastive Regression (MCoRe) framework for the AQA task. This
approach allows us to efficiently extract spatial-temporal information, while
simultaneously reducing computational costs by segmenting the input video into
multiple stages or procedures. Inspired by the graph contrastive learning, we
propose a new stage-wise contrastive learning loss function to enhance
performance. As a result, MCoRe demonstrates the state-of-the-art result so far
on the widely-adopted fine-grained AQA dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CrisisViT: A Robust Vision <span class="highlight-title">Transformer</span> for Crisis Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02838v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02838v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijun Long, Richard McCreadie, Muhammad Imran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In times of emergency, crisis response agencies need to quickly and
accurately assess the situation on the ground in order to deploy relevant
services and resources. However, authorities often have to make decisions based
on limited information, as data on affected regions can be scarce until local
response services can provide first-hand reports. Fortunately, the widespread
availability of smartphones with high-quality cameras has made citizen
journalism through social media a valuable source of information for crisis
responders. However, analyzing the large volume of images posted by citizens
requires more time and effort than is typically available. To address this
issue, this paper proposes the use of state-of-the-art deep neural models for
automatic image classification/tagging, specifically by adapting
transformer-based architectures for crisis image classification (CrisisViT). We
leverage the new Incidents1M crisis image dataset to develop a range of new
transformer-based image classification models. Through experimentation over the
standard Crisis image benchmark dataset, we demonstrate that the CrisisViT
models significantly outperform previous approaches in emergency type, image
relevance, humanitarian category, and damage severity classification.
Additionally, we show that the new Incidents1M dataset can further augment the
CrisisViT models resulting in an additional 1.25% absolute accuracy gain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Two-stage Progressive Residual Dense Attention Network for Image
  Denoising 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wencong Wu, An Ge, Guannan Lv, Yuelong Xia, Yungang Zhang, Wen Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep convolutional neural networks (CNNs) for image denoising can effectively
exploit rich hierarchical features and have achieved great success. However,
many deep CNN-based denoising models equally utilize the hierarchical features
of noisy images without paying attention to the more important and useful
features, leading to relatively low performance. To address the issue, we
design a new Two-stage Progressive Residual Dense Attention Network
(TSP-RDANet) for image denoising, which divides the whole process of denoising
into two sub-tasks to remove noise progressively. Two different attention
mechanism-based denoising networks are designed for the two sequential
sub-tasks: the residual dense attention module (RDAM) is designed for the first
stage, and the hybrid dilated residual dense attention module (HDRDAM) is
proposed for the second stage. The proposed attention modules are able to learn
appropriate local features through dense connection between different
convolutional layers, and the irrelevant features can also be suppressed. The
two sub-networks are then connected by a long skip connection to retain the
shallow feature to enhance the denoising performance. The experiments on seven
benchmark datasets have verified that compared with many state-of-the-art
methods, the proposed TSP-RDANet can obtain favorable results both on synthetic
and real noisy image denoising. The code of our TSP-RDANet is available at
https://github.com/WenCongWu/TSP-RDANet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CRSOT: Cross-Resolution Object Tracking using Unaligned Frame and Event
  Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02826v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02826v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yabin Zhu, Xiao Wang, Chenglong Li, Bo Jiang, Lin Zhu, Zhixiang Huang, Yonghong Tian, Jin Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing datasets for RGB-DVS tracking are collected with DVS346 camera and
their resolution ($346 \times 260$) is low for practical applications.
Actually, only visible cameras are deployed in many practical systems, and the
newly designed neuromorphic cameras may have different resolutions. The latest
neuromorphic sensors can output high-definition event streams, but it is very
difficult to achieve strict alignment between events and frames on both spatial
and temporal views. Therefore, how to achieve accurate tracking with unaligned
neuromorphic and visible sensors is a valuable but unresearched problem. In
this work, we formally propose the task of object tracking using unaligned
neuromorphic and visible cameras. We build the first unaligned frame-event
dataset CRSOT collected with a specially built data acquisition system, which
contains 1,030 high-definition RGB-Event video pairs, 304,974 video frames. In
addition, we propose a novel unaligned object tracking framework that can
realize robust tracking even using the loosely aligned RGB-Event data.
Specifically, we extract the template and search regions of RGB and Event data
and feed them into a unified ViT backbone for feature embedding. Then, we
propose uncertainty perception modules to encode the RGB and Event features,
respectively, then, we propose a modality uncertainty fusion module to
aggregate the two modalities. These three branches are jointly optimized in the
training phase. Extensive experiments demonstrate that our tracker can
collaborate the dual modalities for high-performance tracking even without
strictly temporal and spatial alignment. The source code, dataset, and
pre-trained models will be released at
https://github.com/Event-AHU/Cross_Resolution_SOT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Peer Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Object-Centric Instruction Augmentation for Robotic Manipulation <span class="chip">ICRA2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Wen, Yichen Zhu, Minjie Zhu, Jinming Li, Zhiyuan Xu, Zhengping Che, Chaomin Shen, Yaxin Peng, Dong Liu, Feifei Feng, Jian Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans interpret scenes by recognizing both the identities and positions of
objects in their observations. For a robot to perform tasks such as
\enquote{pick and place}, understanding both what the objects are and where
they are located is crucial. While the former has been extensively discussed in
the literature that uses the large language model to enrich the text
descriptions, the latter remains underexplored. In this work, we introduce the
\textit{Object-Centric Instruction Augmentation (OCI)} framework to augment
highly semantic and information-dense language instruction with position cues.
We utilize a Multi-modal Large Language Model (MLLM) to weave knowledge of
object locations into natural language instruction, thus aiding the policy
network in mastering actions for versatile manipulation. Additionally, we
present a feature reuse mechanism to integrate the vision-language features
from off-the-shelf pre-trained MLLM into policy networks. Through a series of
simulated and real-world robotic tasks, we demonstrate that robotic manipulator
imitation policies trained with our enhanced instructions outperform those
relying solely on traditional language instructions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICRA2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffbody: Diffusion-based Pose and Shape Editing of Human Images <span class="chip">WACV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuta Okuyama, Yuki Endo, Yoshihiro Kanamori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pose and body shape editing in a human image has received increasing
attention. However, current methods often struggle with dataset biases and
deteriorate realism and the person's identity when users make large edits. We
propose a one-shot approach that enables large edits with identity
preservation. To enable large edits, we fit a 3D body model, project the input
image onto the 3D model, and change the body's pose and shape. Because this
initial textured body model has artifacts due to occlusion and the inaccurate
body shape, the rendered image undergoes a diffusion-based refinement, in which
strong noise destroys body structure and identity whereas insufficient noise
does not help. We thus propose an iterative refinement with weak noise, applied
first for the whole body and then for the face. We further enhance the realism
by fine-tuning text embeddings via self-supervised learning. Our quantitative
and qualitative evaluations demonstrate that our method outperforms other
existing methods across various datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WACV 2024, project page:
  https://www.cgg.cs.tsukuba.ac.jp/~okuyama/pub/diffbody/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Subjective and Objective Analysis of Indian Social Media Video Quality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sandeep Mishra, Mukul Jha, Alan C. Bovik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We conducted a large-scale subjective study of the perceptual quality of
User-Generated Mobile Video Content on a set of mobile-originated videos
obtained from the Indian social media platform ShareChat. The content viewed by
volunteer human subjects under controlled laboratory conditions has the benefit
of culturally diversifying the existing corpus of User-Generated Content (UGC)
video quality datasets. There is a great need for large and diverse UGC-VQA
datasets, given the explosive global growth of the visual internet and social
media platforms. This is particularly true in regard to videos obtained by
smartphones, especially in rapidly emerging economies like India. ShareChat
provides a safe and cultural community oriented space for users to generate and
share content in their preferred Indian languages and dialects. Our subjective
quality study, which is based on this data, offers a boost of cultural, visual,
and language diversification to the video quality research community. We expect
that this new data resource will also allow for the development of systems that
can predict the perceived visual quality of Indian social media videos, to
control scaling and compression protocols for streaming, provide better user
recommendations, and guide content analysis and processing. We demonstrate the
value of the new data resource by conducting a study of leading blind video
quality models on it, including a new model, called MoEVA, which deploys a
mixture of experts to predict video quality. Both the new LIVE-ShareChat
dataset and sample source code for MoEVA are being made freely available to the
research community at https://github.com/sandeep-sm/LIVE-SC
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the IEEE Transactions on Image Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weakly Semi-supervised Tool Detection in Minimally Invasive Surgery
  Videos <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryo Fujii, Ryo Hachiuma, Hideo Saito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surgical tool detection is essential for analyzing and evaluating minimally
invasive surgery videos. Current approaches are mostly based on supervised
methods that require large, fully instance-level labels (i.e., bounding boxes).
However, large image datasets with instance-level labels are often limited
because of the burden of annotation. Thus, surgical tool detection is important
when providing image-level labels instead of instance-level labels since
image-level annotations are considerably more time-efficient than
instance-level annotations. In this work, we propose to strike a balance
between the extremely costly annotation burden and detection performance. We
further propose a co-occurrence loss, which considers a characteristic that
some tool pairs often co-occur together in an image to leverage image-level
labels. Encapsulating the knowledge of co-occurrence using the co-occurrence
loss helps to overcome the difficulty in classification that originates from
the fact that some tools have similar shapes and textures. Extensive
experiments conducted on the Endovis2018 dataset in various data settings show
the effectiveness of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fus-MAE: A cross-attention-based data fusion approach for Masked
  Autoencoders in remote sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02764v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02764v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Chan-To-Hing, Bharadwaj Veeravalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised frameworks for representation learning have recently stirred
up interest among the remote sensing community, given their potential to
mitigate the high labeling costs associated with curating large satellite image
datasets. In the realm of multimodal data fusion, while the often used
contrastive learning methods can help bridging the domain gap between different
sensor types, they rely on data augmentations techniques that require expertise
and careful design, especially for multispectral remote sensing data. A
possible but rather scarcely studied way to circumvent these limitations is to
use a masked image modelling based pretraining strategy. In this paper, we
introduce Fus-MAE, a self-supervised learning framework based on masked
autoencoders that uses cross-attention to perform early and feature-level data
fusion between synthetic aperture radar and multispectral optical data - two
modalities with a significant domain gap. Our empirical findings demonstrate
that Fus-MAE can effectively compete with contrastive learning strategies
tailored for SAR-optical data fusion and outperforms other masked-autoencoders
frameworks trained on a larger corpus.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detection and Classification of Diabetic Retinopathy using Deep Learning
  Algorithms for Segmentation to Facilitate Referral Recommendation for Test
  and Treatment Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manoj S H, Arya A Bosale
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research paper addresses the critical challenge of diabetic retinopathy
(DR), a severe complication of diabetes leading to potential blindness. The
proposed methodology leverages transfer learning with convolutional neural
networks (CNNs) for automatic DR detection using a single fundus photograph,
demonstrating high effectiveness with a quadratic weighted kappa score of
0.92546 in the APTOS 2019 Blindness Detection Competition. The paper reviews
existing literature on DR detection, spanning classical computer vision methods
to deep learning approaches, particularly focusing on CNNs. It identifies gaps
in the research, emphasizing the lack of exploration in integrating pretrained
large language models with segmented image inputs for generating
recommendations and understanding dynamic interactions within a web application
context.Objectives include developing a comprehensive DR detection methodology,
exploring model integration, evaluating performance through competition
ranking, contributing significantly to DR detection methodologies, and
identifying research gaps.The methodology involves data preprocessing, data
augmentation, and the use of a U-Net neural network architecture for
segmentation. The U-Net model efficiently segments retinal structures,
including blood vessels, hard and soft exudates, haemorrhages, microaneurysms,
and the optical disc. High evaluation scores in Jaccard, F1, recall, precision,
and accuracy underscore the model's potential for enhancing diagnostic
capabilities in retinal pathology assessment.The outcomes of this research hold
promise for improving patient outcomes through timely diagnosis and
intervention in the fight against diabetic retinopathy, marking a significant
contribution to the field of medical image analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Systematic <span class="highlight-title">review</span> of image segmentation using complex networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02758v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02758v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amin Rezaei, Fatemeh Asadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This review presents various image segmentation methods using complex
networks.
  Image segmentation is one of the important steps in image analysis as it
helps analyze and understand complex images. At first, it has been tried to
classify complex networks based on how it being used in image segmentation.
  In computer vision and image processing applications, image segmentation is
essential for analyzing complex images with irregular shapes, textures, or
overlapping boundaries. Advanced algorithms make use of machine learning,
clustering, edge detection, and region-growing techniques. Graph theory
principles combined with community detection-based methods allow for more
precise analysis and interpretation of complex images. Hybrid approaches
combine multiple techniques for comprehensive, robust segmentation, improving
results in computer vision and image processing tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reading Between the Frames: Multi-Modal Depression Detection in Videos
  from Non-Verbal Cues <span class="chip">ECIR
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Gimeno-Gómez, Ana-Maria Bucur, Adrian Cosma, Carlos-David Martínez-Hinarejos, Paolo Rosso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depression, a prominent contributor to global disability, affects a
substantial portion of the population. Efforts to detect depression from social
media texts have been prevalent, yet only a few works explored depression
detection from user-generated video content. In this work, we address this
research gap by proposing a simple and flexible multi-modal temporal model
capable of discerning non-verbal depression cues from diverse modalities in
noisy, real-world videos. We show that, for in-the-wild videos, using
additional high-level non-verbal cues is crucial to achieving good performance,
and we extracted and processed audio speech embeddings, face emotion
embeddings, face, body and hand landmarks, and gaze and blinking information.
Through extensive experiments, we show that our model achieves state-of-the-art
results on three key benchmark datasets for depression detection from video by
a substantial margin. Our code is publicly available on GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 46th European Conference on Information Retrieval (ECIR
  2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAMI: Multi-Attentional Mutual-Information for Long Sequence Neuron
  Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alfirsa Damasyifa Fauzulhaq, Wahyu Parwitayasa, Joseph Ananda Sugihdharma, M. Fadli Ridhani, Novanto Yudistira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neuron labeling is an approach to visualize the behaviour and respond of a
certain neuron to a certain pattern that activates the neuron. Neuron labeling
extract information about the features captured by certain neurons in a deep
neural network, one of which uses the encoder-decoder image captioning
approach. The encoder used can be a pretrained CNN-based model and the decoder
is an RNN-based model for text generation. Previous work, namely MILAN (Mutual
Information-guided Linguistic Annotation of Neuron), has tried to visualize the
neuron behaviour using modified Show, Attend, and Tell (SAT) model in the
encoder, and LSTM added with Bahdanau attention in the decoder. MILAN can show
great result on short sequence neuron captioning, but it does not show great
result on long sequence neuron captioning, so in this work, we would like to
improve the performance of MILAN even more by utilizing different kind of
attention mechanism and additionally adding several attention result into one,
in order to combine all the advantages from several attention mechanism. Using
our compound dataset, we obtained higher BLEU and F1-Score on our proposed
model, achieving 17.742 and 0.4811 respectively. At some point where the model
converges at the peak, our model obtained BLEU of 21.2262 and BERTScore
F1-Score of 0.4870.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing targeted transferability via feature space fine-tuning <span class="chip">ICASSP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Zeng, Biwei Chen, Anjie Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial examples (AEs) have been extensively studied due to their
potential for privacy protection and inspiring robust neural networks. However,
making a targeted AE transferable across unknown models remains challenging. In
this paper, to alleviate the overfitting dilemma common in an AE crafted by
existing simple iterative attacks, we propose fine-tuning it in the feature
space. Specifically, starting with an AE generated by a baseline attack, we
encourage the features that contribute to the target class and discourage the
features that contribute to the original class in a middle layer of the source
model. Extensive experiments demonstrate that only a few iterations of
fine-tuning can boost existing attacks in terms of targeted transferability
nontrivially and universally. Our results also verify that the simple iterative
attacks can yield comparable or even better transferability than the
resource-intensive methods, which rely on training target-specific classifiers
or generators with additional data. The code is available at:
github.com/zengh5/TA_feature_FT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 10 figures, accepted by 2024ICASSP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting Traffic Flow with Federated Learning and Graph Neural with
  Asynchronous Computations Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02723v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02723v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Yaqub, Shahzad Ahmad, Malik Abdul Manan, Imran Shabir Chuhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time traffic flow prediction holds significant importance within the
domain of Intelligent Transportation Systems (ITS). The task of achieving a
balance between prediction precision and computational efficiency presents a
significant challenge. In this article, we present a novel deep-learning method
called Federated Learning and Asynchronous Graph Convolutional Network
(FLAGCN). Our framework incorporates the principles of asynchronous graph
convolutional networks with federated learning to enhance the accuracy and
efficiency of real-time traffic flow prediction. The FLAGCN model employs a
spatial-temporal graph convolution technique to asynchronously address
spatio-temporal dependencies within traffic data effectively. To efficiently
handle the computational requirements associated with this deep learning model,
this study used a graph federated learning technique known as GraphFL. This
approach is designed to facilitate the training process. The experimental
results obtained from conducting tests on two distinct traffic datasets
demonstrate that the utilization of FLAGCN leads to the optimization of both
training and inference durations while maintaining a high level of prediction
accuracy. FLAGCN outperforms existing models with significant improvements by
achieving up to approximately 6.85% reduction in RMSE, 20.45% reduction in
MAPE, compared to the best-performing existing models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 Figures, 1 Table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Image Demoireing from Unpaired Real Data <span class="chip">AAAI2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02719v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02719v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunshan Zhong, Yuyao Zhou, Yuxin Zhang, Fei Chao, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on addressing the issue of image demoireing. Unlike the
large volume of existing studies that rely on learning from paired real data,
we attempt to learn a demoireing model from unpaired real data, i.e., moire
images associated with irrelevant clean images. The proposed method, referred
to as Unpaired Demoireing (UnDeM), synthesizes pseudo moire images from
unpaired datasets, generating pairs with clean images for training demoireing
models. To achieve this, we divide real moire images into patches and group
them in compliance with their moire complexity. We introduce a novel moire
generation framework to synthesize moire images with diverse moire features,
resembling real moire patches, and details akin to real moire-free images.
Additionally, we introduce an adaptive denoise method to eliminate the
low-quality pseudo moire images that adversely impact the learning of
demoireing models. We conduct extensive experiments on the commonly-used FHDMi
and UHDM datasets. Results manifest that our UnDeM performs better than
existing methods when using existing demoireing models such as MBCNN and
ESDNet-L. Code: https://github.com/zysxmu/UnDeM
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Complementary Information Mutual Learning for Multimodality Medical
  Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuyun Shen, Wenhao Li, Haoqing Chen, Xiaoling Wang, Fengping Zhu, Yuxin Li, Xiangfeng Wang, Bo Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiologists must utilize multiple modal images for tumor segmentation and
diagnosis due to the limitations of medical imaging and the diversity of tumor
signals. This leads to the development of multimodal learning in segmentation.
However, the redundancy among modalities creates challenges for existing
subtraction-based joint learning methods, such as misjudging the importance of
modalities, ignoring specific modal information, and increasing cognitive load.
These thorny issues ultimately decrease segmentation accuracy and increase the
risk of overfitting. This paper presents the complementary information mutual
learning (CIML) framework, which can mathematically model and address the
negative impact of inter-modal redundant information. CIML adopts the idea of
addition and removes inter-modal redundant information through inductive
bias-driven task decomposition and message passing-based redundancy filtering.
CIML first decomposes the multimodal segmentation task into multiple subtasks
based on expert prior knowledge, minimizing the information dependence between
modalities. Furthermore, CIML introduces a scheme in which each modality can
extract information from other modalities additively through message passing.
To achieve non-redundancy of extracted information, the redundant filtering is
transformed into complementary information learning inspired by the variational
information bottleneck. The complementary information learning procedure can be
efficiently solved by variational inference and cross-modal spatial attention.
Numerical results from the verification task and standard benchmarks indicate
that CIML efficiently removes redundant information between modalities,
outperforming SOTA methods regarding validation accuracy and segmentation
effect.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VoxelNextFusion: A Simple, Unified and Effective Voxel Fusion Framework
  for Multi-Modal 3D Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziying Song, Guoxin Zhang, Jun Xie, Lin Liu, Caiyan Jia, Shaoqing Xu, Zhepeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR-camera fusion can enhance the performance of 3D object detection by
utilizing complementary information between depth-aware LiDAR points and
semantically rich images. Existing voxel-based methods face significant
challenges when fusing sparse voxel features with dense image features in a
one-to-one manner, resulting in the loss of the advantages of images, including
semantic and continuity information, leading to sub-optimal detection
performance, especially at long distances. In this paper, we present
VoxelNextFusion, a multi-modal 3D object detection framework specifically
designed for voxel-based methods, which effectively bridges the gap between
sparse point clouds and dense images. In particular, we propose a voxel-based
image pipeline that involves projecting point clouds onto images to obtain both
pixel- and patch-level features. These features are then fused using a
self-attention to obtain a combined representation. Moreover, to address the
issue of background features present in patches, we propose a feature
importance module that effectively distinguishes between foreground and
background features, thus minimizing the impact of the background features.
Extensive experiments were conducted on the widely used KITTI and nuScenes 3D
object detection benchmarks. Notably, our VoxelNextFusion achieved around
+3.20% in AP@0.7 improvement for car detection in hard level compared to the
Voxel R-CNN baseline on the KITTI test dataset
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengying Wu, Yao Mu, Bingxian Wu, Yi Hou, Ji Ma, Shanghang Zhang, Chang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of household robotics, the Zero-Shot Object Navigation (ZSON)
task empowers agents to adeptly traverse unfamiliar environments and locate
objects from novel categories without prior explicit training. This paper
introduces VoroNav, a novel semantic exploration framework that proposes the
Reduced Voronoi Graph to extract exploratory paths and planning nodes from a
semantic map constructed in real time. By harnessing topological and semantic
information, VoroNav designs text-based descriptions of paths and images that
are readily interpretable by a large language model (LLM). Our approach
presents a synergy of path and farsight descriptions to represent the
environmental context, enabling the LLM to apply commonsense reasoning to
ascertain the optimal waypoints for navigation. Extensive evaluation on the
HM3D and HSSD datasets validates that VoroNav surpasses existing ZSON
benchmarks in both success rates and exploration efficiency (+2.8% Success and
+3.7% SPL on HM3D, +2.6% Success and +3.8% SPL on HSSD). Additionally
introduced metrics that evaluate obstacle avoidance proficiency and perceptual
efficiency further corroborate the enhancements achieved by our method in ZSON
planning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PAHD: Perception-Action based Human Decision Making using Explainable
  Graph Neural Networks on SAR Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sasindu Wijeratne, Bingyi Zhang, Rajgopal Kannan, Viktor Prasanna, Carl Busart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic Aperture Radar (SAR) images are commonly utilized in military
applications for automatic target recognition (ATR). Machine learning (ML)
methods, such as Convolutional Neural Networks (CNN) and Graph Neural Networks
(GNN), are frequently used to identify ground-based objects, including battle
tanks, personnel carriers, and missile launchers. Determining the vehicle
class, such as the BRDM2 tank, BMP2 tank, BTR60 tank, and BTR70 tank, is
crucial, as it can help determine whether the target object is an ally or an
enemy. While the ML algorithm provides feedback on the recognized target, the
final decision is left to the commanding officers. Therefore, providing
detailed information alongside the identified target can significantly impact
their actions. This detailed information includes the SAR image features that
contributed to the classification, the classification confidence, and the
probability of the identified object being classified as a different object
type or class. We propose a GNN-based ATR framework that provides the final
classified class and outputs the detailed information mentioned above. This is
the first study to provide a detailed analysis of the classification class,
making final decisions more straightforward. Moreover, our GNN framework
achieves an overall accuracy of 99.2\% when evaluated on the MSTAR dataset,
improving over previous state-of-the-art GNN methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Progressive Knowledge Distillation Of Stable Diffusion XL Using Layer
  Level Loss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yatharth Gupta, Vishnu V. Jaddipal, Harish Prabhala, Sayak Paul, Patrick Von Platen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stable Diffusion XL (SDXL) has become the best open source text-to-image
model (T2I) for its versatility and top-notch image quality. Efficiently
addressing the computational demands of SDXL models is crucial for wider reach
and applicability. In this work, we introduce two scaled-down variants, Segmind
Stable Diffusion (SSD-1B) and Segmind-Vega, with 1.3B and 0.74B parameter
UNets, respectively, achieved through progressive removal using layer-level
losses focusing on reducing the model size while preserving generative quality.
We release these models weights at https://hf.co/Segmind. Our methodology
involves the elimination of residual networks and transformer blocks from the
U-Net structure of SDXL, resulting in significant reductions in parameters, and
latency. Our compact models effectively emulate the original SDXL by
capitalizing on transferred knowledge, achieving competitive results against
larger multi-billion parameter SDXL. Our work underscores the efficacy of
knowledge distillation coupled with layer-level losses in reducing model size
while preserving the high-quality generative capabilities of SDXL, thus
facilitating more accessible deployment in resource-constrained environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GTA: Guided Transfer of Spatial Attention from Object-Centric
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        SeokHyun Seo, Jinwoo Hong, JungWoo Chae, Kyungyul Kim, Sangheum Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Utilizing well-trained representations in transfer learning often results in
superior performance and faster convergence compared to training from scratch.
However, even if such good representations are transferred, a model can easily
overfit the limited training dataset and lose the valuable properties of the
transferred representations. This phenomenon is more severe in ViT due to its
low inductive bias. Through experimental analysis using attention maps in ViT,
we observe that the rich representations deteriorate when trained on a small
dataset. Motivated by this finding, we propose a novel and simple
regularization method for ViT called Guided Transfer of spatial Attention
(GTA). Our proposed method regularizes the self-attention maps between the
source and target models. A target model can fully exploit the knowledge
related to object localization properties through this explicit regularization.
Our experimental results show that the proposed GTA consistently improves the
accuracy across five benchmark datasets especially when the number of training
data is small.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking PathCLIP for Pathology Image Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02651v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02651v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunyi Zheng, Xiaonan Cui, Yuxuan Sun, Jingxiong Li, Honglin Li, Yunlong Zhang, Pingyi Chen, Xueping Jing, Zhaoxiang Ye, Lin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate image classification and retrieval are of importance for clinical
diagnosis and treatment decision-making. The recent contrastive language-image
pretraining (CLIP) model has shown remarkable proficiency in understanding
natural images. Drawing inspiration from CLIP, PathCLIP is specifically
designed for pathology image analysis, utilizing over 200,000 image and text
pairs in training. While the performance the PathCLIP is impressive, its
robustness under a wide range of image corruptions remains unknown. Therefore,
we conduct an extensive evaluation to analyze the performance of PathCLIP on
various corrupted images from the datasets of Osteosarcoma and WSSS4LUAD. In
our experiments, we introduce seven corruption types including brightness,
contrast, Gaussian blur, resolution, saturation, hue, and markup at four
severity levels. Through experiments, we find that PathCLIP is relatively
robustness to image corruptions and surpasses OpenAI-CLIP and PLIP in zero-shot
classification. Among the seven corruptions, blur and resolution can cause
server performance degradation of the PathCLIP. This indicates that ensuring
the quality of images is crucial before conducting a clinical test.
Additionally, we assess the robustness of PathCLIP in the task of image-image
retrieval, revealing that PathCLIP performs less effectively than PLIP on
Osteosarcoma but performs better on WSSS4LUAD under diverse corruptions.
Overall, PathCLIP presents impressive zero-shot classification and retrieval
performance for pathology images, but appropriate care needs to be taken when
using it. We hope this study provides a qualitative impression of PathCLIP and
helps understand its differences from other CLIP models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing 3D-Air Signature by Pen Tip Tail Trajectory Awareness: <span class="highlight-title">Dataset</span>
  and Featuring by Novel Spatio-temporal CNN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saurabh Atreya, Maheswar Bora, Aritra Mukherjee, Abhijit Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work proposes a novel process of using pen tip and tail 3D trajectory
for air signature. To acquire the trajectories we developed a new pen tool and
a stereo camera was used. We proposed SliT-CNN, a novel 2D spatial-temporal
convolutional neural network (CNN) for better featuring of the air signature.
In addition, we also collected an air signature dataset from $45$ signers.
Skilled forgery signatures per user are also collected. A detailed benchmarking
of the proposed dataset using existing techniques and proposed CNN on existing
and proposed dataset exhibit the effectiveness of our methodology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted and presented in IJCB 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recent Advancement in 3D Biometrics using Monocular Camera 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aritra Mukherjee, Abhijit Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent literature has witnessed significant interest towards 3D biometrics
employing monocular vision for robust authentication methods. Motivated by
this, in this work we seek to provide insight on recent development in the area
of 3D biometrics employing monocular vision. We present the similarity and
dissimilarity of 3D monocular biometrics and classical biometrics, listing the
strengths and challenges. Further, we provide an overview of recent techniques
in 3D biometrics with monocular vision, as well as application systems adopted
by the industry. Finally, we discuss open research problems in this area of
research
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted and presented in IJCB 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AG-ReID.v2: Bridging Aerial and Ground Views for Person
  Re-identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huy Nguyen, Kien Nguyen, Sridha Sridharan, Clinton Fookes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aerial-ground person re-identification (Re-ID) presents unique challenges in
computer vision, stemming from the distinct differences in viewpoints, poses,
and resolutions between high-altitude aerial and ground-based cameras. Existing
research predominantly focuses on ground-to-ground matching, with aerial
matching less explored due to a dearth of comprehensive datasets. To address
this, we introduce AG-ReID.v2, a dataset specifically designed for person Re-ID
in mixed aerial and ground scenarios. This dataset comprises 100,502 images of
1,615 unique individuals, each annotated with matching IDs and 15 soft
attribute labels. Data were collected from diverse perspectives using a UAV,
stationary CCTV, and smart glasses-integrated camera, providing a rich variety
of intra-identity variations. Additionally, we have developed an explainable
attention network tailored for this dataset. This network features a
three-stream architecture that efficiently processes pairwise image distances,
emphasizes key top-down features, and adapts to variations in appearance due to
altitude differences. Comparative evaluations demonstrate the superiority of
our approach over existing baselines. We plan to release the dataset and
algorithm source code publicly, aiming to advance research in this specialized
field of computer vision. For access, please visit
https://github.com/huynguyen792/AG-ReID.v2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, Accepted by TIFS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Random Ensemble of Encrypted models for Enhancing Robustness against
  Adversarial Examples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryota Iijima, Sayaka Shiota, Hitoshi Kiya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) are well known to be vulnerable to adversarial
examples (AEs). In addition, AEs have adversarial transferability, which means
AEs generated for a source model can fool another black-box model (target
model) with a non-trivial probability. In previous studies, it was confirmed
that the vision transformer (ViT) is more robust against the property of
adversarial transferability than convolutional neural network (CNN) models such
as ConvMixer, and moreover encrypted ViT is more robust than ViT without any
encryption. In this article, we propose a random ensemble of encrypted ViT
models to achieve much more robust models. In experiments, the proposed scheme
is verified to be more robust against not only black-box attacks but also
white-box ones than convention methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FED-NeRF: Achieve High 3D Consistency and Temporal Coherence for Face
  Video Editing on Dynamic NeRF 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Zhang, Yu-Wing Tai, Chi-Keung Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of the GAN-NeRF structure has enabled face editing on NeRF to
maintain 3D view consistency. However, achieving simultaneously multi-view
consistency and temporal coherence while editing video sequences remains a
formidable challenge. This paper proposes a novel face video editing
architecture built upon the dynamic face GAN-NeRF structure, which effectively
utilizes video sequences to restore the latent code and 3D face geometry. By
editing the latent code, multi-view consistent editing on the face can be
ensured, as validated by multiview stereo reconstruction on the resulting
edited images in our dynamic NeRF. As the estimation of face geometries occurs
on a frame-by-frame basis, this may introduce a jittering issue. We propose a
stabilizer that maintains temporal coherence by preserving smooth changes of
face expressions in consecutive frames. Quantitative and qualitative analyses
reveal that our method, as the pioneering 4D face video editor, achieves
state-of-the-art performance in comparison to existing 2D or 3D-based
approaches independently addressing identity and motion. Codes will be
released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code will be available at: https://github.com/ZHANG1023/FED-NeRF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling and Masking: A New Paradigm of Data Sampling for Image and Video
  Quality Assessment <span class="chip">AAAI2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongxu Liu, Yinghui Quan, Guoyao Xiao, Aobo Li, Jinjian Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quality assessment of images and videos emphasizes both local details and
global semantics, whereas general data sampling methods (e.g., resizing,
cropping or grid-based fragment) fail to catch them simultaneously. To address
the deficiency, current approaches have to adopt multi-branch models and take
as input the multi-resolution data, which burdens the model complexity. In this
work, instead of stacking up models, a more elegant data sampling method (named
as SAMA, scaling and masking) is explored, which compacts both the local and
global content in a regular input size. The basic idea is to scale the data
into a pyramid first, and reduce the pyramid into a regular data dimension with
a masking strategy. Benefiting from the spatial and temporal redundancy in
images and videos, the processed data maintains the multi-scale characteristics
with a regular input size, thus can be processed by a single-branch model. We
verify the sampling method in image and video quality assessment. Experiments
show that our sampling method can improve the performance of current
single-branch models significantly, and achieves competitive performance to the
multi-branch models without extra model complexity. The source code will be
available at https://github.com/Sissuire/SAMA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2024. Code has been released at
  https://github.com/Sissuire/SAMA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MOODv2: Masked Image Modeling for Out-of-Distribution Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyao Li, Pengguang Chen, Shaozuo Yu, Shu Liu, Jiaya Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The crux of effective out-of-distribution (OOD) detection lies in acquiring a
robust in-distribution (ID) representation, distinct from OOD samples. While
previous methods predominantly leaned on recognition-based techniques for this
purpose, they often resulted in shortcut learning, lacking comprehensive
representations. In our study, we conducted a comprehensive analysis, exploring
distinct pretraining tasks and employing various OOD score functions. The
results highlight that the feature representations pre-trained through
reconstruction yield a notable enhancement and narrow the performance gap among
various score functions. This suggests that even simple score functions can
rival complex ones when leveraging reconstruction-based pretext tasks.
Reconstruction-based pretext tasks adapt well to various score functions. As
such, it holds promising potential for further expansion. Our OOD detection
framework, MOODv2, employs the masked image modeling pretext task. Without
bells and whistles, MOODv2 impressively enhances 14.30% AUROC to 95.68% on
ImageNet and achieves 99.98% on CIFAR-10.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DHGCN: Dynamic Hop Graph Convolution Network for <span class="highlight-title">Self-supervised</span> Point
  Cloud Learning <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jincen Jiang, Lizhi Zhao, Xuequan Lu, Wei Hu, Imran Razzak, Meili Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works attempt to extend Graph Convolution Networks (GCNs) to point
clouds for classification and segmentation tasks. These works tend to sample
and group points to create smaller point sets locally and mainly focus on
extracting local features through GCNs, while ignoring the relationship between
point sets. In this paper, we propose the Dynamic Hop Graph Convolution Network
(DHGCN) for explicitly learning the contextual relationships between the
voxelized point parts, which are treated as graph nodes. Motivated by the
intuition that the contextual information between point parts lies in the
pairwise adjacent relationship, which can be depicted by the hop distance of
the graph quantitatively, we devise a novel self-supervised part-level hop
distance reconstruction task and design a novel loss function accordingly to
facilitate training. In addition, we propose the Hop Graph Attention (HGA),
which takes the learned hop distance as input for producing attention weights
to allow edge features to contribute distinctively in aggregation. Eventually,
the proposed DHGCN is a plug-and-play module that is compatible with
point-based backbone networks. Comprehensive experiments on different backbones
and tasks demonstrate that our self-supervised method achieves state-of-the-art
performance. Our source code is available at: https://github.com/Jinec98/DHGCN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Partition-based Nonrigid Registration for 3D Face Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuping Ye, Zhan Song, Juan Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a partition-based surface registration for 3D morphable
model(3DMM). In the 3DMM, it often requires to warp a handcrafted template
model into different captured models. The proposed method first utilizes the
landmarks to partition the template model then scale each part and finally
smooth the boundaries. This method is especially effective when the disparity
between the template model and the target model is huge. The experiment result
shows the method perform well than the traditional warp method and robust to
the local minima.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploiting Polarized Material Cues for Robust Car Detection <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen Dong, Haiyang Mei, Ziqi Wei, Ao Jin, Sen Qiu, Qiang Zhang, Xin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Car detection is an important task that serves as a crucial prerequisite for
many automated driving functions. The large variations in lighting/weather
conditions and vehicle densities of the scenes pose significant challenges to
existing car detection algorithms to meet the highly accurate perception demand
for safety, due to the unstable/limited color information, which impedes the
extraction of meaningful/discriminative features of cars. In this work, we
present a novel learning-based car detection method that leverages trichromatic
linear polarization as an additional cue to disambiguate such challenging
cases. A key observation is that polarization, characteristic of the light
wave, can robustly describe intrinsic physical properties of the scene objects
in various imaging conditions and is strongly linked to the nature of materials
for cars (e.g., metal and glass) and their surrounding environment (e.g., soil
and trees), thereby providing reliable and discriminative features for robust
car detection in challenging scenes. To exploit polarization cues, we first
construct a pixel-aligned RGB-Polarization car detection dataset, which we
subsequently employ to train a novel multimodal fusion network. Our car
detection network dynamically integrates RGB and polarization features in a
request-and-complement manner and can explore the intrinsic material properties
of cars across all learning samples. We extensively validate our method and
demonstrate that it outperforms state-of-the-art detection methods.
Experimental results show that polarization is a powerful cue for car
detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Object-oriented backdoor attack against image captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meiling Li, Nan Zhong, Xinpeng Zhang, Zhenxing Qian, Sheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Backdoor attack against image classification task has been widely studied and
proven to be successful, while there exist little research on the backdoor
attack against vision-language models. In this paper, we explore backdoor
attack towards image captioning models by poisoning training data. Assuming the
attacker has total access to the training dataset, and cannot intervene in
model construction or training process. Specifically, a portion of benign
training samples is randomly selected to be poisoned. Afterwards, considering
that the captions are usually unfolded around objects in an image, we design an
object-oriented method to craft poisons, which aims to modify pixel values by a
slight range with the modification number proportional to the scale of the
current detected object region. After training with the poisoned data, the
attacked model behaves normally on benign images, but for poisoned images, the
model will generate some sentences irrelevant to the given image. The attack
controls the model behavior on specific test images without sacrificing the
generation performance on benign test images. Our method proves the weakness of
image captioning models to backdoor attack and we hope this work can raise the
awareness of defending against backdoor attack in the image captioning field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Characterizing Satellite Geometry via Accelerated 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Van Minh Nguyen, Emma Sandidge, Trupti Mahendrakar, Ryan T. White
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The accelerating deployment of spacecraft in orbit have generated interest in
on-orbit servicing (OOS), inspection of spacecraft, and active debris removal
(ADR). Such missions require precise rendezvous and proximity operations in the
vicinity of non-cooperative, possible unknown, resident space objects. Safety
concerns with manned missions and lag times with ground-based control
necessitate complete autonomy. This requires robust characterization of the
target's geometry. In this article, we present an approach for mapping
geometries of satellites on orbit based on 3D Gaussian Splatting that can run
on computing resources available on current spaceflight hardware. We
demonstrate model training and 3D rendering performance on a
hardware-in-the-loop satellite mock-up under several realistic lighting and
motion conditions. Our model is shown to be capable of training on-board and
rendering higher quality novel views of an unknown satellite nearly 2 orders of
magnitude faster than previous NeRF-based algorithms. Such on-board
capabilities are critical to enable downstream machine intelligence tasks
necessary for autonomous guidance, navigation, and control tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoCoT: Contrastive Chain-of-Thought <span class="highlight-title">Prompt</span>ing for Large Multimodal
  Models with Multiple Image Inputs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daoan Zhang, Junming Yang, Hanjia Lyu, Zijian Jin, Yuan Yao, Mingkai Chen, Jiebo Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When exploring the development of Artificial General Intelligence (AGI), a
critical task for these models involves interpreting and processing information
from multiple image inputs. However, Large Multimodal Models (LMMs) encounter
two issues in such scenarios: (1) a lack of fine-grained perception, and (2) a
tendency to blend information across multiple images. We first extensively
investigate the capability of LMMs to perceive fine-grained visual details when
dealing with multiple input images. The research focuses on two aspects: first,
image-to-image matching (to evaluate whether LMMs can effectively reason and
pair relevant images), and second, multi-image-to-text matching (to assess
whether LMMs can accurately capture and summarize detailed image information).
We conduct evaluations on a range of both open-source and closed-source large
models, including GPT-4V, Gemini, OpenFlamingo, and MMICL. To enhance model
performance, we further develop a Contrastive Chain-of-Thought (CoCoT)
prompting approach based on multi-input multimodal models. This method requires
LMMs to compare the similarities and differences among multiple image inputs,
and then guide the models to answer detailed questions about multi-image inputs
based on the identified similarities and differences. Our experimental results
showcase CoCoT's proficiency in enhancing the multi-image comprehension
capabilities of large multimodal models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MC-ViViT: Multi-branch Classifier-ViViT to detect Mild Cognitive
  Impairment in older adults using facial videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.05292v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.05292v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Sun, Hiroko H. Dodge, Mohammad H. Mahoor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep machine learning models including Convolutional Neural Networks (CNN)
have been successful in the detection of Mild Cognitive Impairment (MCI) using
medical images, questionnaires, and videos. This paper proposes a novel
Multi-branch Classifier-Video Vision Transformer (MC-ViViT) model to
distinguish MCI from those with normal cognition by analyzing facial features.
The data comes from the I-CONECT, a behavioral intervention trial aimed at
improving cognitive function by providing frequent video chats. MC-ViViT
extracts spatiotemporal features of videos in one branch and augments
representations by the MC module. The I-CONECT dataset is challenging as the
dataset is imbalanced containing Hard-Easy and Positive-Negative samples, which
impedes the performance of MC-ViViT. We propose a loss function for Hard-Easy
and Positive-Negative Samples (HP Loss) by combining Focal loss and AD-CORRE
loss to address the imbalanced problem. Our experimental results on the
I-CONECT dataset show the great potential of MC-ViViT in predicting MCI with a
high accuracy of 90.63% accuracy on some of the interview videos.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 tables, 7 figures, 9 equations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TreeLearn: A Comprehensive Deep Learning Method for Segmenting
  Individual Trees from Ground-Based LiDAR Forest Point Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08471v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08471v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Henrich, Jan van Delden, Dominik Seidel, Thomas Kneib, Alexander Ecker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Laser-scanned point clouds of forests make it possible to extract valuable
information for forest management. To consider single trees, a forest point
cloud needs to be segmented into individual tree point clouds. Existing
segmentation methods are usually based on hand-crafted algorithms, such as
identifying trunks and growing trees from them, and face difficulties in dense
forests with overlapping tree crowns. In this study, we propose TreeLearn, a
deep learning-based approach for tree instance segmentation of forest point
clouds. Unlike previous methods, TreeLearn is trained on already segmented
point clouds in a data-driven manner, making it less reliant on predefined
features and algorithms. Furthermore, TreeLearn is implemented as a fully
automatic pipeline and does not rely on extensive hyperparameter tuning, which
makes it easy to use. Additionally, we introduce a new manually segmented
benchmark forest dataset containing 156 full trees, and 79 partial trees, that
have been cleanly segmented by hand. The data is generated by mobile laser
scanning and contributes to create a larger and more diverse data basis for
model development and fine-grained instance segmentation evaluation. We trained
TreeLearn on forest point clouds of 6665 trees, labeled using the Lidar360
software. An evaluation on the benchmark dataset shows that TreeLearn performs
equally well or better than the algorithm used to generate its training data.
Furthermore, the method's performance can be vastly improved by fine-tuning on
the cleanly labeled benchmark dataset. The TreeLearn code is available from
https://github.com/ecker-lab/TreeLearn. The data as well as trained models can
be found at https://doi.org/10.25625/VPMPID.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Network Initialization for Medical AI Models Using
  Large-Scale, Unlabeled Natural Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.07688v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.07688v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soroosh Tayebi Arasteh, Leo Misera, Jakob Nikolas Kather, Daniel Truhn, Sven Nebelung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training datasets, like ImageNet, have become the gold standard in
medical image analysis. However, the emergence of self-supervised learning
(SSL), which leverages unlabeled data to learn robust features, presents an
opportunity to bypass the intensive labeling process. In this study, we
explored if SSL for pre-training on non-medical images can be applied to chest
radiographs and how it compares to supervised pre-training on non-medical
images and on medical images. We utilized a vision transformer and initialized
its weights based on (i) SSL pre-training on natural images (DINOv2), (ii) SL
pre-training on natural images (ImageNet dataset), and (iii) SL pre-training on
chest radiographs from the MIMIC-CXR database. We tested our approach on over
800,000 chest radiographs from six large global datasets, diagnosing more than
20 different imaging findings. Our SSL pre-training on curated images not only
outperformed ImageNet-based pre-training (P<0.001 for all datasets) but, in
certain cases, also exceeded SL on the MIMIC-CXR dataset. Our findings suggest
that selecting the right pre-training strategy, especially with SSL, can be
pivotal for improving artificial intelligence (AI)'s diagnostic accuracy in
medical imaging. By demonstrating the promise of SSL in chest radiograph
analysis, we underline a transformative shift towards more efficient and
accurate AI models in medical imaging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in European Radiology Experimental</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language-free Compositional Action Generation via Decoupling Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03538v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03538v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Liu, Guangyi Chen, Yansong Tang, Guangrun Wang, Ser-Nam Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Composing simple elements into complex concepts is crucial yet challenging,
especially for 3D action generation. Existing methods largely rely on extensive
neural language annotations to discern composable latent semantics, a process
that is often costly and labor-intensive. In this study, we introduce a novel
framework to generate compositional actions without reliance on language
auxiliaries. Our approach consists of three main components: Action Coupling,
Conditional Action Generation, and Decoupling Refinement. Action Coupling
utilizes an energy model to extract the attention masks of each sub-action,
subsequently integrating two actions using these attentions to generate
pseudo-training examples. Then, we employ a conditional generative model, CVAE,
to learn a latent space, facilitating the diverse generation. Finally, we
propose Decoupling Refinement, which leverages a self-supervised pre-trained
model MAE to ensure semantic consistency between the sub-actions and
compositional actions. This refinement process involves rendering generated 3D
actions into 2D space, decoupling these images into two sub-segments, using the
MAE model to restore the complete image from sub-segments, and constraining the
recovered images to match images rendered from raw sub-actions. Due to the lack
of existing datasets containing both sub-actions and compositional actions, we
created two new datasets, named HumanAct-C and UESTC-C, and present a
corresponding evaluation metric. Both qualitative and quantitative assessments
are conducted to show our efficacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Surgical Aggregation: Federated Class-Heterogeneous Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06683v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06683v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranav Kulkarni, Adway Kanhere, Paul H. Yi, Vishwa S. Parekh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The release of numerous chest x-ray datasets has spearheaded the development
of deep learning models with expert-level performance. However, they have
limited interoperability due to class-heterogeneity -- a result of inconsistent
labeling schemes and partial annotations. Therefore, it is challenging to
leverage these datasets in aggregate to train models with a complete
representation of abnormalities that may occur within the thorax. In this work,
we propose surgical aggregation, a federated learning framework for aggregating
knowledge from class-heterogeneous datasets and learn a model that can
simultaneously predict the presence of all disease labels present across the
datasets. We evaluate our method using simulated and real-world
class-heterogeneous datasets across both independent and identically
distributed (iid) and non-iid settings. Our results show that surgical
aggregation outperforms current methods, has better generalizability, and is a
crucial first step towards tackling class-heterogeneity in federated learning
to facilitate the development of clinically-useful models using previously
non-interoperable chest x-ray datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advancing Ischemic Stroke Diagnosis: A Novel Two-Stage Approach for
  Blood Clot Origin Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.13775v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.13775v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Koushik Sivarama Krishnan, P. J. Joe Nikesh, Swathi Gnanasekar, Karthik Sivarama Krishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An innovative two-stage methodology for categorizing blood clot origins is
presented in this paper, which is important for the diagnosis and treatment of
ischemic stroke. First, a background classifier based on MobileNetV3 segments
big whole-slide digital pathology images into numerous tiles to detect the
presence of cellular material. After that, different pre-trained image
classification algorithms are fine-tuned to determine the origin of blood
clots. Due to complex blood flow dynamics and limitations in conventional
imaging methods such as computed tomography (CT), magnetic resonance imaging
(MRI), and ultrasound, identifying the sources of blood clots is a challenging
task. Although these techniques are useful for identifying blood clots, they
are not very good at determining how they originated. To address these
challenges, our method makes use of robust computer vision models that have
been refined using information from whole-slide digital pathology images. Out
of all the models tested, the PoolFormer \cite{yu2022metaformer} performs
better than the others, with 93.4\% accuracy, 93.4\% precision, 93.4\% recall,
and 93.4\% F1-score. Moreover, it achieves the good weighted multi-class
logarithmic loss (WMCLL) of 0.4361, which emphasizes how effective it is in
this particular application. These encouraging findings suggest that our
approach can successfully identify the origin of blood clots in a variety of
vascular locations, potentially advancing ischemic stroke diagnosis and
treatment approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Randomized Smoothing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16221v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16221v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Scholten, Jan Schuchardt, Aleksandar Bojchevski, Stephan Günnemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world data is complex and often consists of objects that can be
decomposed into multiple entities (e.g. images into pixels, graphs into
interconnected nodes). Randomized smoothing is a powerful framework for making
models provably robust against small changes to their inputs - by guaranteeing
robustness of the majority vote when randomly adding noise before
classification. Yet, certifying robustness on such complex data via randomized
smoothing is challenging when adversaries do not arbitrarily perturb entire
objects (e.g. images) but only a subset of their entities (e.g. pixels). As a
solution, we introduce hierarchical randomized smoothing: We partially smooth
objects by adding random noise only on a randomly selected subset of their
entities. By adding noise in a more targeted manner than existing methods we
obtain stronger robustness guarantees while maintaining high accuracy. We
initialize hierarchical smoothing using different noising distributions,
yielding novel robustness certificates for discrete and continuous domains. We
experimentally demonstrate the importance of hierarchical smoothing in image
and node classification, where it yields superior robustness-accuracy
trade-offs. Overall, hierarchical smoothing is an important contribution
towards models that are both - certifiably robust to perturbations and
accurate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ConvNet vs <span class="highlight-title">Transformer</span>, Supervised vs CLIP: Beyond ImageNet Accuracy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09215v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09215v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kirill Vishniakov, Zhiqiang Shen, Zhuang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern computer vision offers a great variety of models to practitioners, and
selecting a model from multiple options for specific applications can be
challenging. Conventionally, competing model architectures and training
protocols are compared by their classification accuracy on ImageNet. However,
this single metric does not fully capture performance nuances critical for
specialized tasks. In this work, we conduct an in-depth comparative analysis of
model behaviors beyond ImageNet accuracy, for both ConvNet and Vision
Transformer architectures, each across supervised and CLIP training paradigms.
Although our selected models have similar ImageNet accuracies and compute
requirements, we find that they differ in many other aspects: types of
mistakes, output calibration, transferability, and feature invariance, among
others. This diversity in model characteristics, not captured by traditional
metrics, highlights the need for more nuanced analysis when choosing among
different models. Our code is available at
https://github.com/kirill-vish/Beyond-INet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://kirill-vish.github.io/beyond-imagenet-accuracy/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Brain tumor segmentation using synthetic MR images -- A comparison of
  GANs and diffusion models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02986v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02986v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Usman Akbar, Måns Larsson, Anders Eklund
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large annotated datasets are required for training deep learning models, but
in medical imaging data sharing is often complicated due to ethics,
anonymization and data protection legislation. Generative AI models, such as
generative adversarial networks (GANs) and diffusion models, can today produce
very realistic synthetic images, and can potentially facilitate data sharing.
However, in order to share synthetic medical images it must first be
demonstrated that they can be used for training different networks with
acceptable performance. Here, we therefore comprehensively evaluate four GANs
(progressive GAN, StyleGAN 1-3) and a diffusion model for the task of brain
tumor segmentation (using two segmentation networks, U-Net and a Swin
transformer). Our results show that segmentation networks trained on synthetic
images reach Dice scores that are 80% - 90% of Dice scores when training with
real images, but that memorization of the training images can be a problem for
diffusion models if the original dataset is too small. Our conclusion is that
sharing synthetic medical images is a viable option to sharing real images, but
that further work is required. The trained generative models and the generated
synthetic images are shared on AIDA data hub
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 Pages. 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep learning in computed tomography pulmonary angiography imaging: a
  dual-pronged approach for pulmonary embolism detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.05197v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.05197v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabiha Bushra, Muhammad E. H. Chowdhury, Rusab Sarmun, Saidul Kabir, Menatalla Said, Sohaib Bassam Zoghoul, Adam Mushtak, Israa Al-Hashimi, Abdulrahman Alqahtani, Anwarul Hasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing reliance on Computed Tomography Pulmonary Angiography (CTPA)
for Pulmonary Embolism (PE) diagnosis presents challenges and a pressing need
for improved diagnostic solutions. The primary objective of this study is to
leverage deep learning techniques to enhance the Computer Assisted Diagnosis
(CAD) of PE. With this aim, we propose a classifier-guided detection approach
that effectively leverages the classifier's probabilistic inference to direct
the detection predictions, marking a novel contribution in the domain of
automated PE diagnosis. Our classification system includes an Attention-Guided
Convolutional Neural Network (AG-CNN) that uses local context by employing an
attention mechanism. This approach emulates a human expert's attention by
looking at both global appearances and local lesion regions before making a
decision. The classifier demonstrates robust performance on the FUMPE dataset,
achieving an AUROC of 0.927, sensitivity of 0.862, specificity of 0.879, and an
F1-score of 0.805 with the Inception-v3 backbone architecture. Moreover, AG-CNN
outperforms the baseline DenseNet-121 model, achieving an 8.1% AUROC gain.
While previous research has mostly focused on finding PE in the main arteries,
our use of cutting-edge object detection models and ensembling techniques
greatly improves the accuracy of detecting small embolisms in the peripheral
arteries. Finally, our proposed classifier-guided detection approach further
refines the detection metrics, contributing new state-of-the-art to the
community: mAP$_{50}$, sensitivity, and F1-score of 0.846, 0.901, and 0.779,
respectively, outperforming the former benchmark with a significant 3.7%
improvement in mAP$_{50}$. Our research aims to elevate PE patient care by
integrating AI solutions into clinical workflows, highlighting the potential of
human-AI collaboration in medical diagnostics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Expert Systems With Applications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Visible-Infrared Person ReID by Collaborative Learning with
  Neighbor-Guided Label Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.12711v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.12711v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        De Cheng, Xiaojian Huang, Nannan Wang, Lingfeng He, Zhihui Li, Xinbo Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised learning visible-infrared person re-identification (USL-VI-ReID)
aims at learning modality-invariant features from unlabeled cross-modality
dataset, which is crucial for practical applications in video surveillance
systems. The key to essentially address the USL-VI-ReID task is to solve the
cross-modality data association problem for further heterogeneous joint
learning. To address this issue, we propose a Dual Optimal Transport Label
Assignment (DOTLA) framework to simultaneously assign the generated labels from
one modality to its counterpart modality. The proposed DOTLA mechanism
formulates a mutual reinforcement and efficient solution to cross-modality data
association, which could effectively reduce the side-effects of some
insufficient and noisy label associations. Besides, we further propose a
cross-modality neighbor consistency guided label refinement and regularization
module, to eliminate the negative effects brought by the inaccurate supervised
signals, under the assumption that the prediction or label distribution of each
example should be similar to its nearest neighbors. Extensive experimental
results on the public SYSU-MM01 and RegDB datasets demonstrate the
effectiveness of the proposed method, surpassing existing state-of-the-art
approach by a large margin of 7.76% mAP on average, which even surpasses some
supervised VI-ReID methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DRKF: Distilled Rotated Kernel Fusion for Efficient Rotation Invariant
  Descriptors in Local Feature Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.10907v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.10907v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ranran Huang, Jiancheng Cai, Chao Li, Zhuoyuan Wu, Xinmin Liu, Zhenhua Chai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of local feature descriptors degrades in the presence of
large rotation variations. To address this issue, we present an efficient
approach to learning rotation invariant descriptors. Specifically, we propose
Rotated Kernel Fusion (RKF) which imposes rotations on the convolution kernel
to improve the inherent nature of CNN. Since RKF can be processed by the
subsequent re-parameterization, no extra computational costs will be introduced
in the inference stage. Moreover, we present Multi-oriented Feature Aggregation
(MOFA) which aggregates features extracted from multiple rotated versions of
the input image and can provide auxiliary knowledge for the training of RKF by
leveraging the distillation strategy. We refer to the distilled RKF model as
DRKF. Besides the evaluation on a rotation-augmented version of the public
dataset HPatches, we also contribute a new dataset named DiverseBEV which is
collected during the drone's flight and consists of bird's eye view images with
large viewpoint changes and camera rotations. Extensive experiments show that
our method can outperform other state-of-the-art techniques when exposed to
large rotation variations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Managing the unknown: a <span class="highlight-title">survey</span> on Open Set Recognition and tangential
  areas 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.08785v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.08785v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcos Barcina-Blanco, Jesus L. Lobo, Pablo Garcia-Bringas, Javier Del Ser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world scenarios classification models are often required to perform
robustly when predicting samples belonging to classes that have not appeared
during its training stage. Open Set Recognition addresses this issue by
devising models capable of detecting unknown classes from samples arriving
during the testing phase, while maintaining a good level of performance in the
classification of samples belonging to known classes. This review
comprehensively overviews the recent literature related to Open Set
Recognition, identifying common practices, limitations, and connections of this
field with other machine learning research areas, such as continual learning,
out-of-distribution detection, novelty detection, and uncertainty estimation.
Our work also uncovers open problems and suggests several research directions
that may motivate and articulate future efforts towards more safe Artificial
Intelligence methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 1 figure, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-Covariate Gait Recognition: A Benchmark <span class="chip">AAAI2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14404v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14404v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shinan Zou, Chao Fan, Jianbo Xiong, Chuanfu Shen, Shiqi Yu, Jin Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gait datasets are essential for gait research. However, this paper observes
that present benchmarks, whether conventional constrained or emerging
real-world datasets, fall short regarding covariate diversity. To bridge this
gap, we undertake an arduous 20-month effort to collect a cross-covariate gait
recognition (CCGR) dataset. The CCGR dataset has 970 subjects and about 1.6
million sequences; almost every subject has 33 views and 53 different
covariates. Compared to existing datasets, CCGR has both population and
individual-level diversity. In addition, the views and covariates are well
labeled, enabling the analysis of the effects of different factors. CCGR
provides multiple types of gait data, including RGB, parsing, silhouette, and
pose, offering researchers a comprehensive resource for exploration. In order
to delve deeper into addressing cross-covariate gait recognition, we propose
parsing-based gait recognition (ParsingGait) by utilizing the newly proposed
parsing data. We have conducted extensive experiments. Our main results show:
1) Cross-covariate emerges as a pivotal challenge for practical applications of
gait recognition. 2) ParsingGait demonstrates remarkable potential for further
advancement. 3) Alarmingly, existing SOTA methods achieve less than 43%
accuracy on the CCGR, highlighting the urgency of exploring cross-covariate
gait recognition. Link: https://github.com/ShinanZou/CCGR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by AAAI2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Context-Aware Iteration Policy Network for Efficient Optical Flow
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07180v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07180v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ri Cheng, Ruian He, Xuhao Jiang, Shili Zhou, Weimin Tan, Bo Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing recurrent optical flow estimation networks are computationally
expensive since they use a fixed large number of iterations to update the flow
field for each sample. An efficient network should skip iterations when the
flow improvement is limited. In this paper, we develop a Context-Aware
Iteration Policy Network for efficient optical flow estimation, which
determines the optimal number of iterations per sample. The policy network
achieves this by learning contextual information to realize whether flow
improvement is bottlenecked or minimal. On the one hand, we use iteration
embedding and historical hidden cell, which include previous iterations
information, to convey how flow has changed from previous iterations. On the
other hand, we use the incremental loss to make the policy network implicitly
perceive the magnitude of optical flow improvement in the subsequent iteration.
Furthermore, the computational complexity in our dynamic network is
controllable, allowing us to satisfy various resource preferences with a single
trained model. Our policy network can be easily integrated into
state-of-the-art optical flow networks. Extensive experiments show that our
method maintains performance while reducing FLOPs by about 40%/20% for the
Sintel/KITTI datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2024, Association for the Advancement of Artificial Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepMerge: Deep-Learning-Based Region-Merging for Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.19787v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.19787v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianwei Lv, Claudio Persello, Wangbin Li, Xiao Huang, Dongping Ming, Alfred Stein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image segmentation aims to partition an image according to the objects in the
scene and is a fundamental step in analysing very high spatial-resolution (VHR)
remote sensing imagery. Current methods struggle to effectively consider land
objects with diverse shapes and sizes. Additionally, the determination of
segmentation scale parameters frequently adheres to a static and empirical
doctrine, posing limitations on the segmentation of large-scale remote sensing
images and yielding algorithms with limited interpretability. To address the
above challenges, we propose a deep-learning-based region merging method dubbed
DeepMerge to handle the segmentation of complete objects in large VHR images by
integrating deep learning and region adjacency graph (RAG). This is the first
method to use deep learning to learn the similarity and merge similar adjacent
super-pixels in RAG. We propose a modified binary tree sampling method to
generate shift-scale data, serving as inputs for transformer-based deep
learning networks, a shift-scale attention with 3-Dimension relative position
embedding to learn features across scales, and an embedding to fuse learned
features with hand-crafted features. DeepMerge can achieve high segmentation
accuracy in a supervised manner from large-scale remotely sensed images and
provides an interpretable optimal scale parameter, which is validated using a
remote sensing image of 0.55 m resolution covering an area of 5,660 km^2. The
experimental results show that DeepMerge achieves the highest F value (0.9550)
and the lowest total error TE (0.0895), correctly segmenting objects of
different sizes and outperforming all competing segmentation methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FENet: Focusing Enhanced Network for Lane Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17163v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17163v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liman Wang, Hanyang Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by human driving focus, this research pioneers networks augmented
with Focusing Sampling, Partial Field of View Evaluation, Enhanced FPN
architecture and Directional IoU Loss - targeted innovations addressing
obstacles to precise lane detection for autonomous driving. Experiments
demonstrate our Focusing Sampling strategy, emphasizing vital distant details
unlike uniform approaches, significantly boosts both benchmark and practical
curved/distant lane recognition accuracy essential for safety. While FENetV1
achieves state-of-the-art conventional metric performance via enhancements
isolating perspective-aware contexts mimicking driver vision, FENetV2 proves
most reliable on the proposed Partial Field analysis. Hence we specifically
recommend V2 for practical lane navigation despite fractional degradation on
standard entire-image measures. Future directions include collecting on-road
data and integrating complementary dual frameworks to further breakthroughs
guided by human perception principles. The Code is available at
https://github.com/HanyangZhong/FENet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages including appendix. The Code is available at
  https://github.com/HanyangZhong/FENet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predicting Parkinson's disease evolution using deep learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17290v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17290v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Frasca, Davide La Torre, Gabriella Pravettoni, Ilaria Cutica
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parkinson's disease is a neurological condition that occurs in nearly 1% of
the world's population. The disease is manifested by a drop in dopamine
production, symptoms are cognitive and behavioural and include a wide range of
personality changes, depressive disorders, memory problems, and emotional
dysregulation, which can occur as the disease progresses. Early diagnosis and
accurate staging of the disease are essential to apply the appropriate
therapeutic approaches to slow cognitive and motor decline.
  Currently, there is not a single blood test or biomarker available to
diagnose Parkinson's disease. Magnetic resonance imaging has been used for the
past three decades to diagnose and distinguish between PD and other
neurological conditions. However, in recent years new possibilities have
arisen: several AI algorithms have been developed to increase the precision and
accuracy of differential diagnosis of PD at an early stage.
  To our knowledge, no AI tools have been designed to identify the stage of
progression. This paper aims to fill this gap. Using the "Parkinson's
Progression Markers Initiative" dataset, which reports the patient's MRI and an
indication of the disease stage, we developed a model to identify the level of
progression. The images and the associated scores were used for training and
assessing different deep-learning models. Our analysis distinguished four
distinct disease progression levels based on a standard scale (Hoehn and Yah
scale). The final architecture consists of the cascading of a 3DCNN network,
adopted to reduce and extract the spatial characteristics of the RMI for
efficient training of the successive LSTM layers, aiming at modelling the
temporal dependencies among the data.
  Our results show that the proposed 3DCNN + LSTM model achieves
state-of-the-art results by classifying the elements with 91.90\% as macro
averaged OVR AUC on four classes
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Research on Multilingual Natural Scene Text Detection Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11153v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11153v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural scene text detection is a significant challenge in computer vision,
with tremendous potential applications in multilingual, diverse, and complex
text scenarios. We propose a multilingual text detection model to address the
issues of low accuracy and high difficulty in detecting multilingual text in
natural scenes. In response to the challenges posed by multilingual text images
with multiple character sets and various font styles, we introduce the SFM Swin
Transformer feature extraction network to enhance the model's robustness in
detecting characters and fonts across different languages. Dealing with the
considerable variation in text scales and complex arrangements in natural scene
text images, we present the AS-HRFPN feature fusion network by incorporating an
Adaptive Spatial Feature Fusion module and a Spatial Pyramid Pooling module.
The feature fusion network improvements enhance the model's ability to detect
text sizes and orientations. Addressing diverse backgrounds and font variations
in multilingual scene text images is a challenge for existing methods. Limited
local receptive fields hinder detection performance. To overcome this, we
propose a Global Semantic Segmentation Branch, extracting and preserving global
features for more effective text detection, aligning with the need for
comprehensive information. In this study, we collected and built a real-world
multilingual natural scene text image dataset and conducted comprehensive
experiments and analyses. The experimental results demonstrate that the
proposed algorithm achieves an F-measure of 85.02\%, which is 4.71\% higher
than the baseline model. We also conducted extensive cross-dataset validation
on MSRA-TD500, ICDAR2017MLT, and ICDAR2015 datasets to verify the generality of
our approach. The code and dataset can be found at
https://github.com/wangmelon/CEMLT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Sorry, we discovered certain mistake and asked that the current
  version be removed in order to perform a thorough reanalysis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tissue Artifact Segmentation and Severity Analysis for Automated
  Diagnosis Using Whole Slide Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01386v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01386v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Galib Muhammad Shahriar Himel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditionally, pathological analysis and diagnosis are performed by manually
eyeballing glass slide specimens under a microscope by an expert. The whole
slide image is the digital specimen produced from the glass slide. Whole slide
image enabled specimens to be observed on a computer screen and led to
computational pathology where computer vision and artificial intelligence are
utilized for automated analysis and diagnosis. With the current computational
advancement, the entire whole slide image can be analyzed autonomously without
human supervision. However, the analysis could fail or lead to wrong diagnosis
if the whole slide image is affected by tissue artifacts such as tissue fold or
air bubbles depending on the severity. Existing artifact detection methods rely
on experts for severity assessment to eliminate artifact affected regions from
the analysis. This process is time consuming, exhausting and undermines the
goal of automated analysis or removal of artifacts without evaluating their
severity, which could result in the loss of diagnostically important data.
Therefore, it is necessary to detect artifacts and then assess their severity
automatically. In this paper, we propose a system that incorporates severity
evaluation with artifact detection utilizing convolutional neural networks. The
proposed system uses DoubleUNet to segment artifacts and an ensemble network of
six fine tuned convolutional neural network models to determine severity. This
method outperformed current state of the art in accuracy by 9 percent for
artifact segmentation and achieved a strong correlation of 97 percent with the
evaluation of pathologists for severity assessment. The robustness of the
system was demonstrated using our proposed heterogeneous dataset and practical
usability was ensured by integrating it with an automated analysis system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>60 pages, 21 figures, 16 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Non-aligned supervision for Real Image Dehazing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04940v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04940v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junkai Fan, Fei Guo, Jianjun Qian, Xiang Li, Jun Li, Jian Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Removing haze from real-world images is challenging due to unpredictable
weather conditions, resulting in the misalignment of hazy and clear image
pairs. In this paper, we propose an innovative dehazing framework that operates
under non-aligned supervision. This framework is grounded in the atmospheric
scattering model, and consists of three interconnected networks: dehazing,
airlight, and transmission networks. In particular, we explore a non-alignment
scenario that a clear reference image, unaligned with the input hazy image, is
utilized to supervise the dehazing network. To implement this, we present a
multi-scale reference loss that compares the feature representations between
the referred image and the dehazed output. Our scenario makes it easier to
collect hazy/clear image pairs in real-world environments, even under
conditions of misalignment and shift views. To showcase the effectiveness of
our scenario, we have collected a new hazy dataset including 415 image pairs
captured by mobile Phone in both rural and urban areas, called "Phone-Hazy".
Furthermore, we introduce a self-attention network based on mean and variance
for modeling real infinite airlight, using the dark channel prior as positional
guidance. Additionally, a channel attention network is employed to estimate the
three-channel transmission. Experimental results demonstrate the superior
performance of our framework over existing state-of-the-art techniques in the
real-world image dehazing task. Phone-Hazy and code will be available at
https://fanjunkai1.github.io/projectpage/NSDNet/index.html.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-attention learning enables real-time nonuniform rotational
  distortion correction in OCT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04512v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04512v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Zhang, Jianlong Yang, Jingqian Zhang, Shiqing Zhao, Aili Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nonuniform rotational distortion (NURD) correction is vital for endoscopic
optical coherence tomography (OCT) imaging and its functional extensions, such
as angiography and elastography. Current NURD correction methods require
time-consuming feature tracking or cross-correlation calculations and thus
sacrifice temporal resolution. Here we propose a cross-attention learning
method for the NURD correction in OCT. Our method is inspired by the recent
success of the self-attention mechanism in natural language processing and
computer vision. By leveraging its ability to model long-range dependencies, we
can directly obtain the correlation between OCT A-lines at any distance, thus
accelerating the NURD correction. We develop an end-to-end stacked
cross-attention network and design three types of optimization constraints. We
compare our method with two traditional feature-based methods and a CNN-based
method, on two publicly-available endoscopic OCT datasets and a private dataset
collected on our home-built endoscopic OCT system. Our method achieved a
$\sim3\times$ speedup to real time ($26\pm 3$ fps), and superior correction
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised CT Metal Artifact Reduction by Plugging Diffusion Priors in
  Dual Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.16742v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.16742v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan Liu, Yaoqin Xie, Songhui Diao, Shan Tan, Xiaokun Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  During the process of computed tomography (CT), metallic implants often cause
disruptive artifacts in the reconstructed images, impeding accurate diagnosis.
Several supervised deep learning-based approaches have been proposed for
reducing metal artifacts (MAR). However, these methods heavily rely on training
with simulated data, as obtaining paired metal artifact CT and clean CT data in
clinical settings is challenging. This limitation can lead to decreased
performance when applying these methods in clinical practice. Existing
unsupervised MAR methods, whether based on learning or not, typically operate
within a single domain, either in the image domain or the sinogram domain. In
this paper, we propose an unsupervised MAR method based on the diffusion model,
a generative model with a high capacity to represent data distributions.
Specifically, we first train a diffusion model using CT images without metal
artifacts. Subsequently, we iteratively utilize the priors embedded within the
pre-trained diffusion model in both the sinogram and image domains to restore
the degraded portions caused by metal artifacts. This dual-domain processing
empowers our approach to outperform existing unsupervised MAR methods,
including another MAR method based on the diffusion model, which we have
qualitatively and quantitatively validated using synthetic datasets. Moreover,
our method demonstrates superior visual results compared to both supervised and
unsupervised methods on clinical datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.07728v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.07728v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seokhyeon Ha, Sunbeom Jung, Jungwoo Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning pre-trained neural network models has become a widely adopted
approach across various domains. However, it can lead to the distortion of
pre-trained feature extractors that already possess strong generalization
capabilities. Mitigating feature distortion during adaptation to new target
domains is crucial. Recent studies have shown promising results in handling
feature distortion by aligning the head layer on in-distribution datasets
before performing fine-tuning. Nonetheless, a significant limitation arises
from the treatment of batch normalization layers during fine-tuning, leading to
suboptimal performance. In this paper, we propose Domain-Aware Fine-Tuning
(DAFT), a novel approach that incorporates batch normalization conversion and
the integration of linear probing and fine-tuning. Our batch normalization
conversion method effectively mitigates feature distortion by reducing
modifications to the neural network during fine-tuning. Additionally, we
introduce the integration of linear probing and fine-tuning to optimize the
head layer with gradual adaptation of the feature extractor. By leveraging
batch normalization layers and integrating linear probing and fine-tuning, our
DAFT significantly mitigates feature distortion and achieves improved model
performance on both in-distribution and out-of-distribution datasets. Extensive
experiments demonstrate that our method outperforms other baseline methods,
demonstrating its effectiveness in not only improving performance but also
mitigating feature distortion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Open and Comprehensive Pipeline for Unified Object Grounding and
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02361v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02361v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyu Zhao, Yicheng Chen, Shilin Xu, Xiangtai Li, Xinjiang Wang, Yining Li, Haian Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grounding-DINO is a state-of-the-art open-set detection model that tackles
multiple vision tasks including Open-Vocabulary Detection (OVD), Phrase
Grounding (PG), and Referring Expression Comprehension (REC). Its effectiveness
has led to its widespread adoption as a mainstream architecture for various
downstream applications. However, despite its significance, the original
Grounding-DINO model lacks comprehensive public technical details due to the
unavailability of its training code. To bridge this gap, we present
MM-Grounding-DINO, an open-source, comprehensive, and user-friendly baseline,
which is built with the MMDetection toolbox. It adopts abundant vision datasets
for pre-training and various detection and grounding datasets for fine-tuning.
We give a comprehensive analysis of each reported result and detailed settings
for reproduction. The extensive experiments on the benchmarks mentioned
demonstrate that our MM-Grounding-DINO-Tiny outperforms the Grounding-DINO-Tiny
baseline. We release all our models to the research community. Codes and
trained models are released at
https://github.com/open-mmlab/mmdetection/tree/main/configs/mm_grounding_dino.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11700v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11700v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi Yan, Delin Qu, Dong Wang, Dan Xu, Zhigang Wang, Bin Zhao, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce $\textbf{GS-SLAM}$ that first utilizes 3D
Gaussian representation in the Simultaneous Localization and Mapping (SLAM)
system. It facilitates a better balance between efficiency and accuracy.
Compared to recent SLAM methods employing neural implicit representations, our
method utilizes a real-time differentiable splatting rendering pipeline that
offers significant speedup to map optimization and RGB-D re-rendering.
Specifically, we propose an adaptive expansion strategy that adds new or
deletes noisy 3D Gaussian in order to efficiently reconstruct new observed
scene geometry and improve the mapping of previously observed areas. This
strategy is essential to extend 3D Gaussian representation to reconstruct the
whole scene rather than synthesize a static object in existing methods.
Moreover, in the pose tracking process, an effective coarse-to-fine technique
is designed to select reliable 3D Gaussian representations to optimize camera
pose, resulting in runtime reduction and robust estimation. Our method achieves
competitive performance compared with existing state-of-the-art real-time
methods on the Replica, TUM-RGBD datasets. The source code will be released
soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ControlDreamer: Stylized 3D Generation with Multi-View ControlNet 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.01129v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.01129v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeongtak Oh, Jooyoung Choi, Yongsung Kim, Minjun Park, Chaehun Shin, Sungroh Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in text-to-3D generation have significantly contributed
to the automation and democratization of 3D content creation. Building upon
these developments, we aim to address the limitations of current methods in
generating 3D models with creative geometry and styles. We introduce multi-view
ControlNet, a novel depth-aware multi-view diffusion model trained on generated
datasets from a carefully curated text corpus. Our multi-view ControlNet is
then integrated into our two-stage pipeline, ControlDreamer, enabling
text-guided generation of stylized 3D models. Additionally, we present a
comprehensive benchmark for 3D style editing, encompassing a broad range of
subjects, including objects, animals, and characters, to further facilitate
research on diverse 3D generation. Our comparative analysis reveals that this
new pipeline outperforms existing text-to-3D methods as evidenced by human
evaluations and CLIP score metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://controldreamer.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MatchDet: A Collaborative Framework for Image Matching and Object
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10983v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10983v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinxiang Lai, Wenlong Wu, Bin-Bin Gao, Jun Liu, Jiawei Zhan, Congchong Nie, Yi Zeng, Chengjie Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image matching and object detection are two fundamental and challenging
tasks, while many related applications consider them two individual tasks (i.e.
task-individual). In this paper, a collaborative framework called MatchDet
(i.e. task-collaborative) is proposed for image matching and object detection
to obtain mutual improvements. To achieve the collaborative learning of the two
tasks, we propose three novel modules, including a Weighted Spatial Attention
Module (WSAM) for Detector, and Weighted Attention Module (WAM) and Box Filter
for Matcher. Specifically, the WSAM highlights the foreground regions of target
image to benefit the subsequent detector, the WAM enhances the connection
between the foreground regions of pair images to ensure high-quality matches,
and Box Filter mitigates the impact of false matches. We evaluate the
approaches on a new benchmark with two datasets called Warp-COCO and
miniScanNet. Experimental results show our approaches are effective and achieve
competitive improvements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TR-DETR: Task-Reciprocal <span class="highlight-title">Transformer</span> for Joint Moment Retrieval and
  Highlight Detection <span class="chip">AAAI-24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02309v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02309v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Sun, Mingyao Zhou, Wenjing Chen, Wei Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video moment retrieval (MR) and highlight detection (HD) based on natural
language queries are two highly related tasks, which aim to obtain relevant
moments within videos and highlight scores of each video clip. Recently,
several methods have been devoted to building DETR-based networks to solve both
MR and HD jointly. These methods simply add two separate task heads after
multi-modal feature extraction and feature interaction, achieving good
performance. Nevertheless, these approaches underutilize the reciprocal
relationship between two tasks. In this paper, we propose a task-reciprocal
transformer based on DETR (TR-DETR) that focuses on exploring the inherent
reciprocity between MR and HD. Specifically, a local-global multi-modal
alignment module is first built to align features from diverse modalities into
a shared latent space. Subsequently, a visual feature refinement is designed to
eliminate query-irrelevant information from visual features for modal
interaction. Finally, a task cooperation module is constructed to refine the
retrieval pipeline and the highlight score prediction process by utilizing the
reciprocity between MR and HD. Comprehensive experiments on QVHighlights,
Charades-STA and TVSum datasets demonstrate that TR-DETR outperforms existing
state-of-the-art methods. Codes are available at
\url{https://github.com/mingyao1120/TR-DETR}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI-24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Incremental Rotation Averaging Revisited and More: A New Rotation
  Averaging Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16924v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16924v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Gao, Hainan Cui, Shuhan Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In order to further advance the accuracy and robustness of the incremental
parameter estimation-based rotation averaging methods, in this paper, a new
member of the Incremental Rotation Averaging (IRA) family is introduced, which
is termed as IRAv4. As the most significant feature of the IRAv4, a
task-specific connected dominating set is extracted to serve as a more reliable
and accurate reference for rotation global alignment. In addition, to further
address the limitations of the existing rotation averaging benchmark of relying
on the slightly outdated Bundler camera calibration results as ground truths
and focusing solely on rotation estimation accuracy, this paper presents a new
COLMAP-based rotation averaging benchmark that incorporates a cross check
between COLMAP and Bundler, and employ the accuracy of both rotation and
downstream location estimation as evaluation metrics, which is desired to
provide a more reliable and comprehensive evaluation tool for the rotation
averaging research. Comprehensive comparisons between the proposed IRAv4 and
other mainstream rotation averaging methods on this new benchmark demonstrate
the effectiveness of our proposed approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Transactions</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast-iTPN: Integrally <span class="highlight-title">Pre-Train</span>ed <span class="highlight-title">Transformer</span> Pyramid Network with Token
  Migration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.12735v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.12735v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunjie Tian, Lingxi Xie, Jihao Qiu, Jianbin Jiao, Yaowei Wang, Qi Tian, Qixiang Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose integrally pre-trained transformer pyramid network (iTPN), towards
jointly optimizing the network backbone and the neck, so that transfer gap
between representation models and downstream tasks is minimal. iTPN is born
with two elaborated designs: 1) The first pre-trained feature pyramid upon
vision transformer (ViT). 2) Multi-stage supervision to the feature pyramid
using masked feature modeling (MFM). iTPN is updated to Fast-iTPN, reducing
computational memory overhead and accelerating inference through two flexible
designs. 1) Token migration: dropping redundant tokens of the backbone while
replenishing them in the feature pyramid without attention operations. 2) Token
gathering: reducing computation cost caused by global attention by introducing
few gathering tokens. The base/large-level Fast-iTPN achieve 88.75%/89.5% top-1
accuracy on ImageNet-1K. With 1x training schedule using DINO, the
base/large-level Fast-iTPN achieves 58.4%/58.8% box AP on COCO object
detection, and a 57.5%/58.7% mIoU on ADE20K semantic segmentation using
MaskDINO. Fast-iTPN can accelerate the inference procedure by up to 70%, with
negligible performance loss, demonstrating the potential to be a powerful
backbone for downstream vision tasks. The code is available at:
github.com/sunsmarterjie/iTPN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The tiny/small/base-level models report new records on ImageNet-1K.
  Code: github.com/sunsmarterjie/iTPN</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning from History: Task-agnostic Model Contrastive Learning for
  Image Restoration <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.06023v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.06023v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gang Wu, Junjun Jiang, Kui Jiang, Xianming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning has emerged as a prevailing paradigm for high-level
vision tasks, which, by introducing properly negative samples, has also been
exploited for low-level vision tasks to achieve a compact optimization space to
account for their ill-posed nature. However, existing methods rely on manually
predefined and task-oriented negatives, which often exhibit pronounced
task-specific biases. To address this challenge, our paper introduces an
innovative method termed 'learning from history', which dynamically generates
negative samples from the target model itself. Our approach, named Model
Contrastive paradigm for Image Restoration (MCIR), rejuvenates latency models
as negative models, making it compatible with diverse image restoration tasks.
We propose the Self-Prior guided Negative loss (SPN) to enable it. This
approach significantly enhances existing models when retrained with the
proposed model contrastive paradigm. The results show significant improvements
in image restoration across various tasks and architectures. For example,
models retrained with SPN outperform the original FFANet and DehazeFormer by
3.41 dB and 0.57 dB on the RESIDE indoor dataset for image dehazing. Similarly,
they achieve notable improvements of 0.47 dB on SPA-Data over IDT for image
deraining and 0.12 dB on Manga109 for a 4x scale super-resolution over
lightweight SwinIR, respectively. Code and retrained models are available at
https://github.com/Aitical/MCIR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera Ready Version. Accepted to The 38th Annual AAAI Conference on
  Artificial Intelligence (AAAI 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploiting Spatial-Temporal Context for Interacting Hand Reconstruction
  on Monocular RGB Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.04074v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.04074v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weichao Zhao, Hezhen Hu, Wengang Zhou, Li li, Houqiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing interacting hands from monocular RGB data is a challenging
task, as it involves many interfering factors, e.g. self- and mutual occlusion
and similar textures. Previous works only leverage information from a single
RGB image without modeling their physically plausible relation, which leads to
inferior reconstruction results. In this work, we are dedicated to explicitly
exploiting spatial-temporal information to achieve better interacting hand
reconstruction. On one hand, we leverage temporal context to complement
insufficient information provided by the single frame, and design a novel
temporal framework with a temporal constraint for interacting hand motion
smoothness. On the other hand, we further propose an interpenetration detection
module to produce kinetically plausible interacting hands without physical
collisions. Extensive experiments are performed to validate the effectiveness
of our proposed framework, which achieves new state-of-the-art performance on
public benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by TOMM 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Applications of Large Scale Foundation Models for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12144v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12144v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Huang, Yue Chen, Zhu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since DARPA Grand Challenges (rural) in 2004/05 and Urban Challenges in 2007,
autonomous driving has been the most active field of AI applications. Recently
powered by large language models (LLMs), chat systems, such as chatGPT and
PaLM, emerge and rapidly become a promising direction to achieve artificial
general intelligence (AGI) in natural language processing (NLP). There comes a
natural thinking that we could employ these abilities to reformulate autonomous
driving. By combining LLM with foundation models, it is possible to utilize the
human knowledge, commonsense and reasoning to rebuild autonomous driving
systems from the current long-tailed AI dilemma. In this paper, we investigate
the techniques of foundation models and LLMs applied for autonomous driving,
categorized as simulation, world model, data annotation and planning or E2E
solutions etc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages. A survey paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GeoLocator: a location-integrated large multimodal model for inferring
  geo-privacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13018v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13018v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Yang, Siqin Wang, Daoyang Li, Yixian Zhang, Shuju Sun, Junzhou He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geographic privacy or geo-privacy refers to the keeping private of one's
geographic location, especially the restriction of geographical data maintained
by personal electronic devices. Geo-privacy is a crucial aspect of personal
security; however, it often goes unnoticed in daily activities. With the surge
in the use of Large Multimodal Models (LMMs), such as GPT-4, for Open Source
Intelligence (OSINT), the potential risks associated with geo-privacy breaches
have intensified. This study develops a location-integrated GPT-4 based model
named GeoLocator and designs four-dimensional experiments to demonstrate its
capability in inferring the locational information of input imageries and/or
social media contents. Our experiments reveal that GeoLocator generates
specific geographic details with high accuracy and consequently embeds the risk
of the model users exposing geospatial information to the public
unintentionally, highlighting the thread of online data sharing, information
gathering technologies and LLMs on geo-privacy. We conclude with the broader
implications of GeoLocator and our findings for individuals and the community
at large, by emphasizing the urgency for enhanced awareness and protective
measures against geo-privacy leakage in the era of advanced AI and widespread
social media usage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large-scale data extraction from the UNOS organ donor documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15752v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15752v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marek Rychlik, Bekir Tanriover, Yan Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we focus on three major task: 1) discussing our methods: Our
method captures a portion of the data in DCD flowsheets, kidney perfusion data,
and Flowsheet data captured peri-organ recovery surgery. 2) demonstrating the
result: We built a comprehensive, analyzable database from 2022 OPTN data. This
dataset is by far larger than any previously available even in this preliminary
phase; and 3) proving that our methods can be extended to all the past OPTN
data and future data.
  The scope of our study is all Organ Procurement and Transplantation Network
(OPTN) data of the USA organ donors since 2008. The data was not analyzable in
a large scale in the past because it was captured in PDF documents known as
``Attachments'', whereby every donor's information was recorded into dozens of
PDF documents in heterogeneous formats. To make the data analyzable, one needs
to convert the content inside these PDFs to an analyzable data format, such as
a standard SQL database. In this paper we will focus on 2022 OPTN data, which
consists of $\approx 400,000$ PDF documents spanning millions of pages. The
entire OPTN data covers 15 years (2008--20022). This paper assumes that readers
are familiar with the content of the OPTN data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Single-subject Multi-contrast MRI Super-resolution via Implicit Neural
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.15065v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.15065v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian McGinnis, Suprosanna Shit, Hongwei Bran Li, Vasiliki Sideri-Lampretsa, Robert Graf, Maik Dannecker, Jiazhen Pan, Nil Stolt Ansó, Mark Mühlau, Jan S. Kirschke, Daniel Rueckert, Benedikt Wiestler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinical routine and retrospective cohorts commonly include multi-parametric
Magnetic Resonance Imaging; however, they are mostly acquired in different
anisotropic 2D views due to signal-to-noise-ratio and scan-time constraints.
Thus acquired views suffer from poor out-of-plane resolution and affect
downstream volumetric image analysis that typically requires isotropic 3D
scans. Combining different views of multi-contrast scans into high-resolution
isotropic 3D scans is challenging due to the lack of a large training cohort,
which calls for a subject-specific framework. This work proposes a novel
solution to this problem leveraging Implicit Neural Representations (INR). Our
proposed INR jointly learns two different contrasts of complementary views in a
continuous spatial function and benefits from exchanging anatomical information
between them. Trained within minutes on a single commodity GPU, our model
provides realistic super-resolution across different pairs of contrasts in our
experiments with three datasets. Using Mutual Information (MI) as a metric, we
find that our model converges to an optimum MI amongst sequences, achieving
anatomically faithful reconstruction. Code is available at:
https://github.com/jqmcginnis/multi_contrast_inr/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SCoRD: Subject-Conditional Relation Detection with Text-Augmented Data <span class="chip">WACV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12910v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12910v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyan Yang, Kushal Kafle, Zhe Lin, Scott Cohen, Zhihong Ding, Vicente Ordonez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Subject-Conditional Relation Detection SCoRD, where conditioned on
an input subject, the goal is to predict all its relations to other objects in
a scene along with their locations. Based on the Open Images dataset, we
propose a challenging OIv6-SCoRD benchmark such that the training and testing
splits have a distribution shift in terms of the occurrence statistics of
$\langle$subject, relation, object$\rangle$ triplets. To solve this problem, we
propose an auto-regressive model that given a subject, it predicts its
relations, objects, and object locations by casting this output as a sequence
of tokens. First, we show that previous scene-graph prediction methods fail to
produce as exhaustive an enumeration of relation-object pairs when conditioned
on a subject on this benchmark. Particularly, we obtain a recall@3 of 83.8% for
our relation-object predictions compared to the 49.75% obtained by a recent
scene graph detector. Then, we show improved generalization on both
relation-object and object-box predictions by leveraging during training
relation-object pairs obtained automatically from textual captions and for
which no object-box annotations are available. Particularly, for
$\langle$subject, relation, object$\rangle$ triplets for which no object
locations are available during training, we are able to obtain a recall@3 of
33.80% for relation-object pairs and 26.75% for their box locations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Plug-in Diffusion Model for Sequential Recommendation <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02913v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02913v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haokai Ma, Ruobing Xie, Lei Meng, Xin Chen, Xu Zhang, Leyu Lin, Zhanhui Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pioneering efforts have verified the effectiveness of the diffusion models in
exploring the informative uncertainty for recommendation. Considering the
difference between recommendation and image synthesis tasks, existing methods
have undertaken tailored refinements to the diffusion and reverse process.
However, these approaches typically use the highest-score item in corpus for
user interest prediction, leading to the ignorance of the user's generalized
preference contained within other items, thereby remaining constrained by the
data sparsity issue. To address this issue, this paper presents a novel Plug-in
Diffusion Model for Recommendation (PDRec) framework, which employs the
diffusion model as a flexible plugin to jointly take full advantage of the
diffusion-generating user preferences on all items. Specifically, PDRec first
infers the users' dynamic preferences on all items via a time-interval
diffusion model and proposes a Historical Behavior Reweighting (HBR) mechanism
to identify the high-quality behaviors and suppress noisy behaviors. In
addition to the observed items, PDRec proposes a Diffusion-based Positive
Augmentation (DPA) strategy to leverage the top-ranked unobserved items as the
potential positive samples, bringing in informative and diverse soft signals to
alleviate data sparsity. To alleviate the false negative sampling issue, PDRec
employs Noise-free Negative Sampling (NNS) to select stable negative samples
for ensuring effective model optimization. Extensive experiments and analyses
on four datasets have verified the superiority of the proposed PDRec over the
state-of-the-art baselines and showcased the universality of PDRec as a
flexible plugin for commonly-used sequential encoders in different
recommendation scenarios. The code is available in
https://github.com/hulkima/PDRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Let's Get It Started: Fostering the Discoverability of New Releases on
  Deezer <span class="chip">ECIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Léa Briand, Théo Bontempelli, Walid Bendada, Mathieu Morlon, François Rigaud, Benjamin Chapus, Thomas Bouabça, Guillaume Salha-Galvan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents our recent initiatives to foster the discoverability of
new releases on the music streaming service Deezer. After introducing our
search and recommendation features dedicated to new releases, we outline our
shift from editorial to personalized release suggestions using cold start
embeddings and contextual bandits. Backed by online experiments, we discuss the
advantages of this shift in terms of recommendation quality and exposure of new
releases on the service.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation as an "Industry Talk" at the 46th European
  Conference on Information Retrieval (ECIR 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DocGraphLM: Documental Graph Language Model for Information Extraction <span class="chip">SIGIR'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongsheng Wang, Zhiqiang Ma, Armineh Nourbakhsh, Kang Gu, Sameena Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in Visually Rich Document Understanding (VrDU) have enabled
information extraction and question answering over documents with complex
layouts. Two tropes of architectures have emerged -- transformer-based models
inspired by LLMs, and Graph Neural Networks. In this paper, we introduce
DocGraphLM, a novel framework that combines pre-trained language models with
graph semantics. To achieve this, we propose 1) a joint encoder architecture to
represent documents, and 2) a novel link prediction approach to reconstruct
document graphs. DocGraphLM predicts both directions and distances between
nodes using a convergent joint loss function that prioritizes neighborhood
restoration and downweighs distant node detection. Our experiments on three
SotA datasets show consistent improvement on IE and QA tasks with the adoption
of graph features. Moreover, we report that adopting the graph features
accelerates convergence in the learning process during training, despite being
solely constructed through link prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at SIGIR'23 (repost for easier access)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">92</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepSeek LLM: Scaling Open-Source Language Models with Longtermism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02954v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02954v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         DeepSeek-AI,  :, Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y. K. Li, Wenfeng Liang, Fangyun Lin, A. X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R. X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, Yuheng Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of open-source large language models (LLMs) has been
truly remarkable. However, the scaling law described in previous literature
presents varying conclusions, which casts a dark cloud over scaling LLMs. We
delve into the study of scaling laws and present our distinctive findings that
facilitate scaling of large scale models in two commonly used open-source
configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek
LLM, a project dedicated to advancing open-source language models with a
long-term perspective. To support the pre-training phase, we have developed a
dataset that currently consists of 2 trillion tokens and is continuously
expanding. We further conduct supervised fine-tuning (SFT) and Direct
Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the
creation of DeepSeek Chat models. Our evaluation results demonstrate that
DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in
the domains of code, mathematics, and reasoning. Furthermore, open-ended
evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance
compared to GPT-3.5.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Tactician's Web of Large-Scale Formal Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02950v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02950v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lasse Blaauwbroek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Tactician's Web is a platform offering a large web of strongly
interconnected, machine-checked, formal mathematical knowledge conveniently
packaged for machine learning, analytics, and proof engineering. Built on top
of the Coq proof assistant, the platform exports a dataset containing a wide
variety of formal theories, presented as a web of definitions, theorems, proof
terms, tactics, and proof states. Theories are encoded both as a semantic graph
(rendered below) and as human-readable text, each with a unique set of
advantages and disadvantages. Proving agents may interact with Coq through the
same rich data representation and can be automatically benchmarked on a set of
theorems. Tight integration with Coq provides the unique possibility to make
agents available to proof engineers as practical tools.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>47 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph2Tac: Learning Hierarchical Representations of Math Concepts in
  Theorem proving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02949v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02949v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jason Rute, Miroslav Olšák, Lasse Blaauwbroek, Fidel Ivan Schaposnik Massolo, Jelle Piepenbrock, Vasily Pestun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concepts abound in mathematics and its applications. They vary greatly
between subject areas, and new ones are introduced in each mathematical paper
or application. A formal theory builds a hierarchy of definitions, theorems and
proofs that reference each other. When an AI agent is proving a new theorem,
most of the mathematical concepts and lemmas relevant to that theorem may have
never been seen during training. This is especially true in the Coq proof
assistant, which has a diverse library of Coq projects, each with its own
definitions, lemmas, and even custom tactic procedures used to prove those
lemmas. It is essential for agents to incorporate such new information into
their knowledge base on the fly. We work towards this goal by utilizing a new,
large-scale, graph-based dataset for machine learning in Coq. We leverage a
faithful graph-representation of Coq terms that induces a directed graph of
dependencies between definitions to create a novel graph neural network,
Graph2Tac (G2T), that takes into account not only the current goal, but also
the entire hierarchy of definitions that led to the current goal. G2T is an
online model that is deeply integrated into the users' workflow and can adapt
in real time to new Coq projects and their definitions. It complements well
with other online models that learn in real time from new proof scripts. Our
novel definition embedding task, which is trained to compute representations of
mathematical concepts not seen during training, boosts the performance of the
neural network to rival state-of-the-art k-nearest neighbor predictors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Digital-analog quantum learning on Rydberg atom arrays 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02940v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02940v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Z. Lu, Lucy Jiao, Kristina Wolinski, Milan Kornjača, Hong-Ye Hu, Sergio Cantu, Fangli Liu, Susanne F. Yelin, Sheng-Tao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose hybrid digital-analog learning algorithms on Rydberg atom arrays,
combining the potentially practical utility and near-term realizability of
quantum learning with the rapidly scaling architectures of neutral atoms. Our
construction requires only single-qubit operations in the digital setting and
global driving according to the Rydberg Hamiltonian in the analog setting. We
perform a comprehensive numerical study of our algorithm on both classical and
quantum data, given respectively by handwritten digit classification and
unsupervised quantum phase boundary learning. We show in the two representative
problems that digital-analog learning is not only feasible in the near term,
but also requires shorter circuit depths and is more robust to realistic error
models as compared to digital learning schemes. Our results suggest that
digital-analog learning opens a promising path towards improved variational
quantum learning experiments in the near term.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 20 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dagma-DCE: Interpretable, Non-Parametric Differentiable Causal Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02930v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02930v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Waxman, Kurt Butler, Petar M. Djuric
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Dagma-DCE, an interpretable and model-agnostic scheme for
differentiable causal discovery. Current non- or over-parametric methods in
differentiable causal discovery use opaque proxies of ``independence'' to
justify the inclusion or exclusion of a causal relationship. We show
theoretically and empirically that these proxies may be arbitrarily different
than the actual causal strength. Juxtaposed to existing differentiable causal
discovery algorithms, \textsc{Dagma-DCE} uses an interpretable measure of
causal strength to define weighted adjacency matrices. In a number of simulated
datasets, we show our method achieves state-of-the-art level performance. We
additionally show that \textsc{Dagma-DCE} allows for principled thresholding
and sparsity penalties by domain-experts. The code for our method is available
open-source at https://github.com/DanWaxman/DAGMA-DCE, and can easily be
adapted to arbitrary differentiable models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures. Accepted to the IEEE Open Journal of Signal
  Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A unified uncertainty-aware exploration: Combining epistemic and
  aleatory uncertainty <span class="chip">ICASSP2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02914v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02914v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parvin Malekzadeh, Ming Hou, Konstantinos N. Plataniotis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploration is a significant challenge in practical reinforcement learning
(RL), and uncertainty-aware exploration that incorporates the quantification of
epistemic and aleatory uncertainty has been recognized as an effective
exploration strategy. However, capturing the combined effect of aleatory and
epistemic uncertainty for decision-making is difficult. Existing works estimate
aleatory and epistemic uncertainty separately and consider the composite
uncertainty as an additive combination of the two. Nevertheless, the additive
formulation leads to excessive risk-taking behavior, causing instability. In
this paper, we propose an algorithm that clarifies the theoretical connection
between aleatory and epistemic uncertainty, unifies aleatory and epistemic
uncertainty estimation, and quantifies the combined effect of both
uncertainties for a risk-sensitive exploration. Our method builds on a novel
extension of distributional RL that estimates a parameterized return
distribution whose parameters are random variables encoding epistemic
uncertainty. Experimental results on tasks with exploration and risk challenges
show that our method outperforms alternative approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ H2G2-Net: A Hierarchical Heterogeneous Graph Generative Network
  Framework for Discovery of Multi-Modal Physiological Responses <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haidong Gu, Nathan Gaw, Yinan Wang, Chancellor Johnstone, Christine Beauchene, Sophia Yuditskaya, Hrishikesh Rao, Chun-An Chou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discovering human cognitive and emotional states using multi-modal
physiological signals draws attention across various research applications.
Physiological responses of the human body are influenced by human cognition and
commonly used to analyze cognitive states. From a network science perspective,
the interactions of these heterogeneous physiological modalities in a graph
structure may provide insightful information to support prediction of cognitive
states. However, there is no clue to derive exact connectivity between
heterogeneous modalities and there exists a hierarchical structure of
sub-modalities. Existing graph neural networks are designed to learn on
non-hierarchical homogeneous graphs with pre-defined graph structures; they
failed to learn from hierarchical, multi-modal physiological data without a
pre-defined graph structure. To this end, we propose a hierarchical
heterogeneous graph generative network (H2G2-Net) that automatically learns a
graph structure without domain knowledge, as well as a powerful representation
on the hierarchical heterogeneous graph in an end-to-end fashion. We validate
the proposed method on the CogPilot dataset that consists of multi-modal
physiological signals. Extensive experiments demonstrate that our proposed
method outperforms the state-of-the-art GNNs by 5%-20% in prediction accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted in Human-Centric Representation Learning workshop at
  AAAI 2024 (https://hcrl-workshop.github.io/2024/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Class-wise Generalization Error: an Information-Theoretic Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02904v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02904v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Firas Laakom, Yuheng Bu, Moncef Gabbouj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing generalization theories of supervised learning typically take a
holistic approach and provide bounds for the expected generalization over the
whole data distribution, which implicitly assumes that the model generalizes
similarly for all the classes. In practice, however, there are significant
variations in generalization performance among different classes, which cannot
be captured by the existing generalization bounds. In this work, we tackle this
problem by theoretically studying the class-generalization error, which
quantifies the generalization performance of each individual class. We derive a
novel information-theoretic bound for class-generalization error using the KL
divergence, and we further obtain several tighter bounds using the conditional
mutual information (CMI), which are significantly easier to estimate in
practice. We empirically validate our proposed bounds in different neural
networks and show that they accurately capture the complex class-generalization
error behavior. Moreover, we show that the theoretical tools developed in this
paper can be applied in several applications beyond this context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Reinforcement Learning for Local Path Following of an Autonomous
  Formula SAE Vehicle 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02903v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02903v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harvey Merton, Thomas Delamore, Karl Stol, Henry Williams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the continued introduction of driverless events to Formula:Society of
Automotive Engineers (F:SAE) competitions around the world, teams are
investigating all aspects of the autonomous vehicle stack. This paper presents
the use of Deep Reinforcement Learning (DRL) and Inverse Reinforcement Learning
(IRL) to map locally-observed cone positions to a desired steering angle for
race track following. Two state-of-the-art algorithms not previously tested in
this context: soft actor critic (SAC) and adversarial inverse reinforcement
learning (AIRL), are used to train models in a representative simulation. Three
novel reward functions for use by RL algorithms in an autonomous racing context
are also discussed. Tests performed in simulation and the real world suggest
that both algorithms can successfully train models for local path following.
Suggestions for future work are presented to allow these models to scale to a
full F:SAE vehicle.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>As presented at the Australasian Conference on Robotics and
  Automation (ACRA 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ State Derivative Normalization for Continuous-Time Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Weigand, Gerben I. Beintema, Jonas Ulmen, Daniel Görges, Roland Tóth, Maarten Schoukens, Martin Ruskowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The importance of proper data normalization for deep neural networks is well
known. However, in continuous-time state-space model estimation, it has been
observed that improper normalization of either the hidden state or hidden state
derivative of the model estimate, or even of the time interval can lead to
numerical and optimization challenges with deep learning based methods. This
results in a reduced model quality. In this contribution, we show that these
three normalization tasks are inherently coupled. Due to the existence of this
coupling, we propose a solution to all three normalization challenges by
introducing a normalization constant at the state derivative level. We show
that the appropriate choice of the normalization constant is related to the
dynamics of the to-be-identified system and we derive multiple methods of
obtaining an effective normalization constant. We compare and discuss all the
normalization strategies on a benchmark problem based on experimental data from
a cascaded tanks system and compare our results with other methods of the
identification literature.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the 20th IFAC Symposium on System
  Identification (SYSID2024) for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nonlinear functional regression by functional deep neural network with
  kernel embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02890v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02890v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongjie Shi, Jun Fan, Linhao Song, Ding-Xuan Zhou, Johan A. K. Suykens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of deep learning in various fields of science and
technology, such as speech recognition, image classification, and natural
language processing, recently it is also widely applied in the functional data
analysis (FDA) with some empirical success. However, due to the infinite
dimensional input, we need a powerful dimension reduction method for functional
learning tasks, especially for the nonlinear functional regression. In this
paper, based on the idea of smooth kernel integral transformation, we propose a
functional deep neural network with an efficient and fully data-dependent
dimension reduction method. The architecture of our functional net consists of
a kernel embedding step: an integral transformation with a data-dependent
smooth kernel; a projection step: a dimension reduction by projection with
eigenfunction basis based on the embedding kernel; and finally an expressive
deep ReLU neural network for the prediction. The utilization of smooth kernel
embedding enables our functional net to be discretization invariant, efficient,
and robust to noisy observations, capable of utilizing information in both
input functions and responses data, and have a low requirement on the number of
discrete points for an unimpaired generalization performance. We conduct
theoretical analysis including approximation error and generalization error
analysis, and numerical simulations to verify these advantages of our
functional net.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Energy-Preserving Reduced Operator Inference for Efficient Design and
  Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02889v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02889v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomoki Koike, Elizabeth Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many-query computations, in which a computational model for an engineering
system must be evaluated many times, are crucial in design and control. For
systems governed by partial differential equations (PDEs), typical
high-fidelity numerical models are high-dimensional and too computationally
expensive for the many-query setting. Thus, efficient surrogate models are
required to enable low-cost computations in design and control. This work
presents a physics-preserving reduced model learning approach that targets PDEs
whose quadratic operators preserve energy, such as those arising in governing
equations in many fluids problems. The approach is based on the Operator
Inference method, which fits reduced model operators to state snapshot and time
derivative data in a least-squares sense. However, Operator Inference does not
generally learn a reduced quadratic operator with the energy-preserving
property of the original PDE. Thus, we propose a new energy-preserving Operator
Inference (EP-OpInf) approach, which imposes this structure on the learned
reduced model via constrained optimization. Numerical results using the viscous
Burgers' and Kuramoto-Sivashinksy equation (KSE) demonstrate that EP-OpInf
learns efficient and accurate reduced models that retain this energy-preserving
structure.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, AIAA SciTech Forum 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Parameter Optimisation for Quantum Kernel Alignment: A
  Sub-sampling Approach in Variational Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02879v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02879v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. Emre Sahin, Benjamin C. B. Symons, Pushpak Pati, Fayyaz Minhas, Declan Millar, Maria Gabrani, Jan Lukas Robertus, Stefano Mensa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum machine learning with quantum kernels for classification problems is
a growing area of research. Recently, quantum kernel alignment techniques that
parameterise the kernel have been developed, allowing the kernel to be trained
and therefore aligned with a specific dataset. While quantum kernel alignment
is a promising technique, it has been hampered by considerable training costs
because the full kernel matrix must be constructed at every training iteration.
Addressing this challenge, we introduce a novel method that seeks to balance
efficiency and performance. We present a sub-sampling training approach that
uses a subset of the kernel matrix at each training step, thereby reducing the
overall computational cost of the training. In this work, we apply the
sub-sampling method to synthetic datasets and a real-world breast cancer
dataset and demonstrate considerable reductions in the number of circuits
required to train the quantum kernel while maintaining classification accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Framework for Variable-lag Motif Following Relation Inference In Time
  Series using Matrix Profile analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naaek Chinpattanakarn, Chainarong Amornbunchornvej
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowing who follows whom and what patterns they are following are crucial
steps to understand collective behaviors (e.g. a group of human, a school of
fish, or a stock market). Time series is one of resources that can be used to
get insight regarding following relations. However, the concept of following
patterns or motifs and the solution to find them in time series are not
obvious. In this work, we formalize a concept of following motifs between two
time series and present a framework to infer following patterns between two
time series. The framework utilizes one of efficient and scalable methods to
retrieve motifs from time series called the Matrix Profile Method. We compare
our proposed framework with several baselines. The framework performs better
than baselines in the simulation datasets. In the dataset of sound recording,
the framework is able to retrieve the following motifs within a pair of time
series that two singers sing following each other. In the cryptocurrency
dataset, the framework is capable of capturing the following motifs within a
pair of time series from two digital currencies, which implies that the values
of one currency follow the values of another currency patterns. Our framework
can be utilized in any field of time series to get insight regarding following
patterns between time series.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Non-Stationary Textures using Self-Rectification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02847v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02847v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Zhou, Rongjun Xiao, Dani Lischinski, Daniel Cohen-Or, Hui Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the challenge of example-based non-stationary texture
synthesis. We introduce a novel twostep approach wherein users first modify a
reference texture using standard image editing tools, yielding an initial rough
target for the synthesis. Subsequently, our proposed method, termed
"self-rectification", automatically refines this target into a coherent,
seamless texture, while faithfully preserving the distinct visual
characteristics of the reference exemplar. Our method leverages a pre-trained
diffusion network, and uses self-attention mechanisms, to gradually align the
synthesized texture with the reference, ensuring the retention of the
structures in the provided target. Through experimental validation, our
approach exhibits exceptional proficiency in handling non-stationary textures,
demonstrating significant advancements in texture synthesis when compared to
existing state-of-the-art techniques. Code is available at
https://github.com/xiaorongjun000/Self-Rectification
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/xiaorongjun000/Self-Rectification</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Thousands of AI Authors on the Future of AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02843v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02843v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katja Grace, Harlan Stewart, Julia Fabienne Sandkühler, Stephen Thomas, Ben Weinstein-Raun, Jan Brauner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the largest survey of its kind, 2,778 researchers who had published in
top-tier artificial intelligence (AI) venues gave predictions on the pace of AI
progress and the nature and impacts of advanced AI systems The aggregate
forecasts give at least a 50% chance of AI systems achieving several milestones
by 2028, including autonomously constructing a payment processing site from
scratch, creating a song indistinguishable from a new song by a popular
musician, and autonomously downloading and fine-tuning a large language model.
If science continues undisrupted, the chance of unaided machines outperforming
humans in every possible task was estimated at 10% by 2027, and 50% by 2047.
The latter estimate is 13 years earlier than that reached in a similar survey
we conducted only one year earlier [Grace et al., 2022]. However, the chance of
all human occupations becoming fully automatable was forecast to reach 10% by
2037, and 50% as late as 2116 (compared to 2164 in the 2022 survey).
  Most respondents expressed substantial uncertainty about the long-term value
of AI progress: While 68.3% thought good outcomes from superhuman AI are more
likely than bad, of these net optimists 48% gave at least a 5% chance of
extremely bad outcomes such as human extinction, and 59% of net pessimists gave
5% or more to extremely good outcomes. Between 38% and 51% of respondents gave
at least a 10% chance to advanced AI leading to outcomes as bad as human
extinction. More than half suggested that "substantial" or "extreme" concern is
warranted about six different AI-related scenarios, including misinformation,
authoritarian control, and inequality. There was disagreement about whether
faster or slower AI progress would be better for the future of humanity.
However, there was broad agreement that research aimed at minimizing potential
risks from AI systems ought to be prioritized more.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The asterisk indicates the corresponding author. The dagger indicates
  equal contribution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Let's Get It Started: Fostering the Discoverability of New Releases on
  Deezer <span class="chip">ECIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Léa Briand, Théo Bontempelli, Walid Bendada, Mathieu Morlon, François Rigaud, Benjamin Chapus, Thomas Bouabça, Guillaume Salha-Galvan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents our recent initiatives to foster the discoverability of
new releases on the music streaming service Deezer. After introducing our
search and recommendation features dedicated to new releases, we outline our
shift from editorial to personalized release suggestions using cold start
embeddings and contextual bandits. Backed by online experiments, we discuss the
advantages of this shift in terms of recommendation quality and exposure of new
releases on the service.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation as an "Industry Talk" at the 46th European
  Conference on Information Retrieval (ECIR 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-Informed Neural Networks for High-Frequency and Multi-Scale
  Problems using Transfer Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdul Hannan Mustajab, Hao Lyu, Zarghaam Rizvi, Frank Wuttke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics-informed neural network (PINN) is a data-driven solver for partial
and ordinary differential equations(ODEs/PDEs). It provides a unified framework
to address both forward and inverse problems. However, the complexity of the
objective function often leads to training failures. This issue is particularly
prominent when solving high-frequency and multi-scale problems. We proposed
using transfer learning to boost the robustness and convergence of training
PINN, starting training from low-frequency problems and gradually approaching
high-frequency problems. Through two case studies, we discovered that transfer
learning can effectively train PINN to approximate solutions from low-frequency
problems to high-frequency problems without increasing network parameters.
Furthermore, it requires fewer data points and less training time. We
elaborately described our training strategy, including optimizer selection, and
suggested guidelines for using transfer learning to train neural networks for
solving more complex problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Credence: Augmenting Datacenter Switch Buffer Sharing with ML
  Predictions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02801v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02801v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vamsi Addanki, Maciej Pacut, Stefan Schmid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Packet buffers in datacenter switches are shared across all the switch ports
in order to improve the overall throughput. The trend of shrinking buffer sizes
in datacenter switches makes buffer sharing extremely challenging and a
critical performance issue. Literature suggests that push-out buffer sharing
algorithms have significantly better performance guarantees compared to
drop-tail algorithms. Unfortunately, switches are unable to benefit from these
algorithms due to lack of support for push-out operations in hardware. Our key
observation is that drop-tail buffers can emulate push-out buffers if the
future packet arrivals are known ahead of time. This suggests that augmenting
drop-tail algorithms with predictions about the future arrivals has the
potential to significantly improve performance.
  This paper is the first research attempt in this direction. We propose
Credence, a drop-tail buffer sharing algorithm augmented with machine-learned
predictions. Credence can unlock the performance only attainable by push-out
algorithms so far. Its performance hinges on the accuracy of predictions.
Specifically, Credence achieves near-optimal performance of the best known
push-out algorithm LQD (Longest Queue Drop) with perfect predictions, but
gracefully degrades to the performance of the simplest drop-tail algorithm
Complete Sharing when the prediction error gets arbitrarily worse. Our
evaluations show that Credence improves throughput by $1.5$x compared to
traditional approaches. In terms of flow completion times, we show that
Credence improves upon the state-of-the-art approaches by up to $95\%$ using
off-the-shelf machine learning techniques that are also practical in today's
hardware. We believe this work opens several interesting future work
opportunities both in systems and theory that we discuss at the end of this
paper.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weakly Semi-supervised Tool Detection in Minimally Invasive Surgery
  Videos <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryo Fujii, Ryo Hachiuma, Hideo Saito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surgical tool detection is essential for analyzing and evaluating minimally
invasive surgery videos. Current approaches are mostly based on supervised
methods that require large, fully instance-level labels (i.e., bounding boxes).
However, large image datasets with instance-level labels are often limited
because of the burden of annotation. Thus, surgical tool detection is important
when providing image-level labels instead of instance-level labels since
image-level annotations are considerably more time-efficient than
instance-level annotations. In this work, we propose to strike a balance
between the extremely costly annotation burden and detection performance. We
further propose a co-occurrence loss, which considers a characteristic that
some tool pairs often co-occur together in an image to leverage image-level
labels. Encapsulating the knowledge of co-occurrence using the co-occurrence
loss helps to overcome the difficulty in classification that originates from
the fact that some tools have similar shapes and textures. Extensive
experiments conducted on the Endovis2018 dataset in various data settings show
the effectiveness of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tackling Electrode Shift In Gesture Recognition with HD-EMG Electrode
  Subsets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02773v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02773v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joao Pereira, Dimitrios Chalatsis, Balint Hodossy, Dario Farina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  sEMG pattern recognition algorithms have been explored extensively in
decoding movement intent, yet are known to be vulnerable to changing recording
conditions, exhibiting significant drops in performance across subjects, and
even across sessions. Multi-channel surface EMG, also referred to as
high-density sEMG (HD-sEMG) systems, have been used to improve performance with
the information collected through the use of additional electrodes. However, a
lack of robustness is ever present due to limited datasets and the difficulties
in addressing sources of variability, such as electrode placement. In this
study, we propose training on a collection of input channel subsets and
augmenting our training distribution with data from different electrode
locations, simultaneously targeting electrode shift and reducing input
dimensionality. Our method increases robustness against electrode shift and
results in significantly higher intersession performance across subjects and
classification algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Powerformer: A Section-adaptive <span class="highlight-title">Transformer</span> for Power Flow Adjustment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaixuan Chen, Wei Luo, Shunyu Liu, Yaoquan Wei, Yihe Zhou, Yunpeng Qing, Quan Zhang, Jie Song, Mingli Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a novel transformer architecture tailored for
learning robust power system state representations, which strives to optimize
power dispatch for the power flow adjustment across different transmission
sections. Specifically, our proposed approach, named Powerformer, develops a
dedicated section-adaptive attention mechanism, separating itself from the
self-attention used in conventional transformers. This mechanism effectively
integrates power system states with transmission section information, which
facilitates the development of robust state representations. Furthermore, by
considering the graph topology of power system and the electrical attributes of
bus nodes, we introduce two customized strategies to further enhance the
expressiveness: graph neural network propagation and multi-factor attention
mechanism. Extensive evaluations are conducted on three power system scenarios,
including the IEEE 118-bus system, a realistic 300-bus system in China, and a
large-scale European system with 9241 buses, where Powerformer demonstrates its
superior performance over several baseline methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fairness-Aware Job Scheduling for Multi-Job Federated Learning <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Shi, Han Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) enables multiple data owners (a.k.a. FL clients) to
collaboratively train machine learning models without disclosing sensitive
private data. Existing FL research mostly focuses on the monopoly scenario in
which a single FL server selects a subset of FL clients to update their local
models in each round of training. In practice, there can be multiple FL servers
simultaneously trying to select clients from the same pool. In this paper, we
propose a first-of-its-kind Fairness-aware Federated Job Scheduling (FairFedJS)
approach to bridge this gap. Based on Lyapunov optimization, it ensures fair
allocation of high-demand FL client datasets to FL jobs in need of them, by
jointly considering the current demand and the job payment bids, in order to
prevent prolonged waiting. Extensive experiments comparing FairFedJS against
four state-of-the-art approaches on two datasets demonstrate its significant
advantages. It outperforms the best baseline by 31.9% and 1.0% on average in
terms of scheduling fairness and convergence time, respectively, while
achieving comparable test accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion Variational Inference: Diffusion Models as Expressive
  Variational Posteriors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Top Piriyakulkij, Yingheng Wang, Volodymyr Kuleshov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose denoising diffusion variational inference (DDVI), an approximate
inference algorithm for latent variable models which relies on diffusion models
as expressive variational posteriors. Our method augments variational
posteriors with auxiliary latents, which yields an expressive class of models
that perform diffusion in latent space by reversing a user-specified noising
process. We fit these models by optimizing a novel lower bound on the marginal
likelihood inspired by the wake-sleep algorithm. Our method is easy to
implement (it fits a regularized extension of the ELBO), is compatible with
black-box variational inference, and outperforms alternative classes of
approximate posteriors based on normalizing flows or adversarial networks. When
applied to deep latent variable models, our method yields the denoising
diffusion VAE (DD-VAE) algorithm. We use this algorithm on a motivating task in
biology -- inferring latent ancestry from human genomes -- outperforming strong
baselines on the Thousand Genomes dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the numerical reliability of nonsmooth autodiff: a MaxPool case study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Boustany
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper considers the reliability of automatic differentiation (AD) for
neural networks involving the nonsmooth MaxPool operation. We investigate the
behavior of AD across different precision levels (16, 32, 64 bits) and
convolutional architectures (LeNet, VGG, and ResNet) on various datasets
(MNIST, CIFAR10, SVHN, and ImageNet). Although AD can be incorrect, recent
research has shown that it coincides with the derivative almost everywhere,
even in the presence of nonsmooth operations (such as MaxPool and ReLU). On the
other hand, in practice, AD operates with floating-point numbers (not real
numbers), and there is, therefore, a need to explore subsets on which AD can be
numerically incorrect. These subsets include a bifurcation zone (where AD is
incorrect over reals) and a compensation zone (where AD is incorrect over
floating-point numbers but correct over reals). Using SGD for the training
process, we study the impact of different choices of the nonsmooth Jacobian for
the MaxPool function on the precision of 16 and 32 bits. These findings suggest
that nonsmooth MaxPool Jacobians with lower norms help maintain stable and
efficient test accuracy, whereas those with higher norms can result in
instability and decreased performance. We also observe that the influence of
MaxPool's nonsmooth Jacobians on learning can be reduced by using batch
normalization, Adam-like optimizers, or increasing the precision level.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shared active subspace for multivariate vector-valued functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khadija Musayeva, Mickael Binois
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes several approaches as baselines to compute a shared
active subspace for multivariate vector-valued functions. The goal is to
minimize the deviation between the function evaluations on the original space
and those on the reconstructed one. This is done either by manipulating the
gradients or the symmetric positive (semi-)definite (SPD) matrices computed
from the gradients of each component function so as to get a single structure
common to all component functions. These approaches can be applied to any data
irrespective of the underlying distribution unlike the existing vector-valued
approach that is constrained to a normal distribution. We test the
effectiveness of these methods on five optimization problems. The experiments
show that, in general, the SPD-level methods are superior to the gradient-level
ones, and are close to the vector-valued approach in the case of a normal
distribution. Interestingly, in most cases it suffices to take the sum of the
SPD matrices to identify the best shared active subspace.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedNS: A Fast Sketching Newton-Type Algorithm for Federated Learning <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02734v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02734v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Li, Yong Liu, Wei Wang, Haoran Wu, Weiping Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent Newton-type federated learning algorithms have demonstrated linear
convergence with respect to the communication rounds. However, communicating
Hessian matrices is often unfeasible due to their quadratic communication
complexity. In this paper, we introduce a novel approach to tackle this issue
while still achieving fast convergence rates. Our proposed method, named as
Federated Newton Sketch methods (FedNS), approximates the centralized Newton's
method by communicating the sketched square-root Hessian instead of the exact
Hessian. To enhance communication efficiency, we reduce the sketch size to
match the effective dimension of the Hessian matrix. We provide convergence
analysis based on statistical learning for the federated Newton sketch
approaches. Specifically, our approaches reach super-linear convergence rates
w.r.t. the communication rounds for the first time. We validate the
effectiveness of our algorithms through various experiments, which coincide
with our theoretical findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting Traffic Flow with Federated Learning and Graph Neural with
  Asynchronous Computations Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02723v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02723v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Yaqub, Shahzad Ahmad, Malik Abdul Manan, Imran Shabir Chuhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time traffic flow prediction holds significant importance within the
domain of Intelligent Transportation Systems (ITS). The task of achieving a
balance between prediction precision and computational efficiency presents a
significant challenge. In this article, we present a novel deep-learning method
called Federated Learning and Asynchronous Graph Convolutional Network
(FLAGCN). Our framework incorporates the principles of asynchronous graph
convolutional networks with federated learning to enhance the accuracy and
efficiency of real-time traffic flow prediction. The FLAGCN model employs a
spatial-temporal graph convolution technique to asynchronously address
spatio-temporal dependencies within traffic data effectively. To efficiently
handle the computational requirements associated with this deep learning model,
this study used a graph federated learning technique known as GraphFL. This
approach is designed to facilitate the training process. The experimental
results obtained from conducting tests on two distinct traffic datasets
demonstrate that the utilization of FLAGCN leads to the optimization of both
training and inference durations while maintaining a high level of prediction
accuracy. FLAGCN outperforms existing models with significant improvements by
achieving up to approximately 6.85% reduction in RMSE, 20.45% reduction in
MAPE, compared to the best-performing existing models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 Figures, 1 Table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Cost-Efficient FPGA Implementation of Tiny <span class="highlight-title">Transformer</span> Model using
  Neural ODE 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02721v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02721v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ikumi Okubo, Keisuke Sugiura, Hiroki Matsutani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer is an emerging neural network model with attention mechanism. It
has been adopted to various tasks and achieved a favorable accuracy compared to
CNNs and RNNs. While the attention mechanism is recognized as a general-purpose
component, many of the Transformer models require a significant number of
parameters compared to the CNN-based ones. To mitigate the computational
complexity, recently, a hybrid approach has been proposed, which uses ResNet as
a backbone architecture and replaces a part of its convolution layers with an
MHSA (Multi-Head Self-Attention) mechanism. In this paper, we significantly
reduce the parameter size of such models by using Neural ODE (Ordinary
Differential Equation) as a backbone architecture instead of ResNet. The
proposed hybrid model reduces the parameter size by 94.6% compared to the
CNN-based ones without degrading the accuracy. We then deploy the proposed
model on a modest-sized FPGA device for edge computing. To further reduce FPGA
resource utilization, we quantize the model following QAT (Quantization Aware
Training) scheme instead of PTQ (Post Training Quantization) to suppress the
accuracy loss. As a result, an extremely lightweight Transformer-based model
can be implemented on resource-limited FPGAs. The weights of the feature
extraction network are stored on-chip to minimize the memory transfer overhead,
allowing faster inference. By eliminating the overhead of memory transfers,
inference can be executed seamlessly, leading to accelerated inference. The
proposed FPGA implementation achieves 12.8x speedup and 9.21x energy efficiency
compared to ARM Cortex-A53 CPU.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Calibration Attack: A Framework For Adversarial Attacks Targeting
  Calibration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stephen Obadinma, Xiaodan Zhu, Hongyu Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new framework of adversarial attacks, named calibration
attacks, in which the attacks are generated and organized to trap victim models
to be miscalibrated without altering their original accuracy, hence seriously
endangering the trustworthiness of the models and any decision-making based on
their confidence scores. Specifically, we identify four novel forms of
calibration attacks: underconfidence attacks, overconfidence attacks, maximum
miscalibration attacks, and random confidence attacks, in both the black-box
and white-box setups. We then test these new attacks on typical victim models
with comprehensive datasets, demonstrating that even with a relatively low
number of queries, the attacks can create significant calibration mistakes. We
further provide detailed analyses to understand different aspects of
calibration attacks. Building on that, we investigate the effectiveness of
widely used adversarial defences and calibration methods against these types of
attacks, which then inspires us to devise two novel defences against such
calibration attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph-level Protein Representation Learning by Structure Knowledge
  Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02713v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02713v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ge Wang, Zelin Zang, Jiangbin Zheng, Jun Xia, Stan Z. Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on learning representation on the whole graph level in an
unsupervised manner. Learning graph-level representation plays an important
role in a variety of real-world issues such as molecule property prediction,
protein structure feature extraction, and social network analysis. The
mainstream method is utilizing contrastive learning to facilitate graph feature
extraction, known as Graph Contrastive Learning (GCL). GCL, although effective,
suffers from some complications in contrastive learning, such as the effect of
false negative pairs. Moreover, augmentation strategies in GCL are weakly
adaptive to diverse graph datasets. Motivated by these problems, we propose a
novel framework called Structure Knowledge Refinement (SKR) which uses data
structure to determine the probability of whether a pair is positive or
negative. Meanwhile, we propose an augmentation strategy that naturally
preserves the semantic meaning of the original data and is compatible with our
SKR framework. Furthermore, we illustrate the effectiveness of our SKR
framework through intuition and experiments. The experimental results on the
tasks of graph-level classification demonstrate that our SKR framework is
superior to most state-of-the-art baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TripleSurv: Triplet Time-adaptive Coordinate Loss for Survival Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liwen Zhang, Lianzhen Zhong, Fan Yang, Di Dong, Hui Hui, Jie Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A core challenge in survival analysis is to model the distribution of
censored time-to-event data, where the event of interest may be a death,
failure, or occurrence of a specific event. Previous studies have showed that
ranking and maximum likelihood estimation (MLE)loss functions are widely-used
for survival analysis. However, ranking loss only focus on the ranking of
survival time and does not consider potential effect of samples for exact
survival time values. Furthermore, the MLE is unbounded and easily subject to
outliers (e.g., censored data), which may cause poor performance of modeling.
To handle the complexities of learning process and exploit valuable survival
time values, we propose a time-adaptive coordinate loss function, TripleSurv,
to achieve adaptive adjustments by introducing the differences in the survival
time between sample pairs into the ranking, which can encourage the model to
quantitatively rank relative risk of pairs, ultimately enhancing the accuracy
of predictions. Most importantly, the TripleSurv is proficient in quantifying
the relative risk between samples by ranking ordering of pairs, and consider
the time interval as a trade-off to calibrate the robustness of model over
sample distribution. Our TripleSurv is evaluated on three real-world survival
datasets and a public synthetic dataset. The results show that our method
outperforms the state-of-the-art methods and exhibits good model performance
and robustness on modeling various sophisticated data distributions with
different censor rates. Our code will be available upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages,6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PAHD: Perception-Action based Human Decision Making using Explainable
  Graph Neural Networks on SAR Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sasindu Wijeratne, Bingyi Zhang, Rajgopal Kannan, Viktor Prasanna, Carl Busart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic Aperture Radar (SAR) images are commonly utilized in military
applications for automatic target recognition (ATR). Machine learning (ML)
methods, such as Convolutional Neural Networks (CNN) and Graph Neural Networks
(GNN), are frequently used to identify ground-based objects, including battle
tanks, personnel carriers, and missile launchers. Determining the vehicle
class, such as the BRDM2 tank, BMP2 tank, BTR60 tank, and BTR70 tank, is
crucial, as it can help determine whether the target object is an ally or an
enemy. While the ML algorithm provides feedback on the recognized target, the
final decision is left to the commanding officers. Therefore, providing
detailed information alongside the identified target can significantly impact
their actions. This detailed information includes the SAR image features that
contributed to the classification, the classification confidence, and the
probability of the identified object being classified as a different object
type or class. We propose a GNN-based ATR framework that provides the final
classified class and outputs the detailed information mentioned above. This is
the first study to provide a detailed analysis of the classification class,
making final decisions more straightforward. Moreover, our GNN framework
achieves an overall accuracy of 99.2\% when evaluated on the MSTAR dataset,
improving over previous state-of-the-art GNN methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Fidelity: Explaining Vulnerability Localization of Learning-based
  Detectors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baijun Cheng, Shengming Zhao, Kailong Wang, Meizhen Wang, Guangdong Bai, Ruitao Feng, Yao Guo, Lei Ma, Haoyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vulnerability detectors based on deep learning (DL) models have proven their
effectiveness in recent years. However, the shroud of opacity surrounding the
decision-making process of these detectors makes it difficult for security
analysts to comprehend. To address this, various explanation approaches have
been proposed to explain the predictions by highlighting important features,
which have been demonstrated effective in other domains such as computer vision
and natural language processing. Unfortunately, an in-depth evaluation of
vulnerability-critical features, such as fine-grained vulnerability-related
code lines, learned and understood by these explanation approaches remains
lacking. In this study, we first evaluate the performance of ten explanation
approaches for vulnerability detectors based on graph and sequence
representations, measured by two quantitative metrics including fidelity and
vulnerability line coverage rate. Our results show that fidelity alone is not
sufficient for evaluating these approaches, as fidelity incurs significant
fluctuations across different datasets and detectors. We subsequently check the
precision of the vulnerability-related code lines reported by the explanation
approaches, and find poor accuracy in this task among all of them. This can be
attributed to the inefficiency of explainers in selecting important features
and the presence of irrelevant artifacts learned by DL-based detectors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Tosem</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Geometric-Facilitated Denoising Diffusion Model for 3D Molecule
  Generation <span class="chip">AAAI-24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02683v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02683v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Can Xu, Haosen Wang, Weigang Wang, Pengfei Zheng, Hongyang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Denoising diffusion models have shown great potential in multiple research
areas. Existing diffusion-based generative methods on de novo 3D molecule
generation face two major challenges. Since majority heavy atoms in molecules
allow connections to multiple atoms through single bonds, solely using
pair-wise distance to model molecule geometries is insufficient. Therefore, the
first one involves proposing an effective neural network as the denoising
kernel that is capable to capture complex multi-body interatomic relationships
and learn high-quality features. Due to the discrete nature of graphs,
mainstream diffusion-based methods for molecules heavily rely on predefined
rules and generate edges in an indirect manner. The second challenge involves
accommodating molecule generation to diffusion and accurately predicting the
existence of bonds. In our research, we view the iterative way of updating
molecule conformations in diffusion process is consistent with molecular
dynamics and introduce a novel molecule generation method named
Geometric-Facilitated Molecular Diffusion (GFMDiff). For the first challenge,
we introduce a Dual-Track Transformer Network (DTN) to fully excevate global
spatial relationships and learn high quality representations which contribute
to accurate predictions of features and geometries. As for the second
challenge, we design Geometric-Facilitated Loss (GFLoss) which intervenes the
formation of bonds during the training period, instead of directly embedding
edges into the latent space. Comprehensive experiments on current benchmarks
demonstrate the superiority of GFMDiff.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures, AAAI-24 Main Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Homophily-Related: Adaptive Hybrid Graph Filter for Multi-View Graph
  Clustering <span class="chip">AAAI2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichen Wen, Yawen Ling, Yazhou Ren, Tianyi Wu, Jianpeng Chen, Xiaorong Pu, Zhifeng Hao, Lifang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently there is a growing focus on graph data, and multi-view graph
clustering has become a popular area of research interest. Most of the existing
methods are only applicable to homophilous graphs, yet the extensive real-world
graph data can hardly fulfill the homophily assumption, where the connected
nodes tend to belong to the same class. Several studies have pointed out that
the poor performance on heterophilous graphs is actually due to the fact that
conventional graph neural networks (GNNs), which are essentially low-pass
filters, discard information other than the low-frequency information on the
graph. Nevertheless, on certain graphs, particularly heterophilous ones,
neglecting high-frequency information and focusing solely on low-frequency
information impedes the learning of node representations. To break this
limitation, our motivation is to perform graph filtering that is closely
related to the homophily degree of the given graph, with the aim of fully
leveraging both low-frequency and high-frequency signals to learn
distinguishable node embedding. In this work, we propose Adaptive Hybrid Graph
Filter for Multi-View Graph Clustering (AHGFC). Specifically, a graph joint
process and graph joint aggregation matrix are first designed by using the
intrinsic node features and adjacency relationship, which makes the low and
high-frequency signals on the graph more distinguishable. Then we design an
adaptive hybrid graph filter that is related to the homophily degree, which
learns the node embedding based on the graph joint aggregation matrix. After
that, the node embedding of each view is weighted and fused into a consensus
embedding for the downstream task. Experimental results show that our proposed
model performs well on six datasets containing homophilous and heterophilous
graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LMaaS: Exploring Pricing Strategy of Large Model as a Service for
  Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02675v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02675v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Panlong Wu, Qi Liu, Yanjie Dong, Fangxin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The next generation of communication is envisioned to be intelligent
communication, that can replace traditional symbolic communication, where
highly condensed semantic information considering both source and channel will
be extracted and transmitted with high efficiency. The recent popular large
models such as GPT4 and the boosting learning techniques lay a solid foundation
for the intelligent communication, and prompt the practical deployment of it in
the near future. Given the characteristics of "training once and widely use" of
those multimodal large language models, we argue that a pay-as-you-go service
mode will be suitable in this context, referred to as Large Model as a Service
(LMaaS). However, the trading and pricing problem is quite complex with
heterogeneous and dynamic customer environments, making the pricing
optimization problem challenging in seeking on-hand solutions. In this paper,
we aim to fill this gap and formulate the LMaaS market trading as a Stackelberg
game with two steps. In the first step, we optimize the seller's pricing
decision and propose an Iterative Model Pricing (IMP) algorithm that optimizes
the prices of large models iteratively by reasoning customers' future rental
decisions, which is able to achieve a near-optimal pricing solution. In the
second step, we optimize customers' selection decisions by designing a robust
selecting and renting (RSR) algorithm, which is guaranteed to be optimal with
rigorous theoretical proof. Extensive experiments confirm the effectiveness and
robustness of our algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Integrated Fine-tuning and Inference when Generative AI meets
  Edge Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ning Chen, Zhipeng Cheng, Xuwei Fan, Xiaoyu Xia, Lianfen Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The high-performance generative artificial intelligence (GAI) represents the
latest evolution of computational intelligence, while the blessing of future 6G
networks also makes edge intelligence (EI) full of development potential. The
inevitable encounter between GAI and EI can unleash new opportunities, where
GAI's pre-training based on massive computing resources and large-scale
unlabeled corpora can provide strong foundational knowledge for EI, while EI
can harness fragmented computing resources to aggregate personalized knowledge
for GAI. However, the natural contradictory features pose significant
challenges to direct knowledge sharing. To address this, in this paper, we
propose the GAI-oriented synthetical network (GaisNet), a collaborative
cloud-edge-end intelligence framework that buffers contradiction leveraging
data-free knowledge relay, where the bidirectional knowledge flow enables GAI's
virtuous-cycle model fine-tuning and task inference, achieving mutualism
between GAI and EI with seamless fusion and collaborative evolution.
Experimental results demonstrate the effectiveness of the proposed mechanisms.
Finally, we discuss the future challenges and directions in the interplay
between GAI and EI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures, and 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-shot Microclimate Prediction with Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02665v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02665v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iman Deznabi, Peeyush Kumar, Madalina Fiterau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weather station data is a valuable resource for climate prediction, however,
its reliability can be limited in remote locations. To compound the issue,
making local predictions often relies on sensor data that may not be accessible
for a new, previously unmonitored location. In response to these challenges, we
propose a novel zero-shot learning approach designed to forecast various
climate measurements at new and unmonitored locations. Our method surpasses
conventional weather forecasting techniques in predicting microclimate
variables by leveraging knowledge extracted from other geographic locations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A backdoor attack against link prediction tasks with graph neural
  networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiazhu Dai, Haoyu Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) are a class of deep learning models capable of
processing graph-structured data, and they have demonstrated significant
performance in a variety of real-world applications. Recent studies have found
that GNN models are vulnerable to backdoor attacks. When specific patterns
(called backdoor triggers, e.g., subgraphs, nodes, etc.) appear in the input
data, the backdoor embedded in the GNN models is activated, which misclassifies
the input data into the target class label specified by the attacker, whereas
when there are no backdoor triggers in the input, the backdoor embedded in the
GNN models is not activated, and the models work normally. Backdoor attacks are
highly stealthy and expose GNN models to serious security risks. Currently,
research on backdoor attacks against GNNs mainly focus on tasks such as graph
classification and node classification, and backdoor attacks against link
prediction tasks are rarely studied. In this paper, we propose a backdoor
attack against the link prediction tasks based on GNNs and reveal the existence
of such security vulnerability in GNN models, which make the backdoored GNN
models to incorrectly predict unlinked two nodes as having a link relationship
when a trigger appear. The method uses a single node as the trigger and poison
selected node pairs in the training graph, and then the backdoor will be
embedded in the GNN models through the training process. In the inference
stage, the backdoor in the GNN models can be activated by simply linking the
trigger node to the two end nodes of the unlinked node pairs in the input data,
causing the GNN models to produce incorrect link prediction results for the
target node pairs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nurse-in-the-Loop Artificial Intelligence for Precision Management of
  Type 2 Diabetes in a Clinical Trial Utilizing Transfer-Learned Predictive
  Digital Twin 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02661v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02661v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Syed Hasib Akhter Faruqui, Adel Alaeddini, Yan Du, Shiyu Li, Kumar Sharma, Jing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background: Type 2 diabetes (T2D) is a prevalent chronic disease with a
significant risk of serious health complications and negative impacts on the
quality of life. Given the impact of individual characteristics and lifestyle
on the treatment plan and patient outcomes, it is crucial to develop precise
and personalized management strategies. Artificial intelligence (AI) provides
great promise in combining patterns from various data sources with nurses'
expertise to achieve optimal care. Methods: This is a 6-month ancillary study
among T2D patients (n = 20, age = 57 +- 10). Participants were randomly
assigned to an intervention (AI, n=10) group to receive daily AI-generated
individualized feedback or a control group without receiving the daily feedback
(non-AI, n=10) in the last three months. The study developed an online
nurse-in-the-loop predictive control (ONLC) model that utilizes a predictive
digital twin (PDT). The PDT was developed using a transfer-learning-based
Artificial Neural Network. The PDT was trained on participants self-monitoring
data (weight, food logs, physical activity, glucose) from the first three
months, and the online control algorithm applied particle swarm optimization to
identify impactful behavioral changes for maintaining the patient's glucose and
weight levels for the next three months. The ONLC provided the intervention
group with individualized feedback and recommendations via text messages. The
PDT was re-trained weekly to improve its performance. Findings: The trained
ONLC model achieved >=80% prediction accuracy across all patients while the
model was tuned online. Participants in the intervention group exhibited a
trend of improved daily steps and stable or improved total caloric and total
carb intake as recommended.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted for review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GTA: Guided Transfer of Spatial Attention from Object-Centric
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        SeokHyun Seo, Jinwoo Hong, JungWoo Chae, Kyungyul Kim, Sangheum Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Utilizing well-trained representations in transfer learning often results in
superior performance and faster convergence compared to training from scratch.
However, even if such good representations are transferred, a model can easily
overfit the limited training dataset and lose the valuable properties of the
transferred representations. This phenomenon is more severe in ViT due to its
low inductive bias. Through experimental analysis using attention maps in ViT,
we observe that the rich representations deteriorate when trained on a small
dataset. Motivated by this finding, we propose a novel and simple
regularization method for ViT called Guided Transfer of spatial Attention
(GTA). Our proposed method regularizes the self-attention maps between the
source and target models. A target model can fully exploit the knowledge
related to object localization properties through this explicit regularization.
Our experimental results show that the proposed GTA consistently improves the
accuracy across five benchmark datasets especially when the number of training
data is small.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Deep Q-Learning based Smart Scheduling of EVs for Demand Response in
  Smart Grids 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02653v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02653v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viorica Rozina Chifu, Tudor Cioara, Cristina Bianca Pop, Horia Rusu, Ionut Anghel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Economic and policy factors are driving the continuous increase in the
adoption and usage of electrical vehicles (EVs). However, despite being a
cleaner alternative to combustion engine vehicles, EVs have negative impacts on
the lifespan of microgrid equipment and energy balance due to increased power
demand and the timing of their usage. In our view grid management should
leverage on EVs scheduling flexibility to support local network balancing
through active participation in demand response programs. In this paper, we
propose a model-free solution, leveraging Deep Q-Learning to schedule the
charging and discharging activities of EVs within a microgrid to align with a
target energy profile provided by the distribution system operator. We adapted
the Bellman Equation to assess the value of a state based on specific rewards
for EV scheduling actions and used a neural network to estimate Q-values for
available actions and the epsilon-greedy algorithm to balance exploitation and
exploration to meet the target energy profile. The results are promising
showing that the proposed solution can effectively schedule the EVs charging
and discharging actions to align with the target profile with a Person
coefficient of 0.99, handling effective EVs scheduling situations that involve
dynamicity given by the e-mobility features, relying only on data with no
knowledge of EVs and microgrid dynamics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Discounting of Training Time Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ridhima Bector, Abhay Aradhya, Chai Quek, Zinovi Rabinovich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Among the most insidious attacks on Reinforcement Learning (RL) solutions are
training-time attacks (TTAs) that create loopholes and backdoors in the learned
behaviour. Not limited to a simple disruption, constructive TTAs (C-TTAs) are
now available, where the attacker forces a specific, target behaviour upon a
training RL agent (victim). However, even state-of-the-art C-TTAs focus on
target behaviours that could be naturally adopted by the victim if not for a
particular feature of the environment dynamics, which C-TTAs exploit. In this
work, we show that a C-TTA is possible even when the target behaviour is
un-adoptable due to both environment dynamics as well as non-optimality with
respect to the victim objective(s). To find efficient attacks in this context,
we develop a specialised flavour of the DDPG algorithm, which we term
gammaDDPG, that learns this stronger version of C-TTA. gammaDDPG dynamically
alters the attack policy planning horizon based on the victim's current
behaviour. This improves effort distribution throughout the attack timeline and
reduces the effect of uncertainty the attacker has about the victim. To
demonstrate the features of our method and better relate the results to prior
research, we borrow a 3D grid domain from a state-of-the-art C-TTA for our
experiments. Code is available at "bit.ly/github-rb-gDDPG".
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving sample efficiency of high dimensional Bayesian optimization
  with MCMC 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeji Yi, Yunyue Wei, Chu Xin Cheng, Kaibo He, Yanan Sui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential optimization methods are often confronted with the curse of
dimensionality in high-dimensional spaces. Current approaches under the
Gaussian process framework are still burdened by the computational complexity
of tracking Gaussian process posteriors and need to partition the optimization
problem into small regions to ensure exploration or assume an underlying
low-dimensional structure. With the idea of transiting the candidate points
towards more promising positions, we propose a new method based on Markov Chain
Monte Carlo to efficiently sample from an approximated posterior. We provide
theoretical guarantees of its convergence in the Gaussian process Thompson
sampling setting. We also show experimentally that both the Metropolis-Hastings
and the Langevin Dynamics version of our algorithm outperform state-of-the-art
methods in high-dimensional sequential optimization and reinforcement learning
benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simple Hierarchical Planning with Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02644v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02644v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Chen, Fei Deng, Kenji Kawaguchi, Caglar Gulcehre, Sungjin Ahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based generative methods have proven effective in modeling
trajectories with offline datasets. However, they often face computational
challenges and can falter in generalization, especially in capturing temporal
abstractions for long-horizon tasks. To overcome this, we introduce the
Hierarchical Diffuser, a simple, fast, yet surprisingly effective planning
method combining the advantages of hierarchical and diffusion-based planning.
Our model adopts a "jumpy" planning strategy at the higher level, which allows
it to have a larger receptive field but at a lower computational cost -- a
crucial factor for diffusion-based planning methods, as we have empirically
verified. Additionally, the jumpy sub-goals guide our low-level planner,
facilitating a fine-tuning stage and further improving our approach's
effectiveness. We conducted empirical evaluations on standard offline
reinforcement learning benchmarks, demonstrating our method's superior
performance and efficiency in terms of training and planning speed compared to
the non-hierarchical Diffuser as well as other hierarchical planning methods.
Moreover, we explore our model's generalization capability, particularly on how
our method improves generalization capabilities on compositional
out-of-distribution tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model-Agnostic Interpretation Framework in Machine Learning: A
  Comparative Study in NBA Sports 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of machine learning has seen tremendous progress in recent years,
with deep learning models delivering exceptional performance across a range of
tasks. However, these models often come at the cost of interpretability, as
they operate as opaque "black boxes" that obscure the rationale behind their
decisions. This lack of transparency can limit understanding of the models'
underlying principles and impede their deployment in sensitive domains, such as
healthcare or finance. To address this challenge, our research team has
proposed an innovative framework designed to reconcile the trade-off between
model performance and interpretability. Our approach is centered around modular
operations on high-dimensional data, which enable end-to-end processing while
preserving interpretability. By fusing diverse interpretability techniques and
modularized data processing, our framework sheds light on the decision-making
processes of complex models without compromising their performance. We have
extensively tested our framework and validated its superior efficacy in
achieving a harmonious balance between computational efficiency and
interpretability. Our approach addresses a critical need in contemporary
machine learning applications by providing unprecedented insights into the
inner workings of complex models, fostering trust, transparency, and
accountability in their deployment across diverse domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Causal Abstractions <span class="chip">AAAI-24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02602v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02602v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Xia, Elias Bareinboim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The abilities of humans to understand the world in terms of cause and effect
relationships, as well as to compress information into abstract concepts, are
two hallmark features of human intelligence. These two topics have been studied
in tandem in the literature under the rubric of causal abstractions theory. In
practice, it remains an open problem how to best leverage abstraction theory in
real-world causal inference tasks, where the true mechanisms are unknown and
only limited data is available. In this paper, we develop a new family of
causal abstractions by clustering variables and their domains. This approach
refines and generalizes previous notions of abstractions to better accommodate
individual causal distributions that are spawned by Pearl's causal hierarchy.
We show that such abstractions are learnable in practical settings through
Neural Causal Models (Xia et al., 2021), enabling the use of the deep learning
toolkit to solve various challenging causal inference tasks -- identification,
estimation, sampling -- at different levels of granularity. Finally, we
integrate these results with representation learning to create more flexible
abstractions, moving these results closer to practical applications. Our
experiments support the theory and illustrate how to scale causal inferences to
high-dimensional settings involving image data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>47 total pages, 20 figures, short version accepted to AAAI-24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guaranteed Nonconvex Factorization Approach for Tensor Train Recovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Qin, Michael B. Wakin, Zhihui Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we provide the first convergence guarantee for the
factorization approach. Specifically, to avoid the scaling ambiguity and to
facilitate theoretical analysis, we optimize over the so-called left-orthogonal
TT format which enforces orthonormality among most of the factors. To ensure
the orthonormal structure, we utilize the Riemannian gradient descent (RGD) for
optimizing those factors over the Stiefel manifold. We first delve into the TT
factorization problem and establish the local linear convergence of RGD.
Notably, the rate of convergence only experiences a linear decline as the
tensor order increases. We then study the sensing problem that aims to recover
a TT format tensor from linear measurements. Assuming the sensing operator
satisfies the restricted isometry property (RIP), we show that with a proper
initialization, which could be obtained through spectral initialization, RGD
also converges to the ground-truth tensor at a linear rate. Furthermore, we
expand our analysis to encompass scenarios involving Gaussian noise in the
measurements. We prove that RGD can reliably recover the ground truth at a
linear rate, with the recovery error exhibiting only polynomial growth in
relation to the tensor order. We conduct various experiments to validate our
theoretical findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthetic Information towards Maximum Posterior Ratio for deep learning
  on Imbalanced Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hung Nguyen, Morris Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study examines the impact of class-imbalanced data on deep learning
models and proposes a technique for data balancing by generating synthetic data
for the minority class. Unlike random-based oversampling, our method
prioritizes balancing the informative regions by identifying high entropy
samples. Generating well-placed synthetic data can enhance machine learning
algorithms accuracy and efficiency, whereas poorly-placed ones may lead to
higher misclassification rates. We introduce an algorithm that maximizes the
probability of generating a synthetic sample in the correct region of its class
by optimizing the class posterior ratio. Additionally, to maintain data
topology, synthetic data are generated within each minority sample's
neighborhood. Our experimental results on forty-one datasets demonstrate the
superior performance of our technique in enhancing deep-learning models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transaction on Artificial Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Learning for distribution skewed data using sample weights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02586v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02586v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hung Nguyen, Peiyuan Wu, Morris Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the most challenging issues in federated learning is that the data is
often not independent and identically distributed (nonIID). Clients are
expected to contribute the same type of data and drawn from one global
distribution. However, data are often collected in different ways from
different resources. Thus, the data distributions among clients might be
different from the underlying global distribution. This creates a weight
divergence issue and reduces federated learning performance. This work focuses
on improving federated learning performance for skewed data distribution across
clients. The main idea is to adjust the client distribution closer to the
global distribution using sample weights. Thus, the machine learning model
converges faster with higher accuracy. We start from the fundamental concept of
empirical risk minimization and theoretically derive a solution for adjusting
the distribution skewness using sample weights. To determine sample weights, we
implicitly exchange density information by leveraging a neural network-based
density estimation model, MADE. The clients data distribution can then be
adjusted without exposing their raw data. Our experiment results on three
real-world datasets show that the proposed method not only improves federated
learning accuracy but also significantly reduces communication costs compared
to the other experimental methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transaction on Artificial Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MC-ViViT: Multi-branch Classifier-ViViT to detect Mild Cognitive
  Impairment in older adults using facial videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.05292v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.05292v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Sun, Hiroko H. Dodge, Mohammad H. Mahoor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep machine learning models including Convolutional Neural Networks (CNN)
have been successful in the detection of Mild Cognitive Impairment (MCI) using
medical images, questionnaires, and videos. This paper proposes a novel
Multi-branch Classifier-Video Vision Transformer (MC-ViViT) model to
distinguish MCI from those with normal cognition by analyzing facial features.
The data comes from the I-CONECT, a behavioral intervention trial aimed at
improving cognitive function by providing frequent video chats. MC-ViViT
extracts spatiotemporal features of videos in one branch and augments
representations by the MC module. The I-CONECT dataset is challenging as the
dataset is imbalanced containing Hard-Easy and Positive-Negative samples, which
impedes the performance of MC-ViViT. We propose a loss function for Hard-Easy
and Positive-Negative Samples (HP Loss) by combining Focal loss and AD-CORRE
loss to address the imbalanced problem. Our experimental results on the
I-CONECT dataset show the great potential of MC-ViViT in predicting MCI with a
high accuracy of 90.63% accuracy on some of the interview videos.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 tables, 7 figures, 9 equations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Network Initialization for Medical AI Models Using
  Large-Scale, Unlabeled Natural Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.07688v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.07688v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soroosh Tayebi Arasteh, Leo Misera, Jakob Nikolas Kather, Daniel Truhn, Sven Nebelung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training datasets, like ImageNet, have become the gold standard in
medical image analysis. However, the emergence of self-supervised learning
(SSL), which leverages unlabeled data to learn robust features, presents an
opportunity to bypass the intensive labeling process. In this study, we
explored if SSL for pre-training on non-medical images can be applied to chest
radiographs and how it compares to supervised pre-training on non-medical
images and on medical images. We utilized a vision transformer and initialized
its weights based on (i) SSL pre-training on natural images (DINOv2), (ii) SL
pre-training on natural images (ImageNet dataset), and (iii) SL pre-training on
chest radiographs from the MIMIC-CXR database. We tested our approach on over
800,000 chest radiographs from six large global datasets, diagnosing more than
20 different imaging findings. Our SSL pre-training on curated images not only
outperformed ImageNet-based pre-training (P<0.001 for all datasets) but, in
certain cases, also exceeded SL on the MIMIC-CXR dataset. Our findings suggest
that selecting the right pre-training strategy, especially with SSL, can be
pivotal for improving artificial intelligence (AI)'s diagnostic accuracy in
medical imaging. By demonstrating the promise of SSL in chest radiograph
analysis, we underline a transformative shift towards more efficient and
accurate AI models in medical imaging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in European Radiology Experimental</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Surgical Aggregation: Federated Class-Heterogeneous Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06683v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06683v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranav Kulkarni, Adway Kanhere, Paul H. Yi, Vishwa S. Parekh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The release of numerous chest x-ray datasets has spearheaded the development
of deep learning models with expert-level performance. However, they have
limited interoperability due to class-heterogeneity -- a result of inconsistent
labeling schemes and partial annotations. Therefore, it is challenging to
leverage these datasets in aggregate to train models with a complete
representation of abnormalities that may occur within the thorax. In this work,
we propose surgical aggregation, a federated learning framework for aggregating
knowledge from class-heterogeneous datasets and learn a model that can
simultaneously predict the presence of all disease labels present across the
datasets. We evaluate our method using simulated and real-world
class-heterogeneous datasets across both independent and identically
distributed (iid) and non-iid settings. Our results show that surgical
aggregation outperforms current methods, has better generalizability, and is a
crucial first step towards tackling class-heterogeneity in federated learning
to facilitate the development of clinically-useful models using previously
non-interoperable chest x-ray datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Distributed Block Chebyshev-Davidson Algorithm for Parallel Spectral
  Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.04443v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.04443v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiyuan Pang, Haizhao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop a distributed Block Chebyshev-Davidson algorithm to solve
large-scale leading eigenvalue problems for spectral analysis in spectral
clustering. First, the efficiency of the Chebyshev-Davidson algorithm relies on
the prior knowledge of the eigenvalue spectrum, which could be expensive to
estimate. This issue can be lessened by the analytic spectrum estimation of the
Laplacian or normalized Laplacian matrices in spectral clustering, making the
proposed algorithm very efficient for spectral clustering. Second, to make the
proposed algorithm capable of analyzing big data, a distributed and parallel
version has been developed with attractive scalability. The speedup by parallel
computing is approximately equivalent to $\sqrt{p}$, where $p$ denotes the
number of processes. {Numerical results will be provided to demonstrate its
efficiency in spectral clustering and scalability advantage over existing
eigensolvers used for spectral clustering in parallel computing environments.}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Application of federated learning techniques for arrhythmia
  classification using 12-lead ECG signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.10993v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.10993v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Mauricio Jimenez Gutierrez, Hafiz Muuhammad Hassan, Lorella Landi, Andrea Vitaletti, Ioannis Chatzigiannakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence-based (AI) analysis of large, curated medical
datasets is promising for providing early detection, faster diagnosis, and more
effective treatment using low-power Electrocardiography (ECG) monitoring
devices information. However, accessing sensitive medical data from diverse
sources is highly restricted since improper use, unsafe storage, or data
leakage could violate a person's privacy. This work uses a Federated Learning
(FL) privacy-preserving methodology to train AI models over heterogeneous sets
of high-definition ECG from 12-lead sensor arrays collected from six
heterogeneous sources. We evaluated the capacity of the resulting models to
achieve equivalent performance compared to state-of-the-art models trained in a
Centralized Learning (CL) fashion. Moreover, we assessed the performance of our
solution over Independent and Identical distributed (IID) and non-IID federated
data. Our methodology involves machine learning techniques based on Deep Neural
Networks and Long-Short-Term Memory models. It has a robust data preprocessing
pipeline with feature engineering, selection, and data balancing techniques.
Our AI models demonstrated comparable performance to models trained using CL,
IID, and non-IID approaches. They showcased advantages in reduced complexity
and faster training time, making them well-suited for cloud-edge architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint of International Symposium on Algorithmic Aspects of Cloud
  Computing (ALGOCLOUD) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoGL: A Library for Automated Graph Learning <span class="chip">ICLR 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.04987v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.04987v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziwei Zhang, Yijian Qin, Zeyang Zhang, Chaoyu Guan, Jie Cai, Heng Chang, Jiyan Jiang, Haoyang Li, Zixin Sun, Beini Xie, Yang Yao, Yipeng Zhang, Xin Wang, Wenwu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed an upsurge in research interests and applications
of machine learning on graphs. However, manually designing the optimal machine
learning algorithms for different graph datasets and tasks is inflexible,
labor-intensive, and requires expert knowledge, limiting its adaptivity and
applicability. Automated machine learning (AutoML) on graphs, aiming to
automatically design the optimal machine learning algorithm for a given graph
dataset and task, has received considerable attention. However, none of the
existing libraries can fully support AutoML on graphs. To fill this gap, we
present Automated Graph Learning (AutoGL), the first dedicated library for
automated machine learning on graphs. AutoGL is open-source, easy to use, and
flexible to be extended. Specifically, we propose a three-layer architecture,
consisting of backends to interface with devices, a complete automated graph
learning pipeline, and supported graph applications. The automated machine
learning pipeline further contains five functional modules: auto feature
engineering, neural architecture search, hyper-parameter optimization, model
training, and auto ensemble, covering the majority of existing AutoML methods
on graphs. For each module, we provide numerous state-of-the-art methods and
flexible base classes and APIs, which allow easy usage and customization. We
further provide experimental results to showcase the usage of our AutoGL
library. We also present AutoGL-light, a lightweight version of AutoGL to
facilitate customizing pipelines and enriching applications, as well as
benchmarks for graph neural architecture search. The codes of AutoGL are
publicly available at https://github.com/THUMNLab/AutoGL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version; initial version published at ICLR 2021 Workshop on
  Geometrical and Topological Representation Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Randomized Smoothing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16221v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16221v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Scholten, Jan Schuchardt, Aleksandar Bojchevski, Stephan Günnemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world data is complex and often consists of objects that can be
decomposed into multiple entities (e.g. images into pixels, graphs into
interconnected nodes). Randomized smoothing is a powerful framework for making
models provably robust against small changes to their inputs - by guaranteeing
robustness of the majority vote when randomly adding noise before
classification. Yet, certifying robustness on such complex data via randomized
smoothing is challenging when adversaries do not arbitrarily perturb entire
objects (e.g. images) but only a subset of their entities (e.g. pixels). As a
solution, we introduce hierarchical randomized smoothing: We partially smooth
objects by adding random noise only on a randomly selected subset of their
entities. By adding noise in a more targeted manner than existing methods we
obtain stronger robustness guarantees while maintaining high accuracy. We
initialize hierarchical smoothing using different noising distributions,
yielding novel robustness certificates for discrete and continuous domains. We
experimentally demonstrate the importance of hierarchical smoothing in image
and node classification, where it yields superior robustness-accuracy
trade-offs. Overall, hierarchical smoothing is an important contribution
towards models that are both - certifiably robust to perturbations and
accurate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span>Bench: A Unified Library for Evaluation of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07910v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07910v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaijie Zhu, Qinlin Zhao, Hao Chen, Jindong Wang, Xing Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evaluation of large language models (LLMs) is crucial to assess their
performance and mitigate potential security risks. In this paper, we introduce
PromptBench, a unified library to evaluate LLMs. It consists of several key
components that are easily used and extended by researchers: prompt
construction, prompt engineering, dataset and model loading, adversarial prompt
attack, dynamic evaluation protocols, and analysis tools. PromptBench is
designed to be an open, general, and flexible codebase for research purposes
that can facilitate original study in creating new benchmarks, deploying
downstream applications, and designing new evaluation protocols. The code is
available at: https://github.com/microsoft/promptbench and will be continuously
supported.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>An extension to PromptBench (arXiv:2306.04528) for unified evaluation
  of LLMs using the same name; code: https://github.com/microsoft/promptbench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PAC-Bayes-Chernoff bounds for unbounded losses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01148v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01148v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ioar Casado, Luis A. Ortega, Andrés R. Masegosa, Aritz Pérez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new high-probability PAC-Bayes oracle bound for unbounded
losses. This result can be understood as a PAC-Bayes version of the Chernoff
bound. The proof technique relies on uniformly bounding the tail of certain
random variable based on the Cram\'er transform of the loss. We highlight two
applications of our main result. First, we show that our bound solves the open
problem of optimizing the free parameter on many PAC-Bayes bounds. Finally, we
show that our approach allows working with flexible assumptions on the loss
function, resulting in novel bounds that generalize previous ones and can be
minimized to obtain Gibbs-like posteriors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, minor typos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Annotation Sensitivity: Training Data Collection Methods Affect Model
  Performance <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.14212v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.14212v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christoph Kern, Stephanie Eckman, Jacob Beck, Rob Chew, Bolei Ma, Frauke Kreuter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When training data are collected from human annotators, the design of the
annotation instrument, the instructions given to annotators, the
characteristics of the annotators, and their interactions can impact training
data. This study demonstrates that design choices made when creating an
annotation instrument also impact the models trained on the resulting
annotations. We introduce the term annotation sensitivity to refer to the
impact of annotation data collection methods on the annotations themselves and
on downstream model performance and predictions. We collect annotations of hate
speech and offensive language in five experimental conditions of an annotation
instrument, randomly assigning annotators to conditions. We then fine-tune BERT
models on each of the five resulting datasets and evaluate model performance on
a holdout portion of each condition. We find considerable differences between
the conditions for 1) the share of hate speech/offensive language annotations,
2) model performance, 3) model predictions, and 4) model learning curves. Our
results emphasize the crucial role played by the annotation instrument which
has received little attention in the machine learning literature. We call for
additional research into how and why the instrument impacts the annotations to
inform the development of best practices in instrument design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2023 Findings:
  https://aclanthology.org/2023.findings-emnlp.992/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ConvNet vs <span class="highlight-title">Transformer</span>, Supervised vs CLIP: Beyond ImageNet Accuracy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09215v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09215v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kirill Vishniakov, Zhiqiang Shen, Zhuang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern computer vision offers a great variety of models to practitioners, and
selecting a model from multiple options for specific applications can be
challenging. Conventionally, competing model architectures and training
protocols are compared by their classification accuracy on ImageNet. However,
this single metric does not fully capture performance nuances critical for
specialized tasks. In this work, we conduct an in-depth comparative analysis of
model behaviors beyond ImageNet accuracy, for both ConvNet and Vision
Transformer architectures, each across supervised and CLIP training paradigms.
Although our selected models have similar ImageNet accuracies and compute
requirements, we find that they differ in many other aspects: types of
mistakes, output calibration, transferability, and feature invariance, among
others. This diversity in model characteristics, not captured by traditional
metrics, highlights the need for more nuanced analysis when choosing among
different models. Our code is available at
https://github.com/kirill-vish/Beyond-INet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://kirill-vish.github.io/beyond-imagenet-accuracy/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cadmium Zinc Telluride (CZT) photon counting detector Characterisation
  for soft tissue imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02106v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02106v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        K. Hameed, Rafidah Zainon, Mahbubunnabi Tamal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of photon counting detection technology has resulted in significant
X-ray imaging research interest in recent years. Computed Tomography (CT)
scanners can benefit from photon-counting detectors, which are new technology
with the potential to overcome key limitations of conventional CT detectors.
Researchers are still studying the effectiveness and sensitivity of
semiconductor detector materials in photon counting detectors for detecting
soft tissue contrasts. This study aimed to characterize the performance of the
Cadmium Zinc Telluride photon counting detector in identifying various tissues.
An optimal frame rate per second (FPS) of CZT detector was evaluated by setting
the X-ray tube voltage and current at 25 keV, 35 keV and 0.5 mA, 1.0 mA
respectively by keeping the optimum FPS fixed, the detector energy thresholds
were set in small steps from 15 keV to 35 keV and the Currents were set for
X-ray tubes in ranges of 0.1 mA to 1.0 mA to find the relationship between
voltage and current of the X-ray source and counts per second (CPS). The
samples i.e., fat, liver, muscles, paraffin wax, and contrast media were
stacked at six different thickness levels in a stair-step chamber made from
Plexi-glass. X-ray transmission at six different thicknesses of tissue samples
was also examined for five different energy (regions) thresholds (21 keV, 25
keV, 29 keV, 31 keV, and 45 keV) to determine the effect on count per second
(CPS). In this study, 12 frames per second is found to be the optimum frame
rate per second (FPS) based on the spectral response of an X-ray source and CPS
has a linear relationship with X-ray tube current as well. It was also noted
that A sample's thickness also affects its X-ray transmission at different
energy thresholds. A high sensitivity and linearity of the detectors make them
suitable for use in both preclinical and medical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages and 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Brain tumor segmentation using synthetic MR images -- A comparison of
  GANs and diffusion models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02986v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02986v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Usman Akbar, Måns Larsson, Anders Eklund
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large annotated datasets are required for training deep learning models, but
in medical imaging data sharing is often complicated due to ethics,
anonymization and data protection legislation. Generative AI models, such as
generative adversarial networks (GANs) and diffusion models, can today produce
very realistic synthetic images, and can potentially facilitate data sharing.
However, in order to share synthetic medical images it must first be
demonstrated that they can be used for training different networks with
acceptable performance. Here, we therefore comprehensively evaluate four GANs
(progressive GAN, StyleGAN 1-3) and a diffusion model for the task of brain
tumor segmentation (using two segmentation networks, U-Net and a Swin
transformer). Our results show that segmentation networks trained on synthetic
images reach Dice scores that are 80% - 90% of Dice scores when training with
real images, but that memorization of the training images can be a problem for
diffusion models if the original dataset is too small. Our conclusion is that
sharing synthetic medical images is a viable option to sharing real images, but
that further work is required. The trained generative models and the generated
synthetic images are shared on AIDA data hub
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 Pages. 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI Control: Improving Safety Despite Intentional Subversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06942v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06942v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Greenblatt, Buck Shlegeris, Kshitij Sachan, Fabien Roger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) become more powerful and are deployed more
autonomously, it will be increasingly important to prevent them from causing
harmful outcomes. Researchers have investigated a variety of safety techniques
for this purpose, e.g. using models to review the outputs of other models, or
red-teaming techniques to surface subtle failure modes. However, researchers
have not evaluated whether such techniques still ensure safety if the model is
itself intentionally trying to subvert them. In this paper, we develop and
evaluate pipelines of safety techniques ("protocols") that are robust to
intentional subversion.
  We investigate a scenario in which we want to solve a sequence of programming
problems, using access to a powerful but untrusted model (in our case, GPT-4),
access to a less powerful trusted model (in our case, GPT-3.5), and limited
access to high-quality trusted labor. We investigate protocols that aim to
never submit solutions containing backdoors, which we operationalize here as
logical errors that are not caught by test cases.
  We investigate a range of protocols and test each against strategies that the
untrusted model could use to subvert them. One protocol is what we call trusted
editing. This protocol first asks GPT-4 to write code, and then asks GPT-3.5 to
rate the suspiciousness of that code. If the code is below some suspiciousness
threshold, it is submitted. Otherwise, GPT-3.5 edits the solution to remove
parts that seem suspicious and then submits the edited code. Another protocol
is untrusted monitoring. This protocol asks GPT-4 to write code, and then asks
another instance of GPT-4 whether the code is backdoored, using various
techniques to prevent the GPT-4 instances from colluding. These protocols
improve substantially on simple baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Edit: Fix minor typos and clarify abstract</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FlashDecoding++: Faster Large Language Model Inference on GPUs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01282v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01282v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li, Jun Liu, Kangdi Chen, Yuhan Dong, Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the Large Language Model (LLM) becomes increasingly important in various
domains. However, the following challenges still remain unsolved in
accelerating LLM inference: (1) Synchronized partial softmax update. The
softmax operation requires a synchronized update operation among each partial
softmax result, leading to ~20% overheads for the attention computation in
LLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices
performing GEMM in LLM inference is flat, leading to under-utilized computation
and >50% performance loss after padding zeros in previous designs. (3)
Performance loss due to static dataflow. Kernel performance in LLM depends on
varied input data features, hardware configurations, etc. A single and static
dataflow may lead to a 50.25% performance loss for GEMMs of different shapes in
LLM inference.
  We present FlashDecoding++, a fast LLM inference engine supporting mainstream
LLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++
creatively proposes: (1) Asynchronized softmax with unified max value.
FlashDecoding++ introduces a unified max value technique for different partial
softmax computations to avoid synchronization. (2) Flat GEMM optimization with
double buffering. FlashDecoding++ points out that flat GEMMs with different
shapes face varied bottlenecks. Then, techniques like double buffering are
introduced. (3) Heuristic dataflow with hardware resource adaptation.
FlashDecoding++ heuristically optimizes dataflow using different hardware
resource considering input dynamics. Due to the versatility of optimizations in
FlashDecoding++, FlashDecoding++ can achieve up to 4.86x and 2.18x speedup on
both NVIDIA and AMD GPUs compared to Hugging Face implementations.
FlashDecoding++ also achieves an average speedup of 1.37x compared to
state-of-the-art LLM inference engines on mainstream LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantum artificial vision for defect detection in manufacturing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.04988v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.04988v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Guijo, Victor Onofre, Gianni Del Bimbo, Samuel Mugel, Daniel Estepa, Xabier De Carlos, Ana Adell, Aizea Lojo, Josu Bilbao, Roman Orus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we consider several algorithms for quantum computer vision
using Noisy Intermediate-Scale Quantum (NISQ) devices, and benchmark them for a
real problem against their classical counterparts. Specifically, we consider
two approaches: a quantum Support Vector Machine (QSVM) on a universal
gate-based quantum computer, and QBoost on a quantum annealer. The quantum
vision systems are benchmarked for an unbalanced dataset of images where the
aim is to detect defects in manufactured car pieces. We see that the quantum
algorithms outperform their classical counterparts in several ways, with QBoost
allowing for larger problems to be analyzed with present-day quantum annealers.
Data preprocessing, including dimensionality reduction and contrast
enhancement, is also discussed, as well as hyperparameter tuning in QBoost. To
the best of our knowledge, this is the first implementation of quantum computer
vision systems for a problem of industrial relevance in a manufacturing
production line.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures, 16 tables, revised version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Managing the unknown: a <span class="highlight-title">survey</span> on Open Set Recognition and tangential
  areas 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.08785v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.08785v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcos Barcina-Blanco, Jesus L. Lobo, Pablo Garcia-Bringas, Javier Del Ser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world scenarios classification models are often required to perform
robustly when predicting samples belonging to classes that have not appeared
during its training stage. Open Set Recognition addresses this issue by
devising models capable of detecting unknown classes from samples arriving
during the testing phase, while maintaining a good level of performance in the
classification of samples belonging to known classes. This review
comprehensively overviews the recent literature related to Open Set
Recognition, identifying common practices, limitations, and connections of this
field with other machine learning research areas, such as continual learning,
out-of-distribution detection, novelty detection, and uncertainty estimation.
Our work also uncovers open problems and suggests several research directions
that may motivate and articulate future efforts towards more safe Artificial
Intelligence methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 1 figure, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tensor Networks for Explainable Machine Learning in Cybersecurity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00867v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00867v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Borja Aizpurua, Roman Orus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we show how tensor networks help in developing explainability
of machine learning algorithms. Specifically, we develop an unsupervised
clustering algorithm based on Matrix Product States (MPS) and apply it in the
context of a real use-case of adversary-generated threat intelligence. Our
investigation proves that MPS rival traditional deep learning models such as
autoencoders and GANs in terms of performance, while providing much richer
model interpretability. Our approach naturally facilitates the extraction of
feature-wise probabilities, Von Neumann Entropy, and mutual information,
offering a compelling narrative for classification of anomalies and fostering
an unprecedented level of transparency and interpretability, something
fundamental to understand the rationale behind artificial intelligence
decisions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 9 figures, 2 table, minor typos corrected</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lower Difficulty and Better Robustness: A Bregman Divergence Perspective
  for Adversarial Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.12511v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.12511v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihui Wu, Haichang Gao, Bingqian Zhou, Xiaoyan Guo, Shudong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate on improving the adversarial robustness
obtained in adversarial training (AT) via reducing the difficulty of
optimization. To better study this problem, we build a novel Bregman divergence
perspective for AT, in which AT can be viewed as the sliding process of the
training data points on the negative entropy curve. Based on this perspective,
we analyze the learning objectives of two typical AT methods, i.e., PGD-AT and
TRADES, and we find that the optimization process of TRADES is easier than
PGD-AT for that TRADES separates PGD-AT. In addition, we discuss the function
of entropy in TRADES, and we find that models with high entropy can be better
robustness learners. Inspired by the above findings, we propose two methods,
i.e., FAIT and MER, which can both not only reduce the difficulty of
optimization under the 10-step PGD adversaries, but also provide better
robustness. Our work suggests that reducing the difficulty of optimization
under the 10-step PGD adversaries is a promising approach for enhancing the
adversarial robustness in AT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MoTCoder: Elevating Large Language Models with Modular of Thought for
  Challenging Programming Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.15960v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.15960v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyao Li, Pengguang Chen, Jiaya Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have showcased impressive capabilities in
handling straightforward programming tasks. However, their performance tends to
falter when confronted with more challenging programming problems. We observe
that conventional models often generate solutions as monolithic code blocks,
restricting their effectiveness in tackling intricate questions. To overcome
this limitation, we present Modular-of-Thought Coder (MoTCoder). We introduce a
pioneering framework for MoT instruction tuning, designed to promote the
decomposition of tasks into logical sub-tasks and sub-modules. Our
investigations reveal that, through the cultivation and utilization of
sub-modules, MoTCoder significantly improves both the modularity and
correctness of the generated solutions, leading to substantial relative pass@1
improvements of 12.9% on APPS and 9.43% on CodeContests. Our codes are
available at https://github.com/dvlab-research/MoTCoder.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Model: https://huggingface.co/JingyaoLi/MoTCoder-15B-v1.0. Code:
  https://github.com/dvlab-research/MoTCoder</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HG<span class="highlight-title">PROMPT</span>: Bridging Homogeneous and Heterogeneous Graphs for Few-shot
  <span class="highlight-title">Prompt</span> Learning <span class="chip">AAAI2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.01878v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.01878v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingtong Yu, Yuan Fang, Zemin Liu, Xinming Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) and heterogeneous graph neural networks (HGNNs)
are prominent techniques for homogeneous and heterogeneous graph representation
learning, yet their performance in an end-to-end supervised framework greatly
depends on the availability of task-specific supervision. To reduce the
labeling cost, pre-training on self-supervised pretext tasks has become a
popular paradigm,but there is often a gap between the pre-trained model and
downstream tasks, stemming from the divergence in their objectives. To bridge
the gap, prompt learning has risen as a promising direction especially in
few-shot settings, without the need to fully fine-tune the pre-trained model.
While there has been some early exploration of prompt-based learning on graphs,
they primarily deal with homogeneous graphs, ignoring the heterogeneous graphs
that are prevalent in downstream applications. In this paper, we propose
HGPROMPT, a novel pre-training and prompting framework to unify not only
pre-training and downstream tasks but also homogeneous and heterogeneous graphs
via a dual-template design. Moreover, we propose dual-prompt in HGPROMPT to
assist a downstream task in locating the most relevant prior to bridge the gaps
caused by not only feature variations but also heterogeneity differences across
tasks. Finally, we thoroughly evaluate and analyze HGPROMPT through extensive
experiments on three public datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Game Theory for Adversarial Attacks and Defenses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.06166v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.06166v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shorya Sharma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial attacks can generate adversarial inputs by applying small but
intentionally worst-case perturbations to samples from the dataset, which leads
to even state-of-the-art deep neural networks outputting incorrect answers with
high confidence. Hence, some adversarial defense techniques are developed to
improve the security and robustness of the models and avoid them being
attacked. Gradually, a game-like competition between attackers and defenders
formed, in which both players would attempt to play their best strategies
against each other while maximizing their own payoffs. To solve the game, each
player would choose an optimal strategy against the opponent based on the
prediction of the opponent's strategy choice. In this work, we are on the
defensive side to apply game-theoretic approaches on defending against attacks.
We use two randomization methods, random initialization and stochastic
activation pruning, to create diversity of networks. Furthermore, we use one
denoising technique, super resolution, to improve models' robustness by
preprocessing images before attacks. Our experimental results indicate that
those three methods can effectively improve the robustness of deep-learning
neural networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>With the agreement of my coauthors, I would like to withdraw the
  manuscript "Game Theory for Adversarial Attacks and Defenses". Some
  experimental procedures were not included in the manuscript, which makes a
  part of important claims not meaningful</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable manifold learning by uniform landmark sampling and constrained
  locally linear embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01100v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01100v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dehua Peng, Zhipeng Gui, Wenzhang Wei, Huayi Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a pivotal approach in machine learning and data science, manifold learning
aims to uncover the intrinsic low-dimensional structure within complex
nonlinear manifolds in high-dimensional space. By exploiting the manifold
hypothesis, various techniques for nonlinear dimension reduction have been
developed to facilitate visualization, classification, clustering, and gaining
key insights. Although existing manifold learning methods have achieved
remarkable successes, they still suffer from extensive distortions incurred in
the global structure, which hinders the understanding of underlying patterns.
Scalability issues also limit their applicability for handling large-scale
data. Here, we propose a scalable manifold learning (scML) method that can
manipulate large-scale and high-dimensional data in an efficient manner. It
starts by seeking a set of landmarks to construct the low-dimensional skeleton
of the entire data, and then incorporates the non-landmarks into the learned
space based on the constrained locally linear embedding (CLLE). We empirically
validated the effectiveness of scML on synthetic datasets and real-world
benchmarks of different types, and applied it to analyze the single-cell
transcriptomics and detect anomalies in electrocardiogram (ECG) signals. scML
scales well with increasing data sizes and embedding dimensions, and exhibits
promising performance in preserving the global structure. The experiments
demonstrate notable robustness in embedding quality as the sample rate
decreases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring the Privacy-Energy Consumption Tradeoff for Split Federated
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09441v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09441v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joohyung Lee, Mohamed Seif, Jungchan Cho, H. Vincent Poor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Split Federated Learning (SFL) has recently emerged as a promising
distributed learning technology, leveraging the strengths of both federated
learning and split learning. It emphasizes the advantages of rapid convergence
while addressing privacy concerns. As a result, this innovation has received
significant attention from both industry and academia. However, since the model
is split at a specific layer, known as a cut layer, into both client-side and
server-side models for the SFL, the choice of the cut layer in SFL can have a
substantial impact on the energy consumption of clients and their privacy, as
it influences the training burden and the output of the client-side models.
Moreover, the design challenge of determining the cut layer is highly
intricate, primarily due to the inherent heterogeneity in the computing and
networking capabilities of clients. In this article, we provide a comprehensive
overview of the SFL process and conduct a thorough analysis of energy
consumption and privacy. This analysis takes into account the influence of
various system parameters on the cut layer selection strategy. Additionally, we
provide an illustrative example of the cut layer selection, aiming to minimize
the risk of clients from reconstructing the raw data at the server while
sustaining energy consumption within the required energy budget, which involve
trade-offs. Finally, we address open challenges in this field. These directions
represent promising avenues for future research and development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AstroLLaMA-Chat: Scaling AstroLLaMA with Conversational and Diverse
  <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01916v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01916v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ernest Perkowski, Rui Pan, Tuan Dung Nguyen, Yuan-Sen Ting, Sandor Kruk, Tong Zhang, Charlie O'Neill, Maja Jablonska, Zechang Sun, Michael J. Smith, Huiling Liu, Kevin Schawinski, Kartheik Iyer, Ioana Ciucă for UniverseTBD
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore the potential of enhancing LLM performance in astronomy-focused
question-answering through targeted, continual pre-training. By employing a
compact 7B-parameter LLaMA-2 model and focusing exclusively on a curated set of
astronomy corpora -- comprising abstracts, introductions, and conclusions -- we
achieve notable improvements in specialized topic comprehension. While general
LLMs like GPT-4 excel in broader question-answering scenarios due to superior
reasoning capabilities, our findings suggest that continual pre-training with
limited resources can still enhance model performance on specialized topics.
Additionally, we present an extension of AstroLLaMA: the fine-tuning of the 7B
LLaMA model on a domain-specific conversational dataset, culminating in the
release of the chat-enabled AstroLLaMA for community use. Comprehensive
quantitative benchmarking is currently in progress and will be detailed in an
upcoming full paper. The model, AstroLLaMA-Chat, is now available at
https://huggingface.co/universeTBD, providing the first open-source
conversational AI tool tailored for the astronomy community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 1 figure, model is available at
  https://huggingface.co/universeTBD, published in RNAAS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Self-supervised</span> <span class="highlight-title">Pretrain</span>ing for Decision Foundation Model: Formulation,
  Pipeline and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00031v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00031v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoqian Liu, Jianbin Jiao, Junge Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decision-making is a dynamic process requiring perception, memory, and
reasoning to make choices and find optimal policies. Traditional approaches to
decision-making suffer from sample efficiency and generalization, while
large-scale self-supervised pretraining has enabled fast adaptation with
fine-tuning or few-shot learning in language and vision. We thus argue to
integrate knowledge acquired from generic large-scale self-supervised
pretraining into downstream decision-making problems. We propose
Pretrain-Then-Adapt pipeline and survey recent work on data collection,
pretraining objectives and adaptation strategies for decision-making
pretraining and downstream inference. Finally, we identify critical challenges
and future directions for developing decision foundation model with the help of
generic and flexible self-supervised pretraining.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tissue Artifact Segmentation and Severity Analysis for Automated
  Diagnosis Using Whole Slide Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01386v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01386v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Galib Muhammad Shahriar Himel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditionally, pathological analysis and diagnosis are performed by manually
eyeballing glass slide specimens under a microscope by an expert. The whole
slide image is the digital specimen produced from the glass slide. Whole slide
image enabled specimens to be observed on a computer screen and led to
computational pathology where computer vision and artificial intelligence are
utilized for automated analysis and diagnosis. With the current computational
advancement, the entire whole slide image can be analyzed autonomously without
human supervision. However, the analysis could fail or lead to wrong diagnosis
if the whole slide image is affected by tissue artifacts such as tissue fold or
air bubbles depending on the severity. Existing artifact detection methods rely
on experts for severity assessment to eliminate artifact affected regions from
the analysis. This process is time consuming, exhausting and undermines the
goal of automated analysis or removal of artifacts without evaluating their
severity, which could result in the loss of diagnostically important data.
Therefore, it is necessary to detect artifacts and then assess their severity
automatically. In this paper, we propose a system that incorporates severity
evaluation with artifact detection utilizing convolutional neural networks. The
proposed system uses DoubleUNet to segment artifacts and an ensemble network of
six fine tuned convolutional neural network models to determine severity. This
method outperformed current state of the art in accuracy by 9 percent for
artifact segmentation and achieved a strong correlation of 97 percent with the
evaluation of pathologists for severity assessment. The robustness of the
system was demonstrated using our proposed heterogeneous dataset and practical
usability was ensured by integrating it with an automated analysis system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>60 pages, 21 figures, 16 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FITS: Modeling Time Series with $10k$ Parameters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03756v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03756v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijian Xu, Ailing Zeng, Qiang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce FITS, a lightweight yet powerful model for time
series analysis. Unlike existing models that directly process raw time-domain
data, FITS operates on the principle that time series can be manipulated
through interpolation in the complex frequency domain. By discarding
high-frequency components with negligible impact on time series data, FITS
achieves performance comparable to state-of-the-art models for time series
forecasting and anomaly detection tasks, while having a remarkably compact size
of only approximately $10k$ parameters. Such a lightweight model can be easily
trained and deployed in edge devices, creating opportunities for various
applications. The code is available in: \url{https://github.com/VEWOXIC/FITS}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fully-Connected Spatial-Temporal Graph for Multivariate Time-Series Data <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.05305v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.05305v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yucheng Wang, Yuecong Xu, Jianfei Yang, Min Wu, Xiaoli Li, Lihua Xie, Zhenghua Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multivariate Time-Series (MTS) data is crucial in various application fields.
With its sequential and multi-source (multiple sensors) properties, MTS data
inherently exhibits Spatial-Temporal (ST) dependencies, involving temporal
correlations between timestamps and spatial correlations between sensors in
each timestamp. To effectively leverage this information, Graph Neural
Network-based methods (GNNs) have been widely adopted. However, existing
approaches separately capture spatial dependency and temporal dependency and
fail to capture the correlations between Different sEnsors at Different
Timestamps (DEDT). Overlooking such correlations hinders the comprehensive
modelling of ST dependencies within MTS data, thus restricting existing GNNs
from learning effective representations. To address this limitation, we propose
a novel method called Fully-Connected Spatial-Temporal Graph Neural Network
(FC-STGNN), including two key components namely FC graph construction and FC
graph convolution. For graph construction, we design a decay graph to connect
sensors across all timestamps based on their temporal distances, enabling us to
fully model the ST dependencies by considering the correlations between DEDT.
Further, we devise FC graph convolution with a moving-pooling GNN layer to
effectively capture the ST dependencies for learning effective representations.
Extensive experiments show the effectiveness of FC-STGNN on multiple MTS
datasets compared to SOTA methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures, Accepted by AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph-Aware Contrasting for Multivariate Time-Series Classification <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.05202v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.05202v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yucheng Wang, Yuecong Xu, Jianfei Yang, Min Wu, Xiaoli Li, Lihua Xie, Zhenghua Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning, as a self-supervised learning paradigm, becomes popular
for Multivariate Time-Series (MTS) classification. It ensures the consistency
across different views of unlabeled samples and then learns effective
representations for these samples. Existing contrastive learning methods mainly
focus on achieving temporal consistency with temporal augmentation and
contrasting techniques, aiming to preserve temporal patterns against
perturbations for MTS data. However, they overlook spatial consistency that
requires the stability of individual sensors and their correlations. As MTS
data typically originate from multiple sensors, ensuring spatial consistency
becomes essential for the overall performance of contrastive learning on MTS
data. Thus, we propose Graph-Aware Contrasting for spatial consistency across
MTS data. Specifically, we propose graph augmentations including node and edge
augmentations to preserve the stability of sensors and their correlations,
followed by graph contrasting with both node- and graph-level contrasting to
extract robust sensor- and global-level features. We further introduce
multi-window temporal contrasting to ensure temporal consistency in the data
for each sensor. Extensive experiments demonstrate that our proposed method
achieves state-of-the-art performance on various MTS classification tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures, Accepted by AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-agent Reinforcement Learning for Cooperative Lane Changing of
  Connected and Autonomous Vehicles in Mixed Traffic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.06318v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.06318v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Zhou, Dong Chen, Jun Yan, Zhaojian Li, Huilin Yin, Wanchen Ge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving has attracted significant research interests in the past
two decades as it offers many potential benefits, including releasing drivers
from exhausting driving and mitigating traffic congestion, among others.
Despite promising progress, lane-changing remains a great challenge for
autonomous vehicles (AV), especially in mixed and dynamic traffic scenarios.
Recently, reinforcement learning (RL), a powerful data-driven control method,
has been widely explored for lane-changing decision makings in AVs with
encouraging results demonstrated. However, the majority of those studies are
focused on a single-vehicle setting, and lane-changing in the context of
multiple AVs coexisting with human-driven vehicles (HDVs) have received scarce
attention. In this paper, we formulate the lane-changing decision making of
multiple AVs in a mixed-traffic highway environment as a multi-agent
reinforcement learning (MARL) problem, where each AV makes lane-changing
decisions based on the motions of both neighboring AVs and HDVs. Specifically,
a multi-agent advantage actor-critic network (MA2C) is developed with a novel
local reward design and a parameter sharing scheme. In particular, a
multi-objective reward function is proposed to incorporate fuel efficiency,
driving comfort, and safety of autonomous driving. Comprehensive experimental
results, conducted under three different traffic densities and various levels
of human driver aggressiveness, show that our proposed MARL framework
consistently outperforms several state-of-the-art benchmarks in terms of
efficiency, safety and driver comfort.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was published on Autonomous Intelligent Systems (Volume 2,
  article number 5, 2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.07728v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.07728v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seokhyeon Ha, Sunbeom Jung, Jungwoo Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning pre-trained neural network models has become a widely adopted
approach across various domains. However, it can lead to the distortion of
pre-trained feature extractors that already possess strong generalization
capabilities. Mitigating feature distortion during adaptation to new target
domains is crucial. Recent studies have shown promising results in handling
feature distortion by aligning the head layer on in-distribution datasets
before performing fine-tuning. Nonetheless, a significant limitation arises
from the treatment of batch normalization layers during fine-tuning, leading to
suboptimal performance. In this paper, we propose Domain-Aware Fine-Tuning
(DAFT), a novel approach that incorporates batch normalization conversion and
the integration of linear probing and fine-tuning. Our batch normalization
conversion method effectively mitigates feature distortion by reducing
modifications to the neural network during fine-tuning. Additionally, we
introduce the integration of linear probing and fine-tuning to optimize the
head layer with gradual adaptation of the feature extractor. By leveraging
batch normalization layers and integrating linear probing and fine-tuning, our
DAFT significantly mitigates feature distortion and achieves improved model
performance on both in-distribution and out-of-distribution datasets. Extensive
experiments demonstrate that our method outperforms other baseline methods,
demonstrating its effectiveness in not only improving performance but also
mitigating feature distortion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Subjectivity in Unsupervised Machine Learning Model Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.00201v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.00201v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanyi Chen, Mary L. Cummings
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model selection is a necessary step in unsupervised machine learning. Despite
numerous criteria and metrics, model selection remains subjective. A high
degree of subjectivity may lead to questions about repeatability and
reproducibility of various machine learning studies and doubts about the
robustness of models deployed in the real world. Yet, the impact of modelers'
preferences on model selection outcomes remains largely unexplored. This study
uses the Hidden Markov Model as an example to investigate the subjectivity
involved in model selection. We asked 33 participants and three Large Language
Models (LLMs) to make model selections in three scenarios. Results revealed
variability and inconsistencies in both the participants' and the LLMs'
choices, especially when different criteria and metrics disagree. Sources of
subjectivity include varying opinions on the importance of different criteria
and metrics, differing views on how parsimonious a model should be, and how the
size of a dataset should influence model selection. The results underscore the
importance of developing a more standardized way to document subjective choices
made in model selection processes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Approximation by non-symmetric networks for cross-domain learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03890v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03890v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hrushikesh Mhaskar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For the past 30 years or so, machine learning has stimulated a great deal of
research in the study of approximation capabilities (expressive power) of a
multitude of processes, such as approximation by shallow or deep neural
networks, radial basis function networks, and a variety of kernel based
methods. Motivated by applications such as invariant learning, transfer
learning, and synthetic aperture radar imaging, we initiate in this paper a
general approach to study the approximation capabilities of kernel based
networks using non-symmetric kernels. While singular value decomposition is a
natural instinct to study such kernels, we consider a more general approach to
include the use of a family of kernels, such as generalized translation
networks (which include neural networks and translation invariant kernels as
special cases) and rotated zonal function kernels. Naturally, unlike
traditional kernel based approximation, we cannot require the kernels to be
positive definite. In particular, we obtain estimates on the accuracy of
uniform approximation of functions in a ($L^2$)-Sobolev class by ReLU$^r$
networks when $r$ is not necessarily an integer. Our general results apply to
the approximation of functions with small smoothness compared to the dimension
of the input space.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Provable Accelerated Convergence of Nesterov's Momentum for Deep ReLU
  Neural Networks <span class="chip">ALT 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.08109v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.08109v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangshuo Liao, Anastasios Kyrillidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current state-of-the-art analyses on the convergence of gradient descent for
training neural networks focus on characterizing properties of the loss
landscape, such as the Polyak-Lojaciewicz (PL) condition and the restricted
strong convexity. While gradient descent converges linearly under such
conditions, it remains an open question whether Nesterov's momentum enjoys
accelerated convergence under similar settings and assumptions. In this work,
we consider a new class of objective functions, where only a subset of the
parameters satisfies strong convexity, and show Nesterov's momentum achieves
acceleration in theory for this objective class. We provide two realizations of
the problem class, one of which is deep ReLU networks, which --to the best of
our knowledge--constitutes this work the first that proves accelerated
convergence rate for non-trivial neural network architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ALT 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Underwater Acoustic Signal Recognition Based on Salient Feature 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.13143v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.13143v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancement of technology, the recognition of underwater
acoustic signals in complex environments has become increasingly crucial.
Currently, mainstream underwater acoustic signal recognition relies primarily
on time-frequency analysis to extract spectral features, finding widespread
applications in the field. However, existing recognition methods heavily depend
on expert systems, facing limitations such as restricted knowledge bases and
challenges in handling complex relationships. These limitations stem from the
complexity and maintenance difficulties associated with rules or inference
engines. Recognizing the potential advantages of deep learning in handling
intricate relationships, this paper proposes a method utilizing neural networks
for underwater acoustic signal recognition. The proposed approach involves
continual learning of features extracted from spectra for the classification of
underwater acoustic signals. Deep learning models can automatically learn
abstract features from data and continually adjust weights during training to
enhance classification performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KwaiAgents: Generalized Information-seeking Agent System with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04889v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04889v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haojie Pan, Zepeng Zhai, Hao Yuan, Yaojia Lv, Ruiji Fu, Ming Liu, Zhongyuan Wang, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Driven by curiosity, humans have continually sought to explore and understand
the world around them, leading to the invention of various tools to satiate
this inquisitiveness. Despite not having the capacity to process and memorize
vast amounts of information in their brains, humans excel in critical thinking,
planning, reflection, and harnessing available tools to interact with and
interpret the world, enabling them to find answers efficiently. The recent
advancements in large language models (LLMs) suggest that machines might also
possess the aforementioned human-like capabilities, allowing them to exhibit
powerful abilities even with a constrained parameter count. In this paper, we
introduce KwaiAgents, a generalized information-seeking agent system based on
LLMs. Within KwaiAgents, we propose an agent system that employs LLMs as its
cognitive core, which is capable of understanding a user's query, behavior
guidelines, and referencing external documents. The agent can also update and
retrieve information from its internal memory, plan and execute actions using a
time-aware search-browse toolkit, and ultimately provide a comprehensive
response. We further investigate the system's performance when powered by LLMs
less advanced than GPT-4, and introduce the Meta-Agent Tuning (MAT) framework,
designed to ensure even an open-sourced 7B or 13B model performs well among
many agent systems. We exploit both benchmark and human evaluations to
systematically validate these capabilities. Extensive experiments show the
superiority of our agent system compared to other autonomous agents and
highlight the enhanced generalized agent-abilities of our fine-tuned LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Estimation for Longitudinal Networks via Adaptive Merging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.07866v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.07866v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Zhang, Junhui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Longitudinal network consists of a sequence of temporal edges among multiple
nodes, where the temporal edges are observed in real time. It has become
ubiquitous with the rise of online social platform and e-commerce, but largely
under-investigated in literature. In this paper, we propose an efficient
estimation framework for longitudinal network, leveraging strengths of adaptive
network merging, tensor decomposition and point process. It merges neighboring
sparse networks so as to enlarge the number of observed edges and reduce
estimation variance, whereas the estimation bias introduced by network merging
is controlled by exploiting local temporal structures for adaptive network
neighborhood. A projected gradient descent algorithm is proposed to facilitate
estimation, where the upper bound of the estimation error in each iteration is
established. A thorough analysis is conducted to quantify the asymptotic
behavior of the proposed method, which shows that it can significantly reduce
the estimation error and also provides guideline for network merging under
various scenarios. We further demonstrate the advantage of the proposed method
through extensive numerical experiments on synthetic datasets and a militarized
interstate dispute dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages and 4 figures; appendix including technical proof will be
  uploaded later</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predicting Drug Solubility Using Different Machine Learning Methods --
  Linear Regression Model with Extracted Chemical Features vs Graph
  Convolutional Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12325v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12325v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Ho, Zhao-Heng Yin, Colin Zhang, Nicole Guo, Yang Ha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting the solubility of given molecules remains crucial in the
pharmaceutical industry. In this study, we revisited this extensively studied
topic, leveraging the capabilities of contemporary computing resources. We
employed two machine learning models: a linear regression model and a graph
convolutional neural network (GCNN) model, using various experimental datasets.
Both methods yielded reasonable predictions, with the GCNN model exhibiting the
highest level of performance. However, the present GCNN model has limited
interpretability while the linear regression model allows scientists for a
greater in-depth analysis of the underlying factors through feature importance
analysis, although more human inputs and evaluations on the overall dataset is
required. From the perspective of chemistry, using the linear regression model,
we elucidated the impact of individual atom species and functional groups on
overall solubility, highlighting the significance of comprehending how chemical
structure influences chemical properties in the drug development process. It is
learned that introducing oxygen atoms can increase the solubility of organic
molecules, while almost all other hetero atoms except oxygen and nitrogen tend
to decrease solubility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CycleGAN Models for MRI Image Translation <span class="chip">ACML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00023v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00023v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cassandra Czobit, Reza Samavi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-to-image translation has gained popularity in the medical field to
transform images from one domain to another. Medical image synthesis via domain
transformation is advantageous in its ability to augment an image dataset where
images for a given class is limited. From the learning perspective, this
process contributes to data-oriented robustness of the model by inherently
broadening the model's exposure to more diverse visual data and enabling it to
learn more generalized features. In the case of generating additional
neuroimages, it is advantageous to obtain unidentifiable medical data and
augment smaller annotated datasets. This study proposes the development of a
CycleGAN model for translating neuroimages from one field strength to another
(e.g., 3 Tesla to 1.5). This model was compared to a model based on DCGAN
architecture. CycleGAN was able to generate the synthetic and reconstructed
images with reasonable accuracy. The mapping function from the source (3 Tesla)
to target domain (1.5 Tesla) performed optimally with an average PSNR value of
25.69 $\pm$ 2.49 dB and an MAE value of 2106.27 $\pm$ 1218.37.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted and presented in ACML PRHA 2023 workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stabilizing RNN Gradients through <span class="highlight-title">Pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12075v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12075v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Herranz-Celotti, Jean Rouat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerous theories of learning propose to prevent the gradient from
exponential growth with depth or time, to stabilize and improve training.
Typically, these analyses are conducted on feed-forward fully-connected neural
networks or simple single-layer recurrent neural networks, given their
mathematical tractability. In contrast, this study demonstrates that
pre-training the network to local stability can be effective whenever the
architectures are too complex for an analytical initialization. Furthermore, we
extend known stability theories to encompass a broader family of deep recurrent
networks, requiring minimal assumptions on data and parameter distribution, a
theory we call the Local Stability Condition (LSC). Our investigation reveals
that the classical Glorot, He, and Orthogonal initialization schemes satisfy
the LSC when applied to feed-forward fully-connected neural networks. However,
analysing deep recurrent networks, we identify a new additive source of
exponential explosion that emerges from counting gradient paths in a
rectangular grid in depth and time. We propose a new approach to mitigate this
issue, that consists on giving a weight of a half to the time and depth
contributions to the gradient, instead of the classical weight of one. Our
empirical results confirm that pre-training both feed-forward and recurrent
networks, for differentiable, neuromorphic and state-space models to fulfill
the LSC, often results in improved final performance. This study contributes to
the field by providing a means to stabilize networks of any complexity. Our
approach can be implemented as an additional step before pre-training on large
augmented datasets, and as an alternative to finding stable initializations
analytically.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CrisisViT: A Robust Vision <span class="highlight-title">Transformer</span> for Crisis Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02838v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02838v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijun Long, Richard McCreadie, Muhammad Imran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In times of emergency, crisis response agencies need to quickly and
accurately assess the situation on the ground in order to deploy relevant
services and resources. However, authorities often have to make decisions based
on limited information, as data on affected regions can be scarce until local
response services can provide first-hand reports. Fortunately, the widespread
availability of smartphones with high-quality cameras has made citizen
journalism through social media a valuable source of information for crisis
responders. However, analyzing the large volume of images posted by citizens
requires more time and effort than is typically available. To address this
issue, this paper proposes the use of state-of-the-art deep neural models for
automatic image classification/tagging, specifically by adapting
transformer-based architectures for crisis image classification (CrisisViT). We
leverage the new Incidents1M crisis image dataset to develop a range of new
transformer-based image classification models. Through experimentation over the
standard Crisis image benchmark dataset, we demonstrate that the CrisisViT
models significantly outperform previous approaches in emergency type, image
relevance, humanitarian category, and damage severity classification.
Additionally, we show that the new Incidents1M dataset can further augment the
CrisisViT models resulting in an additional 1.25% absolute accuracy gain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MusicAOG: an Energy-Based Model for Learning and Sampling a Hierarchical
  Representation of Symbolic Music 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yikai Qian, Tianle Wang, Xinyi Tong, Xin Jin, Duo Xu, Bo Zheng, Tiezheng Ge, Feng Yu, Song-Chun Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In addressing the challenge of interpretability and generalizability of
artificial music intelligence, this paper introduces a novel symbolic
representation that amalgamates both explicit and implicit musical information
across diverse traditions and granularities. Utilizing a hierarchical and-or
graph representation, the model employs nodes and edges to encapsulate a broad
spectrum of musical elements, including structures, textures, rhythms, and
harmonies. This hierarchical approach expands the representability across
various scales of music. This representation serves as the foundation for an
energy-based model, uniquely tailored to learn musical concepts through a
flexible algorithm framework relying on the minimax entropy principle.
Utilizing an adapted Metropolis-Hastings sampling technique, the model enables
fine-grained control over music generation. A comprehensive empirical
evaluation, contrasting this novel approach with existing methodologies,
manifests considerable advancements in interpretability and controllability.
This study marks a substantial contribution to the fields of music analysis,
composition, and computational musicology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling and Masking: A New Paradigm of Data Sampling for Image and Video
  Quality Assessment <span class="chip">AAAI2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongxu Liu, Yinghui Quan, Guoyao Xiao, Aobo Li, Jinjian Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quality assessment of images and videos emphasizes both local details and
global semantics, whereas general data sampling methods (e.g., resizing,
cropping or grid-based fragment) fail to catch them simultaneously. To address
the deficiency, current approaches have to adopt multi-branch models and take
as input the multi-resolution data, which burdens the model complexity. In this
work, instead of stacking up models, a more elegant data sampling method (named
as SAMA, scaling and masking) is explored, which compacts both the local and
global content in a regular input size. The basic idea is to scale the data
into a pyramid first, and reduce the pyramid into a regular data dimension with
a masking strategy. Benefiting from the spatial and temporal redundancy in
images and videos, the processed data maintains the multi-scale characteristics
with a regular input size, thus can be processed by a single-branch model. We
verify the sampling method in image and video quality assessment. Experiments
show that our sampling method can improve the performance of current
single-branch models significantly, and achieves competitive performance to the
multi-branch models without extra model complexity. The source code will be
available at https://github.com/Sissuire/SAMA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2024. Code has been released at
  https://github.com/Sissuire/SAMA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieval-Augmented Text-to-Audio Generation <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08051v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08051v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D. Plumbley, Wenwu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent progress in text-to-audio (TTA) generation, we show that the
state-of-the-art models, such as AudioLDM, trained on datasets with an
imbalanced class distribution, such as AudioCaps, are biased in their
generation performance. Specifically, they excel in generating common audio
classes while underperforming in the rare ones, thus degrading the overall
generation performance. We refer to this problem as long-tailed text-to-audio
generation. To address this issue, we propose a simple retrieval-augmented
approach for TTA models. Specifically, given an input text prompt, we first
leverage a Contrastive Language Audio Pretraining (CLAP) model to retrieve
relevant text-audio pairs. The features of the retrieved audio-text data are
then used as additional conditions to guide the learning of TTA models. We
enhance AudioLDM with our proposed approach and denote the resulting augmented
system as Re-AudioLDM. On the AudioCaps dataset, Re-AudioLDM achieves a
state-of-the-art Frechet Audio Distance (FAD) of 1.37, outperforming the
existing approaches by a large margin. Furthermore, we show that Re-AudioLDM
can generate realistic audio for complex scenes, rare audio classes, and even
unseen audio types, indicating its potential in TTA tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TR-DETR: Task-Reciprocal <span class="highlight-title">Transformer</span> for Joint Moment Retrieval and
  Highlight Detection <span class="chip">AAAI-24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02309v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02309v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Sun, Mingyao Zhou, Wenjing Chen, Wei Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video moment retrieval (MR) and highlight detection (HD) based on natural
language queries are two highly related tasks, which aim to obtain relevant
moments within videos and highlight scores of each video clip. Recently,
several methods have been devoted to building DETR-based networks to solve both
MR and HD jointly. These methods simply add two separate task heads after
multi-modal feature extraction and feature interaction, achieving good
performance. Nevertheless, these approaches underutilize the reciprocal
relationship between two tasks. In this paper, we propose a task-reciprocal
transformer based on DETR (TR-DETR) that focuses on exploring the inherent
reciprocity between MR and HD. Specifically, a local-global multi-modal
alignment module is first built to align features from diverse modalities into
a shared latent space. Subsequently, a visual feature refinement is designed to
eliminate query-irrelevant information from visual features for modal
interaction. Finally, a task cooperation module is constructed to refine the
retrieval pipeline and the highlight score prediction process by utilizing the
reciprocity between MR and HD. Comprehensive experiments on QVHighlights,
Charades-STA and TVSum datasets demonstrate that TR-DETR outperforms existing
state-of-the-art methods. Codes are available at
\url{https://github.com/mingyao1120/TR-DETR}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI-24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ M$^{2}$UGen: Multi-modal Music Understanding and Generation with the
  Power of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11255v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11255v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atin Sakkeer Hussain, Shansong Liu, Chenshuo Sun, Ying Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current landscape of research leveraging large language models (LLMs) is
experiencing a surge. Many works harness the powerful reasoning capabilities of
these models to comprehend various modalities, such as text, speech, images,
videos, etc. They also utilize LLMs to understand human intention and generate
desired outputs like images, videos, and music. However, research that combines
both understanding and generation using LLMs is still limited and in its
nascent stage. To address this gap, we introduce a Multi-modal Music
Understanding and Generation (M$^{2}$UGen) framework that integrates LLM's
abilities to comprehend and generate music for different modalities. The
M$^{2}$UGen framework is purpose-built to unlock creative potential from
diverse sources of inspiration, encompassing music, image, and video through
the use of pretrained MERT, ViT, and ViViT models, respectively. To enable
music generation, we explore the use of AudioLDM 2 and MusicGen. Bridging
multi-modal understanding and music generation is accomplished through the
integration of the LLaMA 2 model. Furthermore, we make use of the MU-LLaMA
model to generate extensive datasets that support text/image/video-to-music
generation, facilitating the training of our M$^{2}$UGen framework. We conduct
a thorough evaluation of our proposed framework. The experimental results
demonstrate that our model achieves or surpasses the performance of the current
state-of-the-art models.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-01-04T00:00:00Z">2024-01-04</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">42</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Task Oriented Dialogue as a Catalyst for <span class="highlight-title">Self-Supervised</span> Automatic
  Speech Recognition <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David M. Chan, Shalini Ghosh, Hitesh Tulsiani, Ariya Rastrow, Björn Hoffmeister
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While word error rates of automatic speech recognition (ASR) systems have
consistently fallen, natural language understanding (NLU) applications built on
top of ASR systems still attribute significant numbers of failures to
low-quality speech recognition results. Existing assistant systems collect
large numbers of these unsuccessful interactions, but these systems usually
fail to learn from these interactions, even in an offline fashion. In this
work, we introduce CLC: Contrastive Learning for Conversations, a family of
methods for contrastive fine-tuning of models in a self-supervised fashion,
making use of easily detectable artifacts in unsuccessful conversations with
assistants. We demonstrate that our CLC family of approaches can improve the
performance of ASR models on OD3, a new public large-scale semi-synthetic
meta-dataset of audio task-oriented dialogues, by up to 19.2%. These gains
transfer to real-world systems as well, where we show that CLC can help to
improve performance by up to 6.7% over baselines. We make OD3 publicly
available at https://github.com/amazon-science/amazon-od3 .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLaMA Pro: Progressive LLaMA with Block Expansion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02415v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02415v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyue Wu, Yukang Gan, Yixiao Ge, Zeyu Lu, Jiahao Wang, Ye Feng, Ping Luo, Ying Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans generally acquire new skills without compromising the old; however,
the opposite holds for Large Language Models (LLMs), e.g., from LLaMA to
CodeLLaMA. To this end, we propose a new post-pretraining method for LLMs with
an expansion of Transformer blocks. We tune the expanded blocks using only new
corpus, efficiently and effectively improving the model's knowledge without
catastrophic forgetting. In this paper, we experiment on the corpus of code and
math, yielding LLaMA Pro-8.3B, a versatile foundation model initialized from
LLaMA2-7B, excelling in general tasks, programming, and mathematics. LLaMA Pro
and its instruction-following counterpart (LLaMA Pro-Instruct) achieve advanced
performance among various benchmarks, demonstrating superiority over existing
open models in the LLaMA family and the immense potential of reasoning and
addressing diverse tasks as an intelligent agent. Our findings provide valuable
insights into integrating natural and programming languages, laying a solid
foundation for developing advanced language agents that operate effectively in
various environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM Augmented LLMs: Expanding Capabilities through Composition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rachit Bansal, Bidisha Samanta, Siddharth Dalmia, Nitish Gupta, Shikhar Vashishth, Sriram Ganapathy, Abhishek Bapna, Prateek Jain, Partha Talukdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundational models with billions of parameters which have been trained on
large corpora of data have demonstrated non-trivial skills in a variety of
domains. However, due to their monolithic structure, it is challenging and
expensive to augment them or impart new skills. On the other hand, due to their
adaptation abilities, several new instances of these models are being trained
towards new domains and tasks. In this work, we study the problem of efficient
and practical composition of existing foundation models with more specific
models to enable newer capabilities. To this end, we propose CALM --
Composition to Augment Language Models -- which introduces cross-attention
between models to compose their representations and enable new capabilities.
Salient features of CALM are: (i) Scales up LLMs on new tasks by 're-using'
existing LLMs along with a few additional parameters and data, (ii) Existing
model weights are kept intact, and hence preserves existing capabilities, and
(iii) Applies to diverse domains and settings. We illustrate that augmenting
PaLM2-S with a smaller model trained on low-resource languages results in an
absolute improvement of up to 13\% on tasks like translation into English and
arithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is
augmented with a code-specific model, we see a relative improvement of 40\%
over the base model for code generation and explanation tasks -- on-par with
fully fine-tuned counterparts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 2 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TinyLlama: An Open-Source Small Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, Wei Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present TinyLlama, a compact 1.1B language model pretrained on around 1
trillion tokens for approximately 3 epochs. Building on the architecture and
tokenizer of Llama 2, TinyLlama leverages various advances contributed by the
open-source community (e.g., FlashAttention), achieving better computational
efficiency. Despite its relatively small size, TinyLlama demonstrates
remarkable performance in a series of downstream tasks. It significantly
outperforms existing open-source language models with comparable sizes. Our
model checkpoints and code are publicly available on GitHub at
https://github.com/jzhang38/TinyLlama.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPEER: Sentence-Level Planning of Long Clinical Summaries via Embedded
  Entity Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02369v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02369v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Griffin Adams, Jason Zucker, Noémie Elhadad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinician must write a lengthy summary each time a patient is discharged from
the hospital. This task is time-consuming due to the sheer number of unique
clinical concepts covered in the admission. Identifying and covering salient
entities is vital for the summary to be clinically useful. We fine-tune
open-source LLMs (Mistral-7B-Instruct and Zephyr-7B-\b{eta}) on the task and
find that they generate incomplete and unfaithful summaries. To increase entity
coverage, we train a smaller, encoder-only model to predict salient entities,
which are treated as content-plans to guide the LLM. To encourage the LLM to
focus on specific mentions in the source notes, we propose SPEER:
Sentence-level Planning via Embedded Entity Retrieval. Specifically, we mark
each salient entity span with special "{{ }}" boundary tags and instruct the
LLM to retrieve marked spans before generating each sentence. Sentence-level
planning acts as a form of state tracking in that the model is explicitly
recording the entities it uses. We fine-tune Mistral and Zephyr variants on a
large-scale, diverse dataset of ~167k in-patient hospital admissions and
evaluate on 3 datasets. SPEER shows gains in both coverage and faithfulness
metrics over non-guided and guided baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Extraction: Contextualising Tabular Data for Efficient
  Summarisation by Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02333v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02333v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Uday Allu, Biddwan Ahmed, Vishesh Tripathi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The conventional use of the Retrieval-Augmented Generation (RAG) architecture
has proven effective for retrieving information from diverse documents.
However, challenges arise in handling complex table queries, especially within
PDF documents containing intricate tabular structures.This research introduces
an innovative approach to enhance the accuracy of complex table queries in
RAG-based systems. Our methodology involves storing PDFs in the retrieval
database and extracting tabular content separately. The extracted tables
undergo a process of context enrichment, concatenating headers with
corresponding values. To ensure a comprehensive understanding of the enriched
data, we employ a fine-tuned version of the Llama-2-chat language model for
summarisation within the RAG architecture. Furthermore, we augment the tabular
data with contextual sense using the ChatGPT 3.5 API through a one-shot prompt.
This enriched data is then fed into the retrieval database alongside other
PDFs. Our approach aims to significantly improve the precision of complex table
queries, offering a promising solution to a longstanding challenge in
information retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLaVA-$φ$: Efficient Multi-Modal Assistant with Small Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichen Zhu, Minjie Zhu, Ning Liu, Zhicai Ou, Xiaofeng Mou, Jian Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce LLaVA-$\phi$ (LLaVA-Phi), an efficient
multi-modal assistant that harnesses the power of the recently advanced small
language model, Phi-2, to facilitate multi-modal dialogues. LLaVA-Phi marks a
notable advancement in the realm of compact multi-modal models. It demonstrates
that even smaller language models, with as few as 2.7B parameters, can
effectively engage in intricate dialogues that integrate both textual and
visual elements, provided they are trained with high-quality corpora. Our model
delivers commendable performance on publicly available benchmarks that
encompass visual comprehension, reasoning, and knowledge-based perception.
Beyond its remarkable performance in multi-modal dialogue tasks, our model
opens new avenues for applications in time-sensitive environments and systems
that require real-time interaction, such as embodied agents. It highlights the
potential of smaller language models to achieve sophisticated levels of
understanding and interaction, while maintaining greater resource
efficiency.The project is available at {https://github.com/zhuyiche/llava-phi}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>technique report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are LLMs Robust for Spoken Dialogues? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyed Mahed Mousavi, Gabriel Roccabruna, Simone Alghisi, Massimo Rizzoli, Mirco Ravanelli, Giuseppe Riccardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Pre-Trained Language Models have demonstrated state-of-the-art
performance in different downstream tasks, including dialogue state tracking
and end-to-end response generation. Nevertheless, most of the publicly
available datasets and benchmarks on task-oriented dialogues focus on written
conversations. Consequently, the robustness of the developed models to spoken
interactions is unknown. In this work, we have evaluated the performance of
LLMs for spoken task-oriented dialogues on the DSTC11 test sets. Due to the
lack of proper spoken dialogue datasets, we have automatically transcribed a
development set of spoken dialogues with a state-of-the-art ASR engine. We have
characterized the ASR-error types and their distributions and simulated these
errors in a large dataset of dialogues. We report the intrinsic (perplexity)
and extrinsic (human evaluation) performance of fine-tuned GPT-2 and T5 models
in two subtasks of response generation and dialogue state tracking,
respectively. The results show that LLMs are not robust to spoken noise by
default, however, fine-tuning/training such models on a proper dataset of
spoken TODs can result in a more robust performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Response Evaluation from Interlocutor's Eye for Open-Domain
  Dialogue Systems <span class="chip">AACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuma Tsuta, Naoki Yoshinaga, Shoetsu Sato, Masashi Toyoda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-domain dialogue systems have started to engage in continuous
conversations with humans. Those dialogue systems are required to be adjusted
to the human interlocutor and evaluated in terms of their perspective. However,
it is questionable whether the current automatic evaluation methods can
approximate the interlocutor's judgments. In this study, we analyzed and
examined what features are needed in an automatic response evaluator from the
interlocutor's perspective. The first experiment on the Hazumi dataset revealed
that interlocutor awareness plays a critical role in making automatic response
evaluation correlate with the interlocutor's judgments. The second experiment
using massive conversations on X (formerly Twitter) confirmed that dialogue
continuity prediction can train an interlocutor-aware response evaluator
without human feedback while revealing the difficulty in evaluating generated
responses compared to human responses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures, 5 tables, Accepted by IJCNLP-AACL 2023 SRW</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ L3Cube-IndicNews: News-based Short Text and Long Document Classification
  <span class="highlight-title">Dataset</span>s in Indic Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aishwarya Mirashi, Srushti Sonavane, Purva Lingayat, Tejas Padhiyar, Raviraj Joshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce L3Cube-IndicNews, a multilingual text
classification corpus aimed at curating a high-quality dataset for Indian
regional languages, with a specific focus on news headlines and articles. We
have centered our work on 10 prominent Indic languages, including Hindi,
Bengali, Marathi, Telugu, Tamil, Gujarati, Kannada, Odia, Malayalam, and
Punjabi. Each of these news datasets comprises 10 or more classes of news
articles. L3Cube-IndicNews offers 3 distinct datasets tailored to handle
different document lengths that are classified as: Short Headlines
Classification (SHC) dataset containing the news headline and news category,
Long Document Classification (LDC) dataset containing the whole news article
and the news category, and Long Paragraph Classification (LPC) containing
sub-articles of the news and the news category. We maintain consistent labeling
across all 3 datasets for in-depth length-based analysis. We evaluate each of
these Indic language datasets using 4 different models including monolingual
BERT, multilingual Indic Sentence BERT (IndicSBERT), and IndicBERT. This
research contributes significantly to expanding the pool of available text
classification datasets and also makes it possible to develop topic
classification models for Indian regional languages. This also serves as an
excellent resource for cross-lingual analysis owing to the high overlap of
labels among languages. The datasets and models are shared publicly at
https://github.com/l3cube-pune/indic-nlp
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the International Conference on Natural Language
  Processing (ICON 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Multi-Facts Reasoning Network For Complex Temporal Question
  Answering Over Knowledge Graph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rikui Huang, Wei Wei, Xiaoye Qu, Wenfeng Xie, Xianling Mao, Dangyang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal Knowledge Graph (TKG) is an extension of regular knowledge graph by
attaching the time scope. Existing temporal knowledge graph question answering
(TKGQA) models solely approach simple questions, owing to the prior assumption
that each question only contains a single temporal fact with explicit/implicit
temporal constraints. Hence, they perform poorly on questions which own
multiple temporal facts. In this paper, we propose \textbf{\underline{J}}oint
\textbf{\underline{M}}ulti \textbf{\underline{F}}acts
\textbf{\underline{R}}easoning \textbf{\underline{N}}etwork (JMFRN), to jointly
reasoning multiple temporal facts for accurately answering \emph{complex}
temporal questions. Specifically, JMFRN first retrieves question-related
temporal facts from TKG for each entity of the given complex question. For
joint reasoning, we design two different attention (\ie entity-aware and
time-aware) modules, which are suitable for universal settings, to aggregate
entities and timestamps information of retrieved facts. Moreover, to filter
incorrect type answers, we introduce an additional answer type discrimination
task. Extensive experiments demonstrate our proposed method significantly
outperforms the state-of-art on the well-known complex temporal question
benchmark TimeQuestions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DIALIGHT: Lightweight Multilingual Development and Evaluation of
  Task-Oriented Dialogue Systems with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songbo Hu, Xiaobin Wang, Zhangdie Yuan, Anna Korhonen, Ivan Vulić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present DIALIGHT, a toolkit for developing and evaluating multilingual
Task-Oriented Dialogue (ToD) systems which facilitates systematic evaluations
and comparisons between ToD systems using fine-tuning of Pretrained Language
Models (PLMs) and those utilising the zero-shot and in-context learning
capabilities of Large Language Models (LLMs). In addition to automatic
evaluation, this toolkit features (i) a secure, user-friendly web interface for
fine-grained human evaluation at both local utterance level and global dialogue
level, and (ii) a microservice-based backend, improving efficiency and
scalability. Our evaluations reveal that while PLM fine-tuning leads to higher
accuracy and coherence, LLM-based systems excel in producing diverse and
likeable responses. However, we also identify significant challenges of LLMs in
adherence to task-specific instructions and generating outputs in multiple
languages, highlighting areas for future research. We hope this open-sourced
toolkit will serve as a valuable resource for researchers aiming to develop and
properly evaluate multilingual ToD systems and will lower, currently still
high, entry barriers in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 tables, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Location Aware Modular Biencoder for Tourism Question Answering <span class="chip">AACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan Li, Martin Tomko, Timothy Baldwin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Answering real-world tourism questions that seek Point-of-Interest (POI)
recommendations is challenging, as it requires both spatial and non-spatial
reasoning, over a large candidate pool. The traditional method of encoding each
pair of question and POI becomes inefficient when the number of candidates
increases, making it infeasible for real-world applications. To overcome this,
we propose treating the QA task as a dense vector retrieval problem, where we
encode questions and POIs separately and retrieve the most relevant POIs for a
question by utilizing embedding space similarity. We use pretrained language
models (PLMs) to encode textual information, and train a location encoder to
capture spatial information of POIs. Experiments on a real-world tourism QA
dataset demonstrate that our approach is effective, efficient, and outperforms
previous methods across all metrics. Enabled by the dense retrieval
architecture, we further build a global evaluation baseline, expanding the
search space by 20 times compared to previous work. We also explore several
factors that impact on the model's performance through follow-up experiments.
Our code and model are publicly available at https://github.com/haonan-li/LAMB.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shayona@SMM4H23: COVID-19 Self diagnosis classification using <span class="highlight-title">BERT</span> and
  LightGBM models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02158v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02158v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rushi Chavda, Darshan Makwana, Vraj Patel, Anupam Shukla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes approaches and results for shared Task 1 and 4 of
SMMH4-23 by Team Shayona. Shared Task-1 was binary classification of english
tweets self-reporting a COVID-19 diagnosis, and Shared Task-4 was Binary
classification of English Reddit posts self-reporting a social anxiety disorder
diagnosis. Our team has achieved the highest f1-score 0.94 in Task-1 among all
participants. We have leveraged the Transformer model (BERT) in combination
with the LightGBM model for both tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Boundary of <span class="highlight-title">GPT</span>-4V on Marine Analysis: A Preliminary Case
  Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02147v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02147v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqiang Zheng, Yiwei Chen, Jipeng Zhang, Tuan-Anh Vu, Huimin Zeng, Yue Him Wong Tim, Sai-Kit Yeung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated a powerful ability to answer
various queries as a general-purpose assistant. The continuous multi-modal
large language models (MLLM) empower LLMs with the ability to perceive visual
signals. The launch of GPT-4 (Generative Pre-trained Transformers) has
generated significant interest in the research communities. GPT-4V(ison) has
demonstrated significant power in both academia and industry fields, as a focal
point in a new artificial intelligence generation. Though significant success
was achieved by GPT-4V, exploring MLLMs in domain-specific analysis (e.g.,
marine analysis) that required domain-specific knowledge and expertise has
gained less attention. In this study, we carry out the preliminary and
comprehensive case study of utilizing GPT-4V for marine analysis. This report
conducts a systematic evaluation of existing GPT-4V, assessing the performance
of GPT-4V on marine research and also setting a new standard for future
developments in MLLMs. The experimental results of GPT-4V show that the
responses generated by GPT-4V are still far away from satisfying the
domain-specific requirements of the marine professions. All images and prompts
used in this study will be available at
https://github.com/hkust-vgd/Marine_GPT-4V_Eval
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>51 pages, 36 figures, Repository:
  https://github.com/hkust-vgd/Marine_GPT-4V_Eval</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DCR-Consistency: Divide-Conquer-Reasoning for Consistency Evaluation and
  Improvement of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wendi Cui, Jiaxin Zhang, Zhuohang Li, Lopez Damien, Kamalika Das, Bradley Malin, Sricharan Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating the quality and variability of text generated by Large Language
Models (LLMs) poses a significant, yet unresolved research challenge.
Traditional evaluation methods, such as ROUGE and BERTScore, which measure
token similarity, often fail to capture the holistic semantic equivalence. This
results in a low correlation with human judgments and intuition, which is
especially problematic in high-stakes applications like healthcare and finance
where reliability, safety, and robust decision-making are highly critical. This
work proposes DCR, an automated framework for evaluating and improving the
consistency of LLM-generated texts using a divide-conquer-reasoning approach.
Unlike existing LLM-based evaluators that operate at the paragraph level, our
method employs a divide-and-conquer evaluator (DCE) that breaks down the
paragraph-to-paragraph comparison between two generated responses into
individual sentence-to-paragraph comparisons, each evaluated based on
predefined criteria. To facilitate this approach, we introduce an automatic
metric converter (AMC) that translates the output from DCE into an
interpretable numeric score. Beyond the consistency evaluation, we further
present a reason-assisted improver (RAI) that leverages the analytical reasons
with explanations identified by DCE to generate new responses aimed at reducing
these inconsistencies. Through comprehensive and systematic empirical analysis,
we show that our approach outperforms state-of-the-art methods by a large
margin (e.g., +19.3% and +24.3% on the SummEval dataset) in evaluating the
consistency of LLM generation across multiple benchmarks in semantic, factual,
and summarization consistency tasks. Our approach also substantially reduces
nearly 90% of output inconsistencies, showing promise for effective
hallucination mitigation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PEFT for Speech: Unveiling Optimal Placement, Merging Strategies, and
  Ensemble Techniques <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02122v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02122v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tzu-Han Lin, How-Shing Wang, Hao-Yung Weng, Kuang-Chen Peng, Zih-Ching Chen, Hung-yi Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-Efficient Fine-Tuning (PEFT) is increasingly recognized as an
effective method in speech processing. However, the optimal approach and the
placement of PEFT methods remain inconclusive. Our study conducts extensive
experiments to compare different PEFT methods and their layer-wise placement
adapting Differentiable Architecture Search (DARTS). We also explore the use of
ensemble learning to leverage diverse PEFT strategies. The results reveal that
DARTS does not outperform the baseline approach, which involves inserting the
same PEFT method into all layers of a Self-Supervised Learning (SSL) model. In
contrast, an ensemble learning approach, particularly one employing majority
voting, demonstrates superior performance. Our statistical evidence indicates
that different PEFT methods learn in varied ways. This variation might explain
why the synergistic integration of various PEFT methods through ensemble
learning can harness their unique learning capabilities more effectively
compared to individual layer-wise optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP 2024 Self-supervision in Audio, Speech and Beyond
  workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using LLM to select the right SQL Query from candidates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02115v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02115v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenwen Li, Tao Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-SQL models can generate a list of candidate SQL queries, and the best
query is often in the candidate list, but not at the top of the list. An
effective re-rank method can select the right SQL query from the candidate list
and improve the model's performance. Previous studies on code generation
automatically generate test cases and use them to re-rank candidate codes.
However, automatic test case generation for text-to-SQL is an understudied
field. We propose an automatic test case generation method that first generates
a database and then uses LLMs to predict the ground truth, which is the
expected execution results of the ground truth SQL query on this database. To
reduce the difficulty for LLMs to predict, we conduct experiments to search for
ways to generate easy databases for LLMs and design easy-to-understand prompts.
Based on our test case generation method, we propose a re-rank method to select
the right SQL query from the candidate list. Given a candidate list, our method
can generate test cases and re-rank the candidate list according to their pass
numbers on these test cases and their generation probabilities. The experiment
results on the validation dataset of Spider show that the performance of some
state-of-the-art models can get a 3.6\% improvement after applying our re-rank
method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Re-evaluating the Memory-balanced Pipeline Parallelism: BPipe 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mincong Huang, Chao Wang, Chi Ma, Yineng Zhang, Peng Zhang, Lei Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pipeline parallelism is an essential technique in the training of large-scale
Transformer models. However, it suffers from imbalanced memory consumption,
leading to insufficient memory utilization. The BPipe technique was proposed to
address this issue and has proven effective in the GPT-3 model. Nevertheless,
our experiments have not yielded similar benefits for LLaMA training.
Additionally, BPipe only yields negligible benefits for GPT-3 training when
applying flash attention. We analyze the underlying causes of the divergent
performance of BPipe on GPT-3 and LLaMA. Furthermore, we introduce a novel
method to estimate the performance of BPipe.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ICE-GRT: Instruction Context Enhancement by Generative Reinforcement
  based <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02072v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02072v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Zheng, Ke Sun, Da Tang, Yukun Ma, Yuyu Zhang, Chenguang Xi, Xun Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of Large Language Models (LLMs) such as ChatGPT and LLaMA
encounter limitations in domain-specific tasks, with these models often lacking
depth and accuracy in specialized areas, and exhibiting a decrease in general
capabilities when fine-tuned, particularly analysis ability in small sized
models. To address these gaps, we introduce ICE-GRT, utilizing Reinforcement
Learning from Human Feedback (RLHF) grounded in Proximal Policy Optimization
(PPO), demonstrating remarkable ability in in-domain scenarios without
compromising general task performance. Our exploration of ICE-GRT highlights
its understanding and reasoning ability to not only generate robust answers but
also to provide detailed analyses of the reasons behind the answer. This
capability marks a significant progression beyond the scope of Supervised
Fine-Tuning models. The success of ICE-GRT is dependent on several crucial
factors, including Appropriate Data, Reward Size Scaling, KL-Control, Advantage
Normalization, etc. The ICE-GRT model exhibits state-of-the-art performance in
domain-specific tasks and across 12 general Language tasks against equivalent
size and even larger size LLMs, highlighting the effectiveness of our approach.
We provide a comprehensive analysis of the ICE-GRT, underscoring the
significant advancements it brings to the field of LLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding LLMs: A Comprehensive <span class="highlight-title">Overview</span> from Training to Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02038v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02038v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiheng Liu, Hao He, Tianle Han, Xu Zhang, Mengyuan Liu, Jiaming Tian, Yutong Zhang, Jiaqi Wang, Xiaohui Gao, Tianyang Zhong, Yi Pan, Shaochen Xu, Zihao Wu, Zhengliang Liu, Xin Zhang, Shu Zhang, Xintao Hu, Tuo Zhang, Ning Qiang, Tianming Liu, Bao Ge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The introduction of ChatGPT has led to a significant increase in the
utilization of Large Language Models (LLMs) for addressing downstream tasks.
There's an increasing focus on cost-efficient training and deployment within
this context. Low-cost training and deployment of LLMs represent the future
development trend. This paper reviews the evolution of large language model
training techniques and inference deployment technologies aligned with this
emerging trend. The discussion on training includes various aspects, including
data preprocessing, training architecture, pre-training tasks, parallel
training, and relevant content related to model fine-tuning. On the inference
side, the paper covers topics such as model compression, parallel computation,
memory scheduling, and structural optimization. It also explores LLMs'
utilization and provides insights into their future development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages,6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text2MDT: Extracting Medical Decision Trees from Medical Texts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02034v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02034v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Zhu, Wenfeng Li, Xing Tian, Pengfei Wang, Xiaoling Wang, Jin Chen, Yuanbin Wu, Yuan Ni, Guotong Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge of the medical decision process, which can be modeled as medical
decision trees (MDTs), is critical to build clinical decision support systems.
However, the current MDT construction methods rely heavily on time-consuming
and laborious manual annotation. In this work, we propose a novel task,
Text2MDT, to explore the automatic extraction of MDTs from medical texts such
as medical guidelines and textbooks. We normalize the form of the MDT and
create an annotated Text-to-MDT dataset in Chinese with the participation of
medical experts. We investigate two different methods for the Text2MDT tasks:
(a) an end-to-end framework which only relies on a GPT style large language
models (LLM) instruction tuning to generate all the node information and tree
structures. (b) The pipeline framework which decomposes the Text2MDT task to
three subtasks. Experiments on our Text2MDT dataset demonstrate that: (a) the
end-to-end method basd on LLMs (7B parameters or larger) show promising
results, and successfully outperform the pipeline methods. (b) The
chain-of-thought (COT) prompting method \cite{Wei2022ChainOT} can improve the
performance of the fine-tuned LLMs on the Text2MDT test set. (c) the
lightweight pipelined method based on encoder-based pretrained models can
perform comparably with LLMs with model complexity two magnititudes smaller.
Our Text2MDT dataset is open-sourced at
\url{https://tianchi.aliyun.com/dataset/95414}, and the source codes are
open-sourced at \url{https://github.com/michael-wzhu/text2dt}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Contrast: Better Reflection Through Inconsistent Solving
  Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqi Zhang, Yongliang Shen, Linjuan Wu, Qiuying Peng, Jun Wang, Yueting Zhuang, Weiming Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The reflection capacity of Large Language Model (LLM) has garnered extensive
attention. A post-hoc prompting strategy, e.g., reflexion and self-refine,
refines LLM's response based on self-evaluated or external feedback. However,
recent research indicates without external feedback, LLM's intrinsic reflection
is unstable. Our investigation unveils that the key bottleneck is the quality
of the self-evaluated feedback. We find LLMs often exhibit overconfidence or
high randomness when self-evaluate, offering stubborn or inconsistent feedback,
which causes poor reflection. To remedy this, we advocate Self-Contrast: It
adaptively explores diverse solving perspectives tailored to the request,
contrasts the differences, and summarizes these discrepancies into a checklist
which could be used to re-examine and eliminate discrepancies. Our method
endows LLM with diverse perspectives to alleviate stubborn biases. Moreover,
their discrepancies indicate potential errors or inherent uncertainties that
LLM often overlooks. Reflecting upon these can catalyze more accurate and
stable reflection. Experiments conducted on a series of reasoning and
translation tasks with different LLMs serve to underscore the effectiveness and
generality of our strategy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Memory, Consciousness and Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02509v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02509v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jitang Li, Jinzheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development in cognitive science and Large Language Models (LLMs),
increasing connections have come to light between these two distinct fields.
Building upon these connections, we propose a conjecture suggesting the
existence of a duality between LLMs and Tulving's theory of memory. We identify
a potential correspondence between Tulving's synergistic ecphory model (SEM) of
retrieval and the emergent abilities observed in LLMs, serving as supporting
evidence for our conjecture. Furthermore, we speculate that consciousness may
be considered a form of emergent ability based on this duality. We also discuss
how other theories of consciousness intersect with our research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Language-Model Agents on Realistic Autonomous Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11671v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11671v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Megan Kinniment, Lucas Jun Koba Sato, Haoxing Du, Brian Goodrich, Max Hasin, Lawrence Chan, Luke Harold Miles, Tao R. Lin, Hjalmar Wijk, Joel Burget, Aaron Ho, Elizabeth Barnes, Paul Christiano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this report, we explore the ability of language model agents to acquire
resources, create copies of themselves, and adapt to novel challenges they
encounter in the wild. We refer to this cluster of capabilities as "autonomous
replication and adaptation" or ARA. We believe that systems capable of ARA
could have wide-reaching and hard-to-anticipate consequences, and that
measuring and forecasting ARA may be useful for informing measures around
security, monitoring, and alignment. Additionally, once a system is capable of
ARA, placing bounds on a system's capabilities may become significantly more
difficult.
  We construct four simple example agents that combine language models with
tools that allow them to take actions in the world. We then evaluate these
agents on 12 tasks relevant to ARA. We find that these language model agents
can only complete the easiest tasks from this list, although they make some
progress on the more challenging tasks. Unfortunately, these evaluations are
not adequate to rule out the possibility that near-future agents will be
capable of ARA. In particular, we do not think that these evaluations provide
good assurance that the ``next generation'' of language models (e.g. 100x
effective compute scaleup on existing models) will not yield agents capable of
ARA, unless intermediate evaluations are performed during pretraining.
Relatedly, we expect that fine-tuning of the existing models could produce
substantially more competent agents, even if the fine-tuning is not directly
targeted at ARA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One Shot Learning as Instruction Data Prospector for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10302v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10302v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunshui Li, Binyuan Hui, Xiaobo Xia, Jiaxi Yang, Min Yang, Lei Zhang, Shuzheng Si, Junhao Liu, Tongliang Liu, Fei Huang, Yongbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning large language models(LLMs) with human is a critical step in
effectively utilizing their pre-trained capabilities across a wide array of
language tasks. Current instruction tuning practices often rely on expanding
dataset size without a clear strategy for ensuring data quality, which can
inadvertently introduce noise and degrade model performance. To address this
challenge, we introduce Nuggets, a novel and efficient methodology that employs
one shot learning to select high-quality instruction data from expansive
datasets. Nuggets assesses the potential of individual instruction examples to
act as effective one shot examples, thereby identifying those that can
significantly enhance diverse task performance. Nuggets utilizes a scoring
system based on the impact of candidate examples on the perplexity of a diverse
anchor set, facilitating the selection of the most beneficial data for
instruction tuning. Through rigorous testing on two benchmarks, including
MT-Bench and Alpaca-Eval, we demonstrate that instruction tuning with the top
1% of Nuggets-curated examples substantially outperforms conventional methods
that use the full dataset. These findings advocate for a data selection
paradigm that prioritizes quality, offering a more efficient pathway to align
LLMs with humans.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vietnamese Poem Generation & The Prospect Of Cross-Language Poem-To-Poem
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01078v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01078v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Triet Minh Huynh, Quan Le Bao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Poetry generation has been a challenging task in the field of Natural
Language Processing, as it requires the model to understand the nuances of
language, sentiment, and style. In this paper, we propose using Large Language
Models to generate Vietnamese poems of various genres from natural language
prompts, thereby facilitating an intuitive process with enhanced content
control. Our most efficacious model, the GPT-3 Babbage variant, achieves a
custom evaluation score of 0.8, specifically tailored to the "luc bat" genre of
Vietnamese poetry. Furthermore, we also explore the idea of paraphrasing poems
into normal text prompts and yield a relatively high score of 0.781 in the "luc
bat" genre. This experiment presents the potential for cross-Language
poem-to-poem translation with translated poems as the inputs while concurrently
maintaining complete control over the generated content.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarial Data Poisoning for Fake News Detection: How to Make a Model
  Misclassify a Target News without Modifying It 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.15228v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.15228v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Siciliano, Luca Maiano, Lorenzo Papa, Federica Baccini, Irene Amerini, Fabrizio Silvestri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fake news detection models are critical to countering disinformation but can
be manipulated through adversarial attacks. In this position paper, we analyze
how an attacker can compromise the performance of an online learning detector
on specific news content without being able to manipulate the original target
news. In some contexts, such as social networks, where the attacker cannot
exert complete control over all the information, this scenario can indeed be
quite plausible. Therefore, we show how an attacker could potentially introduce
poisoning data into the training data to manipulate the behavior of an online
learning method. Our initial findings reveal varying susceptibility of logistic
regression models based on complexity and attack type.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How do media talk about the Covid-19 pandemic? Metaphorical thematic
  clustering in Italian online newspapers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.02106v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.02106v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucia Busso, Ottavia Tordini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The contribution presents a study on figurative language of the first months
of the COVID-19 crisis in Italian online newspapers. Particularly, we contrast
topics and metaphorical language used by journalists in the first and second
phase of the government response to the pandemic in Spring 2020. The analysis
is conducted on a journalistic corpus collected between February 24th and June
3rd, 2020. The analysis is performed using both quantitative and qualitative
approaches, combining Structural Topic Modelling (Roberts et al. 2016),
Conceptual Metaphor Theory (Lakoff & Johnson, 1980), and qualitative-corpus
based metaphor analysis (Charteris-Black, 2004). We find a significant shift in
topics discussed across Phase 1 and Phase 2, and interesting overlaps in
topic-specific metaphors. Using qualitative corpus analysis, we present a more
in-depth case study discussing metaphorical collocations of the topics of
Economy and Society
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unleashing the Emergent Cognitive Synergy in Large Language Models: A
  Task-Solving Agent through Multi-Persona Self-Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05300v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05300v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human intelligence thrives on cognitive synergy, where collaboration among
different minds yield superior outcomes compared to isolated individuals. In
this work, we propose Solo Performance Prompting (SPP), which transforms a
single LLM into a cognitive synergist by engaging in multi-turn
self-collaboration with multiple personas. A cognitive synergist is an
intelligent agent that collaboratively combines multiple minds' strengths and
knowledge to enhance problem-solving in complex tasks. By dynamically
identifying and simulating different personas based on task inputs, SPP
unleashes the potential of cognitive synergy in LLMs. Our in-depth analysis
shows that assigning multiple fine-grained personas in LLMs improves
problem-solving abilities compared to using a single or fixed number of
personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing,
Codenames Collaborative, and Logic Grid Puzzle, encompassing both
knowledge-intensive and reasoning-intensive types. Unlike previous works, such
as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs,
experimental results demonstrate that SPP effectively reduces factual
hallucination, and maintains strong reasoning capabilities. Additionally,
comparative experiments show that cognitive synergy only emerges in GPT-4 and
does not appear in less capable models, such as GPT-3.5-turbo and
Llama2-13b-chat, which draws an interesting analogy to human development. Code,
data, and prompts can be found at:
https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using
  EmotionBench 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.03656v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.03656v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jen-tse Huang, Man Ho Lam, Eric John Li, Shujie Ren, Wenxuan Wang, Wenxiang Jiao, Zhaopeng Tu, Michael R. Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating Large Language Models' (LLMs) anthropomorphic capabilities has
become increasingly important in contemporary discourse. Utilizing the emotion
appraisal theory from psychology, we propose to evaluate the empathy ability of
LLMs, i.e., how their feelings change when presented with specific situations.
After a careful and comprehensive survey, we collect a dataset containing over
400 situations that have proven effective in eliciting the eight emotions
central to our study. Categorizing the situations into 36 factors, we conduct a
human evaluation involving more than 1,200 subjects worldwide. With the human
evaluation results as references, our evaluation includes five LLMs, covering
both commercial and open-source models, including variations in model sizes,
featuring the latest iterations, such as GPT-4 and LLaMA-2. We find that,
despite several misalignments, LLMs can generally respond appropriately to
certain situations. Nevertheless, they fall short in alignment with the
emotional behaviors of human beings and cannot establish connections between
similar situations. Our collected dataset of situations, the human evaluation
results, and the code of our testing framework, dubbed EmotionBench, is made
openly accessible via https://github.com/CUHK-ARISE/EmotionBench. We aspire to
contribute to the advancement of LLMs regarding better alignment with the
emotional behaviors of human beings, thereby enhancing their utility and
applicability as intelligent assistants.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages. Added demographic distribution of the user study. Added
  ethics statements and limitations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Aligned Multimodal Learning for NER on Tweet Posts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.08372v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.08372v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peipei Liu, Hong Li, Yimo Ren, Jie Liu, Shuaizong Si, Hongsong Zhu, Limin Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mining structured knowledge from tweets using named entity recognition (NER)
can be beneficial for many down stream applications such as recommendation and
intention understanding. With tweet posts tending to be multimodal, multimodal
named entity recognition (MNER) has attracted more attention. In this paper, we
propose a novel approach, which can dynamically align the image and text
sequence and achieve the multi-level cross-modal learning to augment textual
word representation for MNER improvement. To be specific, our framework can be
split into three main stages: the first stage focuses on intra-modality
representation learning to derive the implicit global and local knowledge of
each modality, the second evaluates the relevance between the text and its
accompanying image and integrates different grained visual information based on
the relevance, the third enforces semantic refinement via iterative cross-modal
interactions and co-attention. We conduct experiments on two open datasets, and
the results and detailed analysis demonstrate the advantage of our model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UstanceBR: a multimodal language resource for stance prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06374v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06374v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Camila Pereira, Matheus Pavan, Sungwon Yoon, Ricelli Ramos, Pablo Costa, Lais Cavalheiro, Ivandre Paraboni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces UstanceBR, a multimodal corpus in the Brazilian
Portuguese Twitter domain for target-based stance prediction. The corpus
comprises 86.8 k labelled stances towards selected target topics, and extensive
network information about the users who published these stances on social
media. In this article we describe the corpus multimodal data, and a number of
usage examples in both in-domain and zero-shot stance prediction based on text-
and network-related information, which are intended to provide initial baseline
results for future studies in the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.04589v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.04589v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Yang, Yingxue Zhang, Fandong Meng, Jie Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite Multi-modal Large Language Models (MM-LLMs) have made exciting
strides recently, they are still struggling to efficiently model the
interactions among multi-modal inputs and the generation in non-textual
modalities. In this work, we propose TEAL (Tokenize and Embed ALl)}, an
approach to treat the input from any modality as a token sequence and learn a
joint embedding space for all modalities. Specifically, for the input from any
modality, TEAL first discretizes it into a token sequence with the
off-the-shelf tokenizer and embeds the token sequence into a joint embedding
space with a learnable embedding matrix. MM-LLMs just need to predict the
multi-modal tokens autoregressively as the textual LLMs do. Finally, the
corresponding de-tokenizer is applied to generate the output in each modality
based on the predicted token sequence. With the joint embedding space, TEAL
enables the frozen LLMs to perform both understanding and generation tasks
involving non-textual modalities, such as image and audio. Thus, the textual
LLM can just work as an interface and maintain its high performance in textual
understanding and generation. Experiments show that TEAL achieves substantial
improvements in multi-modal understanding, and implements a simple scheme for
multi-modal generations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Multi-modal, Large Language Models, Tokenizer, Understanding and
  Generation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ T-Eval: Evaluating the Tool Utilization Capability Step by Step 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14033v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14033v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehui Chen, Weihua Du, Wenwei Zhang, Kuikun Liu, Jiangning Liu, Miao Zheng, Jingming Zhuo, Songyang Zhang, Dahua Lin, Kai Chen, Feng Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLM) have achieved remarkable performance on various
NLP tasks and are augmented by tools for broader applications. Yet, how to
evaluate and analyze the tool-utilization capability of LLMs is still
under-explored. In contrast to previous works that evaluate models
holistically, we comprehensively decompose the tool utilization into multiple
sub-processes, including instruction following, planning, reasoning, retrieval,
understanding, and review. Based on that, we further introduce T-Eval to
evaluate the tool utilization capability step by step. T-Eval disentangles the
tool utilization evaluation into several sub-domains along model capabilities,
facilitating the inner understanding of both holistic and isolated competency
of LLMs. We conduct extensive experiments on T-Eval and in-depth analysis of
various LLMs. T-Eval not only exhibits consistency with the outcome-oriented
evaluation but also provides a more fine-grained analysis of the capabilities
of LLMs, providing a new perspective in LLM evaluation on tool-utilization
ability. The benchmark will be available at
https://github.com/open-compass/T-Eval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/open-compass/T-Eval; Website:
  https://open-compass.github.io/T-Eval</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video Understanding with Large Language Models: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17432v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17432v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan Liang, Teng Wang, Daoan Zhang, Jie An, Jingyang Lin, Rongyi Zhu, Ali Vosoughi, Chao Huang, Zeliang Zhang, Feng Zheng, Jianguo Zhang, Ping Luo, Jiebo Luo, Chenliang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the burgeoning growth of online video platforms and the escalating
volume of video content, the demand for proficient video understanding tools
has intensified markedly. Given the remarkable capabilities of Large Language
Models (LLMs) in language and multimodal tasks, this survey provides a detailed
overview of the recent advancements in video understanding harnessing the power
of LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly
advanced, particularly their ability for open-ended spatial-temporal reasoning
combined with commonsense knowledge, suggesting a promising path for future
video understanding. We examine the unique characteristics and capabilities of
Vid-LLMs, categorizing the approaches into four main types: LLM-based Video
Agents, Vid-LLMs Pretraining, Vid-LLMs Instruction Tuning, and Hybrid Methods.
Furthermore, this survey presents a comprehensive study of the tasks, datasets,
and evaluation methodologies for Vid-LLMs. Additionally, it explores the
expansive applications of Vid-LLMs across various domains, highlighting their
remarkable scalability and versatility in real-world video understanding
challenges. Finally, it summarizes the limitations of existing Vid-LLMs and
outlines directions for future research. For more information, readers are
recommended to visit the repository at
https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Theory of Hallucinations based on Equivariance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14504v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14504v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hisaichi Shibata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study aims to acquire knowledge for creating very large language models
that are immune to hallucinations. Hallucinations in contemporary large
language models are often attributed to a misunderstanding of real-world social
relationships. Therefore, I hypothesize that very large language models capable
of thoroughly grasping all these relationships will be free from
hallucinations. Additionally, I propose that certain types of equivariant
language models are adept at learning and understanding these relationships.
Building on this, I have developed a specialized cross-entropy error function
to create a hallucination scale for language models, which measures their
extent of equivariance acquisition. Utilizing this scale, I tested language
models for their ability to acquire character-level equivariance. In
particular, I introduce and employ a novel technique based on T5 (Text To Text
Transfer Transformer) that efficiently understands permuted input texts without
the need for explicit dictionaries to convert token IDs (integers) to texts
(strings). This T5 model demonstrated a moderate ability to acquire
character-level equivariance. Additionally, I discovered scale laws that can
aid in developing hallucination-free language models at the character level.
This methodology can be extended to assess equivariance acquisition at the word
level, paving the way for very large language models that can comprehensively
understand relationships and, consequently, avoid hallucinations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GIT-Mol: A Multi-modal Large Language Model for Molecular Science with
  Graph, Image, and Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.06911v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.06911v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengfei Liu, Yiming Ren, Zhixiang Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have made significant strides in natural language
processing, enabling innovative applications in molecular science by processing
textual representations of molecules. However, most existing language models
cannot capture the rich information with complex molecular structures or
images. In this paper, we introduce GIT-Mol, a multi-modal large language model
that integrates the Graph, Image, and Text information. To facilitate the
integration of multi-modal molecular data, we propose GIT-Former, a novel
architecture that is capable of aligning all modalities into a unified latent
space. We achieve a 5%-10% accuracy increase in properties prediction and a
20.2% boost in molecule generation validity compared to the baselines. With the
any-to-language molecular translation strategy, our model has the potential to
perform more downstream tasks, such as compound name recognition and chemical
reaction prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VideoChat: Chat-Centric Video Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.06355v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.06355v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we initiate an attempt of developing an end-to-end
chat-centric video understanding system, coined as VideoChat. It integrates
video foundation models and large language models via a learnable neural
interface, excelling in spatiotemporal reasoning, event localization, and
causal relationship inference. To instructively tune this system, we build a
video-centric instruction dataset, composed of thousands of videos associated
with detailed descriptions and conversations. This dataset emphasizes
spatiotemporal reasoning and captures causal relationships, providing a
valuable asset for training our chat-centric video understanding system.
Preliminary qualitative experiments demonstrate the potential of our system
across a broad spectrum of video applications, which could serve as a simple
prototype system for future research on chat-centric video understanding.
Access our code and data at https://github.com/OpenGVLab/Ask-Anything
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-target Stance Detection by Exploiting Target Analytical
  Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01761v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01761v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daijun Ding, Rong Chen, Liwen Jing, Bowen Zhang, Xu Huang, Li Dong, Xiaowen Zhao, Ge Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-target stance detection (CTSD) is an important task, which infers the
attitude of the destination target by utilizing annotated data derived from the
source target. One important approach in CTSD is to extract domain-invariant
features to bridge the knowledge gap between multiple targets. However, the
analysis of informal and short text structure, and implicit expressions,
complicate the extraction of domain-invariant knowledge. In this paper, we
propose a Multi-Perspective Prompt-Tuning (MPPT) model for CTSD that uses the
analysis perspective as a bridge to transfer knowledge. First, we develop a
two-stage instruct-based chain-of-thought method (TsCoT) to elicit target
analysis perspectives and provide natural language explanations (NLEs) from
multiple viewpoints by formulating instructions based on large language model
(LLM). Second, we propose a multi-perspective prompt-tuning framework
(MultiPLN) to fuse the NLEs into the stance predictor. Extensive experiments
results demonstrate the superiority of MPPT against the state-of-the-art
baseline methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interactive Text-to-SQL Generation via Editable Step-by-Step
  Explanations <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.07372v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.07372v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Tian, Zheng Zhang, Zheng Ning, Toby Jia-Jun Li, Jonathan K. Kummerfeld, Tianyi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relational databases play an important role in business, science, and more.
However, many users cannot fully unleash the analytical power of relational
databases, because they are not familiar with database languages such as SQL.
Many techniques have been proposed to automatically generate SQL from natural
language, but they suffer from two issues: (1) they still make many mistakes,
particularly for complex queries, and (2) they do not provide a flexible way
for non-expert users to validate and refine incorrect queries. To address these
issues, we introduce a new interaction mechanism that allows users to directly
edit a step-by-step explanation of a query to fix errors. Our experiments on
multiple datasets, as well as a user study with 24 participants, demonstrate
that our approach can achieve better performance than multiple SOTA approaches.
Our code and datasets are available at https://github.com/magic-YuanTian/STEPS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM in a flash: Efficient Large Language Model Inference with Limited
  Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11514v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11514v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keivan Alizadeh, Iman Mirzadeh, Dmitry Belenko, Karen Khatamifard, Minsik Cho, Carlo C Del Mundo, Mohammad Rastegari, Mehrdad Farajtabar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are central to modern natural language
processing, delivering exceptional performance in various tasks. However, their
substantial computational and memory requirements present challenges,
especially for devices with limited DRAM capacity. This paper tackles the
challenge of efficiently running LLMs that exceed the available DRAM capacity
by storing the model parameters in flash memory, but bringing them on demand to
DRAM. Our method involves constructing an inference cost model that takes into
account the characteristics of flash memory, guiding us to optimize in two
critical areas: reducing the volume of data transferred from flash and reading
data in larger, more contiguous chunks. Within this hardware-informed
framework, we introduce two principal techniques. First, "windowing"
strategically reduces data transfer by reusing previously activated neurons,
and second, "row-column bundling", tailored to the sequential data access
strengths of flash memory, increases the size of data chunks read from flash
memory. These methods collectively enable running models up to twice the size
of the available DRAM, with a 4-5x and 20-25x increase in inference speed
compared to naive loading approaches in CPU and GPU, respectively. Our
integration of sparsity awareness, context-adaptive loading, and a
hardware-oriented design paves the way for effective inference of LLMs on
devices with limited memory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">112</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to <span class="highlight-title">Prompt</span> with Text Only Supervision for Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02418v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02418v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Uzair Khattak, Muhammad Ferjad Naeem, Muzammal Naseer, Luc Van Gool, Federico Tombari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundational vision-language models such as CLIP are becoming a new paradigm
in vision, due to their excellent generalization abilities. However, adapting
these models for downstream tasks while maintaining their generalization
remains a challenge. In literature, one branch of methods adapts CLIP by
learning prompts using visual information. While effective, most of these works
require labeled data which is not practical, and often struggle to generalize
towards new datasets due to over-fitting on the source data. An alternative
approach resorts to training-free methods by generating class descriptions from
large language models (LLMs) and perform prompt ensembling. However, these
methods often generate class specific prompts that cannot be transferred to
other classes, which incur higher costs by generating LLM descriptions for each
class separately. In this work, we propose to combine the strengths of these
both streams of methods by learning prompts using only text data derived from
LLMs. As supervised training of prompts is not trivial due to absence of
images, we develop a training approach that allows prompts to extract rich
contextual knowledge from LLM data. Moreover, with LLM contextual data mapped
within the learned prompts, it enables zero-shot transfer of prompts to new
classes and datasets potentially cutting the LLM prompt engineering cost. To
the best of our knowledge, this is the first work that learns generalized
prompts using text only data. We perform extensive evaluations on 4 benchmarks
where our method improves over prior ensembling works while being competitive
to those utilizing labeled images. Our code and pre-trained models are
available at https://github.com/muzairkhattak/ProText.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://muzairkhattak.github.io/ProText/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ODIN: A Single Model for 2D and 3D Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02416v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02416v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayush Jain, Pushkal Katara, Nikolaos Gkanatsios, Adam W. Harley, Gabriel Sarch, Kriti Aggarwal, Vishrav Chaudhary, Katerina Fragkiadaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art models on contemporary 3D perception benchmarks like ScanNet
consume and label dataset-provided 3D point clouds, obtained through post
processing of sensed multiview RGB-D images. They are typically trained
in-domain, forego large-scale 2D pre-training and outperform alternatives that
featurize the posed RGB-D multiview images instead. The gap in performance
between methods that consume posed images versus post-processed 3D point clouds
has fueled the belief that 2D and 3D perception require distinct model
architectures. In this paper, we challenge this view and propose ODIN
(Omni-Dimensional INstance segmentation), a model that can segment and label
both 2D RGB images and 3D point clouds, using a transformer architecture that
alternates between 2D within-view and 3D cross-view information fusion. Our
model differentiates 2D and 3D feature operations through the positional
encodings of the tokens involved, which capture pixel coordinates for 2D patch
tokens and 3D coordinates for 3D feature tokens. ODIN achieves state-of-the-art
performance on ScanNet200, Matterport3D and AI2THOR 3D instance segmentation
benchmarks, and competitive performance on ScanNet, S3DIS and COCO. It
outperforms all previous works by a wide margin when the sensed 3D point cloud
is used in place of the point cloud sampled from 3D mesh. When used as the 3D
perception engine in an instructable embodied agent architecture, it sets a new
state-of-the-art on the TEACh action-from-dialogue benchmark. Our code and
checkpoints can be found at the project website: https://odin-seg.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bring Metric Functions into Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02414v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02414v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie An, Zhengyuan Yang, Jianfeng Wang, Linjie Li, Zicheng Liu, Lijuan Wang, Jiebo Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a Cascaded Diffusion Model (Cas-DM) that improves a Denoising
Diffusion Probabilistic Model (DDPM) by effectively incorporating additional
metric functions in training. Metric functions such as the LPIPS loss have been
proven highly effective in consistency models derived from the score matching.
However, for the diffusion counterparts, the methodology and efficacy of adding
extra metric functions remain unclear. One major challenge is the mismatch
between the noise predicted by a DDPM at each step and the desired clean image
that the metric function works well on. To address this problem, we propose
Cas-DM, a network architecture that cascades two network modules to effectively
apply metric functions to the diffusion model training. The first module,
similar to a standard DDPM, learns to predict the added noise and is unaffected
by the metric function. The second cascaded module learns to predict the clean
image, thereby facilitating the metric function computation. Experiment results
show that the proposed diffusion model backbone enables the effective use of
the LPIPS loss, leading to state-of-the-art image quality (FID, sFID, IS) on
various established benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM Augmented LLMs: Expanding Capabilities through Composition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rachit Bansal, Bidisha Samanta, Siddharth Dalmia, Nitish Gupta, Shikhar Vashishth, Sriram Ganapathy, Abhishek Bapna, Prateek Jain, Partha Talukdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundational models with billions of parameters which have been trained on
large corpora of data have demonstrated non-trivial skills in a variety of
domains. However, due to their monolithic structure, it is challenging and
expensive to augment them or impart new skills. On the other hand, due to their
adaptation abilities, several new instances of these models are being trained
towards new domains and tasks. In this work, we study the problem of efficient
and practical composition of existing foundation models with more specific
models to enable newer capabilities. To this end, we propose CALM --
Composition to Augment Language Models -- which introduces cross-attention
between models to compose their representations and enable new capabilities.
Salient features of CALM are: (i) Scales up LLMs on new tasks by 're-using'
existing LLMs along with a few additional parameters and data, (ii) Existing
model weights are kept intact, and hence preserves existing capabilities, and
(iii) Applies to diverse domains and settings. We illustrate that augmenting
PaLM2-S with a smaller model trained on low-resource languages results in an
absolute improvement of up to 13\% on tasks like translation into English and
arithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is
augmented with a code-specific model, we see a relative improvement of 40\%
over the base model for code generation and explanation tasks -- on-par with
fully fine-tuned counterparts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 2 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What You See is What You GAN: Rendering Every Pixel for High-Fidelity
  Geometry in 3D GANs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02411v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02411v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Trevithick, Matthew Chan, Towaki Takikawa, Umar Iqbal, Shalini De Mello, Manmohan Chandraker, Ravi Ramamoorthi, Koki Nagano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D-aware Generative Adversarial Networks (GANs) have shown remarkable
progress in learning to generate multi-view-consistent images and 3D geometries
of scenes from collections of 2D images via neural volume rendering. Yet, the
significant memory and computational costs of dense sampling in volume
rendering have forced 3D GANs to adopt patch-based training or employ
low-resolution rendering with post-processing 2D super resolution, which
sacrifices multiview consistency and the quality of resolved geometry.
Consequently, 3D GANs have not yet been able to fully resolve the rich 3D
geometry present in 2D images. In this work, we propose techniques to scale
neural volume rendering to the much higher resolution of native 2D images,
thereby resolving fine-grained 3D geometry with unprecedented detail. Our
approach employs learning-based samplers for accelerating neural rendering for
3D GAN training using up to 5 times fewer depth samples. This enables us to
explicitly "render every pixel" of the full-resolution image during training
and inference without post-processing superresolution in 2D. Together with our
strategy to learn high-quality surface geometry, our method synthesizes
high-resolution 3D geometry and strictly view-consistent images while
maintaining image quality on par with baselines relying on post-processing
super resolution. We demonstrate state-of-the-art 3D gemetric quality on FFHQ
and AFHQ, setting a new standard for unsupervised learning of 3D shapes in 3D
GANs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>See our project page: https://research.nvidia.com/labs/nxp/wysiwyg/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Open-Vocabulary Panoptic Segmentation with 2D-3D Vision-Language
  Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Xiao, Longlong Jing, Shangxuan Wu, Alex Zihao Zhu, Jingwei Ji, Chiyu Max Jiang, Wei-Chih Hung, Thomas Funkhouser, Weicheng Kuo, Anelia Angelova, Yin Zhou, Shiwei Sheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D panoptic segmentation is a challenging perception task, which aims to
predict both semantic and instance annotations for 3D points in a scene.
Although prior 3D panoptic segmentation approaches have achieved great
performance on closed-set benchmarks, generalizing to novel categories remains
an open problem. For unseen object categories, 2D open-vocabulary segmentation
has achieved promising results that solely rely on frozen CLIP backbones and
ensembling multiple classification outputs. However, we find that simply
extending these 2D models to 3D does not achieve good performance due to poor
per-mask classification quality on novel categories. In this paper, we propose
the first method to tackle 3D open-vocabulary panoptic segmentation. Our model
takes advantage of the fusion between learnable LiDAR features and dense frozen
vision CLIP features, using a single classification head to make predictions
for both base and novel classes. To further improve the classification
performance on novel classes and leverage the CLIP model, we propose two novel
loss functions: object-level distillation loss and voxel-level distillation
loss. Our experiments on the nuScenes and SemanticKITTI datasets show that our
method outperforms strong baselines by a large margin.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning the 3D Fauna of the Web 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02400v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02400v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zizhang Li, Dor Litvak, Ruining Li, Yunzhi Zhang, Tomas Jakab, Christian Rupprecht, Shangzhe Wu, Andrea Vedaldi, Jiajun Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning 3D models of all animals on the Earth requires massively scaling up
existing solutions. With this ultimate goal in mind, we develop 3D-Fauna, an
approach that learns a pan-category deformable 3D animal model for more than
100 animal species jointly. One crucial bottleneck of modeling animals is the
limited availability of training data, which we overcome by simply learning
from 2D Internet images. We show that prior category-specific attempts fail to
generalize to rare species with limited training images. We address this
challenge by introducing the Semantic Bank of Skinned Models (SBSM), which
automatically discovers a small set of base animal shapes by combining
geometric inductive priors with semantic knowledge implicitly captured by an
off-the-shelf self-supervised feature extractor. To train such a model, we also
contribute a new large-scale dataset of diverse animal species. At inference
time, given a single image of any quadruped animal, our model reconstructs an
articulated 3D mesh in a feed-forward fashion within seconds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally to this work. The last
  three authors contributed equally. Project page:
  https://kyleleey.github.io/3DFauna/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChartAssisstant: A Universal Chart Multimodal Language Model via
  Chart-to-Table <span class="highlight-title">Pre-train</span>ing and Multitask Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fanqing Meng, Wenqi Shao, Quanfeng Lu, Peng Gao, Kaipeng Zhang, Yu Qiao, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Charts play a vital role in data visualization, understanding data patterns,
and informed decision-making. However, their unique combination of graphical
elements (e.g., bars, lines) and textual components (e.g., labels, legends)
poses challenges for general-purpose multimodal models. While vision-language
models trained on chart data excel in comprehension, they struggle with
generalization and require task-specific fine-tuning. To address these
challenges, we propose ChartAssistant, a chart-based vision-language model for
universal chart comprehension and reasoning. ChartAssistant leverages ChartSFT,
a comprehensive dataset covering diverse chart-related tasks with basic and
specialized chart types. It undergoes a two-stage training process, starting
with pre-training on chart-to-table parsing to align chart and text, followed
by multitask instruction-following fine-tuning. This approach enables
ChartAssistant to achieve competitive performance across various chart tasks
without task-specific fine-tuning. Experimental results demonstrate significant
performance gains over the state-of-the-art UniChart method, outperforming
OpenAI's GPT-4V(ision) on real-world chart data. The code and data are
available at https://github.com/OpenGVLab/ChartAst.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Survey</span> of 3D Human Body Pose and Shape Estimation Methods for
  Contemporary Dance Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Darshan Venkatrayappa, Alain Tremeau, Damien Muselet, Philippe Colantoni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D human body shape and pose estimation from RGB images is a challenging
problem with potential applications in augmented/virtual reality, healthcare
and fitness technology and virtual retail. Recent solutions have focused on
three types of inputs: i) single images, ii) multi-view images and iii) videos.
In this study, we surveyed and compared 3D body shape and pose estimation
methods for contemporary dance and performing arts, with a special focus on
human body pose and dressing, camera viewpoint, illumination conditions and
background conditions. We demonstrated that multi-frame methods, such as PHALP,
provide better results than single-frame method for pose estimation when
dancers are performing contemporary dances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2008.09062 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Open and Comprehensive Pipeline for Unified Object Grounding and
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyu Zhao, Yicheng Chen, Shilin Xu, Xiangtai Li, Xinjiang Wang, Yining Li, Haian Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grounding-DINO is a state-of-the-art open-set detection model that tackles
multiple vision tasks including Open-Vocabulary Detection (OVD), Phrase
Grounding (PG), and Referring Expression Comprehension (REC). Its effectiveness
has led to its widespread adoption as a mainstream architecture for various
downstream applications. However, despite its significance, the original
Grounding-DINO model lacks comprehensive public technical details due to the
unavailability of its training code. To bridge this gap, we present
MM-Grounding-DINO, an open-source, comprehensive, and user-friendly baseline,
which is built with the MMDetection toolbox. It adopts abundant vision datasets
for pre-training and various detection and grounding datasets for fine-tuning.
We give a comprehensive analysis of each reported result and detailed settings
for reproduction. The extensive experiments on the benchmarks mentioned
demonstrate that our MM-Grounding-DINO-Tiny outperforms the Grounding-DINO-Tiny
baseline. We release all our models to the research community. Codes and
trained models are released at
https://github.com/open-mmlab/mmdetection/configs/mm_grounding_dino.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A novel method to enhance pneumonia detection via a model-level
  ensembling of CNN and vision <span class="highlight-title">transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02358v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02358v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sandeep Angara, Nishith Reddy Mannuru, Aashrith Mannuru, Sharath Thirunagaru
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pneumonia remains a leading cause of morbidity and mortality worldwide. Chest
X-ray (CXR) imaging is a fundamental diagnostic tool, but traditional analysis
relies on time-intensive expert evaluation. Recently, deep learning has shown
immense potential for automating pneumonia detection from CXRs. This paper
explores applying neural networks to improve CXR-based pneumonia diagnosis. We
developed a novel model fusing Convolution Neural networks (CNN) and Vision
Transformer networks via model-level ensembling. Our fusion architecture
combines a ResNet34 variant and a Multi-Axis Vision Transformer small model.
Both base models are initialized with ImageNet pre-trained weights. The output
layers are removed, and features are combined using a flattening layer before
final classification. Experiments used the Kaggle pediatric pneumonia dataset
containing 1,341 normal and 3,875 pneumonia CXR images. We compared our model
against standalone ResNet34, Vision Transformer, and Swin Transformer Tiny
baseline models using identical training procedures. Extensive data
augmentation, Adam optimization, learning rate warmup, and decay were employed.
The fusion model achieved a state-of-the-art accuracy of 94.87%, surpassing the
baselines. We also attained excellent sensitivity, specificity, kappa score,
and positive predictive value. Confusion matrix analysis confirms fewer
misclassifications. The ResNet34 and Vision Transformer combination enables
jointly learning robust features from CNNs and Transformer paradigms. This
model-level ensemble technique effectively integrates their complementary
strengths for enhanced pneumonia classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fit-NGP: Fitting Object Models to Neural Graphics Primitives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02357v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02357v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marwan Taher, Ignacio Alzugaray, Andrew J. Davison
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate 3D object pose estimation is key to enabling many robotic
applications that involve challenging object interactions. In this work, we
show that the density field created by a state-of-the-art efficient radiance
field reconstruction method is suitable for highly accurate and robust pose
estimation for objects with known 3D models, even when they are very small and
with challenging reflective surfaces. We present a fully automatic object pose
estimation system based on a robot arm with a single wrist-mounted camera,
which can scan a scene from scratch, detect and estimate the 6-Degrees of
Freedom (DoF) poses of multiple objects within a couple of minutes of
operation. Small objects such as bolts and nuts are estimated with accuracy on
order of 1mm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mining Fine-Grained Image-Text Alignment for Zero-Shot Captioning via
  Text-Only Training <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longtian Qiu, Shan Ning, Xuming He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image captioning aims at generating descriptive and meaningful textual
descriptions of images, enabling a broad range of vision-language applications.
Prior works have demonstrated that harnessing the power of Contrastive Image
Language Pre-training (CLIP) offers a promising approach to achieving zero-shot
captioning, eliminating the need for expensive caption annotations. However,
the widely observed modality gap in the latent space of CLIP harms the
performance of zero-shot captioning by breaking the alignment between paired
image-text features. To address this issue, we conduct an analysis on the CLIP
latent space which leads to two findings. Firstly, we observe that the CLIP's
visual feature of image subregions can achieve closer proximity to the paired
caption due to the inherent information loss in text descriptions. In addition,
we show that the modality gap between a paired image-text can be empirically
modeled as a zero-mean Gaussian distribution. Motivated by the findings, we
propose a novel zero-shot image captioning framework with text-only training to
reduce the modality gap. In particular, we introduce a subregion feature
aggregation to leverage local region information, which produces a compact
visual representation for matching text representation. Moreover, we
incorporate a noise injection and CLIP reranking strategy to boost captioning
performance. We also extend our framework to build a zero-shot VQA pipeline,
demonstrating its generality. Through extensive experiments on common
captioning and VQA datasets such as MSCOCO, Flickr30k and VQAV2, we show that
our method achieves remarkable performance improvements. Code is available at
https://github.com/Artanic30/MacCap.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2024.Open sourced, Code and Model Available</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Linguistic Profiling of Deepfakes: An Open Database for Next-Generation
  Deepfake Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yabin Wang, Zhiwu Huang, Zhiheng Ma, Xiaopeng Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of text-to-image generative models has revolutionized the field
of deepfakes, enabling the creation of realistic and convincing visual content
directly from textual descriptions. However, this advancement presents
considerably greater challenges in detecting the authenticity of such content.
Existing deepfake detection datasets and methods often fall short in
effectively capturing the extensive range of emerging deepfakes and offering
satisfactory explanatory information for detection. To address the significant
issue, this paper introduces a deepfake database (DFLIP-3K) for the development
of convincing and explainable deepfake detection. It encompasses about 300K
diverse deepfake samples from approximately 3K generative models, which boasts
the largest number of deepfake models in the literature. Moreover, it collects
around 190K linguistic footprints of these deepfakes. The two distinguished
features enable DFLIP-3K to develop a benchmark that promotes progress in
linguistic profiling of deepfakes, which includes three sub-tasks namely
deepfake detection, model identification, and prompt prediction. The deepfake
model and prompt are two essential components of each deepfake, and thus
dissecting them linguistically allows for an invaluable exploration of
trustworthy and interpretable evidence in deepfake detection, which we believe
is the key for the next-generation deepfake detection. Furthermore, DFLIP-3K is
envisioned as an open database that fosters transparency and encourages
collaborative efforts to further enhance its growth. Our extensive experiments
on the developed benchmark verify that our DFLIP-3K database is capable of
serving as a standardized resource for evaluating and comparing
linguistic-based deepfake detection, identification, and prompt prediction
techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLaVA-$φ$: Efficient Multi-Modal Assistant with Small Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichen Zhu, Minjie Zhu, Ning Liu, Zhicai Ou, Xiaofeng Mou, Jian Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce LLaVA-$\phi$ (LLaVA-Phi), an efficient
multi-modal assistant that harnesses the power of the recently advanced small
language model, Phi-2, to facilitate multi-modal dialogues. LLaVA-Phi marks a
notable advancement in the realm of compact multi-modal models. It demonstrates
that even smaller language models, with as few as 2.7B parameters, can
effectively engage in intricate dialogues that integrate both textual and
visual elements, provided they are trained with high-quality corpora. Our model
delivers commendable performance on publicly available benchmarks that
encompass visual comprehension, reasoning, and knowledge-based perception.
Beyond its remarkable performance in multi-modal dialogue tasks, our model
opens new avenues for applications in time-sensitive environments and systems
that require real-time interaction, such as embodied agents. It highlights the
potential of smaller language models to achieve sophisticated levels of
understanding and interaction, while maintaining greater resource
efficiency.The project is available at {https://github.com/zhuyiche/llava-phi}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>technique report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ClassWise-SAM-Adapter: Parameter Efficient Fine-tuning Adapts Segment
  Anything to SAR Domain for Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyang Pu, Hecheng Jia, Linghao Zheng, Feng Wang, Feng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of artificial intelligence, the emergence of foundation models,
backed by high computing capabilities and extensive data, has been
revolutionary. Segment Anything Model (SAM), built on the Vision Transformer
(ViT) model with millions of parameters and vast training dataset SA-1B, excels
in various segmentation scenarios relying on its significance of semantic
information and generalization ability. Such achievement of visual foundation
model stimulates continuous researches on specific downstream tasks in computer
vision. The ClassWise-SAM-Adapter (CWSAM) is designed to adapt the
high-performing SAM for landcover classification on space-borne Synthetic
Aperture Radar (SAR) images. The proposed CWSAM freezes most of SAM's
parameters and incorporates lightweight adapters for parameter efficient
fine-tuning, and a classwise mask decoder is designed to achieve semantic
segmentation task. This adapt-tuning method allows for efficient landcover
classification of SAR images, balancing the accuracy with computational demand.
In addition, the task specific input module injects low frequency information
of SAR images by MLP-based layers to improve the model performance. Compared to
conventional state-of-the-art semantic segmentation algorithms by extensive
experiments, CWSAM showcases enhanced performance with fewer computing
resources, highlighting the potential of leveraging foundational models like
SAM for specific downstream tasks in the SAR domain. The source code is
available at: https://github.com/xypu98/CWSAM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BA-SAM: Scalable Bias-Mode Attention Mask for Segment Anything Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiran Song, Qianyu Zhou, Xiangtai Li, Deng-Ping Fan, Xuequan Lu, Lizhuang Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address the challenge of image resolution variation for the
Segment Anything Model (SAM). SAM, known for its zero-shot generalizability,
exhibits a performance degradation when faced with datasets with varying image
sizes. Previous approaches tend to resize the image to a fixed size or adopt
structure modifications, hindering the preservation of SAM's rich prior
knowledge. Besides, such task-specific tuning necessitates a complete
retraining of the model, which is cost-expensive and unacceptable for
deployment in the downstream tasks. In this paper, we reformulate this issue as
a length extrapolation problem, where token sequence length varies while
maintaining a consistent patch size for images of different sizes. To this end,
we propose Scalable Bias-Mode Attention Mask (BA-SAM) to enhance SAM's
adaptability to varying image resolutions while eliminating the need for
structure modifications. Firstly, we introduce a new scaling factor to ensure
consistent magnitude in the attention layer's dot product values when the token
sequence length changes. Secondly, we present a bias-mode attention mask that
allows each token to prioritize neighboring information, mitigating the impact
of untrained distant information. Our BA-SAM demonstrates efficacy in two
scenarios: zero-shot and fine-tuning. Extensive evaluation on diverse datasets,
including DIS5K, DUTS, ISIC, COD10K, and COCO, reveals its ability to
significantly mitigate performance degradation in the zero-shot setting and
achieve state-of-the-art performance with minimal fine-tuning. Furthermore, we
propose a generalized model and benchmark, showcasing BA-SAM's generalizability
across all four datasets simultaneously.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SuperEdge: Towards a Generalization Model for <span class="highlight-title">Self-Supervised</span> Edge
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leng Kai, Zhang Zhijie, Liu Jie, Zed Boukhers, Sui Wei, Cong Yang, Li Zhijun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Edge detection is a fundamental technique in various computer vision tasks.
Edges are indeed effectively delineated by pixel discontinuity and can offer
reliable structural information even in textureless areas. State-of-the-art
heavily relies on pixel-wise annotations, which are labor-intensive and subject
to inconsistencies when acquired manually. In this work, we propose a novel
self-supervised approach for edge detection that employs a multi-level,
multi-homography technique to transfer annotations from synthetic to real-world
datasets. To fully leverage the generated edge annotations, we developed
SuperEdge, a streamlined yet efficient model capable of concurrently extracting
edges at pixel-level and object-level granularity. Thanks to self-supervised
training, our method eliminates the dependency on manual annotated edge labels,
thereby enhancing its generalizability across diverse datasets. Comparative
evaluations reveal that SuperEdge advances edge detection, demonstrating
improvements of 4.9% in ODS and 3.3% in OIS over the existing STEdge method on
BIPEDv2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TR-DETR: Task-Reciprocal <span class="highlight-title">Transformer</span> for Joint Moment Retrieval and
  Highlight Detection <span class="chip">AAAI-24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Sun, Mingyao Zhou, Wenjing Chen, Wei Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video moment retrieval (MR) and highlight detection (HD) based on natural
language queries are two highly related tasks, which aim to obtain relevant
moments within videos and highlight scores of each video clip. Recently,
several methods have been devoted to building DETR-based networks to solve both
MR and HD jointly. These methods simply add two separate task heads after
multi-modal feature extraction and feature interaction, achieving good
performance. Nevertheless, these approaches underutilize the reciprocal
relationship between two tasks. In this paper, we propose a task-reciprocal
transformer based on DETR (TR-DETR) that focuses on exploring the inherent
reciprocity between MR and HD. Specifically, a local-global multi-modal
alignment module is first built to align features from diverse modalities into
a shared latent space. Subsequently, a visual feature refinement is designed to
eliminate query-irrelevant information from visual features for modal
interaction. Finally, a task cooperation module is constructed to refine the
retrieval pipeline and the highlight score prediction process by utilizing the
reciprocity between MR and HD. Comprehensive experiments on QVHighlights,
Charades-STA and TVSum datasets demonstrate that TR-DETR outperforms existing
state-of-the-art methods. Codes are available at
\url{https://github.com/mingyao1120/TR-DETR}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI-24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GridFormer: Point-Grid <span class="highlight-title">Transformer</span> for Surface Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengtao Li, Ge Gao, Yudong Liu, Yu-Shen Liu, Ming Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit neural networks have emerged as a crucial technology in 3D surface
reconstruction. To reconstruct continuous surfaces from discrete point clouds,
encoding the input points into regular grid features (plane or volume) has been
commonly employed in existing approaches. However, these methods typically use
the grid as an index for uniformly scattering point features. Compared with the
irregular point features, the regular grid features may sacrifice some
reconstruction details but improve efficiency. To take full advantage of these
two types of features, we introduce a novel and high-efficiency attention
mechanism between the grid and point features named Point-Grid Transformer
(GridFormer). This mechanism treats the grid as a transfer point connecting the
space and point cloud. Our method maximizes the spatial expressiveness of grid
features and maintains computational efficiency. Furthermore, optimizing
predictions over the entire space could potentially result in blurred
boundaries. To address this issue, we further propose a boundary optimization
strategy incorporating margin binary cross-entropy loss and boundary sampling.
This approach enables us to achieve a more precise representation of the object
structure. Our experiments validate that our method is effective and
outperforms the state-of-the-art approaches under widely used benchmarks by
producing more precise geometry reconstructions. The code is available at
https://github.com/list17/GridFormer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distillation-based fabric anomaly detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02287v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02287v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Thomine, Hichem Snoussi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised texture anomaly detection has been a concerning topic in a vast
amount of industrial processes. Patterned textures inspection, particularly in
the context of fabric defect detection, is indeed a widely encountered use
case. This task involves handling a diverse spectrum of colors and textile
types, encompassing a wide range of fabrics. Given the extensive variability in
colors, textures, and defect types, fabric defect detection poses a complex and
challenging problem in the field of patterned textures inspection. In this
article, we propose a knowledge distillation-based approach tailored
specifically for addressing the challenge of unsupervised anomaly detection in
textures resembling fabrics. Our method aims to redefine the recently
introduced reverse distillation approach, which advocates for an
encoder-decoder design to mitigate classifier bias and to prevent the student
from reconstructing anomalies. In this study, we present a new reverse
distillation technique for the specific task of fabric defect detection. Our
approach involves a meticulous design selection that strategically highlights
high-level features. To demonstrate the capabilities of our approach both in
terms of performance and inference speed, we conducted a series of experiments
on multiple texture datasets, including MVTEC AD, AITEX, and TILDA, alongside
conducting experiments on a dataset acquired from a textile manufacturing
facility. The main contributions of this paper are the following: a robust
texture anomaly detector utilizing a reverse knowledge-distillation technique
suitable for both anomaly detection and domain generalization and a novel
dataset encompassing a diverse range of fabrics and defects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Textile Research Journal. 2023;0(0)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PEGASUS: Physically Enhanced Gaussian Splatting Simulation System for
  6DOF Object Pose <span class="highlight-title">Dataset</span> Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Meyer, Floris Erich, Yusuke Yoshiyasu, Marc Stamminger, Noriaki Ando, Yukiyasu Domae
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Physically Enhanced Gaussian Splatting Simulation System
(PEGASUS) for 6DOF object pose dataset generation, a versatile dataset
generator based on 3D Gaussian Splatting. Environment and object
representations can be easily obtained using commodity cameras to reconstruct
with Gaussian Splatting. PEGASUS allows the composition of new scenes by
merging the respective underlying Gaussian Splatting point cloud of an
environment with one or multiple objects. Leveraging a physics engine enables
the simulation of natural object placement within a scene through interaction
between meshes extracted for the objects and the environment. Consequently, an
extensive amount of new scenes - static or dynamic - can be created by
combining different environments and objects. By rendering scenes from various
perspectives, diverse data points such as RGB images, depth maps, semantic
masks, and 6DoF object poses can be extracted. Our study demonstrates that
training on data generated by PEGASUS enables pose estimation networks to
successfully transfer from synthetic data to real-world data. Moreover, we
introduce the Ramen dataset, comprising 30 Japanese cup noodle items. This
dataset includes spherical scans that captures images from both object
hemisphere and the Gaussian Splatting reconstruction, making them compatible
with PEGASUS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://meyerls.github.io/pegasus_web</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lightweight Fish Classification Model for Sustainable Marine Management:
  Indonesian Case 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Febrian Kurniawan, Gandeva Bayu Satrya, Firuz Kamalov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The enormous demand for seafood products has led to exploitation of marine
resources and near-extinction of some species. In particular, overfishing is
one the main issues in sustainable marine development. In alignment with the
protection of marine resources and sustainable fishing, this study proposes to
advance fish classification techniques that support identifying protected fish
species using state-of-the-art machine learning. We use a custom modification
of the MobileNet model to design a lightweight classifier called M-MobileNet
that is capable of running on limited hardware. As part of the study, we
compiled a labeled dataset of 37,462 images of fish found in the waters of the
Indonesian archipelago. The proposed model is trained on the dataset to
classify images of the captured fish into their species and give
recommendations on whether they are consumable or not. Our modified MobileNet
model uses only 50\% of the top layer parameters with about 42% GTX 860M
utility and achieves up to 97% accuracy in fish classification and determining
its consumability. Given the limited computing capacity available on many
fishing vessels, the proposed model provides a practical solution to on-site
fish classification. In addition, synchronized implementation of the proposed
model on multiple vessels can supply valuable information about the movement
and location of different species of fish.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ShapeAug: Occlusion Augmentation for Event Camera Data <span class="chip">ICPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katharina Bendig, René Schuster, Didier Stricker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Dynamic Vision Sensors (DVSs) sparked a lot of interest due to
their inherent advantages over conventional RGB cameras. These advantages
include a low latency, a high dynamic range and a low energy consumption.
Nevertheless, the processing of DVS data using Deep Learning (DL) methods
remains a challenge, particularly since the availability of event training data
is still limited. This leads to a need for event data augmentation techniques
in order to improve accuracy as well as to avoid over-fitting on the training
data. Another challenge especially in real world automotive applications is
occlusion, meaning one object is hindering the view onto the object behind it.
In this paper, we present a novel event data augmentation approach, which
addresses this problem by introducing synthetic events for randomly moving
objects in a scene. We test our method on multiple DVS classification datasets,
resulting in an relative improvement of up to 6.5 % in top1-accuracy. Moreover,
we apply our augmentation technique on the real world Gen1 Automotive Event
Dataset for object detection, where we especially improve the detection of
pedestrians by up to 5 %.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICPRAM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Slot-guided Volumetric Object Radiance Fields <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Di Qi, Tong Yang, Xiangyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel framework for 3D object-centric representation learning.
Our approach effectively decomposes complex scenes into individual objects from
a single image in an unsupervised fashion. This method, called slot-guided
Volumetric Object Radiance Fields (sVORF), composes volumetric object radiance
fields with object slots as a guidance to implement unsupervised 3D scene
decomposition. Specifically, sVORF obtains object slots from a single image via
a transformer module, maps these slots to volumetric object radiance fields
with a hypernetwork and composes object radiance fields with the guidance of
object slots at a 3D location. Moreover, sVORF significantly reduces memory
requirement due to small-sized pixel rendering during training. We demonstrate
the effectiveness of our approach by showing top results in scene decomposition
and generation tasks of complex synthetic datasets (e.g., Room-Diverse).
Furthermore, we also confirm the potential of sVORF to segment objects in
real-world scenes (e.g., the LLFF dataset). We hope our approach can provide
preliminary understanding of the physical world and help ease future research
in 3D object-centric representation learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nodule detection and generation on chest X-rays: NODE21 Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ecem Sogancioglu, Bram van Ginneken, Finn Behrendt, Marcel Bengs, Alexander Schlaefer, Miron Radu, Di Xu, Ke Sheng, Fabien Scalzo, Eric Marcus, Samuele Papa, Jonas Teuwen, Ernst Th. Scholten, Steven Schalekamp, Nils Hendrix, Colin Jacobs, Ward Hendrix, Clara I Sánchez, Keelin Murphy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pulmonary nodules may be an early manifestation of lung cancer, the leading
cause of cancer-related deaths among both men and women. Numerous studies have
established that deep learning methods can yield high-performance levels in the
detection of lung nodules in chest X-rays. However, the lack of gold-standard
public datasets slows down the progression of the research and prevents
benchmarking of methods for this task. To address this, we organized a public
research challenge, NODE21, aimed at the detection and generation of lung
nodules in chest X-rays. While the detection track assesses state-of-the-art
nodule detection systems, the generation track determines the utility of nodule
generation algorithms to augment training data and hence improve the
performance of the detection systems. This paper summarizes the results of the
NODE21 challenge and performs extensive additional experiments to examine the
impact of the synthetically generated nodule training images on the detection
algorithm performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span> Decoupling for Text-to-Image Person Re-identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02173v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02173v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihao Li, Lei Tan, Pingyang Dai, Yan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image person re-identification (TIReID) aims to retrieve the target
person from an image gallery via a textual description query. Recently,
pre-trained vision-language models like CLIP have attracted significant
attention and have been widely utilized for this task due to their robust
capacity for semantic concept learning and rich multi-modal knowledge. However,
recent CLIP-based TIReID methods commonly rely on direct fine-tuning of the
entire network to adapt the CLIP model for the TIReID task. Although these
methods show competitive performance on this topic, they are suboptimal as they
necessitate simultaneous domain adaptation and task adaptation. To address this
issue, we attempt to decouple these two processes during the training stage.
Specifically, we introduce the prompt tuning strategy to enable domain
adaptation and propose a two-stage training approach to disentangle domain
adaptation from task adaptation. In the first stage, we freeze the two encoders
from CLIP and solely focus on optimizing the prompts to alleviate domain gap
between the original training data of CLIP and downstream tasks. In the second
stage, we maintain the fixed prompts and fine-tune the CLIP model to prioritize
capturing fine-grained information, which is more suitable for TIReID task.
Finally, we evaluate the effectiveness of our method on three widely used
datasets. Compared to the directly fine-tuned approach, our method achieves
significant improvements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Frequency Domain Nuances Mining for Visible-Infrared Person
  Re-identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukang Zhang, Yang Lu, Yan Yan, Hanzi Wang, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The key of visible-infrared person re-identification (VIReID) lies in how to
minimize the modality discrepancy between visible and infrared images. Existing
methods mainly exploit the spatial information while ignoring the
discriminative frequency information. To address this issue, this paper aims to
reduce the modality discrepancy from the frequency domain perspective.
Specifically, we propose a novel Frequency Domain Nuances Mining (FDNM) method
to explore the cross-modality frequency domain information, which mainly
includes an amplitude guided phase (AGP) module and an amplitude nuances mining
(ANM) module. These two modules are mutually beneficial to jointly explore
frequency domain visible-infrared nuances, thereby effectively reducing the
modality discrepancy in the frequency domain. Besides, we propose a
center-guided nuances mining loss to encourage the ANM module to preserve
discriminative identity information while discovering diverse cross-modality
nuances. To the best of our knowledge, this is the first work that explores the
potential frequency information for VIReID research. Extensive experiments show
that the proposed FDNM has significant advantages in improving the performance
of VIReID. Specifically, our method outperforms the second-best method by 5.2\%
in Rank-1 accuracy and 5.8\% in mAP on the SYSU-MM01 dataset under the indoor
search mode, respectively. Besides, we also validate the effectiveness and
generalization of our method on the challenging visible-infrared face
recognition task. \textcolor{magenta}{The code will be available.}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing RAW-to-sRGB with Decoupled Style Structure in Fourier Domain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02161v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02161v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanhua He, Tao Hu, Guoli Wang, Zejin Wang, Run Wang, Qian Zhang, Keyu Yan, Ziyi Chen, Rui Li, Chenjun Xie, Jie Zhang, Man Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  RAW to sRGB mapping, which aims to convert RAW images from smartphones into
RGB form equivalent to that of Digital Single-Lens Reflex (DSLR) cameras, has
become an important area of research. However, current methods often ignore the
difference between cell phone RAW images and DSLR camera RGB images, a
difference that goes beyond the color matrix and extends to spatial structure
due to resolution variations. Recent methods directly rebuild color mapping and
spatial structure via shared deep representation, limiting optimal performance.
Inspired by Image Signal Processing (ISP) pipeline, which distinguishes image
restoration and enhancement, we present a novel Neural ISP framework, named
FourierISP. This approach breaks the image down into style and structure within
the frequency domain, allowing for independent optimization. FourierISP is
comprised of three subnetworks: Phase Enhance Subnet for structural refinement,
Amplitude Refine Subnet for color learning, and Color Adaptation Subnet for
blending them in a smooth manner. This approach sharpens both color and
structure, and extensive evaluations across varied datasets confirm that our
approach realizes state-of-the-art results. Code will be available at
~\url{https://github.com/alexhe101/FourierISP}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Frequency-Adaptive Pan-Sharpening with Mixture of Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanhua He, Keyu Yan, Rui Li, Chengjun Xie, Jie Zhang, Man Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pan-sharpening involves reconstructing missing high-frequency information in
multi-spectral images with low spatial resolution, using a higher-resolution
panchromatic image as guidance. Although the inborn connection with frequency
domain, existing pan-sharpening research has not almost investigated the
potential solution upon frequency domain. To this end, we propose a novel
Frequency Adaptive Mixture of Experts (FAME) learning framework for
pan-sharpening, which consists of three key components: the Adaptive Frequency
Separation Prediction Module, the Sub-Frequency Learning Expert Module, and the
Expert Mixture Module. In detail, the first leverages the discrete cosine
transform to perform frequency separation by predicting the frequency mask. On
the basis of generated mask, the second with low-frequency MOE and
high-frequency MOE takes account for enabling the effective low-frequency and
high-frequency information reconstruction. Followed by, the final fusion module
dynamically weights high-frequency and low-frequency MOE knowledge to adapt to
remote sensing images with significant content variations. Quantitative and
qualitative experiments over multiple datasets demonstrate that our method
performs the best against other state-of-the-art ones and comprises a strong
generalization ability for real-world scenes. Code will be made publicly at
\url{https://github.com/alexhe101/FAME-Net}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Marginal Debiased Network for Fair Visual Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mei Wang, Weihong Deng, Sen Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) are often prone to learn the spurious
correlations between target classes and bias attributes, like gender and race,
inherent in a major portion of training data (bias-aligned samples), thus
showing unfair behavior and arising controversy in the modern pluralistic and
egalitarian society. In this paper, we propose a novel marginal debiased
network (MDN) to learn debiased representations. More specifically, a marginal
softmax loss (MSL) is designed by introducing the idea of margin penalty into
the fairness problem, which assigns a larger margin for bias-conflicting
samples (data without spurious correlations) than for bias-aligned ones, so as
to deemphasize the spurious correlations and improve generalization on unbiased
test criteria. To determine the margins, our MDN is optimized through a meta
learning framework. We propose a meta equalized loss (MEL) to perceive the
model fairness, and adaptively update the margin parameters by metaoptimization
which requires the trained model guided by the optimal margins should minimize
MEL computed on an unbiased meta-validation set. Extensive experiments on
BiasedMNIST, Corrupted CIFAR-10, CelebA and UTK-Face datasets demonstrate that
our MDN can achieve a remarkable performance on under-represented samples and
obtain superior debiased results against the previous approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Boundary of <span class="highlight-title">GPT</span>-4V on Marine Analysis: A Preliminary Case
  Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02147v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02147v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqiang Zheng, Yiwei Chen, Jipeng Zhang, Tuan-Anh Vu, Huimin Zeng, Yue Him Wong Tim, Sai-Kit Yeung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated a powerful ability to answer
various queries as a general-purpose assistant. The continuous multi-modal
large language models (MLLM) empower LLMs with the ability to perceive visual
signals. The launch of GPT-4 (Generative Pre-trained Transformers) has
generated significant interest in the research communities. GPT-4V(ison) has
demonstrated significant power in both academia and industry fields, as a focal
point in a new artificial intelligence generation. Though significant success
was achieved by GPT-4V, exploring MLLMs in domain-specific analysis (e.g.,
marine analysis) that required domain-specific knowledge and expertise has
gained less attention. In this study, we carry out the preliminary and
comprehensive case study of utilizing GPT-4V for marine analysis. This report
conducts a systematic evaluation of existing GPT-4V, assessing the performance
of GPT-4V on marine research and also setting a new standard for future
developments in MLLMs. The experimental results of GPT-4V show that the
responses generated by GPT-4V are still far away from satisfying the
domain-specific requirements of the marine professions. All images and prompts
used in this study will be available at
https://github.com/hkust-vgd/Marine_GPT-4V_Eval
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>51 pages, 36 figures, Repository:
  https://github.com/hkust-vgd/Marine_GPT-4V_Eval</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GUESS:GradUally Enriching SyntheSis for Text-Driven Human Motion
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02142v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02142v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuehao Gao, Yang Yang, Zhenyu Xie, Shaoyi Du, Zhongqian Sun, Yang Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel cascaded diffusion-based generative
framework for text-driven human motion synthesis, which exploits a strategy
named GradUally Enriching SyntheSis (GUESS as its abbreviation). The strategy
sets up generation objectives by grouping body joints of detailed skeletons in
close semantic proximity together and then replacing each of such joint group
with a single body-part node. Such an operation recursively abstracts a human
pose to coarser and coarser skeletons at multiple granularity levels. With
gradually increasing the abstraction level, human motion becomes more and more
concise and stable, significantly benefiting the cross-modal motion synthesis
task. The whole text-driven human motion synthesis problem is then divided into
multiple abstraction levels and solved with a multi-stage generation framework
with a cascaded latent diffusion model: an initial generator first generates
the coarsest human motion guess from a given text description; then, a series
of successive generators gradually enrich the motion details based on the
textual description and the previous synthesized results. Notably, we further
integrate GUESS with the proposed dynamic multi-condition fusion mechanism to
dynamically balance the cooperative effects of the given textual condition and
synthesized coarse motion prompt in different generation stages. Extensive
experiments on large-scale datasets verify that GUESS outperforms existing
state-of-the-art methods by large margins in terms of accuracy, realisticness,
and diversity. Code is available at https://github.com/Xuehao-Gao/GUESS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Visualization and Computer Graphics
  (2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian Intrinsic Groupwise Image Registration: Unsupervised
  Disentanglement of Anatomy and Geometry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02141v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02141v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinzhe Luo, Xin Wang, Linda Shapiro, Chun Yuan, Jianfeng Feng, Xiahai Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article presents a general Bayesian learning framework for multi-modal
groupwise registration on medical images. The method builds on probabilistic
modelling of the image generative process, where the underlying common anatomy
and geometric variations of the observed images are explicitly disentangled as
latent variables. Thus, groupwise registration is achieved through the solution
to Bayesian inference. We propose a novel hierarchical variational
auto-encoding architecture to realize the inference procedure of the latent
variables, where the registration parameters can be calculated in a
mathematically interpretable fashion. Remarkably, this new paradigm can learn
groupwise registration in an unsupervised closed-loop self-reconstruction
process, sparing the burden of designing complex intensity-based similarity
measures. The computationally efficient disentangled architecture is also
inherently scalable and flexible, allowing for groupwise registration on
large-scale image groups with variable sizes. Furthermore, the inferred
structural representations from disentanglement learning are capable of
capturing the latent anatomy of the observations with visual semantics.
Extensive experiments were conducted to validate the proposed framework,
including four datasets from cardiac, brain and abdominal medical images. The
results have demonstrated the superiority of our method over conventional
similarity-based approaches in terms of accuracy, efficiency, scalability and
interpretability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explore Human Parsing Modality for Action Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02138v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02138v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinfu Liu, Runwei Ding, Yuhang Wen, Nan Dai, Fanyang Meng, Shen Zhao, Mengyuan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal-based action recognition methods have achieved high success using
pose and RGB modality. However, skeletons sequences lack appearance depiction
and RGB images suffer irrelevant noise due to modality limitations. To address
this, we introduce human parsing feature map as a novel modality, since it can
selectively retain effective semantic features of the body parts, while
filtering out most irrelevant noise. We propose a new dual-branch framework
called Ensemble Human Parsing and Pose Network (EPP-Net), which is the first to
leverage both skeletons and human parsing modalities for action recognition.
The first human pose branch feeds robust skeletons in graph convolutional
network to model pose features, while the second human parsing branch also
leverages depictive parsing feature maps to model parsing festures via
convolutional backbones. The two high-level features will be effectively
combined through a late fusion strategy for better action recognition.
Extensive experiments on NTU RGB+D and NTU RGB+D 120 benchmarks consistently
verify the effectiveness of our proposed EPP-Net, which outperforms the
existing action recognition methods. Our code is available at:
https://github.com/liujf69/EPP-Net-Action.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2307.07977</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SyCoCa: Symmetrizing Contrastive Captioners with Attentive Masking for
  Multimodal Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziping Ma, Furong Xu, Jian Liu, Ming Yang, Qingpei Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal alignment between language and vision is the fundamental topic in
current vision-language model research. Contrastive Captioners (CoCa), as a
representative method, integrates Contrastive Language-Image Pretraining (CLIP)
and Image Caption (IC) into a unified framework, resulting in impressive
results. CLIP imposes a bidirectional constraints on global representation of
entire images and sentences. Although IC conducts an unidirectional
image-to-text generation on local representation, it lacks any constraint on
local text-to-image reconstruction, which limits the ability to understand
images at a fine-grained level when aligned with texts. To achieve multimodal
alignment from both global and local perspectives, this paper proposes
Symmetrizing Contrastive Captioners (SyCoCa), which introduces bidirectional
interactions on images and texts across the global and local representation
levels. Specifically, we expand a Text-Guided Masked Image Modeling (TG-MIM)
head based on ITC and IC heads. The improved SyCoCa can further leverage
textual cues to reconstruct contextual images and visual cues to predict
textual contents. When implementing bidirectional local interactions, the local
contents of images tend to be cluttered or unrelated to their textual
descriptions. Thus, we employ an attentive masking strategy to select effective
image patches for interaction. Extensive experiments on five vision-language
tasks, including image-text retrieval, image-captioning, visual question
answering, and zero-shot/finetuned image classification, validate the
effectiveness of our proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unified Diffusion-Based Rigid and Non-Rigid Editing with Text and Image
  Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02126v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02126v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Wang, Ping Liu, Wei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing text-to-image editing methods tend to excel either in rigid or
non-rigid editing but encounter challenges when combining both, resulting in
misaligned outputs with the provided text prompts. In addition, integrating
reference images for control remains challenging. To address these issues, we
present a versatile image editing framework capable of executing both rigid and
non-rigid edits, guided by either textual prompts or reference images. We
leverage a dual-path injection scheme to handle diverse editing scenarios and
introduce an integrated self-attention mechanism for fusion of appearance and
structural information. To mitigate potential visual artifacts, we further
employ latent fusion techniques to adjust intermediate latents. Compared to
previous work, our approach represents a significant advance in achieving
precise and versatile image editing. Comprehensive experiments validate the
efficacy of our method, showcasing competitive or superior results in
text-based editing and appearance transfer tasks, encompassing both rigid and
non-rigid settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost
  Whole-Body Teleoperation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zipeng Fu, Tony Z. Zhao, Chelsea Finn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning from human demonstrations has shown impressive performance
in robotics. However, most results focus on table-top manipulation, lacking the
mobility and dexterity necessary for generally useful tasks. In this work, we
develop a system for imitating mobile manipulation tasks that are bimanual and
require whole-body control. We first present Mobile ALOHA, a low-cost and
whole-body teleoperation system for data collection. It augments the ALOHA
system with a mobile base, and a whole-body teleoperation interface. Using data
collected with Mobile ALOHA, we then perform supervised behavior cloning and
find that co-training with existing static ALOHA datasets boosts performance on
mobile manipulation tasks. With 50 demonstrations for each task, co-training
can increase success rates by up to 90%, allowing Mobile ALOHA to autonomously
complete complex mobile manipulation tasks such as sauteing and serving a piece
of shrimp, opening a two-door wall cabinet to store heavy cooking pots, calling
and entering an elevator, and lightly rinsing a used pan using a kitchen
faucet. Project website: https://mobile-aloha.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://mobile-aloha.github.io (Zipeng Fu and Tony
  Z. Zhao are project co-leads, Chelsea Finn is the advisor)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Source-Free Online Domain Adaptive Semantic Segmentation of Satellite
  Images under Image Degradation <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02113v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02113v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fahim Faisal Niloy, Kishor Kumar Bhaumik, Simon S. Woo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online adaptation to distribution shifts in satellite image segmentation
stands as a crucial yet underexplored problem. In this paper, we address
source-free and online domain adaptation, i.e., test-time adaptation (TTA), for
satellite images, with the focus on mitigating distribution shifts caused by
various forms of image degradation. Towards achieving this goal, we propose a
novel TTA approach involving two effective strategies. First, we progressively
estimate the global Batch Normalization (BN) statistics of the target
distribution with incoming data stream. Leveraging these statistics during
inference has the ability to effectively reduce domain gap. Furthermore, we
enhance prediction quality by refining the predicted masks using global class
centers. Both strategies employ dynamic momentum for fast and stable
convergence. Notably, our method is backpropagation-free and hence fast and
lightweight, making it highly suitable for on-the-fly adaptation to new domain.
Through comprehensive experiments across various domain adaptation scenarios,
we demonstrate the robust performance of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Significance of Anatomical Constraints in Virtual Try-On 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02110v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02110v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debapriya Roy, Sanchayan Santra, Diganta Mukherjee, Bhabatosh Chanda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The system of Virtual Try-ON (VTON) allows a user to try a product virtually.
In general, a VTON system takes a clothing source and a person's image to
predict the try-on output of the person in the given clothing. Although
existing methods perform well for simple poses, in case of bent or crossed arms
posture or when there is a significant difference between the alignment of the
source clothing and the pose of the target person, these methods fail by
generating inaccurate clothing deformations. In the VTON methods that employ
Thin Plate Spline (TPS) based clothing transformations, this mainly occurs for
two reasons - (1)~the second-order smoothness constraint of TPS that restricts
the bending of the object plane. (2)~Overlaps among different clothing parts
(e.g., sleeves and torso) can not be modeled by a single TPS transformation, as
it assumes the clothing as a single planar object; therefore, disregards the
independence of movement of different clothing parts. To this end, we make two
major contributions. Concerning the bending limitations of TPS, we propose a
human AnaTomy-Aware Geometric (ATAG) transformation. Regarding the overlap
issue, we propose a part-based warping approach that divides the clothing into
independently warpable parts to warp them separately and later combine them.
Extensive analysis shows the efficacy of this approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2208.08076</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLAPP: Contrastive Language-Audio <span class="highlight-title">Pre-train</span>ing in Passive Underwater
  Vessel Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Li, Jingsheng Gao, Tong Yu, Suncheng Xiang, Jiacheng Ruan, Ting Liu, Yuzhuo Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing research on audio classification faces challenges in recognizing
attributes of passive underwater vessel scenarios and lacks well-annotated
datasets due to data privacy concerns. In this study, we introduce CLAPP
(Contrastive Language-Audio Pre-training in Passive Underwater Vessel
Classification), a novel model. Our aim is to train a neural network using a
wide range of vessel audio and vessel state text pairs obtained from an
oceanship dataset. CLAPP is capable of directly learning from raw vessel audio
data and, when available, from carefully curated labels, enabling improved
recognition of vessel attributes in passive underwater vessel scenarios.
Model's zero-shot capability allows predicting the most relevant vessel state
description for a given vessel audio, without directly optimizing for the task.
Our approach aims to solve 2 challenges: vessel audio-text classification and
passive underwater vessel audio attribute recognition. The proposed method
achieves new state-of-the-art results on both Deepship and Shipsear public
datasets, with a notable margin of about 7%-13% for accuracy compared to prior
methods on zero-shot task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preserving Image Properties Through Initializations in Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02097v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02097v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeffrey Zhang, Shao-Yu Chang, Kedan Li, David Forsyth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retail photography imposes specific requirements on images. For instance,
images may need uniform background colors, consistent model poses, centered
products, and consistent lighting. Minor deviations from these standards impact
a site's aesthetic appeal, making the images unsuitable for use. We show that
Stable Diffusion methods, as currently applied, do not respect these
requirements. The usual practice of training the denoiser with a very noisy
image and starting inference with a sample of pure noise leads to inconsistent
generated images during inference. This inconsistency occurs because it is easy
to tell the difference between samples of the training and inference
distributions. As a result, a network trained with centered retail product
images with uniform backgrounds generates images with erratic backgrounds. The
problem is easily fixed by initializing inference with samples from an
approximation of noisy images. However, in using such an approximation, the
joint distribution of text and noisy image at inference time still slightly
differs from that at training time. This discrepancy is corrected by training
the network with samples from the approximate noisy image distribution.
Extensive experiments on real application data show significant qualitative and
quantitative improvements in performance from adopting these procedures.
Finally, our procedure can interact well with other control-based methods to
further enhance the controllability of diffusion-based methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Class-Incremental Learning with Prototype Guided <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02094v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02094v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiyang Guo, Fei Zhu, Wenzhuo Liu, Xu-Yao Zhang, Cheng-Lin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing federated learning methods have effectively addressed decentralized
learning in scenarios involving data privacy and non-IID data. However, in
real-world situations, each client dynamically learns new classes, requiring
the global model to maintain discriminative capabilities for both new and old
classes. To effectively mitigate the effects of catastrophic forgetting and
data heterogeneity under low communication costs, we designed a simple and
effective method named PLoRA. On the one hand, we adopt prototype learning to
learn better feature representations and leverage the heuristic information
between prototypes and class features to design a prototype re-weight module to
solve the classifier bias caused by data heterogeneity without retraining the
classification layer. On the other hand, our approach utilizes a pre-trained
model as the backbone and utilizes LoRA to fine-tune with a tiny amount of
parameters when learning new classes. Moreover, PLoRA does not rely on
similarity-based module selection strategies, thereby further reducing
communication overhead. Experimental results on standard datasets indicate that
our method outperforms the state-of-the-art approaches significantly. More
importantly, our method exhibits strong robustness and superiority in various
scenarios and degrees of data heterogeneity. Our code will be publicly
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging SAM for Single-Source Domain Generalization in Medical Image
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02076v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02076v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanhui Wang, Huaize Ye, Yi Xia, Xueyan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain Generalization (DG) aims to reduce domain shifts between domains to
achieve promising performance on the unseen target domain, which has been
widely practiced in medical image segmentation. Single-source domain
generalization (SDG) is the most challenging setting that trains on only one
source domain. Although existing methods have made considerable progress on SDG
of medical image segmentation, the performances are still far from the
applicable standards when faced with a relatively large domain shift. In this
paper, we leverage the Segment Anything Model (SAM) to SDG to greatly improve
the ability of generalization. Specifically, we introduce a parallel framework,
the source images are sent into the SAM module and normal segmentation module
respectively. To reduce the calculation resources, we apply a merging strategy
before sending images to the SAM module. We extract the bounding boxes from the
segmentation module and send the refined version as prompts to the SAM module.
We evaluate our model on a classic DG dataset and achieve competitive results
compared to other state-of-the-art DG methods. Furthermore, We conducted a
series of ablation experiments to prove the effectiveness of the proposed
method. The code is publicly available at https://github.com/SARIHUST/SAMMed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalizable vision-language <span class="highlight-title">pre-train</span>ing for annotation-free pathology
  localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Yang, Hong-Yu Zhou, Cheng Li, Weijian Huang, Jiarun Liu, Shanshan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Locating pathologies automatically from medical images aids the understanding
of the emergence and progression of diseases, and such an ability can
significantly benefit clinical diagnostics. However, existing deep learning
models heavily rely on expert annotations and lack generalization capabilities
in open clinical environments. In this study, we present a generalizable
vision-language pre-training model for Annotation-Free pathology Localization
(AFLoc). The core strength of AFLoc lies in its image annotation-free
multi-level semantic structure-based contrastive learning, which
comprehensively aligns multi-granularity medical concepts from reports with
abundant image features, to adapt to the diverse expressions of observed and
emerging unseen pathologies. We conducted extensive experimental validation
across 4 distinct external datasets, encompassing 11 types of chest
pathologies, to verify its generalization ability. The results demonstrate that
AFLoc surpasses 6 state-of-the-art methods and even outperforms the human
benchmark in locating 5 different pathologies, underscoring its suitability for
complex clinical environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Cloud-edge Collaborative Inference for Object
  Re-identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanming Wang, Yuxin Yang, Mengshi Qi, Huadong Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current object re-identification (ReID) system follows the centralized
processing paradigm, i.e., all computations are conducted in the cloud server
and edge devices are only used to capture and send images. As the number of
videos experiences a rapid escalation, this paradigm has become impractical due
to the finite computational resources. In such a scenario, the ReID system
should be converted to fit in the cloud-edge collaborative processing paradigm,
which is crucial to boost the scalability and practicality of ReID systems.
However, current relevant work lacks research on this issue, making it
challenging for ReID methods to be adapted effectively. Therefore, we pioneer a
cloud-edge collaborative inference framework for ReID systems and particularly
propose a distribution-aware correlation modeling network (DaCM) to make the
desired image return to the cloud server as soon as possible via learning to
model the spatial-temporal correlations among instances. DaCM embeds the
spatial-temporal correlations implicitly included in the timestamps into a
graph structure, and it can be applied in the cloud to regulate the size of the
upload window and on the edge device to adjust the sequence of images,
respectively. Traditional ReID methods can be combined with DaCM seamlessly,
enabling their application within our proposed edge-cloud collaborative
framework. Extensive experiments demonstrate that our method obviously reduces
transmission overhead and significantly improves performance. We will release
our code and model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffusionEdge: Diffusion Probabilistic Model for Crisp Edge Detection <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02032v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02032v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunfan Ye, Kai Xu, Yuhang Huang, Renjiao Yi, Zhiping Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Limited by the encoder-decoder architecture, learning-based edge detectors
usually have difficulty predicting edge maps that satisfy both correctness and
crispness. With the recent success of the diffusion probabilistic model (DPM),
we found it is especially suitable for accurate and crisp edge detection since
the denoising process is directly applied to the original image size.
Therefore, we propose the first diffusion model for the task of general edge
detection, which we call DiffusionEdge. To avoid expensive computational
resources while retaining the final performance, we apply DPM in the latent
space and enable the classic cross-entropy loss which is uncertainty-aware in
pixel level to directly optimize the parameters in latent space in a
distillation manner. We also adopt a decoupled architecture to speed up the
denoising process and propose a corresponding adaptive Fourier filter to adjust
the latent features of specific frequencies. With all the technical designs,
DiffusionEdge can be stably trained with limited resources, predicting crisp
and accurate edge maps with much fewer augmentation strategies. Extensive
experiments on four edge detection benchmarks demonstrate the superiority of
DiffusionEdge both in correctness and crispness. On the NYUDv2 dataset,
compared to the second best, we increase the ODS, OIS (without post-processing)
and AC by 30.2%, 28.1% and 65.1%, respectively. Code:
https://github.com/GuHuangAI/DiffusionEdge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spy-Watermark: Robust Invisible Watermarking for Backdoor Attack <span class="chip">ICASSP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02031v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02031v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruofei Wang, Renjie Wan, Zongyu Guo, Qing Guo, Rui Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Backdoor attack aims to deceive a victim model when facing backdoor instances
while maintaining its performance on benign data. Current methods use manual
patterns or special perturbations as triggers, while they often overlook the
robustness against data corruption, making backdoor attacks easy to defend in
practice. To address this issue, we propose a novel backdoor attack method
named Spy-Watermark, which remains effective when facing data collapse and
backdoor defense. Therein, we introduce a learnable watermark embedded in the
latent domain of images, serving as the trigger. Then, we search for a
watermark that can withstand collapse during image decoding, cooperating with
several anti-collapse operations to further enhance the resilience of our
trigger against data corruption. Extensive experiments are conducted on
CIFAR10, GTSRB, and ImageNet datasets, demonstrating that Spy-Watermark
overtakes ten state-of-the-art methods in terms of robustness and stealthiness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spikformer V2: Join the High Accuracy Club on ImageNet with an SNN
  Ticket 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02020v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02020v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaokun Zhou, Kaiwei Che, Wei Fang, Keyu Tian, Yuesheng Zhu, Shuicheng Yan, Yonghong Tian, Li Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking Neural Networks (SNNs), known for their biologically plausible
architecture, face the challenge of limited performance. The self-attention
mechanism, which is the cornerstone of the high-performance Transformer and
also a biologically inspired structure, is absent in existing SNNs. To this
end, we explore the potential of leveraging both self-attention capability and
biological properties of SNNs, and propose a novel Spiking Self-Attention (SSA)
and Spiking Transformer (Spikformer). The SSA mechanism eliminates the need for
softmax and captures the sparse visual feature employing spike-based Query,
Key, and Value. This sparse computation without multiplication makes SSA
efficient and energy-saving. Further, we develop a Spiking Convolutional Stem
(SCS) with supplementary convolutional layers to enhance the architecture of
Spikformer. The Spikformer enhanced with the SCS is referred to as Spikformer
V2. To train larger and deeper Spikformer V2, we introduce a pioneering
exploration of Self-Supervised Learning (SSL) within the SNN. Specifically, we
pre-train Spikformer V2 with masking and reconstruction style inspired by the
mainstream self-supervised Transformer, and then finetune the Spikformer V2 on
the image classification on ImageNet. Extensive experiments show that
Spikformer V2 outperforms other previous surrogate training and ANN2SNN
methods. An 8-layer Spikformer V2 achieves an accuracy of 80.38% using 4 time
steps, and after SSL, a 172M 16-layer Spikformer V2 reaches an accuracy of
81.10% with just 1 time step. To the best of our knowledge, this is the first
time that the SNN achieves 80+% accuracy on ImageNet. The code will be
available at Spikformer V2.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Diffusion-Based Image Synthesis with Context Prediction <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02015v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02015v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ling Yang, Jingwei Liu, Shenda Hong, Zhilong Zhang, Zhilin Huang, Zheming Cai, Wentao Zhang, Bin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models are a new class of generative models, and have dramatically
promoted image generation with unprecedented quality and diversity. Existing
diffusion models mainly try to reconstruct input image from a corrupted one
with a pixel-wise or feature-wise constraint along spatial axes. However, such
point-based reconstruction may fail to make each predicted pixel/feature fully
preserve its neighborhood context, impairing diffusion-based image synthesis.
As a powerful source of automatic supervisory signal, context has been well
studied for learning representations. Inspired by this, we for the first time
propose ConPreDiff to improve diffusion-based image synthesis with context
prediction. We explicitly reinforce each point to predict its neighborhood
context (i.e., multi-stride features/tokens/pixels) with a context decoder at
the end of diffusion denoising blocks in training stage, and remove the decoder
for inference. In this way, each point can better reconstruct itself by
preserving its semantic connections with neighborhood context. This new
paradigm of ConPreDiff can generalize to arbitrary discrete and continuous
diffusion backbones without introducing extra parameters in sampling procedure.
Extensive experiments are conducted on unconditional image generation,
text-to-image generation and image inpainting tasks. Our ConPreDiff
consistently outperforms previous methods and achieves a new SOTA text-to-image
generation results on MS-COCO, with a zero-shot FID score of 6.21.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vulnerabilities Unveiled: Adversarially Attacking a Multimodal Vision
  Langauge Model for Pathology Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02565v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02565v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jai Prakash Veerla, Poojitha Thota, Partha Sai Guttikonda, Shirin Nilizadeh, Jacob M. Luber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the dynamic landscape of medical artificial intelligence, this study
explores the vulnerabilities of the Pathology Language-Image Pretraining (PLIP)
model, a Vision Language Foundation model, under targeted adversarial
conditions. Leveraging the Kather Colon dataset with 7,180 H&E images across
nine tissue types, our investigation employs Projected Gradient Descent (PGD)
adversarial attacks to intentionally induce misclassifications. The outcomes
reveal a 100% success rate in manipulating PLIP's predictions, underscoring its
susceptibility to adversarial perturbations. The qualitative analysis of
adversarial examples delves into the interpretability challenges, shedding
light on nuanced changes in predictions induced by adversarial manipulations.
These findings contribute crucial insights into the interpretability, domain
adaptation, and trustworthiness of Vision Language Models in medical imaging.
The study emphasizes the pressing need for robust defenses to ensure the
reliability of AI models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting Future States with Spatial Point Processes in Single Molecule
  Resolution Spatial Transcriptomics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parisa Boodaghi Malidarreh, Biraaj Rout, Mohammad Sadegh Nasr, Priyanshi Borad, Jillur Rahman Saurav, Jai Prakash Veerla, Kelli Fenelon, Theodora Koromila, Jacob M. Luber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a pipeline based on Random Forest Regression to
predict the future distribution of cells that are expressed by the Sog-D gene
(active cells) in both the Anterior to posterior (AP) and the Dorsal to Ventral
(DV) axis of the Drosophila in embryogenesis process. This method provides
insights about how cells and living organisms control gene expression in super
resolution whole embryo spatial transcriptomics imaging at sub cellular, single
molecule resolution. A Random Forest Regression model was used to predict the
next stage active distribution based on the previous one. To achieve this goal,
we leveraged temporally resolved, spatial point processes by including Ripley's
K-function in conjunction with the cell's state in each stage of embryogenesis,
and found average predictive accuracy of active cell distribution. This tool is
analogous to RNA Velocity for spatially resolved developmental biology, from
one data point we can predict future spatially resolved gene expression using
features from the spatial point processes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OptFlow: Fast Optimization-based Scene Flow Estimation without
  Supervision <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahul Ahuja, Chris Baker, Wilko Schwarting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene flow estimation is a crucial component in the development of autonomous
driving and 3D robotics, providing valuable information for environment
perception and navigation. Despite the advantages of learning-based scene flow
estimation techniques, their domain specificity and limited generalizability
across varied scenarios pose challenges. In contrast, non-learning
optimization-based methods, incorporating robust priors or regularization,
offer competitive scene flow estimation performance, require no training, and
show extensive applicability across datasets, but suffer from lengthy inference
times. In this paper, we present OptFlow, a fast optimization-based scene flow
estimation method. Without relying on learning or any labeled datasets, OptFlow
achieves state-of-the-art performance for scene flow estimation on popular
autonomous driving benchmarks. It integrates a local correlation weight matrix
for correspondence matching, an adaptive correspondence threshold limit for
nearest-neighbor search, and graph prior rigidity constraints, resulting in
expedited convergence and improved point correspondence identification.
Moreover, we demonstrate how integrating a point cloud registration function
within our objective function bolsters accuracy and differentiates between
static and dynamic points without relying on external odometry data.
Consequently, OptFlow outperforms the baseline graph-prior method by
approximately 20% and the Neural Scene Flow Prior method by 5%-7% in accuracy,
all while offering the fastest inference time among all non-learning scene flow
estimation methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the proceedings of the IEEE/CVF Winter Conference on
  Applications of Computer Vision (WACV), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robot-Assisted Deep Venous Thrombosis Ultrasound Examination using
  Virtual Fixture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dianye Huang, Chenguang Yang, Mingchuan Zhou, Angelos Karlas, Nassir Navab, Zhongliang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Venous Thrombosis (DVT) is a common vascular disease with blood clots
inside deep veins, which may block blood flow or even cause a life-threatening
pulmonary embolism. A typical exam for DVT using ultrasound (US) imaging is by
pressing the target vein until its lumen is fully compressed. However, the
compression exam is highly operator-dependent. To alleviate intra- and
inter-variations, we present a robotic US system with a novel hybrid force
motion control scheme ensuring position and force tracking accuracy, and soft
landing of the probe onto the target surface. In addition, a path-based virtual
fixture is proposed to realize easy human-robot interaction for repeat
compression operation at the lesion location. To ensure the biometric
measurements obtained in different examinations are comparable, the 6D scanning
path is determined in a coarse-to-fine manner using both an external RGBD
camera and US images. The RGBD camera is first used to extract a rough scanning
path on the object. Then, the segmented vascular lumen from US images are used
to optimize the scanning path to ensure the visibility of the target object. To
generate a continuous scan path for developing virtual fixtures, an arc-length
based path fitting model considering both position and orientation is proposed.
Finally, the whole system is evaluated on a human-like arm phantom with an
uneven surface.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted Paper IEEE T-ASE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using Singular Value Decomposition in a Convolutional Neural Network to
  Improve Brain Tumor Segmentation Accuracy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pegah Ahadian, Maryam Babaei, Kourosh Parand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A brain tumor consists of cells showing abnormal brain growth. The area of
the brain tumor significantly affects choosing the type of treatment and
following the course of the disease during the treatment. At the same time,
pictures of Brain MRIs are accompanied by noise. Eliminating existing noises
can significantly impact the better segmentation and diagnosis of brain tumors.
In this work, we have tried using the analysis of eigenvalues. We have used the
MSVD algorithm, reducing the image noise and then using the deep neural network
to segment the tumor in the images. The proposed method's accuracy was
increased by 2.4% compared to using the original images. With Using the MSVD
method, convergence speed has also increased, showing the proposed method's
effectiveness
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Novel End-to-End Production-Ready Machine Learning Flow for
  Nanolithography Modeling and Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed S. E. Habib, Hossam A. H. Fahmy, Mohamed F. Abu-ElYazeed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optical lithography is the main enabler to semiconductor manufacturing. It
requires extensive processing to perform the Resolution Enhancement Techniques
(RETs) required to transfer the design data to a working Integrated Circuits
(ICs). The processing power and computational runtime for RETs tasks is ever
increasing due to the continuous reduction of the feature size and the
expansion of the chip area. State-of-the-art research sought Machine Learning
(ML) technologies to reduce runtime and computational power, however they are
still not used in production yet. In this study, we analyze the reasons holding
back ML computational lithography from being production ready and present a
novel highly scalable end-to-end flow that enables production ready ML-RET
correction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Branched Variational Autoencoder Classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Salah, David Yevick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a modified variational autoencoder (VAEs) that contains
an additional neural network branch. The resulting branched VAE (BVAE)
contributes a classification component based on the class labels to the total
loss and therefore imparts categorical information to the latent
representation. As a result, the latent space distributions of the input
classes are separated and ordered, thereby enhancing the classification
accuracy. The degree of improvement is quantified by numerical calculations
employing the benchmark MNIST dataset for both unrotated and rotated digits.
The proposed technique is then compared to and then incorporated into a VAE
with fixed output distributions. This procedure is found to yield improved
performance for a wide range of output distributions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comprehensive Exploration of Synthetic Data Generation: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        André Bauer, Simon Trapp, Michael Stenger, Robert Leppich, Samuel Kounev, Mark Leznik, Kyle Chard, Ian Foster
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed a surge in the popularity of Machine Learning
(ML), applied across diverse domains. However, progress is impeded by the
scarcity of training data due to expensive acquisition and privacy legislation.
Synthetic data emerges as a solution, but the abundance of released models and
limited overview literature pose challenges for decision-making. This work
surveys 417 Synthetic Data Generation (SDG) models over the last decade,
providing a comprehensive overview of model types, functionality, and
improvements. Common attributes are identified, leading to a classification and
trend analysis. The findings reveal increased model performance and complexity,
with neural network-based approaches prevailing, except for privacy-preserving
data generation. Computer vision dominates, with GANs as primary generative
models, while diffusion models, transformers, and RNNs compete. Implications
from our performance evaluation highlight the scarcity of common metrics and
datasets, making comparisons challenging. Additionally, the neglect of training
and computational costs in literature necessitates attention in future
research. This work serves as a guide for SDG model selection and identifies
crucial areas for future exploration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image-based Deep Learning for Smart Digital Twins: a <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02523v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02523v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Ruman Islam, Mahadevan Subramaniam, Pei-Chi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Smart Digital twins (SDTs) are being increasingly used to virtually replicate
and predict the behaviors of complex physical systems through continual data
assimilation enabling the optimization of the performance of these systems by
controlling the actions of systems. Recently, deep learning (DL) models have
significantly enhanced the capabilities of SDTs, particularly for tasks such as
predictive maintenance, anomaly detection, and optimization. In many domains,
including medicine, engineering, and education, SDTs use image data
(image-based SDTs) to observe and learn system behaviors and control their
behaviors. This paper focuses on various approaches and associated challenges
in developing image-based SDTs by continually assimilating image data from
physical systems. The paper also discusses the challenges involved in designing
and implementing DL models for SDTs, including data acquisition, processing,
and interpretation. In addition, insights into the future directions and
opportunities for developing new image-based DL approaches to develop robust
SDTs are provided. This includes the potential for using generative models for
data augmentation, developing multi-modal DL models, and exploring the
integration of DL with other technologies, including 5G, edge computing, and
IoT. In this paper, we describe the image-based SDTs, which enable broader
adoption of the digital twin DT paradigms across a broad spectrum of areas and
the development of new methods to improve the abilities of SDTs in replicating,
predicting, and optimizing the behavior of complex systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 2 figures, and 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The cell signaling structure function 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Layton Aho, Mark Winter, Marc DeCarlo, Agne Frismantiene, Yannick Blum, Paolo Armando Gagliardi, Olivier Pertz, Andrew R. Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Live cell microscopy captures 5-D $(x,y,z,channel,time)$ movies that display
patterns of cellular motion and signaling dynamics. We present here an approach
to finding spatiotemporal patterns of cell signaling dynamics in 5-D live cell
microscopy movies unique in requiring no \emph{a priori} knowledge of expected
pattern dynamics, and no training data. The proposed cell signaling structure
function (SSF) is a Kolmogorov structure function that optimally measures cell
signaling state as nuclear intensity w.r.t. surrounding cytoplasm, a
significant improvement compared to the current state-of-the-art cytonuclear
ratio. SSF kymographs store at each spatiotemporal cell centroid the SSF value,
or a functional output such as velocity. Patterns of similarity are identified
via the metric normalized compression distance (NCD). The NCD is a reproducing
kernel for a Hilbert space that represents the input SSF kymographs as points
in a low dimensional embedding that optimally captures the pattern similarity
identified by the NCD throughout the space. The only parameter is the expected
cell radii ($\mu m$). A new formulation of the cluster structure function
optimally estimates how meaningful an embedding from the RKHS representation.
Results are presented quantifying the impact of ERK and AKT signaling between
different oncogenic mutations, and by the relation between ERK signaling and
cellular velocity patterns for movies of 2-D monolayers of human breast
epithelial (MCF10A) cells, 3-D MCF10A spheroids under optogenetic manipulation
of ERK, and human induced pluripotent stem cells .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VASE: Object-Centric Appearance and Shape Manipulation of Real Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02473v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02473v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elia Peruzzo, Vidit Goel, Dejia Xu, Xingqian Xu, Yifan Jiang, Zhangyang Wang, Humphrey Shi, Nicu Sebe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, several works tackled the video editing task fostered by the
success of large-scale text-to-image generative models. However, most of these
methods holistically edit the frame using the text, exploiting the prior given
by foundation diffusion models and focusing on improving the temporal
consistency across frames. In this work, we introduce a framework that is
object-centric and is designed to control both the object's appearance and,
notably, to execute precise and explicit structural modifications on the
object. We build our framework on a pre-trained image-conditioned diffusion
model, integrate layers to handle the temporal dimension, and propose training
strategies and architectural modifications to enable shape control. We evaluate
our method on the image-driven video editing task showing similar performance
to the state-of-the-art, and showcasing novel shape-editing capabilities.
Further details, code and examples are available on our project page:
https://helia95.github.io/vase-website/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page https://helia95.github.io/vase-website/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Not all Minorities are Equal: Empty-Class-Aware Distillation for
  Heterogeneous Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuangpu Guo, Yuhe Ding, Jian Liang, Ran He, Zilei Wang, Tieniu Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data heterogeneity, characterized by disparities in local data distribution
across clients, poses a significant challenge in federated learning.
Substantial efforts have been devoted to addressing the heterogeneity in local
label distribution. As minority classes suffer from worse accuracy due to
overfitting on local imbalanced data, prior methods often incorporate
class-balanced learning techniques during local training. Despite the improved
mean accuracy across all classes, we observe that empty classes-referring to
categories absent from a client's data distribution-are still not well
recognized. This paper introduces FedED, a novel approach in heterogeneous
federated learning that integrates both empty-class distillation and logit
suppression simultaneously. Specifically, empty-class distillation leverages
knowledge distillation during local training on each client to retain essential
information related to empty classes from the global model. Moreover, logit
suppression directly penalizes network logits for non-label classes,
effectively addressing misclassifications in minority classes that may be
biased toward majority classes. Extensive experiments validate the efficacy of
FedED, surpassing previous state-of-the-art methods across diverse datasets
with varying degrees of label distribution shift.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Anatomy-aware and acquisition-agnostic joint registration with
  SynthMorph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11329v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11329v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malte Hoffmann, Andrew Hoopes, Douglas N. Greve, Bruce Fischl, Adrian V. Dalca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Affine image registration is a cornerstone of medical-image analysis. While
classical algorithms can achieve excellent accuracy, they solve a
time-consuming optimization for every image pair. Deep-learning (DL) methods
learn a function that maps an image pair to an output transform. Evaluating the
function is fast, but capturing large transforms can be challenging, and
networks tend to struggle if a test-image characteristic shifts from the
training domain, such as resolution. Most affine methods are agnostic to
anatomy, meaning the registration will be inaccurate if algorithms consider all
structures in the image.
  We address these shortcomings with SynthMorph, an easy-to-use DL tool for
joint affine-deformable registration of any brain image without preprocessing,
right off the MRI scanner. First, we leverage a strategy to train networks with
wildly varying images synthesized from label maps, yielding robust performance
across acquisition specifics unseen at training. Second, we optimize the
spatial overlap of select anatomical labels. This enables networks to
distinguish anatomy of interest from irrelevant structures, removing the need
for preprocessing that excludes content which would impinge on anatomy-specific
registration. Third, we combine the affine model with a deformable hypernetwork
that lets users choose the optimal deformation-field regularity for their
specific data, at registration time, in a fraction of the time required by
classical methods.
  We rigorously analyze how competing architectures learn affine transforms and
compare state-of-the-art registration tools across an extremely diverse set of
neuroimaging data, aiming to truly capture the behavior of methods in the real
world. SynthMorph demonstrates consistent and improved accuracy. It is
available at https://w3id.org/synthmorph, as a single complete end-to-end
solution for registration of brain MRI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 22 figures, 4 tables, affine registration, deformable
  registration, deep learning, hypernetwork, domain shift, neuroimaging</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Generalize towards Unseen Domains via a Content-Aware Style
  Invariant Model for Disease Detection from Chest X-rays 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.13991v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.13991v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Zunaed, Md. Aynal Haque, Taufiq Hasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Performance degradation due to distribution discrepancy is a longstanding
challenge in intelligent imaging, particularly for chest X-rays (CXRs). Recent
studies have demonstrated that CNNs are biased toward styles (e.g.,
uninformative textures) rather than content (e.g., shape), in stark contrast to
the human vision system. Radiologists tend to learn visual cues from CXRs and
thus perform well across multiple domains. Motivated by this, we employ the
novel on-the-fly style randomization modules at both image (SRM-IL) and feature
(SRM-FL) levels to create rich style perturbed features while keeping the
content intact for robust cross-domain performance. Previous methods simulate
unseen domains by constructing new styles via interpolation or swapping styles
from existing data, limiting them to available source domains during training.
However, SRM-IL samples the style statistics from the possible value range of a
CXR image instead of the training data to achieve more diversified
augmentations. Moreover, we utilize pixel-wise learnable parameters in the
SRM-FL compared to pre-defined channel-wise mean and standard deviations as
style embeddings for capturing more representative style features.
Additionally, we leverage consistency regularizations on global semantic
features and predictive distributions from with and without style-perturbed
versions of the same CXR to tweak the model's sensitivity toward content
markers for accurate predictions. Our proposed method, trained on CheXpert and
MIMIC-CXR datasets, achieves 77.32$\pm$0.35, 88.38$\pm$0.19, 82.63$\pm$0.13
AUCs(%) on the unseen domain test datasets, i.e., BRAX, VinDr-CXR, and NIH
chest X-ray14, respectively, compared to 75.56$\pm$0.80, 87.57$\pm$0.46,
82.07$\pm$0.19 from state-of-the-art models on five-fold cross-validation with
statistically significant results in thoracic disease classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UpFusion: Novel View Diffusion from Unposed Sparse View Observations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06661v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06661v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bharath Raj Nagoor Kani, Hsin-Ying Lee, Sergey Tulyakov, Shubham Tulsiani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose UpFusion, a system that can perform novel view synthesis and infer
3D representations for an object given a sparse set of reference images without
corresponding pose information. Current sparse-view 3D inference methods
typically rely on camera poses to geometrically aggregate information from
input views, but are not robust in-the-wild when such information is
unavailable/inaccurate. In contrast, UpFusion sidesteps this requirement by
learning to implicitly leverage the available images as context in a
conditional generative model for synthesizing novel views. We incorporate two
complementary forms of conditioning into diffusion models for leveraging the
input views: a) via inferring query-view aligned features using a scene-level
transformer, b) via intermediate attentional layers that can directly observe
the input image tokens. We show that this mechanism allows generating
high-fidelity novel views while improving the synthesis quality given
additional (unposed) images. We evaluate our approach on the Co3Dv2 and Google
Scanned Objects datasets and demonstrate the benefits of our method over
pose-reliant sparse-view methods as well as single-view methods that cannot
leverage additional views. Finally, we also show that our learned model can
generalize beyond the training categories and even allow reconstruction from
self-captured images of generic objects in-the-wild.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://upfusion3d.github.io/ v2: Fixed a citation
  mistake</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Audiovisual Masked Autoencoders <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05922v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05922v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mariana-Iuliana Georgescu, Eduardo Fonseca, Radu Tudor Ionescu, Mario Lucic, Cordelia Schmid, Anurag Arnab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Can we leverage the audiovisual information already present in video to
improve self-supervised representation learning? To answer this question, we
study various pretraining architectures and objectives within the masked
autoencoding framework, motivated by the success of similar methods in natural
language and image understanding. We show that we can achieve significant
improvements on audiovisual downstream classification tasks, surpassing the
state-of-the-art on VGGSound and AudioSet. Furthermore, we can leverage our
audiovisual pretraining scheme for multiple unimodal downstream tasks using a
single audiovisual pretrained model. We additionally demonstrate the
transferability of our representations, achieving state-of-the-art audiovisual
results on Epic Kitchens without pretraining specifically for this dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From 2D Images to 3D Model:Weakly Supervised Multi-View Face
  Reconstruction with Deep Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.03842v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.03842v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiguang Zhao, Chaolong Yang, Jianan Ye, Rui Zhang, Yuyao Yan, Xi Yang, Bin Dong, Amir Hussain, Kaizhu Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While weakly supervised multi-view face reconstruction (MVR) is garnering
increased attention, one critical issue still remains open: how to effectively
fuse multiple image information to reconstruct high-precision 3D models. In
this regard, we propose a novel model called Deep Fusion MVR (DF-MVR) and
design a multi-view encoding to single decoding framework with skip
connections, able to extract, integrate, and compensate deep features with
attention from multi-view images. Furthermore, we adopt the involution kernel
to enrich deep fusion features with channel features. In addition, we develop
the face parse network to learn, identify, and emphasize the critical common
face area within multi-view images. Experiments on Pixel-Face and Bosphorus
datasets indicate the superiority of our model. Without 3D annotation, DF-MVR
achieves 5.2% and 3.0% RMSE improvement over the existing weakly supervised
MVRs respectively on Pixel-Face and Bosphorus dataset. Code will be available
publicly at https://github.com/weiguangzhao/DF_MVR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Brain Tumor Segmentation (BraTS) Challenge 2023: Focus on Pediatrics
  (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17033v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17033v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anahita Fathi Kazerooni, Nastaran Khalili, Xinyang Liu, Debanjan Haldar, Zhifan Jiang, Syed Muhammed Anwar, Jake Albrecht, Maruf Adewole, Udunna Anazodo, Hannah Anderson, Sina Bagheri, Ujjwal Baid, Timothy Bergquist, Austin J. Borja, Evan Calabrese, Verena Chung, Gian-Marco Conte, Farouk Dako, James Eddy, Ivan Ezhov, Ariana Familiar, Keyvan Farahani, Shuvanjan Haldar, Juan Eugenio Iglesias, Anastasia Janas, Elaine Johansen, Blaise V Jones, Florian Kofler, Dominic LaBella, Hollie Anne Lai, Koen Van Leemput, Hongwei Bran Li, Nazanin Maleki, Aaron S McAllister, Zeke Meier, Bjoern Menze, Ahmed W Moawad, Khanak K Nandolia, Julija Pavaine, Marie Piraud, Tina Poussaint, Sanjay P Prabhu, Zachary Reitman, Andres Rodriguez, Jeffrey D Rudie, Ibraheem Salman Shaikh, Lubdha M. Shah, Nakul Sheth, Russel Taki Shinohara, Wenxin Tu, Karthik Viswanathan, Chunhao Wang, Jeffrey B Ware, Benedikt Wiestler, Walter Wiggins, Anna Zapaishchykova, Mariam Aboian, Miriam Bornhorst, Peter de Blank, Michelle Deutsch, Maryam Fouladi, Lindsey Hoffman, Benjamin Kann, Margot Lazow, Leonie Mikael, Ali Nabavizadeh, Roger Packer, Adam Resnick, Brian Rood, Arastoo Vossough, Spyridon Bakas, Marius George Linguraru
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pediatric tumors of the central nervous system are the most common cause of
cancer-related death in children. The five-year survival rate for high-grade
gliomas in children is less than 20\%. Due to their rarity, the diagnosis of
these entities is often delayed, their treatment is mainly based on historic
treatment concepts, and clinical trials require multi-institutional
collaborations. The MICCAI Brain Tumor Segmentation (BraTS) Challenge is a
landmark community benchmark event with a successful history of 12 years of
resource creation for the segmentation and analysis of adult glioma. Here we
present the CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023 challenge, which
represents the first BraTS challenge focused on pediatric brain tumors with
data acquired across multiple international consortia dedicated to pediatric
neuro-oncology and clinical trials. The BraTS-PEDs 2023 challenge focuses on
benchmarking the development of volumentric segmentation algorithms for
pediatric brain glioma through standardized quantitative performance evaluation
metrics utilized across the BraTS 2023 cluster of challenges. Models gaining
knowledge from the BraTS-PEDs multi-parametric structural MRI (mpMRI) training
data will be evaluated on separate validation and unseen test mpMRI dataof
high-grade pediatric glioma. The CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023
challenge brings together clinicians and AI/imaging scientists to lead to
faster development of automated segmentation techniques that could benefit
clinical trials, and ultimately the care of children with brain tumors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Generate Training <span class="highlight-title">Dataset</span>s for Robust Semantic Segmentation <span class="chip">WACV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.02535v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.02535v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marwane Hariat, Olivier Laurent, Rémi Kazmierczak, Shihao Zhang, Andrei Bursuc, Angela Yao, Gianni Franchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation methods have advanced significantly. Still, their
robustness to real-world perturbations and object types not seen during
training remains a challenge, particularly in safety-critical applications. We
propose a novel approach to improve the robustness of semantic segmentation
techniques by leveraging the synergy between label-to-image generators and
image-to-label segmentation models. Specifically, we design Robusta, a novel
robust conditional generative adversarial network to generate realistic and
plausible perturbed images that can be used to train reliable segmentation
models. We conduct in-depth studies of the proposed generative model, assess
the performance and robustness of the downstream segmentation network, and
demonstrate that our approach can significantly enhance the robustness in the
face of real-world perturbations, distribution shifts, and out-of-distribution
samples. Our results suggest that this approach could be valuable in
safety-critical applications, where the reliability of perception modules such
as semantic segmentation is of utmost importance and comes with a limited
computational budget in inference. We release our code at
https://github.com/ENSTA-U2IS/robusta.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at WACV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> and Benchmark of Automatic Surface Reconstruction from Point
  Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.13656v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.13656v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raphael Sulzer, Renaud Marlet, Bruno Vallet, Loic Landrieu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a comprehensive survey and benchmark of both traditional and
learning-based methods for surface reconstruction from point clouds. This task
is particularly challenging for real-world acquisitions due to factors like
noise, outliers, non-uniform sampling, and missing data. Traditional approaches
often simplify the problem by imposing handcrafted priors on either the input
point clouds or the resulting surface, a process that can necessitate tedious
hyperparameter tuning. Conversely, deep learning models have the capability to
directly learn the properties of input point clouds and desired surfaces from
data. We study the influence of these handcrafted and learned priors on the
precision and robustness of surface reconstruction techniques. We evaluate
various time-tested and contemporary methods in a standardized manner. When
both trained and evaluated on point clouds with identical characteristics, the
learning-based models consistently produce superior surfaces compared to their
traditional counterparts$\unicode{x2013}$even in scenarios involving novel
shape categories. However, traditional methods demonstrate greater resilience
to the diverse array of point cloud anomalies commonly found in real-world 3D
acquisitions. For the benefit of the research community, we make our code and
datasets available, inviting further enhancements to learning-based surface
reconstruction. This can be accessed at
https://github.com/raphaelsulzer/dsr-benchmark .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HawkRover: An Autonomous mmWave Vehicular Communication Testbed with
  Multi-sensor Fusion and Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01822v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01822v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Zhu, Haijian Sun, Mingyue Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Connected and automated vehicles (CAVs) have become a transformative
technology that can change our daily life. Currently, millimeter-wave (mmWave)
bands are identified as the promising CAV connectivity solution. While it can
provide high data rate, their realization faces many challenges such as high
attenuation during mmWave signal propagation and mobility management. Existing
solution has to initiate pilot signal to measure channel information, then
apply signal processing to calculate the best narrow beam towards the receiver
end to guarantee sufficient signal power. This process takes significant
overhead and time, hence not suitable for vehicles. In this study, we propose
an autonomous and low-cost testbed to collect extensive co-located mmWave
signal and other sensors data such as LiDAR (Light Detection and Ranging),
cameras, ultrasonic, etc, traditionally for ``automated'', to facilitate mmWave
vehicular communications. Intuitively, these sensors can build a 3D map around
the vehicle and signal propagation path can be estimated, eliminating iterative
the process via pilot signals. This multimodal data fusion, together with AI,
is expected to bring significant advances in ``connected'' research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE conferences for future publications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamically Masked Discriminator for Generative Adversarial Networks <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.07716v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.07716v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentian Zhang, Haozhe Liu, Bing Li, Jinheng Xie, Yawen Huang, Yuexiang Li, Yefeng Zheng, Bernard Ghanem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training Generative Adversarial Networks (GANs) remains a challenging
problem. The discriminator trains the generator by learning the distribution of
real/generated data. However, the distribution of generated data changes
throughout the training process, which is difficult for the discriminator to
learn. In this paper, we propose a novel method for GANs from the viewpoint of
online continual learning. We observe that the discriminator model, trained on
historically generated data, often slows down its adaptation to the changes in
the new arrival generated data, which accordingly decreases the quality of
generated results. By treating the generated data in training as a stream, we
propose to detect whether the discriminator slows down the learning of new
knowledge in generated data. Therefore, we can explicitly enforce the
discriminator to learn new knowledge fast. Particularly, we propose a new
discriminator, which automatically detects its retardation and then dynamically
masks its features, such that the discriminator can adaptively learn the
temporally-vary distribution of generated data. Experimental results show our
method outperforms the state-of-the-art approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated v2 -- NeurIPS 2023 camera ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Few-shot Adaptation of Multi-modal Foundation Models: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01736v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01736v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Liu, Tianshu Zhang, Wenwen Dai, Wenwen Cai, Xiaocong Zhou, Delong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal (vision-language) models, such as CLIP, are replacing traditional
supervised pre-training models (e.g., ImageNet-based pre-training) as the new
generation of visual foundation models. These models with robust and aligned
semantic representations learned from billions of internet image-text pairs and
can be applied to various downstream tasks in a zero-shot manner. However, in
some fine-grained domains like medical imaging and remote sensing, the
performance of multi-modal foundation models often leaves much to be desired.
Consequently, many researchers have begun to explore few-shot adaptation
methods for these models, gradually deriving three main technical approaches:
1) prompt-based methods, 2) adapter-based methods, and 3) external
knowledge-based methods. Nevertheless, this rapidly developing field has
produced numerous results without a comprehensive survey to systematically
organize the research progress. Therefore, in this survey, we introduce and
analyze the research advancements in few-shot adaptation methods for
multi-modal models, summarizing commonly used datasets and experimental setups,
and comparing the results of different methods. In addition, due to the lack of
reliable theoretical support for existing methods, we derive the few-shot
adaptation generalization error bound for multi-modal models. The theorem
reveals that the generalization error of multi-modal foundation models is
constrained by three factors: domain gap, model capacity, and sample size.
Based on this, we propose three possible solutions from the following aspects:
1) adaptive domain generalization, 2) adaptive model selection, and 3) adaptive
knowledge utilization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Expressive Speech-driven Facial Animation with controllable emotions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.02008v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.02008v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutong Chen, Junhong Zhao, Wei-Qiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is in high demand to generate facial animation with high realism, but it
remains a challenging task. Existing approaches of speech-driven facial
animation can produce satisfactory mouth movement and lip synchronization, but
show weakness in dramatic emotional expressions and flexibility in emotion
control. This paper presents a novel deep learning-based approach for
expressive facial animation generation from speech that can exhibit
wide-spectrum facial expressions with controllable emotion type and intensity.
We propose an emotion controller module to learn the relationship between the
emotion variations (e.g., types and intensity) and the corresponding facial
expression parameters. It enables emotion-controllable facial animation, where
the target expression can be continuously adjusted as desired. The qualitative
and quantitative evaluations show that the animation generated by our method is
rich in facial emotional expressiveness while retaining accurate lip movement,
outperforming other state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Fully Decoupled End-to-End Person Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.04967v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.04967v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengcheng Zhang, Xiao Bai, Jin Zheng, Xin Ning
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end person search aims to jointly detect and re-identify a target
person in raw scene images with a unified model. The detection task unifies all
persons while the re-id task discriminates different identities, resulting in
conflict optimal objectives. Existing works proposed to decouple end-to-end
person search to alleviate such conflict. Yet these methods are still
sub-optimal on one or two of the sub-tasks due to their partially decoupled
models, which limits the overall person search performance. In this paper, we
propose to fully decouple person search towards optimal person search. A
task-incremental person search network is proposed to incrementally construct
an end-to-end model for the detection and re-id sub-task, which decouples the
model architecture for the two sub-tasks. The proposed task-incremental network
allows task-incremental training for the two conflicting tasks. This enables
independent learning for different objectives thus fully decoupled the model
for persons earch. Comprehensive experimental evaluations demonstrate the
effectiveness of the proposed fully decoupled models for end-to-end person
search.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>DICTA 2023 Best Student Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Certification of Vision-Language Models Using Incremental
  Randomized Smoothing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09024v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09024v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        A K Nirala, A Joshi, C Hegde, S Sarkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A key benefit of deep vision-language models such as CLIP is that they enable
zero-shot open vocabulary classification; the user has the ability to define
novel class labels via natural language prompts at inference time. However,
while CLIP-based zero-shot classifiers have demonstrated competitive
performance across a range of domain shifts, they remain highly vulnerable to
adversarial attacks. Therefore, ensuring the robustness of such models is
crucial for their reliable deployment in the wild.
  In this work, we introduce Open Vocabulary Certification (OVC), a fast
certification method designed for open-vocabulary models like CLIP via
randomized smoothing techniques. Given a base "training" set of prompts and
their corresponding certified CLIP classifiers, OVC relies on the observation
that a classifier with a novel prompt can be viewed as a perturbed version of
nearby classifiers in the base training set. Therefore, OVC can rapidly certify
the novel classifier using a variation of incremental randomized smoothing. By
using a caching trick, we achieve approximately two orders of magnitude
acceleration in the certification process for novel prompts. To achieve further
(heuristic) speedups, OVC approximates the embedding space at a given input
using a multivariate normal distribution bypassing the need for sampling via
forward passes through the vision backbone. We demonstrate the effectiveness of
OVC on through experimental evaluation using multiple vision-language backbones
on the CIFAR-10 and ImageNet test datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TSGAN: An Optical-to-SAR Dual Conditional GAN for Optical based SAR
  Temporal Shifting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00440v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00440v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moien Rangzan, Sara Attarchi, Richard Gloaguen, Seyed Kazem Alavipanah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In contrast to the well-investigated field of SAR-to-Optical translation,
this study explores the lesser-investigated domain of Optical-to-SAR
translation, a challenging field due to the ill-posed nature of this
translation. The complexity arises as a single optical data can have multiple
SAR representations based on the SAR viewing geometry. We propose a novel
approach, termed SAR Temporal Shifting, which inputs an optical data from the
desired timestamp along with a SAR data from a different temporal point but
with a consistent viewing geometry as the expected SAR data, both complemented
with a change map of optical data during the intervening period. This model
modifies the SAR data based on the changes observed in optical data to generate
the SAR data for the desired timestamp. Our model, a dual conditional
Generative Adversarial Network (GAN), named Temporal Shifting GAN (TSGAN),
incorporates a siamese encoder in both the Generator and the Discriminator. To
prevent the model from overfitting on the input SAR data, we employed a change
weighted loss function. Our approach surpasses traditional translation methods
by eliminating the GAN's fiction phenomenon, particularly in unchanged regions,
resulting in higher SSIM and PSNR in these areas. Additionally, modifications
to the Pix2Pix architecture and the inclusion of attention mechanisms have
enhanced the model's performance on all regions of the data. This research
paves the way for leveraging legacy optical datasets, the most abundant and
longstanding source of Earth imagery data, extending their use to SAR domains
and temporal analyses. To foster further research, we provide the code,
datasets used in our study, and a framework for generating paired SAR-Optical
datasets for new regions of interest. These resources are available on
github.com/moienr/TemporalGAN
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Comments: Added acknowledgments and corrected a typo. No changes to
  the main content</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SLP-Net:An efficient lightweight network for segmentation of skin
  lesions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12789v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12789v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Yang, Hong Peng, Chenggang Guo, Xiaohui Luo, Jun Wang, Xianzhong Long
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt treatment for melanoma is crucial. To assist physicians in identifying
lesion areas precisely in a quick manner, we propose a novel skin lesion
segmentation technique namely SLP-Net, an ultra-lightweight segmentation
network based on the spiking neural P(SNP) systems type mechanism. Most
existing convolutional neural networks achieve high segmentation accuracy while
neglecting the high hardware cost. SLP-Net, on the contrary, has a very small
number of parameters and a high computation speed. We design a lightweight
multi-scale feature extractor without the usual encoder-decoder structure.
Rather than a decoder, a feature adaptation module is designed to replace it
and implement multi-scale information decoding. Experiments at the ISIC2018
challenge demonstrate that the proposed model has the highest Acc and DSC among
the state-of-the-art methods, while experiments on the PH2 dataset also
demonstrate a favorable generalization ability. Finally, we compare the
computational complexity as well as the computational speed of the models in
experiments, where SLP-Net has the highest overall superiority
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-stages attention Breast cancer classification based on nonlinear
  spiking neural P neurons with autapses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12804v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12804v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Yang, Hong Peng, Xiaohui Luo, Jun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Breast cancer(BC) is a prevalent type of malignant tumor in women. Early
diagnosis and treatment are vital for enhancing the patients' survival rate.
Downsampling in deep networks may lead to loss of information, so for
compensating the detail and edge information and allowing convolutional neural
networks to pay more attention to seek the lesion region, we propose a
multi-stages attention architecture based on NSNP neurons with autapses. First,
unlike the single-scale attention acquisition methods of existing methods, we
set up spatial attention acquisition at each feature map scale of the
convolutional network to obtain an fusion global information on attention
guidance. Then we introduce a new type of NSNP variants called NSNP neurons
with autapses. Specifically, NSNP systems are modularized as feature encoders,
recoding the features extracted from convolutional neural network as well as
the fusion of attention information and preserve the key characteristic
elements in feature maps. This ensures the retention of valuable data while
gradually transforming high-dimensional complicated info into low-dimensional
ones. The proposed method is evaluated on the public dataset BreakHis at
various magnifications and classification tasks. It achieves a classification
accuracy of 96.32% at all magnification cases, outperforming state-of-the-art
methods. Ablation studies are also performed, verifying the proposed model's
efficacy. The source code is available at
XhuBobYoung/Breast-cancer-Classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training-free Content Injection using h-space in Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.15403v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.15403v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaeseok Jeong, Mingi Kwon, Youngjung Uh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models (DMs) synthesize high-quality images in various domains.
However, controlling their generative process is still hazy because the
intermediate variables in the process are not rigorously studied. Recently, the
bottleneck feature of the U-Net, namely $h$-space, is found to convey the
semantics of the resulting image. It enables StyleCLIP-like latent editing
within DMs. In this paper, we explore further usage of $h$-space beyond
attribute editing, and introduce a method to inject the content of one image
into another image by combining their features in the generative processes.
Briefly, given the original generative process of the other image, 1) we
gradually blend the bottleneck feature of the content with proper
normalization, and 2) we calibrate the skip connections to match the injected
content. Unlike custom-diffusion approaches, our method does not require
time-consuming optimization or fine-tuning. Instead, our method manipulates
intermediate features within a feed-forward generative process. Furthermore,
our method does not require supervision from external networks. The code is
available at https://curryjung.github.io/InjectFusion/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Handling Noisy Labels via One-Step Abductive Multi-Target Learning and
  Its Application to Helicobacter Pylori Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2011.14956v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2011.14956v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongquan Yang, Yiming Yang, Jie Chen, Jiayi Zheng, Zhongxi Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning from noisy labels is an important concern in plenty of real-world
scenarios. Various approaches for this concern first make corrections
corresponding to potentially noisy-labeled instances, and then update
predictive model with information of the made corrections. However, in specific
areas, such as medical histopathology whole slide image analysis (MHWSIA), it
is often difficult or impossible for experts to manually achieve the noisy-free
ground-truth labels which leads to labels with complex noise. This situation
raises two more difficult problems: 1) the methodology of approaches making
corrections corresponding to potentially noisy-labeled instances has
limitations due to the complex noise existing in labels; and 2) the appropriate
evaluation strategy for validation/testing is unclear because of the great
difficulty in collecting the noisy-free ground-truth labels. For the problem
1), we present one-step abductive multi-target learning (OSAMTL) that imposes a
one-step logical reasoning upon machine learning via a multi-target learning
procedure to constrain the predictions of the learning model to be subject to
our prior knowledge about the true target. For the problem 2), we propose a
logical assessment formula (LAF) that evaluates the logical rationality of the
outputs of an approach by estimating the consistencies between the predictions
of the learning model and the logical facts narrated from the results of the
one-step logical reasoning of OSAMTL. Based on the Helicobacter pylori (H.
pylori) segmentation task in MHWSIA, we show that OSAMTL enables the machine
learning model achieving logically more rational predictions, which is beyond
various state-of-the-art approaches in handling complex noisy labels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spiking NeRF: Representing the Real-World Geometry by a Discontinuous
  Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09077v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09077v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanfeng Liao, Qian Zheng, Yan Liu, Gang Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A crucial reason for the success of existing NeRF-based methods is to build a
neural density field for the geometry representation via multiple perceptron
layers (MLPs). MLPs are continuous functions, however, real geometry or density
field is frequently discontinuous at the interface between the air and the
surface. Such a contrary brings the problem of unfaithful geometry
representation. To this end, this paper proposes spiking NeRF, which leverages
spiking neurons and a hybrid Artificial Neural Network (ANN)-Spiking Neural
Network (SNN) framework to build a discontinuous density field for faithful
geometry representation. Specifically, we first demonstrate the reason why
continuous density fields will bring inaccuracy. Then, we propose to use the
spiking neurons to build a discontinuous density field. We conduct a
comprehensive analysis for the problem of existing spiking neuron models and
then provide the numerical relationship between the parameter of the spiking
neuron and the theoretical accuracy of geometry. Based on this, we propose a
bounded spiking neuron to build the discontinuous density field. Our method
achieves SOTA performance. The source code and the supplementary material are
available at https://github.com/liaozhanfeng/Spiking-NeRF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shadow Generation with Decomposed Mask Prediction and Attentive Shadow
  Filling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.17358v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.17358v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinhao Tao, Junyan Cao, Yan Hong, Li Niu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image composition refers to inserting a foreground object into a background
image to obtain a composite image. In this work, we focus on generating
plausible shadows for the inserted foreground object to make the composite
image more realistic. To supplement the existing small-scale dataset, we create
a large-scale dataset called RdSOBA with rendering techniques. Moreover, we
design a two-stage network named DMASNet with decomposed mask prediction and
attentive shadow filling. Specifically, in the first stage, we decompose shadow
mask prediction into box prediction and shape prediction. In the second stage,
we attend to reference background shadow pixels to fill the foreground shadow.
Abundant experiments prove that our DMASNet achieves better visual effects and
generalizes well to real composite images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLASS-M: Adaptive stain separation-based contrastive learning with
  pseudo-labeling for histopathological image classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06978v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06978v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bodong Zhang, Hamid Manoochehri, Man Minh Ho, Fahimeh Fooladgar, Yosep Chong, Beatrice S. Knudsen, Deepika Sirohi, Tolga Tasdizen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Histopathological image classification is an important task in medical image
analysis. Recent approaches generally rely on weakly supervised learning due to
the ease of acquiring case-level labels from pathology reports. However,
patch-level classification is preferable in applications where only a limited
number of cases are available or when local prediction accuracy is critical. On
the other hand, acquiring extensive datasets with localized labels for training
is not feasible. In this paper, we propose a semi-supervised patch-level
histopathological image classification model, named CLASS-M, that does not
require extensively labeled datasets. CLASS-M is formed by two main parts: a
contrastive learning module that uses separated Hematoxylin and Eosin images
generated through an adaptive stain separation process, and a module with
pseudo-labels using MixUp. We compare our model with other state-of-the-art
models on two clear cell renal cell carcinoma datasets. We demonstrate that our
CLASS-M model has the best performance on both datasets. Our code is available
at github.com/BzhangURU/Paper_CLASS-M/tree/main
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Depth-Regularized Optimization for 3D Gaussian Splatting in Few-Shot
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13398v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13398v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaeyoung Chung, Jeongtaek Oh, Kyoung Mu Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a method to optimize Gaussian splatting with a
limited number of images while avoiding overfitting. Representing a 3D scene by
combining numerous Gaussian splats has yielded outstanding visual quality.
However, it tends to overfit the training views when only a small number of
images are available. To address this issue, we introduce a dense depth map as
a geometry guide to mitigate overfitting. We obtained the depth map using a
pre-trained monocular depth estimation model and aligning the scale and offset
using sparse COLMAP feature points. The adjusted depth aids in the color-based
optimization of 3D Gaussian splatting, mitigating floating artifacts, and
ensuring adherence to geometric constraints. We verify the proposed method on
the NeRF-LLFF dataset with varying numbers of few images. Our approach
demonstrates robust geometry compared to the original method that relies solely
on images. Project page: robot0321.github.io/DepthRegGS
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures; Project page: robot0321.github.io/DepthRegGS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Text-to-Image Editing via Cache-Enabled Sparse Diffusion
  Inference <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17423v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17423v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Yu, Haoyang Li, Fangcheng Fu, Xupeng Miao, Bin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the recent success of diffusion models, text-to-image generation is
becoming increasingly popular and achieves a wide range of applications. Among
them, text-to-image editing, or continuous text-to-image generation, attracts
lots of attention and can potentially improve the quality of generated images.
It's common to see that users may want to slightly edit the generated image by
making minor modifications to their input textual descriptions for several
rounds of diffusion inference. However, such an image editing process suffers
from the low inference efficiency of many existing diffusion models even using
GPU accelerators. To solve this problem, we introduce Fast Image Semantically
Edit (FISEdit), a cached-enabled sparse diffusion model inference engine for
efficient text-to-image editing. The key intuition behind our approach is to
utilize the semantic mapping between the minor modifications on the input text
and the affected regions on the output image. For each text editing step,
FISEdit can automatically identify the affected image regions and utilize the
cached unchanged regions' feature map to accelerate the inference process.
Extensive empirical results show that FISEdit can be $3.4\times$ and
$4.4\times$ faster than existing methods on NVIDIA TITAN RTX and A100 GPUs
respectively, and even generates more satisfactory images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A General Implicit Framework for Fast NeRF Composition and Rendering <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.04669v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.04669v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Gao, Ziyi Yang, Yunlu Zhao, Yuxiang Sun, Xiaogang Jin, Changqing Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A variety of Neural Radiance Fields (NeRF) methods have recently achieved
remarkable success in high render speed. However, current accelerating methods
are specialized and incompatible with various implicit methods, preventing
real-time composition over various types of NeRF works. Because NeRF relies on
sampling along rays, it is possible to provide general guidance for
acceleration. To that end, we propose a general implicit pipeline for composing
NeRF objects quickly. Our method enables the casting of dynamic shadows within
or between objects using analytical light sources while allowing multiple NeRF
objects to be seamlessly placed and rendered together with any arbitrary rigid
transformations. Mainly, our work introduces a new surface representation known
as Neural Depth Fields (NeDF) that quickly determines the spatial relationship
between objects by allowing direct intersection computation between rays and
implicit surfaces. It leverages an intersection neural network to query NeRF
for acceleration instead of depending on an explicit spatial structure.Our
proposed method is the first to enable both the progressive and interactive
composition of NeRF objects. Additionally, it also serves as a previewing
plugin for a range of existing NeRF works.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fusion of Single and Integral Multispectral Aerial Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17515v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17515v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Youssef, Oliver Bimber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A novel hybrid (model- and learning-based) architecture is presented for
fusing the most significant features from conventional aerial images with the
ones from integral aerial images that are the result of synthetic aperture
sensing for removing occlusion. It combines the environment's spatial
references with features of unoccluded targets that would normally be hidden by
dense vegetation. Our method out-beats state-of-the-art two-channel and
multi-channel fusion approaches visually and quantitatively in common metrics,
such as mutual information, visual information fidelity, and peak
signal-to-noise ratio. The proposed model does not require manually tuned
parameters, can be extended to an arbitrary number and combinations of spectral
channels, and is reconfigurable for addressing different use cases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High-Fidelity Diffusion-based Image Editing <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.15707v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.15707v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Hou, Guoqiang Wei, Zhibo Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have attained remarkable success in the domains of image
generation and editing. It is widely recognized that employing larger inversion
and denoising steps in diffusion model leads to improved image reconstruction
quality. However, the editing performance of diffusion models tends to be no
more satisfactory even with increasing denoising steps. The deficiency in
editing could be attributed to the conditional Markovian property of the
editing process, where errors accumulate throughout denoising steps. To tackle
this challenge, we first propose an innovative framework where a rectifier
module is incorporated to modulate diffusion model weights with residual
features, thereby providing compensatory information to bridge the fidelity
gap. Furthermore, we introduce a novel learning paradigm aimed at minimizing
error propagation during the editing process, which trains the editing
procedure in a manner similar to denoising score-matching. Extensive
experiments demonstrate that our proposed framework and training strategy
achieve high-fidelity reconstruction and editing results across various levels
of denoising steps, meanwhile exhibits exceptional performance in terms of both
quantitative metric and qualitative assessments. Moreover, we explore our
model's generalization through several applications like image-to-image
translation and out-of-domain image editing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InstructTA: Instruction-Tuned Targeted Attack for Large Vision-Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.01886v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.01886v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xunguang Wang, Zhenlan Ji, Pingchuan Ma, Zongjie Li, Shuai Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models (LVLMs) have demonstrated their incredible
capability in image understanding and response generation. However, this rich
visual interaction also makes LVLMs vulnerable to adversarial examples. In this
paper, we formulate a novel and practical gray-box attack scenario that the
adversary can only access the visual encoder of the victim LVLM, without the
knowledge of its prompts (which are often proprietary for service providers and
not publicly available) and its underlying large language model (LLM). This
practical setting poses challenges to the cross-prompt and cross-model
transferability of targeted adversarial attack, which aims to confuse the LVLM
to output a response that is semantically similar to the attacker's chosen
target text. To this end, we propose an instruction-tuned targeted attack
(dubbed InstructTA) to deliver the targeted adversarial attack on LVLMs with
high transferability. Initially, we utilize a public text-to-image generative
model to "reverse" the target response into a target image, and employ GPT-4 to
infer a reasonable instruction $\boldsymbol{p}^\prime$ from the target
response. We then form a local surrogate model (sharing the same visual encoder
with the victim LVLM) to extract instruction-aware features of an adversarial
image example and the target image, and minimize the distance between these two
features to optimize the adversarial example. To further improve the
transferability, we augment the instruction $\boldsymbol{p}^\prime$ with
instructions paraphrased from an LLM. Extensive experiments demonstrate the
superiority of our proposed method in targeted attack performance and
transferability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rewrite Caption Semantics: Bridging Semantic Gaps for
  Language-Supervised Semantic Segmentation <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13505v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13505v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun Xing, Jian Kang, Aoran Xiao, Jiahao Nie, Ling Shao, Shijian Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Pre-training has demonstrated its remarkable zero-shot
recognition ability and potential to learn generalizable visual representations
from language supervision. Taking a step ahead, language-supervised semantic
segmentation enables spatial localization of textual inputs by learning pixel
grouping solely from image-text pairs. Nevertheless, the state-of-the-art
suffers from clear semantic gaps between visual and textual modality: plenty of
visual concepts appeared in images are missing in their paired captions. Such
semantic misalignment circulates in pre-training, leading to inferior zero-shot
performance in dense predictions due to insufficient visual concepts captured
in textual representations. To close such semantic gap, we propose Concept
Curation (CoCu), a pipeline that leverages CLIP to compensate for the missing
semantics. For each image-text pair, we establish a concept archive that
maintains potential visually-matched concepts with our proposed vision-driven
expansion and text-to-vision-guided ranking. Relevant concepts can thus be
identified via cluster-guided sampling and fed into pre-training, thereby
bridging the gap between visual and textual semantics. Extensive experiments
over a broad suite of 8 segmentation benchmarks show that CoCu achieves superb
zero-shot transfer performance and greatly boosts language-supervised
segmentation baseline by a large margin, suggesting the value of bridging
semantic gap in pre-training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023. Code is available at
  https://github.com/xing0047/rewrite</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual Learning: Forget-free Winning Subnetworks for Video
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11973v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11973v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haeyong Kang, Jaehong Yoon, Sung Ju Hwang, Chang D. Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by the Lottery Ticket Hypothesis (LTH), which highlights the
existence of efficient subnetworks within larger, dense networks, a
high-performing Winning Subnetwork (WSN) in terms of task performance under
appropriate sparsity conditions is considered for various continual learning
tasks. It leverages pre-existing weights from dense networks to achieve
efficient learning in Task Incremental Learning (TIL) scenarios. In Few-Shot
Class Incremental Learning (FSCIL), a variation of WSN referred to as the Soft
subnetwork (SoftNet) is designed to prevent overfitting when the data samples
are scarce. Furthermore, the sparse reuse of WSN weights is considered for
Video Incremental Learning (VIL). The use of Fourier Subneural Operator (FSO)
within WSN is considered. It enables compact encoding of videos and identifies
reusable subnetworks across varying bandwidths. We have integrated FSO into
different architectural frameworks for continual learning, including VIL, TIL,
and FSCIL. Our comprehensive experiments demonstrate FSO's effectiveness,
significantly improving task performance at various convolutional
representational levels. Specifically, FSO enhances higher-layer performance in
TIL and FSCIL and lower-layer performance in VIL
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2303.14962,
  arXiv:2306.11305</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VSFormer: Visual-Spatial Fusion <span class="highlight-title">Transformer</span> for Correspondence Pruning <span class="chip">AAAI2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.08774v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.08774v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tangfei Liao, Xiaoqin Zhang, Li Zhao, Tao Wang, Guobao Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Correspondence pruning aims to find correct matches (inliers) from an initial
set of putative correspondences, which is a fundamental task for many
applications. The process of finding is challenging, given the varying inlier
ratios between scenes/image pairs due to significant visual differences.
However, the performance of the existing methods is usually limited by the
problem of lacking visual cues (\eg texture, illumination, structure) of
scenes. In this paper, we propose a Visual-Spatial Fusion Transformer
(VSFormer) to identify inliers and recover camera poses accurately. Firstly, we
obtain highly abstract visual cues of a scene with the cross attention between
local features of two-view images. Then, we model these visual cues and
correspondences by a joint visual-spatial fusion module, simultaneously
embedding visual cues into correspondences for pruning. Additionally, to mine
the consistency of correspondences, we also design a novel module that combines
the KNN-based graph and the transformer, effectively capturing both local and
global contexts. Extensive experiments have demonstrated that the proposed
VSFormer outperforms state-of-the-art methods on outdoor and indoor benchmarks.
Our code is provided at the following repository:
https://github.com/sugar-fly/VSFormer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HEAP: Unsupervised Object Discovery and Localization with Contrastive
  Grouping <span class="chip">AAAI24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17492v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17492v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zhang, Jinheng Xie, Yuan Yuan, Michael Bi Mi, Robby T. Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised object discovery and localization aims to detect or segment
objects in an image without any supervision. Recent efforts have demonstrated a
notable potential to identify salient foreground objects by utilizing
self-supervised transformer features. However, their scopes only build upon
patch-level features within an image, neglecting region/image-level and
cross-image relationships at a broader scale. Moreover, these methods cannot
differentiate various semantics from multiple instances. To address these
problems, we introduce Hierarchical mErging framework via contrAstive grouPing
(HEAP). Specifically, a novel lightweight head with cross-attention mechanism
is designed to adaptively group intra-image patches into semantically coherent
regions based on correlation among self-supervised features. Further, to ensure
the distinguishability among various regions, we introduce a region-level
contrastive clustering loss to pull closer similar regions across images. Also,
an image-level contrastive loss is present to push foreground and background
representations apart, with which foreground objects and background are
accordingly discovered. HEAP facilitates efficient hierarchical image
decomposition, which contributes to more accurate object discovery while also
enabling differentiation among objects of various classes. Extensive
experimental results on semantic segmentation retrieval, unsupervised object
discovery, and saliency detection tasks demonstrate that HEAP achieves
state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accurate Leukocyte Detection Based on Deformable-DETR and Multi-Level
  Feature Fusion for Aiding Diagnosis of Blood Diseases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00926v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00926v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Chen, Chenyan Zhang, Ben Chen, Yiyu Huang, Yifei Sun, Changmiao Wang, Xianjun Fu, Yuxing Dai, Feiwei Qin, Yong Peng, Yu Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In standard hospital blood tests, the traditional process requires doctors to
manually isolate leukocytes from microscopic images of patients' blood using
microscopes. These isolated leukocytes are then categorized via automatic
leukocyte classifiers to determine the proportion and volume of different types
of leukocytes present in the blood samples, aiding disease diagnosis. This
methodology is not only time-consuming and labor-intensive, but it also has a
high propensity for errors due to factors such as image quality and
environmental conditions, which could potentially lead to incorrect subsequent
classifications and misdiagnosis. To address these issues, this paper proposes
an innovative method of leukocyte detection: the Multi-level Feature Fusion and
Deformable Self-attention DETR (MFDS-DETR). To tackle the issue of leukocyte
scale disparity, we designed the High-level Screening-feature Fusion Pyramid
(HS-FPN), enabling multi-level fusion. This model uses high-level features as
weights to filter low-level feature information via a channel attention module
and then merges the screened information with the high-level features, thus
enhancing the model's feature expression capability. Further, we address the
issue of leukocyte feature scarcity by incorporating a multi-scale deformable
self-attention module in the encoder and using the self-attention and
cross-deformable attention mechanisms in the decoder, which aids in the
extraction of the global features of the leukocyte feature maps. The
effectiveness, superiority, and generalizability of the proposed MFDS-DETR
method are confirmed through comparisons with other cutting-edge leukocyte
detection models using the private WBCDD, public LISC and BCCD datasets. Our
source code and private WBCCD dataset are available at
https://github.com/JustlfC03/MFDS-DETR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures, accept Computers in Biology and Medicine 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InternVid: A Large-scale Video-Text <span class="highlight-title">Dataset</span> for Multimodal Understanding
  and Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06942v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06942v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, Conghui He, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces InternVid, a large-scale video-centric multimodal
dataset that enables learning powerful and transferable video-text
representations for multimodal understanding and generation. The InternVid
dataset contains over 7 million videos lasting nearly 760K hours, yielding 234M
video clips accompanied by detailed descriptions of total 4.1B words. Our core
contribution is to develop a scalable approach to autonomously build a
high-quality video-text dataset with large language models (LLM), thereby
showcasing its efficacy in learning video-language representation at scale.
Specifically, we utilize a multi-scale approach to generate video-related
descriptions. Furthermore, we introduce ViCLIP, a video-text representation
learning model based on ViT-L. Learned on InternVid via contrastive learning,
this model demonstrates leading zero-shot action recognition and competitive
video retrieval performance. Beyond basic video understanding tasks like
recognition and retrieval, our dataset and model have broad applications. They
are particularly beneficial for generating interleaved video-text data for
learning a video-centric dialogue system, advancing video-to-text and
text-to-video generation research. These proposed resources provide a tool for
researchers and practitioners interested in multimodal video understanding and
generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Data and Code:
  https://github.com/OpenGVLab/InternVideo/tree/main/Data/InternVid</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BLADE: Box-Level Supervised Amodal Segmentation through Directed
  Expansion <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01642v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01642v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaochen Liu, Zhixuan Li, Tingting Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perceiving the complete shape of occluded objects is essential for human and
machine intelligence. While the amodal segmentation task is to predict the
complete mask of partially occluded objects, it is time-consuming and
labor-intensive to annotate the pixel-level ground truth amodal masks.
Box-level supervised amodal segmentation addresses this challenge by relying
solely on ground truth bounding boxes and instance classes as supervision,
thereby alleviating the need for exhaustive pixel-level annotations.
Nevertheless, current box-level methodologies encounter limitations in
generating low-resolution masks and imprecise boundaries, failing to meet the
demands of practical real-world applications. We present a novel solution to
tackle this problem by introducing a directed expansion approach from visible
masks to corresponding amodal masks. Our approach involves a hybrid end-to-end
network based on the overlapping region - the area where different instances
intersect. Diverse segmentation strategies are applied for overlapping regions
and non-overlapping regions according to distinct characteristics. To guide the
expansion of visible masks, we introduce an elaborately-designed connectivity
loss for overlapping regions, which leverages correlations with visible masks
and facilitates accurate amodal segmentation. Experiments are conducted on
several challenging datasets and the results show that our proposed method can
outperform existing state-of-the-art methods with large margins.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Frequency Domain Modality-invariant Feature Learning for
  Visible-infrared Person Re-Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01839v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01839v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulin Li, Tianzhu Zhang, Yongdong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visible-infrared person re-identification (VI-ReID) is challenging due to the
significant cross-modality discrepancies between visible and infrared images.
While existing methods have focused on designing complex network architectures
or using metric learning constraints to learn modality-invariant features, they
often overlook which specific component of the image causes the modality
discrepancy problem. In this paper, we first reveal that the difference in the
amplitude component of visible and infrared images is the primary factor that
causes the modality discrepancy and further propose a novel Frequency Domain
modality-invariant feature learning framework (FDMNet) to reduce modality
discrepancy from the frequency domain perspective. Our framework introduces two
novel modules, namely the Instance-Adaptive Amplitude Filter (IAF) module and
the Phrase-Preserving Normalization (PPNorm) module, to enhance the
modality-invariant amplitude component and suppress the modality-specific
component at both the image- and feature-levels. Extensive experimental results
on two standard benchmarks, SYSU-MM01 and RegDB, demonstrate the superior
performance of our FDMNet against state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video Understanding with Large Language Models: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17432v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17432v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan Liang, Teng Wang, Daoan Zhang, Jie An, Jingyang Lin, Rongyi Zhu, Ali Vosoughi, Chao Huang, Zeliang Zhang, Feng Zheng, Jianguo Zhang, Ping Luo, Jiebo Luo, Chenliang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the burgeoning growth of online video platforms and the escalating
volume of video content, the demand for proficient video understanding tools
has intensified markedly. Given the remarkable capabilities of Large Language
Models (LLMs) in language and multimodal tasks, this survey provides a detailed
overview of the recent advancements in video understanding harnessing the power
of LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly
advanced, particularly their ability for open-ended spatial-temporal reasoning
combined with commonsense knowledge, suggesting a promising path for future
video understanding. We examine the unique characteristics and capabilities of
Vid-LLMs, categorizing the approaches into four main types: LLM-based Video
Agents, Vid-LLMs Pretraining, Vid-LLMs Instruction Tuning, and Hybrid Methods.
Furthermore, this survey presents a comprehensive study of the tasks, datasets,
and evaluation methodologies for Vid-LLMs. Additionally, it explores the
expansive applications of Vid-LLMs across various domains, highlighting their
remarkable scalability and versatility in real-world video understanding
challenges. Finally, it summarizes the limitations of existing Vid-LLMs and
outlines directions for future research. For more information, readers are
recommended to visit the repository at
https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MLN-net: A multi-source medical image segmentation method for clustered
  microcalcifications using multiple layer normalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.02742v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.02742v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Wang, Zanting Ye, Xiang Xie, Haidong Cui, Tao Chen, Banteng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate segmentation of clustered microcalcifications in mammography is
crucial for the diagnosis and treatment of breast cancer. Despite exhibiting
expert-level accuracy, recent deep learning advancements in medical image
segmentation provide insufficient contribution to practical applications, due
to the domain shift resulting from differences in patient postures, individual
gland density, and imaging modalities of mammography etc. In this paper, a
novel framework named MLN-net, which can accurately segment multi-source images
using only single source images, is proposed for clustered microcalcification
segmentation. We first propose a source domain image augmentation method to
generate multi-source images, leading to improved generalization. And a
structure of multiple layer normalization (LN) layers is used to construct the
segmentation network, which can be found efficient for clustered
microcalcification segmentation in different domains. Additionally, a branch
selection strategy is designed for measuring the similarity of the source
domain data and the target domain data. To validate the proposed MLN-net,
extensive analyses including ablation experiments are performed, comparison of
12 baseline methods. Extensive experiments validate the effectiveness of
MLN-net in segmenting clustered microcalcifications from different domains and
the its segmentation accuracy surpasses state-of-the-art methods. Code will be
available at https://github.com/yezanting/MLN-NET-VERSON1.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 9 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploiting Spatial-Temporal Context for Interacting Hand Reconstruction
  on Monocular RGB Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.04074v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.04074v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weichao Zhao, Hezhen Hu, Wengang Zhou, Li li, Houqiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing interacting hands from monocular RGB data is a challenging
task, as it involves many interfering factors, e.g. self- and mutual occlusion
and similar textures. Previous works only leverage information from a single
RGB image without modeling their physically plausible relation, which leads to
inferior reconstruction results. In this work, we are dedicated to explicitly
exploiting spatial-temporal information to achieve better interacting hand
reconstruction. On one hand, we leverage temporal context to complement
insufficient information provided by the single frame, and design a novel
temporal framework with a temporal constraint for interacting hand motion
smoothness. On the other hand, we further propose an interpenetration detection
module to produce kinetically plausible interacting hands without physical
collisions. Extensive experiments are performed to validate the effectiveness
of our proposed framework, which achieves new state-of-the-art performance on
public benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VideoChat: Chat-Centric Video Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.06355v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.06355v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we initiate an attempt of developing an end-to-end
chat-centric video understanding system, coined as VideoChat. It integrates
video foundation models and large language models via a learnable neural
interface, excelling in spatiotemporal reasoning, event localization, and
causal relationship inference. To instructively tune this system, we build a
video-centric instruction dataset, composed of thousands of videos associated
with detailed descriptions and conversations. This dataset emphasizes
spatiotemporal reasoning and captures causal relationships, providing a
valuable asset for training our chat-centric video understanding system.
Preliminary qualitative experiments demonstrate the potential of our system
across a broad spectrum of video applications, which could serve as a simple
prototype system for future research on chat-centric video understanding.
Access our code and data at https://github.com/OpenGVLab/Ask-Anything
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantifying Deep Learning Model Uncertainty in Conformal Prediction <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.00876v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.00876v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamed Karimi, Reza Samavi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precise estimation of predictive uncertainty in deep neural networks is a
critical requirement for reliable decision-making in machine learning and
statistical modeling, particularly in the context of medical AI. Conformal
Prediction (CP) has emerged as a promising framework for representing the model
uncertainty by providing well-calibrated confidence levels for individual
predictions. However, the quantification of model uncertainty in conformal
prediction remains an active research area, yet to be fully addressed. In this
paper, we explore state-of-the-art CP methodologies and their theoretical
foundations. We propose a probabilistic approach in quantifying the model
uncertainty derived from the produced prediction sets in conformal prediction
and provide certified boundaries for the computed uncertainty. By doing so, we
allow model uncertainty measured by CP to be compared by other uncertainty
quantification methods such as Bayesian (e.g., MC-Dropout and DeepEnsemble) and
Evidential approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in AAAI Second Symposium on Human Partnership with Medical
  AI: Design, Operationalization, and Ethics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-Shot Video Editing Using Off-The-Shelf Image Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17599v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17599v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen Wang, Yan Jiang, Kangyang Xie, Zide Liu, Hao Chen, Yue Cao, Xinlong Wang, Chunhua Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale text-to-image diffusion models achieve unprecedented success in
image generation and editing. However, how to extend such success to video
editing is unclear. Recent initial attempts at video editing require
significant text-to-video data and computation resources for training, which is
often not accessible. In this work, we propose vid2vid-zero, a simple yet
effective method for zero-shot video editing. Our vid2vid-zero leverages
off-the-shelf image diffusion models, and doesn't require training on any
video. At the core of our method is a null-text inversion module for
text-to-video alignment, a cross-frame modeling module for temporal
consistency, and a spatial regularization module for fidelity to the original
video. Without any training, we leverage the dynamic nature of the attention
mechanism to enable bi-directional temporal modeling at test time. Experiments
and analyses show promising results in editing attributes, subjects, places,
etc., in real-world videos. Code is made available at
\url{https://github.com/baaivision/vid2vid-zero}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Add customized video editing. Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FactoFormer: Factorized Hyperspectral <span class="highlight-title">Transformer</span>s with <span class="highlight-title">Self-Supervised</span>
  <span class="highlight-title">Pretrain</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09431v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09431v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaheer Mohamed, Maryam Haghighat, Tharindu Fernando, Sridha Sridharan, Clinton Fookes, Peyman Moghadam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral images (HSIs) contain rich spectral and spatial information.
Motivated by the success of transformers in the field of natural language
processing and computer vision where they have shown the ability to learn long
range dependencies within input data, recent research has focused on using
transformers for HSIs. However, current state-of-the-art hyperspectral
transformers only tokenize the input HSI sample along the spectral dimension,
resulting in the under-utilization of spatial information. Moreover,
transformers are known to be data-hungry and their performance relies heavily
on large-scale pretraining, which is challenging due to limited annotated
hyperspectral data. Therefore, the full potential of HSI transformers has not
been fully realized. To overcome these limitations, we propose a novel
factorized spectral-spatial transformer that incorporates factorized
self-supervised pretraining procedures, leading to significant improvements in
performance. The factorization of the inputs allows the spectral and spatial
transformers to better capture the interactions within the hyperspectral data
cubes. Inspired by masked image modeling pretraining, we also devise efficient
masking strategies for pretraining each of the spectral and spatial
transformers. We conduct experiments on six publicly available datasets for HSI
classification task and demonstrate that our model achieves state-of-the-art
performance in all the datasets. The code for our model will be made available
at https://github.com/csiro-robotics/factoformer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transactions on Geoscience and Remote Sensing in
  December 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Histopathology Slide Indexing and Search: Are We There Yet? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.17019v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.17019v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Helen H. Shang, Mohammad Sadegh Nasr, Jai Prakash Veerla, Parisa Boodaghi Malidarreh, MD Jillur Rahman Saurav, Amir Hajighasemi, Manfred Huber, Chace Moleta, Jitin Makker, Jacob M. Luber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The search and retrieval of digital histopathology slides is an important
task that has yet to be solved. In this case study, we investigate the clinical
readiness of three state-of-the-art histopathology slide search engines,
Yottixel, SISH, and RetCCL, on three patients with solid tumors. We provide a
qualitative assessment of each model's performance in providing retrieval
results that are reliable and useful to pathologists. We found that all three
image search engines fail to produce consistently reliable results and have
difficulties in capturing granular and subtle features of malignancy, limiting
their diagnostic accuracy. Based on our findings, we also propose a minimal set
of requirements to further advance the development of accurate and reliable
histopathology image search engines for successful clinical adoption.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language-Assisted Deep Learning for Autistic Behaviors Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09310v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09310v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andong Deng, Taojiannan Yang, Chen Chen, Qian Chen, Leslie Neely, Sakiko Oyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Correctly recognizing the behaviors of children with Autism Spectrum Disorder
(ASD) is of vital importance for the diagnosis of Autism and timely early
intervention. However, the observation and recording during the treatment from
the parents of autistic children may not be accurate and objective. In such
cases, automatic recognition systems based on computer vision and machine
learning (in particular deep learning) technology can alleviate this issue to a
large extent. Existing human action recognition models can now achieve
persuasive performance on challenging activity datasets, e.g. daily activity,
and sports activity. However, problem behaviors in children with ASD are very
different from these general activities, and recognizing these problem
behaviors via computer vision is less studied. In this paper, we first evaluate
a strong baseline for action recognition, i.e. Video Swin Transformer, on two
autism behaviors datasets (SSBD and ESBD) and show that it can achieve high
accuracy and outperform the previous methods by a large margin, demonstrating
the feasibility of vision-based problem behaviors recognition. Moreover, we
propose language-assisted training to further enhance the action recognition
performance. Specifically, we develop a two-branch multimodal deep learning
framework by incorporating the "freely available" language description for each
type of problem behavior. Experimental results demonstrate that incorporating
additional language supervision can bring an obvious performance boost for the
autism problem behaviors recognition task as compared to using the video
information only (i.e. 3.49% improvement on ESBD and 1.46% on SSBD).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Smart Health Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion Models as Masked Audio-Video Learners <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03937v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03937v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elvis Nunez, Yanzi Jin, Mohammad Rastegari, Sachin Mehta, Maxwell Horton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the past several years, the synchronization between audio and visual
signals has been leveraged to learn richer audio-visual representations. Aided
by the large availability of unlabeled videos, many unsupervised training
frameworks have demonstrated impressive results in various downstream audio and
video tasks. Recently, Masked Audio-Video Learners (MAViL) has emerged as a
state-of-the-art audio-video pre-training framework. MAViL couples contrastive
learning with masked autoencoding to jointly reconstruct audio spectrograms and
video frames by fusing information from both modalities. In this paper, we
study the potential synergy between diffusion models and MAViL, seeking to
derive mutual benefits from these two frameworks. The incorporation of
diffusion into MAViL, combined with various training efficiency methodologies
that include the utilization of a masking ratio curriculum and adaptive batch
sizing, results in a notable 32% reduction in pre-training Floating-Point
Operations (FLOPS) and an 18% decrease in pre-training wall clock time.
Crucially, this enhanced efficiency does not compromise the model's performance
in downstream audio-classification tasks when compared to MAViL's performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version for the Machine Learning for Audio Workshop at
  NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Supervision by Denoising for Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.02952v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.02952v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sean I. Young, Adrian V. Dalca, Enzo Ferrante, Polina Golland, Christopher A. Metzler, Bruce Fischl, Juan Eugenio Iglesias
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning-based image reconstruction models, such as those based on the U-Net,
require a large set of labeled images if good generalization is to be
guaranteed. In some imaging domains, however, labeled data with pixel- or
voxel-level label accuracy are scarce due to the cost of acquiring them. This
problem is exacerbated further in domains like medical imaging, where there is
no single ground truth label, resulting in large amounts of repeat variability
in the labels. Therefore, training reconstruction networks to generalize better
by learning from both labeled and unlabeled examples (called semi-supervised
learning) is problem of practical and theoretical interest. However,
traditional semi-supervised learning methods for image reconstruction often
necessitate handcrafting a differentiable regularizer specific to some given
imaging problem, which can be extremely time-consuming. In this work, we
propose "supervision by denoising" (SUD), a framework that enables us to
supervise reconstruction models using their own denoised output as soft labels.
SUD unifies stochastic averaging and spatial denoising techniques under a
spatio-temporal denoising framework and alternates denoising and model weight
update steps in an optimization framework for semi-supervision. As example
applications, we apply SUD to two problems arising from biomedical imaging --
anatomical brain reconstruction (3D) and cortical parcellation (2D) -- to
demonstrate a significant improvement in the image reconstructions over
supervised-only and stochastic averaging baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the IEEE Transactions on Pattern Analysis and Machine
  Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision-Language Models can Identify Distracted Driver Behavior from
  Naturalistic Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.10159v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.10159v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Zahid Hasan, Jiajing Chen, Jiyang Wang, Mohammed Shaiqur Rahman, Ameya Joshi, Senem Velipasalar, Chinmay Hegde, Anuj Sharma, Soumik Sarkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recognizing the activities causing distraction in real-world driving
scenarios is critical for ensuring the safety and reliability of both drivers
and pedestrians on the roadways. Conventional computer vision techniques are
typically data-intensive and require a large volume of annotated training data
to detect and classify various distracted driving behaviors, thereby limiting
their efficiency and scalability. We aim to develop a generalized framework
that showcases robust performance with access to limited or no annotated
training data. Recently, vision-language models have offered large-scale
visual-textual pretraining that can be adapted to task-specific learning like
distracted driving activity recognition. Vision-language pretraining models,
such as CLIP, have shown significant promise in learning natural
language-guided visual representations. This paper proposes a CLIP-based driver
activity recognition approach that identifies driver distraction from
naturalistic driving images and videos. CLIP's vision embedding offers
zero-shot transfer and task-based finetuning, which can classify distracted
activities from driving video data. Our results show that this framework offers
state-of-the-art performance on zero-shot transfer and video-based CLIP for
predicting the driver's state on two public datasets. We propose both
frame-based and video-based frameworks developed on top of the CLIP's visual
representation for distracted driving detection and classification tasks and
report the results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ R-MAE: Regions Meet Masked Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05411v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05411v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duy-Kien Nguyen, Vaibhav Aggarwal, Yanghao Li, Martin R. Oswald, Alexander Kirillov, Cees G. M. Snoek, Xinlei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we explore regions as a potential visual analogue of words for
self-supervised image representation learning. Inspired by Masked Autoencoding
(MAE), a generative pre-training baseline, we propose masked region
autoencoding to learn from groups of pixels or regions. Specifically, we design
an architecture which efficiently addresses the one-to-many mapping between
images and regions, while being highly effective especially with high-quality
regions. When integrated with MAE, our approach (R-MAE) demonstrates consistent
improvements across various pre-training datasets and downstream detection and
segmentation benchmarks, with negligible computational overheads. Beyond the
quantitative evaluation, our analysis indicates the models pre-trained with
masked region autoencoding unlock the potential for interactive segmentation.
The code is provided at https://github.com/facebookresearch/r-mae.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training Diffusion Models with Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.13301v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.13301v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, Sergey Levine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models are a class of flexible generative models trained with an
approximation to the log-likelihood objective. However, most use cases of
diffusion models are not concerned with likelihoods, but instead with
downstream objectives such as human-perceived image quality or drug
effectiveness. In this paper, we investigate reinforcement learning methods for
directly optimizing diffusion models for such objectives. We describe how
posing denoising as a multi-step decision-making problem enables a class of
policy gradient algorithms, which we refer to as denoising diffusion policy
optimization (DDPO), that are more effective than alternative reward-weighted
likelihood approaches. Empirically, DDPO is able to adapt text-to-image
diffusion models to objectives that are difficult to express via prompting,
such as image compressibility, and those derived from human feedback, such as
aesthetic quality. Finally, we show that DDPO can improve prompt-image
alignment using feedback from a vision-language model without the need for
additional data collection or human annotation. The project's website can be
found at http://rl-diffusion.github.io .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Neural Networks for Tabular Data Learning: A <span class="highlight-title">Survey</span> with Taxonomy
  and Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng-Te Li, Yu-Che Tsai, Chih-Yao Chen, Jay Chiehen Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this survey, we dive into Tabular Data Learning (TDL) using Graph Neural
Networks (GNNs), a domain where deep learning-based approaches have
increasingly shown superior performance in both classification and regression
tasks compared to traditional methods. The survey highlights a critical gap in
deep neural TDL methods: the underrepresentation of latent correlations among
data instances and feature values. GNNs, with their innate capability to model
intricate relationships and interactions between diverse elements of tabular
data, have garnered significant interest and application across various TDL
domains. Our survey provides a systematic review of the methods involved in
designing and implementing GNNs for TDL (GNN4TDL). It encompasses a detailed
investigation into the foundational aspects and an overview of GNN-based TDL
methods, offering insights into their evolving landscape. We present a
comprehensive taxonomy focused on constructing graph structures and
representation learning within GNN-based TDL methods. In addition, the survey
examines various training plans, emphasizing the integration of auxiliary tasks
to enhance the effectiveness of instance representations. A critical part of
our discussion is dedicated to the practical application of GNNs across a
spectrum of GNN4TDL scenarios, demonstrating their versatility and impact.
Lastly, we discuss the limitations and propose future research directions,
aiming to spur advancements in GNN4TDL. This survey serves as a resource for
researchers and practitioners, offering a thorough understanding of GNNs' role
in revolutionizing TDL and pointing towards future innovations in this
promising area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review, ongoing work, Github page:
  https://github.com/Roytsai27/awesome-GNN4TDL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spectral-based Graph Neutral Networks for Complementary Item
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haitong Luo, Xuying Meng, Suhang Wang, Hanyun Cao, Weiyao Zhang, Yequan Wang, Yujun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling complementary relationships greatly helps recommender systems to
accurately and promptly recommend the subsequent items when one item is
purchased. Unlike traditional similar relationships, items with complementary
relationships may be purchased successively (such as iPhone and Airpods Pro),
and they not only share relevance but also exhibit dissimilarity. Since the two
attributes are opposites, modeling complementary relationships is challenging.
Previous attempts to exploit these relationships have either ignored or
oversimplified the dissimilarity attribute, resulting in ineffective modeling
and an inability to balance the two attributes. Since Graph Neural Networks
(GNNs) can capture the relevance and dissimilarity between nodes in the
spectral domain, we can leverage spectral-based GNNs to effectively understand
and model complementary relationships. In this study, we present a novel
approach called Spectral-based Complementary Graph Neural Networks (SComGNN)
that utilizes the spectral properties of complementary item graphs. We make the
first observation that complementary relationships consist of low-frequency and
mid-frequency components, corresponding to the relevance and dissimilarity
attributes, respectively. Based on this spectral observation, we design
spectral graph convolutional networks with low-pass and mid-pass filters to
capture the low-frequency and mid-frequency components. Additionally, we
propose a two-stage attention mechanism to adaptively integrate and balance the
two attributes. Experimental results on four e-commerce datasets demonstrate
the effectiveness of our model, with SComGNN significantly outperforming
existing baseline models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Starling: An I/O-Efficient Disk-Resident Graph Index Framework for
  High-Dimensional Vector Similarity Search on Data Segment <span class="chip">SIGMOD 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengzhao Wang, Weizhi Xu, Xiaomeng Yi, Songlin Wu, Zhangyang Peng, Xiangyu Ke, Yunjun Gao, Xiaoliang Xu, Rentong Guo, Charles Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-dimensional vector similarity search (HVSS) is receiving a spotlight as
a powerful tool for various data science and AI applications. As vector data
grows larger, in-memory indexes become extremely expensive because they
necessitate substantial expansion of main memory resources. One possible
solution is to use disk-based implementation, which stores and searches vector
data in high-performance devices like NVMe SSDs. However, HVSS for data
segments is still challenging in vector databases, where one machine has
multiple segments for system features (like scaling) purposes. In this setting,
each segment has limited memory and disk space, so HVSS on the data segment
needs to balance accuracy, efficiency, and space cost. Existing disk-based
methods are sub-optimal because they do not consider all these requirements
together. In this paper, we present Starling, an I/O-efficient disk-resident
graph index framework that optimizes data layout and search strategy in the
segment. It has two main components: (1) a data layout that includes an
in-memory navigation graph and a reordered disk-based graph with locality
enhancement, which reduces the search path length and disk bandwidth wastage;
and (2) a block search strategy that minimizes expensive disk I/Os when
executing a vector query. We conduct extensive experiments to verify Starling's
effectiveness, efficiency, and scalability. On a data segment with 2GB memory
and 10GB disk capacity, Starling can maintain up to 33 million vectors in 128
dimensions, and serve HVSS with more than 0.9 average precision and top-10
recall rate, and latency of under 1 millisecond. The results show that Starling
exhibits 43.9$\times$ higher throughput with 98% lower query latency than
state-of-the-art methods under the same accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by SIGMOD 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lookahead: An Inference Acceleration Framework for Large Language Model
  with Lossless Generation Accuracy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12728v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12728v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Zhao, Zhitian Xie, Chenyi Zhuang, Jinjie Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) have made significant advancements across
various tasks, such as question answering, translation, text summarization, and
dialogue systems, the need for accuracy in information becomes crucial,
especially for serious financial products serving billions of users like
Alipay. To address this, Alipay has developed a Retrieval-Augmented Generation
(RAG) system that grounds LLMs on the most accurate and up-to-date information.
However, for a real-world product serving millions of users, the inference
speed of LLMs becomes a critical factor compared to a mere experimental model.
  Hence, this paper presents a generic framework for accelerating the inference
process, resulting in a substantial increase in speed and cost reduction for
our RAG system, with lossless generation accuracy. In the traditional inference
process, each token is generated sequentially by the LLM, leading to a time
consumption proportional to the number of generated tokens. To enhance this
process, our framework, named \textit{lookahead}, introduces a
\textit{multi-branch} strategy. Instead of generating a single token at a time,
we propose a \textit{Trie-based Retrieval} (TR) process that enables the
generation of multiple branches simultaneously, each of which is a sequence of
tokens. Subsequently, for each branch, a \textit{Verification and Accept} (VA)
process is performed to identify the longest correct sub-sequence as the final
output. Our strategy offers two distinct advantages: (1) it guarantees absolute
correctness of the output, avoiding any approximation algorithms, and (2) the
worst-case performance of our approach is equivalent to the conventional
process. We conduct extensive experiments to demonstrate the significant
improvements achieved by applying our inference acceleration framework. Code is
avaliable: https://github.com/alipay/PainlessInferenceAcceleration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">119</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Task Oriented Dialogue as a Catalyst for <span class="highlight-title">Self-Supervised</span> Automatic
  Speech Recognition <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David M. Chan, Shalini Ghosh, Hitesh Tulsiani, Ariya Rastrow, Björn Hoffmeister
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While word error rates of automatic speech recognition (ASR) systems have
consistently fallen, natural language understanding (NLU) applications built on
top of ASR systems still attribute significant numbers of failures to
low-quality speech recognition results. Existing assistant systems collect
large numbers of these unsuccessful interactions, but these systems usually
fail to learn from these interactions, even in an offline fashion. In this
work, we introduce CLC: Contrastive Learning for Conversations, a family of
methods for contrastive fine-tuning of models in a self-supervised fashion,
making use of easily detectable artifacts in unsuccessful conversations with
assistants. We demonstrate that our CLC family of approaches can improve the
performance of ASR models on OD3, a new public large-scale semi-synthetic
meta-dataset of audio task-oriented dialogues, by up to 19.2%. These gains
transfer to real-world systems as well, where we show that CLC can help to
improve performance by up to 6.7% over baselines. We make OD3 publicly
available at https://github.com/amazon-science/amazon-od3 .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ODIN: A Single Model for 2D and 3D Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02416v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02416v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayush Jain, Pushkal Katara, Nikolaos Gkanatsios, Adam W. Harley, Gabriel Sarch, Kriti Aggarwal, Vishrav Chaudhary, Katerina Fragkiadaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art models on contemporary 3D perception benchmarks like ScanNet
consume and label dataset-provided 3D point clouds, obtained through post
processing of sensed multiview RGB-D images. They are typically trained
in-domain, forego large-scale 2D pre-training and outperform alternatives that
featurize the posed RGB-D multiview images instead. The gap in performance
between methods that consume posed images versus post-processed 3D point clouds
has fueled the belief that 2D and 3D perception require distinct model
architectures. In this paper, we challenge this view and propose ODIN
(Omni-Dimensional INstance segmentation), a model that can segment and label
both 2D RGB images and 3D point clouds, using a transformer architecture that
alternates between 2D within-view and 3D cross-view information fusion. Our
model differentiates 2D and 3D feature operations through the positional
encodings of the tokens involved, which capture pixel coordinates for 2D patch
tokens and 3D coordinates for 3D feature tokens. ODIN achieves state-of-the-art
performance on ScanNet200, Matterport3D and AI2THOR 3D instance segmentation
benchmarks, and competitive performance on ScanNet, S3DIS and COCO. It
outperforms all previous works by a wide margin when the sensed 3D point cloud
is used in place of the point cloud sampled from 3D mesh. When used as the 3D
perception engine in an instructable embodied agent architecture, it sets a new
state-of-the-art on the TEACh action-from-dialogue benchmark. Our code and
checkpoints can be found at the project website: https://odin-seg.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simulation-Based Inference with Quantile Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02413v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02413v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Neural Quantile Estimation (NQE), a novel Simulation-Based
Inference (SBI) method based on conditional quantile regression. NQE
autoregressively learns individual one dimensional quantiles for each posterior
dimension, conditioned on the data and previous posterior dimensions. Posterior
samples are obtained by interpolating the predicted quantiles using monotonic
cubic Hermite spline, with specific treatment for the tail behavior and
multi-modal distributions. We introduce an alternative definition for the
Bayesian credible region using the local Cumulative Density Function (CDF),
offering substantially faster evaluation than the traditional Highest Posterior
Density Region (HPDR). In case of limited simulation budget and/or known model
misspecification, a post-processing broadening step can be integrated into NQE
to ensure the unbiasedness of the posterior estimation with negligible
additional computational cost. We demonstrate that the proposed NQE method
achieves state-of-the-art performance on a variety of benchmark problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8+13 pages, 7+7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM Augmented LLMs: Expanding Capabilities through Composition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rachit Bansal, Bidisha Samanta, Siddharth Dalmia, Nitish Gupta, Shikhar Vashishth, Sriram Ganapathy, Abhishek Bapna, Prateek Jain, Partha Talukdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundational models with billions of parameters which have been trained on
large corpora of data have demonstrated non-trivial skills in a variety of
domains. However, due to their monolithic structure, it is challenging and
expensive to augment them or impart new skills. On the other hand, due to their
adaptation abilities, several new instances of these models are being trained
towards new domains and tasks. In this work, we study the problem of efficient
and practical composition of existing foundation models with more specific
models to enable newer capabilities. To this end, we propose CALM --
Composition to Augment Language Models -- which introduces cross-attention
between models to compose their representations and enable new capabilities.
Salient features of CALM are: (i) Scales up LLMs on new tasks by 're-using'
existing LLMs along with a few additional parameters and data, (ii) Existing
model weights are kept intact, and hence preserves existing capabilities, and
(iii) Applies to diverse domains and settings. We illustrate that augmenting
PaLM2-S with a smaller model trained on low-resource languages results in an
absolute improvement of up to 13\% on tasks like translation into English and
arithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is
augmented with a code-specific model, we see a relative improvement of 40\%
over the base model for code generation and explanation tasks -- on-par with
fully fine-tuned counterparts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 2 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What You See is What You GAN: Rendering Every Pixel for High-Fidelity
  Geometry in 3D GANs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02411v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02411v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Trevithick, Matthew Chan, Towaki Takikawa, Umar Iqbal, Shalini De Mello, Manmohan Chandraker, Ravi Ramamoorthi, Koki Nagano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D-aware Generative Adversarial Networks (GANs) have shown remarkable
progress in learning to generate multi-view-consistent images and 3D geometries
of scenes from collections of 2D images via neural volume rendering. Yet, the
significant memory and computational costs of dense sampling in volume
rendering have forced 3D GANs to adopt patch-based training or employ
low-resolution rendering with post-processing 2D super resolution, which
sacrifices multiview consistency and the quality of resolved geometry.
Consequently, 3D GANs have not yet been able to fully resolve the rich 3D
geometry present in 2D images. In this work, we propose techniques to scale
neural volume rendering to the much higher resolution of native 2D images,
thereby resolving fine-grained 3D geometry with unprecedented detail. Our
approach employs learning-based samplers for accelerating neural rendering for
3D GAN training using up to 5 times fewer depth samples. This enables us to
explicitly "render every pixel" of the full-resolution image during training
and inference without post-processing superresolution in 2D. Together with our
strategy to learn high-quality surface geometry, our method synthesizes
high-resolution 3D geometry and strictly view-consistent images while
maintaining image quality on par with baselines relying on post-processing
super resolution. We demonstrate state-of-the-art 3D gemetric quality on FFHQ
and AFHQ, setting a new standard for unsupervised learning of 3D shapes in 3D
GANs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>See our project page: https://research.nvidia.com/labs/nxp/wysiwyg/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-Time 2D Temperature Field Prediction in Metal Additive
  Manufacturing Using Physics-Informed Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02403v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02403v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pouyan Sajadi, Mostafa Rahmani Dehaghani, Yifan Tang, G. Gary Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately predicting the temperature field in metal additive manufacturing
(AM) processes is critical to preventing overheating, adjusting process
parameters, and ensuring process stability. While physics-based computational
models offer precision, they are often time-consuming and unsuitable for
real-time predictions and online control in iterative design scenarios.
Conversely, machine learning models rely heavily on high-quality datasets,
which can be costly and challenging to obtain within the metal AM domain. Our
work addresses this by introducing a physics-informed neural network framework
specifically designed for temperature field prediction in metal AM. This
framework incorporates a physics-informed input, physics-informed loss
function, and a Convolutional Long Short-Term Memory (ConvLSTM) architecture.
Utilizing real-time temperature data from the process, our model predicts 2D
temperature fields for future timestamps across diverse geometries, deposition
patterns, and process parameters. We validate the proposed framework in two
scenarios: full-field temperature prediction for a thin wall and 2D temperature
field prediction for cylinder and cubic parts, demonstrating errors below 3%
and 1%, respectively. Our proposed framework exhibits the flexibility to be
applied across diverse scenarios with varying process parameters, geometries,
and deposition patterns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, 13 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating synthetic data for neural operators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erisa Hasani, Rachel A. Ward
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerous developments in the recent literature show the promising potential
of deep learning in obtaining numerical solutions to partial differential
equations (PDEs) beyond the reach of current numerical solvers. However,
data-driven neural operators all suffer from the same problem: the data needed
to train a network depends on classical numerical solvers such as finite
difference or finite element, among others. In this paper, we propose a new
approach to generating synthetic functional training data that does not require
solving a PDE numerically. The way we do this is simple: we draw a large number
$N$ of independent and identically distributed `random functions' $u_j$ from
the underlying solution space (e.g., $H_0^1(\Omega)$) in which we know the
solution lies according to classical theory. We then plug each such random
candidate solution into the equation and get a corresponding right-hand side
function $f_j$ for the equation, and consider $(f_j, u_j)_{j=1}^N$ as
supervised training data for learning the underlying inverse problem $f
\rightarrow u$. This `backwards' approach to generating training data only
requires derivative computations, in contrast to standard `forward' approaches,
which require a numerical PDE solver, enabling us to generate a large number of
such data points quickly and efficiently. While the idea is simple, we hope
that this method will expand the potential for developing neural PDE solvers
that do not depend on classical numerical solvers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integration of physics-informed operator learning and finite element
  method for parametric learning of partial differential equations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02363v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02363v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahed Rezaei, Ahmad Moeineddin, Michael Kaliske, Markus Apel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a method that employs physics-informed deep learning techniques
for parametrically solving partial differential equations. The focus is on the
steady-state heat equations within heterogeneous solids exhibiting significant
phase contrast. Similar equations manifest in diverse applications like
chemical diffusion, electrostatics, and Darcy flow. The neural network aims to
establish the link between the complex thermal conductivity profiles and
temperature distributions, as well as heat flux components within the
microstructure, under fixed boundary conditions. A distinctive aspect is our
independence from classical solvers like finite element methods for data. A
noteworthy contribution lies in our novel approach to defining the loss
function, based on the discretized weak form of the governing equation. This
not only reduces the required order of derivatives but also eliminates the need
for automatic differentiation in the construction of loss terms, accepting
potential numerical errors from the chosen discretization method. As a result,
the loss function in this work is an algebraic equation that significantly
enhances training efficiency. We benchmark our methodology against the standard
finite element method, demonstrating accurate yet faster predictions using the
trained neural network for temperature and flux profiles. We also show higher
accuracy by using the proposed method compared to purely data-driven approaches
for unforeseen scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> Analyzing Generalization in Deep Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ezgi Korkmaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning research obtained significant success and attention
with the utilization of deep neural networks to solve problems in high
dimensional state or action spaces. While deep reinforcement learning policies
are currently being deployed in many different fields from medical applications
to self driving vehicles, there are still ongoing questions the field is trying
to answer on the generalization capabilities of deep reinforcement learning
policies. In this paper, we will outline the fundamental reasons why deep
reinforcement learning policies encounter overfitting problems that limit their
robustness and generalization capabilities. Furthermore, we will formalize and
unify the diverse solution approaches to increase generalization, and overcome
overfitting in state-action value functions. We believe our study can provide a
compact systematic unified analysis for the current advancements in deep
reinforcement learning, and help to construct robust deep neural policies with
improved generalization abilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Source Domain Adaptation with <span class="highlight-title">Transformer</span>-based Feature Generation
  for Subject-Independent EEG-based Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shadi Sartipi, Mujdat Cetin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although deep learning-based algorithms have demonstrated excellent
performance in automated emotion recognition via electroencephalogram (EEG)
signals, variations across brain signal patterns of individuals can diminish
the model's effectiveness when applied across different subjects. While
transfer learning techniques have exhibited promising outcomes, they still
encounter challenges related to inadequate feature representations and may
overlook the fact that source subjects themselves can possess distinct
characteristics. In this work, we propose a multi-source domain adaptation
approach with a transformer-based feature generator (MSDA-TF) designed to
leverage information from multiple sources. The proposed feature generator
retains convolutional layers to capture shallow spatial, temporal, and spectral
EEG data representations, while self-attention mechanisms extract global
dependencies within these features. During the adaptation process, we group the
source subjects based on correlation values and aim to align the moments of the
target subject with each source as well as within the sources. MSDA-TF is
validated on the SEED dataset and is shown to yield promising results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evasive Hardware Trojan through Adversarial Power Trace 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02342v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02342v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Behnam Omidi, Khaled N. Khasawneh, Ihsen Alouani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The globalization of the Integrated Circuit (IC) supply chain, driven by
time-to-market and cost considerations, has made ICs vulnerable to hardware
Trojans (HTs). Against this threat, a promising approach is to use Machine
Learning (ML)-based side-channel analysis, which has the advantage of being a
non-intrusive method, along with efficiently detecting HTs under golden
chip-free settings. In this paper, we question the trustworthiness of ML-based
HT detection via side-channel analysis. We introduce a HT obfuscation (HTO)
approach to allow HTs to bypass this detection method. Rather than
theoretically misleading the model by simulated adversarial traces, a key
aspect of our approach is the design and implementation of adversarial noise as
part of the circuitry, alongside the HT. We detail HTO methodologies for ASICs
and FPGAs, and evaluate our approach using TrustHub benchmark. Interestingly,
we found that HTO can be implemented with only a single transistor for ASIC
designs to generate adversarial power traces that can fool the defense with
100% efficiency. We also efficiently implemented our approach on a Spartan 6
Xilinx FPGA using 2 different variants: (i) DSP slices-based, and (ii)
ring-oscillator-based design. Additionally, we assess the efficiency of
countermeasures like spectral domain analysis, and we show that an adaptive
attacker can still design evasive HTOs by constraining the design with a
spectral noise budget. In addition, while adversarial training (AT) offers
higher protection against evasive HTs, AT models suffer from a considerable
utility loss, potentially rendering them unsuitable for such security
application. We believe this research represents a significant step in
understanding and exploiting ML vulnerabilities in a hardware security context,
and we make all resources and designs openly available online:
https://dev.d18uu4lqwhbmka.amplifyapp.com
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Extraction: Contextualising Tabular Data for Efficient
  Summarisation by Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02333v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02333v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Uday Allu, Biddwan Ahmed, Vishesh Tripathi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The conventional use of the Retrieval-Augmented Generation (RAG) architecture
has proven effective for retrieving information from diverse documents.
However, challenges arise in handling complex table queries, especially within
PDF documents containing intricate tabular structures.This research introduces
an innovative approach to enhance the accuracy of complex table queries in
RAG-based systems. Our methodology involves storing PDFs in the retrieval
database and extracting tabular content separately. The extracted tables
undergo a process of context enrichment, concatenating headers with
corresponding values. To ensure a comprehensive understanding of the enriched
data, we employ a fine-tuned version of the Llama-2-chat language model for
summarisation within the RAG architecture. Furthermore, we augment the tabular
data with contextual sense using the ChatGPT 3.5 API through a one-shot prompt.
This enriched data is then fed into the retrieval database alongside other
PDFs. Our approach aims to significantly improve the precision of complex table
queries, offering a promising solution to a longstanding challenge in
information retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Not all Minorities are Equal: Empty-Class-Aware Distillation for
  Heterogeneous Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuangpu Guo, Yuhe Ding, Jian Liang, Ran He, Zilei Wang, Tieniu Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data heterogeneity, characterized by disparities in local data distribution
across clients, poses a significant challenge in federated learning.
Substantial efforts have been devoted to addressing the heterogeneity in local
label distribution. As minority classes suffer from worse accuracy due to
overfitting on local imbalanced data, prior methods often incorporate
class-balanced learning techniques during local training. Despite the improved
mean accuracy across all classes, we observe that empty classes-referring to
categories absent from a client's data distribution-are still not well
recognized. This paper introduces FedED, a novel approach in heterogeneous
federated learning that integrates both empty-class distillation and logit
suppression simultaneously. Specifically, empty-class distillation leverages
knowledge distillation during local training on each client to retain essential
information related to empty classes from the global model. Moreover, logit
suppression directly penalizes network logits for non-label classes,
effectively addressing misclassifications in minority classes that may be
biased toward majority classes. Extensive experiments validate the efficacy of
FedED, surpassing previous state-of-the-art methods across diverse datasets
with varying degrees of label distribution shift.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Robust Quantile Huber Loss With Interpretable Parameter Adjustment In
  Distributional Reinforcement Learning <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02325v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02325v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parvin Malekzadeh, Konstantinos N. Plataniotis, Zissis Poulos, Zeyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributional Reinforcement Learning (RL) estimates return distribution
mainly by learning quantile values via minimizing the quantile Huber loss
function, entailing a threshold parameter often selected heuristically or via
hyperparameter search, which may not generalize well and can be suboptimal.
This paper introduces a generalized quantile Huber loss function derived from
Wasserstein distance (WD) calculation between Gaussian distributions, capturing
noise in predicted (current) and target (Bellman-updated) quantile values.
Compared to the classical quantile Huber loss, this innovative loss function
enhances robustness against outliers. Notably, the classical Huber loss
function can be seen as an approximation of our proposed loss, enabling
parameter adjustment by approximating the amount of noise in the data during
the learning process. Empirical tests on Atari games, a common application in
distributional RL, and a recent hedging strategy using distributional RL,
validate the effectiveness of our proposed loss function and its potential for
parameter adjustments in distributional RL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 1 figure, to be published in ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Agent Context Learning Strategy for Interference-Aware Beam
  Allocation in mmWave Vehicular Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02323v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02323v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdulkadir Kose, Haeyoung Lee, Chuan Heng Foh, Mohammad Shojafar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Millimeter wave (mmWave) has been recognized as one of key technologies for
5G and beyond networks due to its potential to enhance channel bandwidth and
network capacity. The use of mmWave for various applications including
vehicular communications has been extensively discussed. However, applying
mmWave to vehicular communications faces challenges of high mobility nodes and
narrow coverage along the mmWave beams. Due to high mobility in dense networks,
overlapping beams can cause strong interference which leads to performance
degradation. As a remedy, beam switching capability in mmWave can be utilized.
Then, frequent beam switching and cell change become inevitable to manage
interference, which increase computational and signalling complexity. In order
to deal with the complexity in interference control, we develop a new strategy
called Multi-Agent Context Learning (MACOL), which utilizes Contextual Bandit
to manage interference while allocating mmWave beams to serve vehicles in the
network. Our approach demonstrates that by leveraging knowledge of neighbouring
beam status, the machine learning agent can identify and avoid potential
interfering transmissions to other ongoing transmissions. Furthermore, we show
that even under heavy traffic loads, our proposed MACOL strategy is able to
maintain low interference levels at around 10%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in IEEE Transactions on Intelligent Transportation Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Physics Informed Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02300v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02300v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcin Łoś, Maciej Paszyński
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a Robust version of the Physics-Informed Neural Networks
(RPINNs) to approximate the Partial Differential Equations (PDEs) solution.
Standard Physics Informed Neural Networks (PINN) takes into account the
governing physical laws described by PDE during the learning process. The
network is trained on a data set that consists of randomly selected points in
the physical domain and its boundary. PINNs have been successfully applied to
solve various problems described by PDEs with boundary conditions. The loss
function in traditional PINNs is based on the strong residuals of the PDEs.
This loss function in PINNs is generally not robust with respect to the true
error. The loss function in PINNs can be far from the true error, which makes
the training process more difficult. In particular, we do not know if the
training process has already converged to the solution with the required
accuracy. This is especially true if we do not know the exact solution, so we
cannot estimate the true error during the training. This paper introduces a
different way of defining the loss function. It incorporates the residual and
the inverse of the Gram matrix, computed using the energy norm. We test our
RPINN algorithm on two Laplace problems and one advection-diffusion problem in
two spatial dimensions. We conclude that RPINN is a robust method. The proposed
loss coincides well with the true error of the solution, as measured in the
energy norm. Thus, we know if our training process goes well, and we know when
to stop the training to obtain the neural network approximation of the solution
of the PDE with the true error of required accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training Single-Layer Morphological Perceptron Using Convex-Concave
  Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Iara Cunha, Marcos Eduardo Valle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper concerns the training of a single-layer morphological perceptron
using disciplined convex-concave programming (DCCP). We introduce an algorithm
referred to as K-DDCCP, which combines the existing single-layer morphological
perceptron (SLMP) model proposed by Ritter and Urcid with the weighted
disciplined convex-concave programming (WDCCP) algorithm by Charisopoulos and
Maragos. The proposed training algorithm leverages the disciplined
convex-concave procedure (DCCP) and formulates a non-convex optimization
problem for binary classification. To tackle this problem, the constraints are
expressed as differences of convex functions, enabling the application of the
DCCP package. The experimental results confirm the effectiveness of the K-DDCCP
algorithm in solving binary classification problems. Overall, this work
contributes to the field of morphological neural networks by proposing an
algorithm that extends the capabilities of the SLMP model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Path-based Explanation for Knowledge Graph Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02290v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02290v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heng Chang, Jiangnan Ye, Alejo Lopez Avila, Jinhua Du, Jia Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have achieved great success in Knowledge Graph
Completion (KGC) by modelling how entities and relations interact in recent
years. However, the explanation of the predicted facts has not caught the
necessary attention. Proper explanations for the results of GNN-based KGC
models increase model transparency and help researchers develop more reliable
models. Existing practices for explaining KGC tasks rely on
instance/subgraph-based approaches, while in some scenarios, paths can provide
more user-friendly and interpretable explanations. Nonetheless, the methods for
generating path-based explanations for KGs have not been well-explored. To
address this gap, we propose Power-Link, the first path-based KGC explainer
that explores GNN-based models. We design a novel simplified graph-powering
technique, which enables the generation of path-based explanations with a fully
parallelisable and memory-efficient training scheme. We further introduce three
new metrics for quantitative evaluation of the explanations, together with a
qualitative human evaluation. Extensive experiments demonstrate that Power-Link
outperforms the SOTA baselines in interpretability, efficiency, and
scalability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DEM: A Method for Certifying Deep Neural Network Classifier Outputs in
  Aerospace 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02283v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02283v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guy Katz, Natan Levy, Idan Refaeli, Raz Yerushalmi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Software development in the aerospace domain requires adhering to strict,
high-quality standards. While there exist regulatory guidelines for commercial
software in this domain (e.g., ARP-4754 and DO-178), these do not apply to
software with deep neural network (DNN) components. Consequently, it is unclear
how to allow aerospace systems to benefit from the deep learning revolution.
Our work here seeks to address this challenge with a novel, output-centric
approach for DNN certification. Our method employs statistical verification
techniques, and has the key advantage of being able to flag specific inputs for
which the DNN's output may be unreliable - so that they may be later inspected
by a human expert. To achieve this, our method conducts a statistical analysis
of the DNN's predictions for other, nearby inputs, in order to detect
inconsistencies. This is in contrast to existing techniques, which typically
attempt to certify the entire DNN, as opposed to individual outputs. Our method
uses the DNN as a black-box, and makes no assumptions about its topology. We
hope that this work constitutes another step towards integrating DNNs in
safety-critical applications - especially in the aerospace domain, where high
standards of quality and reliability are crucial.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lightweight Fish Classification Model for Sustainable Marine Management:
  Indonesian Case 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Febrian Kurniawan, Gandeva Bayu Satrya, Firuz Kamalov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The enormous demand for seafood products has led to exploitation of marine
resources and near-extinction of some species. In particular, overfishing is
one the main issues in sustainable marine development. In alignment with the
protection of marine resources and sustainable fishing, this study proposes to
advance fish classification techniques that support identifying protected fish
species using state-of-the-art machine learning. We use a custom modification
of the MobileNet model to design a lightweight classifier called M-MobileNet
that is capable of running on limited hardware. As part of the study, we
compiled a labeled dataset of 37,462 images of fish found in the waters of the
Indonesian archipelago. The proposed model is trained on the dataset to
classify images of the captured fish into their species and give
recommendations on whether they are consumable or not. Our modified MobileNet
model uses only 50\% of the top layer parameters with about 42% GTX 860M
utility and achieves up to 97% accuracy in fish classification and determining
its consumability. Given the limited computing capacity available on many
fishing vessels, the proposed model provides a practical solution to on-site
fish classification. In addition, synchronized implementation of the proposed
model on multiple vessels can supply valuable information about the movement
and location of different species of fish.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Universal Approximation Theorem for Vector- and Hypercomplex-Valued
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02277v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02277v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcos Eduardo Valle, Wington L. Vital, Guilherme Vieira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The universal approximation theorem states that a neural network with one
hidden layer can approximate continuous functions on compact sets with any
desired precision. This theorem supports using neural networks for various
applications, including regression and classification tasks. Furthermore, it is
valid for real-valued neural networks and some hypercomplex-valued neural
networks such as complex-, quaternion-, tessarine-, and Clifford-valued neural
networks. However, hypercomplex-valued neural networks are a type of
vector-valued neural network defined on an algebra with additional algebraic or
geometric properties. This paper extends the universal approximation theorem
for a wide range of vector-valued neural networks, including
hypercomplex-valued models as particular instances. Precisely, we introduce the
concept of non-degenerate algebra and state the universal approximation theorem
for neural networks defined on such algebras.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty-Aware Deep Attention Recurrent Neural Network for
  Heterogeneous Time Series Imputation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linglong Qian, Zina Ibrahim, Richard Dobson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Missingness is ubiquitous in multivariate time series and poses an obstacle
to reliable downstream analysis. Although recurrent network imputation achieved
the SOTA, existing models do not scale to deep architectures that can
potentially alleviate issues arising in complex data. Moreover, imputation
carries the risk of biased estimations of the ground truth. Yet, confidence in
the imputed values is always unmeasured or computed post hoc from model output.
We propose DEep Attention Recurrent Imputation (DEARI), which jointly estimates
missing values and their associated uncertainty in heterogeneous multivariate
time series. By jointly representing feature-wise correlations and temporal
dynamics, we adopt a self attention mechanism, along with an effective residual
component, to achieve a deep recurrent neural network with good imputation
performance and stable convergence. We also leverage self-supervised metric
learning to boost performance by optimizing sample similarity. Finally, we
transform DEARI into a Bayesian neural network through a novel Bayesian
marginalization strategy to produce stochastic DEARI, which outperforms its
deterministic equivalent. Experiments show that DEARI surpasses the SOTA in
diverse imputation tasks using real-world datasets, namely air quality control,
healthcare and traffic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Balancing Continual Learning and Fine-tuning for Human Activity
  Recognition <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi Ian Tang, Lorena Qendro, Dimitris Spathis, Fahim Kawsar, Akhil Mathur, Cecilia Mascolo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wearable-based Human Activity Recognition (HAR) is a key task in
human-centric machine learning due to its fundamental understanding of human
behaviours. Due to the dynamic nature of human behaviours, continual learning
promises HAR systems that are tailored to users' needs. However, because of the
difficulty in collecting labelled data with wearable sensors, existing
approaches that focus on supervised continual learning have limited
applicability, while unsupervised continual learning methods only handle
representation learning while delaying classifier training to a later stage.
This work explores the adoption and adaptation of CaSSLe, a continual
self-supervised learning model, and Kaizen, a semi-supervised continual
learning model that balances representation learning and down-stream
classification, for the task of wearable-based HAR. These schemes re-purpose
contrastive learning for knowledge retention and, Kaizen combines that with
self-training in a unified scheme that can leverage unlabelled and labelled
data for continual learning. In addition to comparing state-of-the-art
self-supervised continual learning schemes, we further investigated the
importance of different loss terms and explored the trade-off between knowledge
retention and learning from new tasks. In particular, our extensive evaluation
demonstrated that the use of a weighting factor that reflects the ratio between
learned and new classes achieves the best overall trade-off in continual
learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2024 HCRL (Human-Centric Representation Learning) Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ L3Cube-IndicNews: News-based Short Text and Long Document Classification
  <span class="highlight-title">Dataset</span>s in Indic Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aishwarya Mirashi, Srushti Sonavane, Purva Lingayat, Tejas Padhiyar, Raviraj Joshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce L3Cube-IndicNews, a multilingual text
classification corpus aimed at curating a high-quality dataset for Indian
regional languages, with a specific focus on news headlines and articles. We
have centered our work on 10 prominent Indic languages, including Hindi,
Bengali, Marathi, Telugu, Tamil, Gujarati, Kannada, Odia, Malayalam, and
Punjabi. Each of these news datasets comprises 10 or more classes of news
articles. L3Cube-IndicNews offers 3 distinct datasets tailored to handle
different document lengths that are classified as: Short Headlines
Classification (SHC) dataset containing the news headline and news category,
Long Document Classification (LDC) dataset containing the whole news article
and the news category, and Long Paragraph Classification (LPC) containing
sub-articles of the news and the news category. We maintain consistent labeling
across all 3 datasets for in-depth length-based analysis. We evaluate each of
these Indic language datasets using 4 different models including monolingual
BERT, multilingual Indic Sentence BERT (IndicSBERT), and IndicBERT. This
research contributes significantly to expanding the pool of available text
classification datasets and also makes it possible to develop topic
classification models for Indian regional languages. This also serves as an
excellent resource for cross-lingual analysis owing to the high overlap of
labels among languages. The datasets and models are shared publicly at
https://github.com/l3cube-pune/indic-nlp
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the International Conference on Natural Language
  Processing (ICON 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Policy-regularized Offline Multi-objective Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02244v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02244v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Lin, Chao Yu, Zongkai Liu, Zifan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we aim to utilize only offline trajectory data to train a
policy for multi-objective RL. We extend the offline policy-regularized method,
a widely-adopted approach for single-objective offline RL problems, into the
multi-objective setting in order to achieve the above goal. However, such
methods face a new challenge in offline MORL settings, namely the
preference-inconsistent demonstration problem. We propose two solutions to this
problem: 1) filtering out preference-inconsistent demonstrations via
approximating behavior preferences, and 2) adopting regularization techniques
with high policy expressiveness. Moreover, we integrate the
preference-conditioned scalarized update method into policy-regularized offline
RL, in order to simultaneously learn a set of policies using a single policy
network, thus reducing the computational cost induced by the training of a
large number of individual policies for various preferences. Finally, we
introduce Regularization Weight Adaptation to dynamically determine appropriate
regularization weights for arbitrary target preferences during deployment.
Empirical results on various multi-objective datasets demonstrate the
capability of our approach in solving offline MORL problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ U-Mixer: An Unet-Mixer Architecture with Stationarity Correction for
  Time Series Forecasting <span class="chip">AAAI2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02236v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02236v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Ma, Xuemei Li, Lexin Fang, Tianlong Zhao, Caiming Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series forecasting is a crucial task in various domains. Caused by
factors such as trends, seasonality, or irregular fluctuations, time series
often exhibits non-stationary. It obstructs stable feature propagation through
deep layers, disrupts feature distributions, and complicates learning data
distribution changes. As a result, many existing models struggle to capture the
underlying patterns, leading to degraded forecasting performance. In this
study, we tackle the challenge of non-stationarity in time series forecasting
with our proposed framework called U-Mixer. By combining Unet and Mixer,
U-Mixer effectively captures local temporal dependencies between different
patches and channels separately to avoid the influence of distribution
variations among channels, and merge low- and high-levels features to obtain
comprehensive data representations. The key contribution is a novel
stationarity correction method, explicitly restoring data distribution by
constraining the difference in stationarity between the data before and after
model processing to restore the non-stationarity information, while ensuring
the temporal dependencies are preserved. Through extensive experiments on
various real-world time series datasets, U-Mixer demonstrates its effectiveness
and robustness, and achieves 14.5\% and 7.7\% improvements over
state-of-the-art (SOTA) methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trajectory-Oriented Policy Optimization with Sparse Rewards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02225v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02225v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guojian Wang, Faguo Wu, Xiao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep reinforcement learning (DRL) remains challenging in tasks with sparse
rewards. These sparse rewards often only indicate whether the task is partially
or fully completed, meaning that many exploration actions must be performed
before the agent obtains useful feedback. Hence, most existing DRL algorithms
fail to learn feasible policies within a reasonable time frame. To overcome
this problem, we develop an approach that exploits offline demonstration
trajectories for faster and more efficient online RL in sparse reward settings.
Our key insight is that by regarding offline demonstration trajectories as
guidance, instead of imitating them, our method learns a policy whose
state-action visitation marginal distribution matches that of offline
demonstrations. Specifically, we introduce a novel trajectory distance based on
maximum mean discrepancy (MMD) and formulate policy optimization as a
distance-constrained optimization problem. Then, we show that this
distance-constrained optimization problem can be reduced into a policy-gradient
algorithm with shaped rewards learned from offline demonstrations. The proposed
algorithm is evaluated on extensive discrete and continuous control tasks with
sparse and deceptive rewards. The experimental results indicate that our
proposed algorithm is significantly better than the baseline methods regarding
diverse exploration and learning the optimal policy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust bilinear factor analysis based on the matrix-variate $t$
  distribution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan Ma, Jianhua Zhao, Changchun Shang, Fen Jiang, Philip L. H. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Factor Analysis based on multivariate $t$ distribution ($t$fa) is a useful
robust tool for extracting common factors on heavy-tailed or contaminated data.
However, $t$fa is only applicable to vector data. When $t$fa is applied to
matrix data, it is common to first vectorize the matrix observations. This
introduces two challenges for $t$fa: (i) the inherent matrix structure of the
data is broken, and (ii) robustness may be lost, as vectorized matrix data
typically results in a high data dimension, which could easily lead to the
breakdown of $t$fa. To address these issues, starting from the intrinsic matrix
structure of matrix data, a novel robust factor analysis model, namely bilinear
factor analysis built on the matrix-variate $t$ distribution ($t$bfa), is
proposed in this paper. The novelty is that it is capable to simultaneously
extract common factors for both row and column variables of interest on
heavy-tailed or contaminated matrix data. Two efficient algorithms for maximum
likelihood estimation of $t$bfa are developed. Closed-form expression for the
Fisher information matrix to calculate the accuracy of parameter estimates are
derived. Empirical studies are conducted to understand the proposed $t$bfa
model and compare with related competitors. The results demonstrate the
superiority and practicality of $t$bfa. Importantly, $t$bfa exhibits a
significantly higher breakdown point than $t$fa, making it more suitable for
matrix data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LADRI: LeArning-based Dynamic Risk Indicator in Automated Driving System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02199v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02199v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anil Ranjitbhai Patel, Peter Liggesmeyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the horizon of intelligent transportation expands with the evolution of
Automated Driving Systems (ADS), ensuring paramount safety becomes more
imperative than ever. Traditional risk assessment methodologies, primarily
crafted for human-driven vehicles, grapple to adequately adapt to the
multifaceted, evolving environments of ADS. This paper introduces a framework
for real-time Dynamic Risk Assessment (DRA) in ADS, harnessing the potency of
Artificial Neural Networks (ANNs).
  Our proposed solution transcends these limitations, drawing upon ANNs, a
cornerstone of deep learning, to meticulously analyze and categorize risk
dimensions using real-time On-board Sensor (OBS) data. This learning-centric
approach not only elevates the ADS's situational awareness but also enriches
its understanding of immediate operational contexts. By dissecting OBS data,
the system is empowered to pinpoint its current risk profile, thereby enhancing
safety prospects for onboard passengers and the broader traffic ecosystem.
  Through this framework, we chart a direction in risk assessment, bridging the
conventional voids and enhancing the proficiency of ADS. By utilizing ANNs, our
methodology offers a perspective, allowing ADS to adeptly navigate and react to
potential risk factors, ensuring safer and more informed autonomous journeys.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2023 IEEE International Test Conference, 8th Edition of Automotive,
  Reliability, Test & Safety Workshop in Disneyland, Anaheim, CA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nodule detection and generation on chest X-rays: NODE21 Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ecem Sogancioglu, Bram van Ginneken, Finn Behrendt, Marcel Bengs, Alexander Schlaefer, Miron Radu, Di Xu, Ke Sheng, Fabien Scalzo, Eric Marcus, Samuele Papa, Jonas Teuwen, Ernst Th. Scholten, Steven Schalekamp, Nils Hendrix, Colin Jacobs, Ward Hendrix, Clara I Sánchez, Keelin Murphy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pulmonary nodules may be an early manifestation of lung cancer, the leading
cause of cancer-related deaths among both men and women. Numerous studies have
established that deep learning methods can yield high-performance levels in the
detection of lung nodules in chest X-rays. However, the lack of gold-standard
public datasets slows down the progression of the research and prevents
benchmarking of methods for this task. To address this, we organized a public
research challenge, NODE21, aimed at the detection and generation of lung
nodules in chest X-rays. While the detection track assesses state-of-the-art
nodule detection systems, the generation track determines the utility of nodule
generation algorithms to augment training data and hence improve the
performance of the detection systems. This paper summarizes the results of the
NODE21 challenge and performs extensive additional experiments to examine the
impact of the synthetically generated nodule training images on the detection
algorithm performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FairGridSearch: A Framework to Compare Fairness-Enhancing Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shih-Chi Ma, Tatiana Ermakova, Benjamin Fabian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models are increasingly used in critical decision-making
applications. However, these models are susceptible to replicating or even
amplifying bias present in real-world data. While there are various bias
mitigation methods and base estimators in the literature, selecting the optimal
model for a specific application remains challenging.
  This paper focuses on binary classification and proposes FairGridSearch, a
novel framework for comparing fairness-enhancing models. FairGridSearch enables
experimentation with different model parameter combinations and recommends the
best one. The study applies FairGridSearch to three popular datasets (Adult,
COMPAS, and German Credit) and analyzes the impacts of metric selection, base
estimator choice, and classification threshold on model fairness.
  The results highlight the significance of selecting appropriate accuracy and
fairness metrics for model evaluation. Additionally, different base estimators
and classification threshold values affect the effectiveness of bias mitigation
methods and fairness stability respectively, but the effects are not consistent
across all datasets. Based on these findings, future research on fairness in
machine learning should consider a broader range of factors when building fair
models, going beyond bias mitigation methods alone.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disentangle Estimation of Causal Effects from Cross-Silo Data <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02154v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02154v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Liu, Haozhao Wang, Shuang Wang, Zhiming He, Wenchao Xu, Jialiang Zhu, Fan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating causal effects among different events is of great importance to
critical fields such as drug development. Nevertheless, the data features
associated with events may be distributed across various silos and remain
private within respective parties, impeding direct information exchange between
them. This, in turn, can result in biased estimations of local causal effects,
which rely on the characteristics of only a subset of the covariates. To tackle
this challenge, we introduce an innovative disentangle architecture designed to
facilitate the seamless cross-silo transmission of model parameters, enriched
with causal mechanisms, through a combination of shared and private branches.
Besides, we introduce global constraints into the equation to effectively
mitigate bias within the various missing domains, thereby elevating the
accuracy of our causal effect estimation. Extensive experiments conducted on
new semi-synthetic datasets show that our method outperforms state-of-the-art
baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Neural Networks for Tabular Data Learning: A <span class="highlight-title">Survey</span> with Taxonomy
  and Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng-Te Li, Yu-Che Tsai, Chih-Yao Chen, Jay Chiehen Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this survey, we dive into Tabular Data Learning (TDL) using Graph Neural
Networks (GNNs), a domain where deep learning-based approaches have
increasingly shown superior performance in both classification and regression
tasks compared to traditional methods. The survey highlights a critical gap in
deep neural TDL methods: the underrepresentation of latent correlations among
data instances and feature values. GNNs, with their innate capability to model
intricate relationships and interactions between diverse elements of tabular
data, have garnered significant interest and application across various TDL
domains. Our survey provides a systematic review of the methods involved in
designing and implementing GNNs for TDL (GNN4TDL). It encompasses a detailed
investigation into the foundational aspects and an overview of GNN-based TDL
methods, offering insights into their evolving landscape. We present a
comprehensive taxonomy focused on constructing graph structures and
representation learning within GNN-based TDL methods. In addition, the survey
examines various training plans, emphasizing the integration of auxiliary tasks
to enhance the effectiveness of instance representations. A critical part of
our discussion is dedicated to the practical application of GNNs across a
spectrum of GNN4TDL scenarios, demonstrating their versatility and impact.
Lastly, we discuss the limitations and propose future research directions,
aiming to spur advancements in GNN4TDL. This survey serves as a resource for
researchers and practitioners, offering a thorough understanding of GNNs' role
in revolutionizing TDL and pointing towards future innovations in this
promising area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review, ongoing work, Github page:
  https://github.com/Roytsai27/awesome-GNN4TDL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PosCUDA: Position based Convolution for Unlearnable Audio <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02135v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02135v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vignesh Gokul, Shlomo Dubnov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models require large amounts of clean data to acheive good
performance. To avoid the cost of expensive data acquisition, researchers use
the abundant data available on the internet. This raises significant privacy
concerns on the potential misuse of personal data for model training without
authorisation. Recent works such as CUDA propose solutions to this problem by
adding class-wise blurs to make datasets unlearnable, i.e a model can never use
the acquired dataset for learning. However these methods often reduce the
quality of the data making it useless for practical applications. We introduce
PosCUDA, a position based convolution for creating unlearnable audio datasets.
PosCUDA uses class-wise convolutions on small patches of audio. The location of
the patches are based on a private key for each class, hence the model learns
the relations between positional blurs and labels, while failing to generalize.
We empirically show that PosCUDA can achieve unlearnability while maintaining
the quality of the original audio datasets. Our proposed method is also robust
to different audio feature representations such as MFCC, raw audio and
different architectures such as transformers, convolutional networks etc.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ACP-ESM: A novel framework for classification of anticancer peptides
  using protein-oriented <span class="highlight-title">transformer</span> approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02124v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02124v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeynep Hilal Kilimci, Mustafa Yalcin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anticancer peptides (ACPs) are a class of molecules that have gained
significant attention in the field of cancer research and therapy. ACPs are
short chains of amino acids, the building blocks of proteins, and they possess
the ability to selectively target and kill cancer cells. One of the key
advantages of ACPs is their ability to selectively target cancer cells while
sparing healthy cells to a greater extent. This selectivity is often attributed
to differences in the surface properties of cancer cells compared to normal
cells. That is why ACPs are being investigated as potential candidates for
cancer therapy. ACPs may be used alone or in combination with other treatment
modalities like chemotherapy and radiation therapy. While ACPs hold promise as
a novel approach to cancer treatment, there are challenges to overcome,
including optimizing their stability, improving selectivity, and enhancing
their delivery to cancer cells, continuous increasing in number of peptide
sequences, developing a reliable and precise prediction model. In this work, we
propose an efficient transformer-based framework to identify anticancer
peptides for by performing accurate a reliable and precise prediction model.
For this purpose, four different transformer models, namely ESM, ProtBert,
BioBERT, and SciBERT are employed to detect anticancer peptides from amino acid
sequences. To demonstrate the contribution of the proposed framework, extensive
experiments are carried on widely-used datasets in the literature, two versions
of AntiCp2, cACP-DeepGram, ACP-740. Experiment results show the usage of
proposed model enhances classification accuracy when compared to the
state-of-the-art studies. The proposed framework, ESM, exhibits 96.45 of
accuracy for AntiCp2 dataset, 97.66 of accuracy for cACP-DeepGram dataset, and
88.51 of accuracy for ACP-740 dataset, thence determining new state-of-the-art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost
  Whole-Body Teleoperation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zipeng Fu, Tony Z. Zhao, Chelsea Finn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning from human demonstrations has shown impressive performance
in robotics. However, most results focus on table-top manipulation, lacking the
mobility and dexterity necessary for generally useful tasks. In this work, we
develop a system for imitating mobile manipulation tasks that are bimanual and
require whole-body control. We first present Mobile ALOHA, a low-cost and
whole-body teleoperation system for data collection. It augments the ALOHA
system with a mobile base, and a whole-body teleoperation interface. Using data
collected with Mobile ALOHA, we then perform supervised behavior cloning and
find that co-training with existing static ALOHA datasets boosts performance on
mobile manipulation tasks. With 50 demonstrations for each task, co-training
can increase success rates by up to 90%, allowing Mobile ALOHA to autonomously
complete complex mobile manipulation tasks such as sauteing and serving a piece
of shrimp, opening a two-door wall cabinet to store heavy cooking pots, calling
and entering an elevator, and lightly rinsing a used pan using a kitchen
faucet. Project website: https://mobile-aloha.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://mobile-aloha.github.io (Zipeng Fu and Tony
  Z. Zhao are project co-leads, Chelsea Finn is the advisor)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cadmium Zinc Telluride (CZT) photon counting detector Characterisation
  for soft tissue imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02106v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02106v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kamran Hameed, Rafidah Zainon, Mahbubunnabi Tamal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of photon counting detection technology has resulted in significant
X-ray imaging research interest in recent years. Computed Tomography (CT)
scanners can benefit from photon-counting detectors, which are new technology
with the potential to overcome key limitations of conventional CT detectors.
Researchers are still studying the effectiveness and sensitivity of
semiconductor detector materials in photon counting detectors for detecting
soft tissue contrasts. This study aimed to characterize the performance of the
Cadmium Zinc Telluride photon counting detector in identifying various tissues.
An optimal frame rate per second (FPS) of CZT detector was evaluated by setting
the X-ray tube voltage and current at 25 keV, 35 keV and 0.5 mA, 1.0 mA
respectively by keeping the optimum FPS fixed, the detector energy thresholds
were set in small steps from 15 keV to 35 keV and the Currents were set for
X-ray tubes in ranges of 0.1 mA to 1.0 mA to find the relationship between
voltage and current of the X-ray source and counts per second (CPS). The
samples i.e., fat, liver, muscles, paraffin wax, and contrast media were
stacked at six different thickness levels in a stair-step chamber made from
Plexi-glass. X-ray transmission at six different thicknesses of tissue samples
was also examined for five different energy (regions) thresholds (21 keV, 25
keV, 29 keV, 31 keV, and 45 keV) to determine the effect on count per second
(CPS). In this study, 12 frames per second is found to be the optimum frame
rate per second (FPS) based on the spectral response of an X-ray source and CPS
has a linear relationship with X-ray tube current as well. It was also noted
that A sample's thickness also affects its X-ray transmission at different
energy thresholds. A high sensitivity and linearity of the detectors make them
suitable for use in both preclinical and medical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages and 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Re-evaluating the Memory-balanced Pipeline Parallelism: BPipe 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mincong Huang, Chao Wang, Chi Ma, Yineng Zhang, Peng Zhang, Lei Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pipeline parallelism is an essential technique in the training of large-scale
Transformer models. However, it suffers from imbalanced memory consumption,
leading to insufficient memory utilization. The BPipe technique was proposed to
address this issue and has proven effective in the GPT-3 model. Nevertheless,
our experiments have not yielded similar benefits for LLaMA training.
Additionally, BPipe only yields negligible benefits for GPT-3 training when
applying flash attention. We analyze the underlying causes of the divergent
performance of BPipe on GPT-3 and LLaMA. Furthermore, we introduce a novel
method to estimate the performance of BPipe.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ View-based Explanations for Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02086v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02086v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingyang Chen, Dazhuo Qiu, Yinghui Wu, Arijit Khan, Xiangyu Ke, Yunjun Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating explanations for graph neural networks (GNNs) has been studied to
understand their behavior in analytical tasks such as graph classification.
Existing approaches aim to understand the overall results of GNNs rather than
providing explanations for specific class labels of interest, and may return
explanation structures that are hard to access, nor directly queryable.
  We propose GVEX, a novel paradigm that generates Graph Views for EXplanation.
(1) We design a two-tier explanation structure called explanation views. An
explanation view consists of a set of graph patterns and a set of induced
explanation subgraphs. Given a database G of multiple graphs and a specific
class label l assigned by a GNN-based classifier M, it concisely describes the
fraction of G that best explains why l is assigned by M. (2) We propose quality
measures and formulate an optimization problem to compute optimal explanation
views for GNN explanation. We show that the problem is $\Sigma^2_P$-hard. (3)
We present two algorithms. The first one follows an explain-and-summarize
strategy that first generates high-quality explanation subgraphs which best
explain GNNs in terms of feature influence maximization, and then performs a
summarization step to generate patterns. We show that this strategy provides an
approximation ratio of 1/2. Our second algorithm performs a single-pass to an
input node stream in batches to incrementally maintain explanation views,
having an anytime quality guarantee of 1/4 approximation. Using real-world
benchmark data, we experimentally demonstrate the effectiveness, efficiency,
and scalability of GVEX. Through case studies, we showcase the practical
applications of GVEX.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Energy based diffusion generator for efficient sampling of Boltzmann
  distributions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02080v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02080v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Wang, Ling Guo, Hao Wu, Tao Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel sampler called the energy based diffusion generator for
generating samples from arbitrary target distributions. The sampling model
employs a structure similar to a variational autoencoder, utilizing a decoder
to transform latent variables from a simple distribution into random variables
approximating the target distribution, and we design an encoder based on the
diffusion model. Leveraging the powerful modeling capacity of the diffusion
model for complex distributions, we can obtain an accurate variational estimate
of the Kullback-Leibler divergence between the distributions of the generated
samples and the target. Moreover, we propose a decoder based on generalized
Hamiltonian dynamics to further enhance sampling performance. Through empirical
evaluation, we demonstrate the effectiveness of our method across various
complex distribution functions, showcasing its superiority compared to existing
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ U-Trustworthy Models.Reliability, Competence, and Confidence in
  Decision-Making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02062v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02062v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ritwik Vashistha, Arya Farahi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With growing concerns regarding bias and discrimination in predictive models,
the AI community has increasingly focused on assessing AI system
trustworthiness. Conventionally, trustworthy AI literature relies on the
probabilistic framework and calibration as prerequisites for trustworthiness.
In this work, we depart from this viewpoint by proposing a novel trust
framework inspired by the philosophy literature on trust. We present a precise
mathematical definition of trustworthiness, termed
$\mathcal{U}$-trustworthiness, specifically tailored for a subset of tasks
aimed at maximizing a utility function. We argue that a model's
$\mathcal{U}$-trustworthiness is contingent upon its ability to maximize Bayes
utility within this task subset. Our first set of results challenges the
probabilistic framework by demonstrating its potential to favor less
trustworthy models and introduce the risk of misleading trustworthiness
assessments. Within the context of $\mathcal{U}$-trustworthiness, we prove that
properly-ranked models are inherently $\mathcal{U}$-trustworthy. Furthermore,
we advocate for the adoption of the AUC metric as the preferred measure of
trustworthiness. By offering both theoretical guarantees and experimental
validation, AUC enables robust evaluation of trustworthiness, thereby enhancing
model selection and hyperparameter tuning to yield more trustworthy outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Collapse for Cross-entropy Class-Imbalanced Learning with
  Unconstrained ReLU Feature Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02058v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02058v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hien Dang, Tho Tran, Tan Nguyen, Nhat Ho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current paradigm of training deep neural networks for classification
tasks includes minimizing the empirical risk that pushes the training loss
value towards zero, even after the training error has been vanished. In this
terminal phase of training, it has been observed that the last-layer features
collapse to their class-means and these class-means converge to the vertices of
a simplex Equiangular Tight Frame (ETF). This phenomenon is termed as Neural
Collapse (NC). To theoretically understand this phenomenon, recent works employ
a simplified unconstrained feature model to prove that NC emerges at the global
solutions of the training problem. However, when the training dataset is
class-imbalanced, some NC properties will no longer be true. For example, the
class-means geometry will skew away from the simplex ETF when the loss
converges. In this paper, we generalize NC to imbalanced regime for
cross-entropy loss under the unconstrained ReLU feature model. We prove that,
while the within-class features collapse property still holds in this setting,
the class-means will converge to a structure consisting of orthogonal vectors
with different lengths. Furthermore, we find that the classifier weights are
aligned to the scaled and centered class-means with scaling factors depend on
the number of training samples of each class, which generalizes NC in the
class-balanced setting. We empirically prove our results through experiments on
practical architectures and dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spikformer V2: Join the High Accuracy Club on ImageNet with an SNN
  Ticket 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02020v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02020v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaokun Zhou, Kaiwei Che, Wei Fang, Keyu Tian, Yuesheng Zhu, Shuicheng Yan, Yonghong Tian, Li Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking Neural Networks (SNNs), known for their biologically plausible
architecture, face the challenge of limited performance. The self-attention
mechanism, which is the cornerstone of the high-performance Transformer and
also a biologically inspired structure, is absent in existing SNNs. To this
end, we explore the potential of leveraging both self-attention capability and
biological properties of SNNs, and propose a novel Spiking Self-Attention (SSA)
and Spiking Transformer (Spikformer). The SSA mechanism eliminates the need for
softmax and captures the sparse visual feature employing spike-based Query,
Key, and Value. This sparse computation without multiplication makes SSA
efficient and energy-saving. Further, we develop a Spiking Convolutional Stem
(SCS) with supplementary convolutional layers to enhance the architecture of
Spikformer. The Spikformer enhanced with the SCS is referred to as Spikformer
V2. To train larger and deeper Spikformer V2, we introduce a pioneering
exploration of Self-Supervised Learning (SSL) within the SNN. Specifically, we
pre-train Spikformer V2 with masking and reconstruction style inspired by the
mainstream self-supervised Transformer, and then finetune the Spikformer V2 on
the image classification on ImageNet. Extensive experiments show that
Spikformer V2 outperforms other previous surrogate training and ANN2SNN
methods. An 8-layer Spikformer V2 achieves an accuracy of 80.38% using 4 time
steps, and after SSL, a 172M 16-layer Spikformer V2 reaches an accuracy of
81.10% with just 1 time step. To the best of our knowledge, this is the first
time that the SNN achieves 80+% accuracy on ImageNet. The code will be
available at Spikformer V2.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Function to Distribution Modeling: A PAC-Generative Approach to
  Offline Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02019v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02019v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Zhang, Ruida Zhou, Yang Shen, Tie Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper considers the problem of offline optimization, where the objective
function is unknown except for a collection of ``offline" data examples. While
recent years have seen a flurry of work on applying various machine learning
techniques to the offline optimization problem, the majority of these work
focused on learning a surrogate of the unknown objective function and then
applying existing optimization algorithms. While the idea of modeling the
unknown objective function is intuitive and appealing, from the learning point
of view it also makes it very difficult to tune the objective of the learner
according to the objective of optimization. Instead of learning and then
optimizing the unknown objective function, in this paper we take on a less
intuitive but more direct view that optimization can be thought of as a process
of sampling from a generative model. To learn an effective generative model
from the offline data examples, we consider the standard technique of
``re-weighting", and our main technical contribution is a probably
approximately correct (PAC) lower bound on the natural optimization objective,
which allows us to jointly learn a weight function and a score-based generative
model. The robustly competitive performance of the proposed approach is
demonstrated via empirical studies using the standard offline optimization
benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SwitchTab: Switched Autoencoders Are Effective Tabular Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02013v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02013v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Wu, Suiyao Chen, Qi Zhao, Renat Sergazinov, Chen Li, Shengjie Liu, Chongchao Zhao, Tianpei Xie, Hanqing Guo, Cheng Ji, Daniel Cociorva, Hakan Brunzel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised representation learning methods have achieved significant
success in computer vision and natural language processing, where data samples
exhibit explicit spatial or semantic dependencies. However, applying these
methods to tabular data is challenging due to the less pronounced dependencies
among data samples. In this paper, we address this limitation by introducing
SwitchTab, a novel self-supervised method specifically designed to capture
latent dependencies in tabular data. SwitchTab leverages an asymmetric
encoder-decoder framework to decouple mutual and salient features among data
pairs, resulting in more representative embeddings. These embeddings, in turn,
contribute to better decision boundaries and lead to improved results in
downstream tasks. To validate the effectiveness of SwitchTab, we conduct
extensive experiments across various domains involving tabular data. The
results showcase superior performance in end-to-end prediction tasks with
fine-tuning. Moreover, we demonstrate that pre-trained salient embeddings can
be utilized as plug-and-play features to enhance the performance of various
traditional classification methods (e.g., Logistic Regression, XGBoost, etc.).
Lastly, we highlight the capability of SwitchTab to create explainable
representations through visualization of decoupled mutual and salient features
in the latent space.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast & Fair: Efficient Second-Order Robust Optimization for Fairness in
  Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02012v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02012v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Allen Minch, Hung Anh Vu, Anne Marie Warren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This project explores adversarial training techniques to develop fairer Deep
Neural Networks (DNNs) to mitigate the inherent bias they are known to exhibit.
DNNs are susceptible to inheriting bias with respect to sensitive attributes
such as race and gender, which can lead to life-altering outcomes (e.g.,
demographic bias in facial recognition software used to arrest a suspect). We
propose a robust optimization problem, which we demonstrate can improve
fairness in several datasets, both synthetic and real-world, using an affine
linear model. Leveraging second order information, we are able to find a
solution to our optimization problem more efficiently than a purely first order
method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decentralized Multi-Task Online Convex Optimization Under Random Link
  Failures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02011v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02011v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjing Yan, Xuanyu Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decentralized optimization methods often entail information exchange between
neighbors. Transmission failures can happen due to network congestion,
hardware/software issues, communication outage, and other factors. In this
paper, we investigate the random link failure problem in decentralized
multi-task online convex optimization, where agents have individual decisions
that are coupled with each other via pairwise constraints. Although widely used
in constrained optimization, conventional saddle-point algorithms are not
directly applicable here because of random packet dropping. To address this
issue, we develop a robust decentralized saddle-point algorithm against random
link failures with heterogeneous probabilities by replacing the missing
decisions of neighbors with their latest received values. Then, by judiciously
bounding the accumulated deviation stemming from this replacement, we first
establish that our algorithm achieves $\mathcal{O}(\sqrt{T})$ regret and
$\mathcal{O}(T^\frac{3}{4})$ constraint violations for the full information
scenario, where the complete information on the local cost function is revealed
to each agent at the end of each time slot. These two bounds match, in order
sense, the performance bounds of algorithms with perfect communications.
Further, we extend our algorithm and analysis to the two-point bandit feedback
scenario, where only the values of the local cost function at two random points
are disclosed to each agent sequentially. Performance bounds of the same orders
as the full information case are derived. Finally, we corroborate the efficacy
of the proposed algorithms and the analytical results through numerical
simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages. 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Two-Stage Surrogate Modeling for Data-Driven Design Optimization with
  Application to Composite Microstructure Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02008v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02008v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farhad Pourkamali-Anaraki, Jamal F. Husseini, Evan J. Pineda, Brett A. Bednarcyk, Scott E. Stapleton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel two-stage machine learning-based surrogate
modeling framework to address inverse problems in scientific and engineering
fields. In the first stage of the proposed framework, a machine learning model
termed the "learner" identifies a limited set of candidates within the input
design space whose predicted outputs closely align with desired outcomes.
Subsequently, in the second stage, a separate surrogate model, functioning as
an "evaluator," is employed to assess the reduced candidate space generated in
the first stage. This evaluation process eliminates inaccurate and uncertain
solutions, guided by a user-defined coverage level. The framework's distinctive
contribution is the integration of conformal inference, providing a versatile
and efficient approach that can be widely applicable. To demonstrate the
effectiveness of the proposed framework compared to conventional single-stage
inverse problems, we conduct several benchmark tests and investigate an
engineering application focused on the micromechanical modeling of
fiber-reinforced composites. The results affirm the superiority of our proposed
framework, as it consistently produces more reliable solutions. Therefore, the
introduced framework offers a unique perspective on fostering interactions
between machine learning-based surrogate models in real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ t-DGR: A Trajectory-Based Deep Generative Replay Method for Continual
  Learning in Decision Making <span class="chip">NeurIPS
  2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02576v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02576v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Yue, Bo Liu, Peter Stone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep generative replay has emerged as a promising approach for continual
learning in decision-making tasks. This approach addresses the problem of
catastrophic forgetting by leveraging the generation of trajectories from
previously encountered tasks to augment the current dataset. However, existing
deep generative replay methods for continual learning rely on autoregressive
models, which suffer from compounding errors in the generated trajectories. In
this paper, we propose a simple, scalable, and non-autoregressive method for
continual learning in decision-making tasks using a generative model that
generates task samples conditioned on the trajectory timestep. We evaluate our
method on Continual World benchmarks and find that our approach achieves
state-of-the-art performance on the average success rate metric among continual
learning methods. Code is available at https://github.com/WilliamYue37/t-DGR .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2nd Workshop on Agent Learning in Open-Endedness (ALOE) at NeurIPS
  2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models for Social Networks: Applications, Challenges, and
  Solutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02575v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02575v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingying Zeng, Richard Huang, Waleed Malik, Langxuan Yin, Bojan Babic, Danny Shacham, Xiao Yan, Jaewon Yang, Qi He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are transforming the way people generate,
explore, and engage with content. We study how we can develop LLM applications
for online social networks. Despite LLMs' successes in other domains, it is
challenging to develop LLM-based products for social networks for numerous
reasons, and it has been relatively under-reported in the research community.
We categorize LLM applications for social networks into three categories. First
is knowledge tasks where users want to find new knowledge and information, such
as search and question-answering. Second is entertainment tasks where users
want to consume interesting content, such as getting entertaining notification
content. Third is foundational tasks that need to be done to moderate and
operate the social networks, such as content annotation and LLM monitoring. For
each task, we share the challenges we found, solutions we developed, and
lessons we learned. To the best of our knowledge, this is the first
comprehensive paper about developing LLM applications for social networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Siamese Residual Neural Network for Musical Shape Evaluation in Piano
  Performance Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoquan Li, Stephan Weiss, Yijun Yan, Yinhe Li, Jinchang Ren, John Soraghan, Ming Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding and identifying musical shape plays an important role in music
education and performance assessment. To simplify the otherwise time- and
cost-intensive musical shape evaluation, in this paper we explore how
artificial intelligence (AI) driven models can be applied. Considering musical
shape evaluation as a classification problem, a light-weight Siamese residual
neural network (S-ResNN) is proposed to automatically identify musical shapes.
To assess the proposed approach in the context of piano musical shape
evaluation, we have generated a new dataset, containing 4116 music pieces
derived by 147 piano preparatory exercises and performed in 28 categories of
musical shapes. The experimental results show that the S-ResNN significantly
outperforms a number of benchmark methods in terms of the precision, recall and
F1 score.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>X.Li, S.Weiss, Y.Yan, Y.Li, J.Ren, J.Soraghan, M.Gong,"Siamese
  residual neural network for musical shape evaluation in piano performance
  assessment" in Proc. of the 31st European Signal Processing Conference,
  Helsinki, Finland</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MeTA: Multi-source Test Time Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sk Miraj Ahmed, Fahim Faisal Niloy, Dripta S. Raychaudhuri, Samet Oymak, Amit K. Roy-Chowdhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test time adaptation is the process of adapting, in an unsupervised manner, a
pre-trained source model to each incoming batch of the test data (i.e., without
requiring a substantial portion of the test data to be available, as in
traditional domain adaptation) and without access to the source data. Since it
works with each batch of test data, it is well-suited for dynamic environments
where decisions need to be made as the data is streaming in. Current test time
adaptation methods are primarily focused on a single source model. We propose
the first completely unsupervised Multi-source Test Time Adaptation (MeTA)
framework that handles multiple source models and optimally combines them to
adapt to the test data. MeTA has two distinguishing features. First, it
efficiently obtains the optimal combination weights to combine the source
models to adapt to the test data distribution. Second, it identifies which of
the source model parameters to update so that only the model which is most
correlated to the target data is adapted, leaving the less correlated ones
untouched; this mitigates the issue of "forgetting" the source model parameters
by focusing only on the source model that exhibits the strongest correlation
with the test batch distribution. Experiments on diverse datasets demonstrate
that the combination of multiple source models does at least as well as the
best source (with hindsight knowledge), and performance does not degrade as the
test data distribution changes over time (robust to forgetting).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-term Fairness For Real-time Decision Making: A Constrained Online
  Optimization Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruijie Du, Deepan Muthirayan, Pramod P. Khargonekar, Yanning Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) has demonstrated remarkable capabilities across many
real-world systems, from predictive modeling to intelligent automation.
However, the widespread integration of machine learning also makes it necessary
to ensure machine learning-driven decision-making systems do not violate
ethical principles and values of society in which they operate. As ML-driven
decisions proliferate, particularly in cases involving sensitive attributes
such as gender, race, and age, to name a few, the need for equity and
impartiality has emerged as a fundamental concern. In situations demanding
real-time decision-making, fairness objectives become more nuanced and complex:
instantaneous fairness to ensure equity in every time slot, and long-term
fairness to ensure fairness over a period of time. There is a growing awareness
that real-world systems that operate over long periods and require fairness
over different timelines. However, existing approaches mainly address dynamic
costs with time-invariant fairness constraints, often disregarding the
challenges posed by time-varying fairness constraints. To bridge this gap, this
work introduces a framework for ensuring long-term fairness within dynamic
decision-making systems characterized by time-varying fairness constraints. We
formulate the decision problem with fairness constraints over a period as a
constrained online optimization problem. A novel online algorithm, named
LoTFair, is presented that solves the problem 'on the fly'. We prove that
LoTFair can make overall fairness violations negligible while maintaining the
performance over the long run.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hyperparameter Estimation for Sparse Bayesian Learning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02544v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02544v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Yu, Lixin Shen, Guohui Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse Bayesian Learning (SBL) models are extensively used in signal
processing and machine learning for promoting sparsity through hierarchical
priors. The hyperparameters in SBL models are crucial for the model's
performance, but they are often difficult to estimate due to the non-convexity
and the high-dimensionality of the associated objective function. This paper
presents a comprehensive framework for hyperparameter estimation in SBL models,
encompassing well-known algorithms such as the expectation-maximization (EM),
MacKay, and convex bounding (CB) algorithms. These algorithms are cohesively
interpreted within an alternating minimization and linearization (AML)
paradigm, distinguished by their unique linearized surrogate functions.
Additionally, a novel algorithm within the AML framework is introduced, showing
enhanced efficiency, especially under low signal noise ratios. This is further
improved by a new alternating minimization and quadratic approximation (AMQ)
paradigm, which includes a proximal regularization term. The paper
substantiates these advancements with thorough convergence analysis and
numerical experiments, demonstrating the algorithm's effectiveness in various
noise conditions and signal-to-noise ratios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Novel End-to-End Production-Ready Machine Learning Flow for
  Nanolithography Modeling and Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed S. E. Habib, Hossam A. H. Fahmy, Mohamed F. Abu-ElYazeed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optical lithography is the main enabler to semiconductor manufacturing. It
requires extensive processing to perform the Resolution Enhancement Techniques
(RETs) required to transfer the design data to a working Integrated Circuits
(ICs). The processing power and computational runtime for RETs tasks is ever
increasing due to the continuous reduction of the feature size and the
expansion of the chip area. State-of-the-art research sought Machine Learning
(ML) technologies to reduce runtime and computational power, however they are
still not used in production yet. In this study, we analyze the reasons holding
back ML computational lithography from being production ready and present a
novel highly scalable end-to-end flow that enables production ready ML-RET
correction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Branched Variational Autoencoder Classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Salah, David Yevick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a modified variational autoencoder (VAEs) that contains
an additional neural network branch. The resulting branched VAE (BVAE)
contributes a classification component based on the class labels to the total
loss and therefore imparts categorical information to the latent
representation. As a result, the latent space distributions of the input
classes are separated and ordered, thereby enhancing the classification
accuracy. The degree of improvement is quantified by numerical calculations
employing the benchmark MNIST dataset for both unrotated and rotated digits.
The proposed technique is then compared to and then incorporated into a VAE
with fixed output distributions. This procedure is found to yield improved
performance for a wide range of output distributions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comprehensive Exploration of Synthetic Data Generation: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        André Bauer, Simon Trapp, Michael Stenger, Robert Leppich, Samuel Kounev, Mark Leznik, Kyle Chard, Ian Foster
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed a surge in the popularity of Machine Learning
(ML), applied across diverse domains. However, progress is impeded by the
scarcity of training data due to expensive acquisition and privacy legislation.
Synthetic data emerges as a solution, but the abundance of released models and
limited overview literature pose challenges for decision-making. This work
surveys 417 Synthetic Data Generation (SDG) models over the last decade,
providing a comprehensive overview of model types, functionality, and
improvements. Common attributes are identified, leading to a classification and
trend analysis. The findings reveal increased model performance and complexity,
with neural network-based approaches prevailing, except for privacy-preserving
data generation. Computer vision dominates, with GANs as primary generative
models, while diffusion models, transformers, and RNNs compete. Implications
from our performance evaluation highlight the scarcity of common metrics and
datasets, making comparisons challenging. Additionally, the neglect of training
and computational costs in literature necessitates attention in future
research. This work serves as a guide for SDG model selection and identifies
crucial areas for future exploration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image-based Deep Learning for Smart Digital Twins: a <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02523v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02523v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Ruman Islam, Mahadevan Subramaniam, Pei-Chi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Smart Digital twins (SDTs) are being increasingly used to virtually replicate
and predict the behaviors of complex physical systems through continual data
assimilation enabling the optimization of the performance of these systems by
controlling the actions of systems. Recently, deep learning (DL) models have
significantly enhanced the capabilities of SDTs, particularly for tasks such as
predictive maintenance, anomaly detection, and optimization. In many domains,
including medicine, engineering, and education, SDTs use image data
(image-based SDTs) to observe and learn system behaviors and control their
behaviors. This paper focuses on various approaches and associated challenges
in developing image-based SDTs by continually assimilating image data from
physical systems. The paper also discusses the challenges involved in designing
and implementing DL models for SDTs, including data acquisition, processing,
and interpretation. In addition, insights into the future directions and
opportunities for developing new image-based DL approaches to develop robust
SDTs are provided. This includes the potential for using generative models for
data augmentation, developing multi-modal DL models, and exploring the
integration of DL with other technologies, including 5G, edge computing, and
IoT. In this paper, we describe the image-based SDTs, which enable broader
adoption of the digital twin DT paradigms across a broad spectrum of areas and
the development of new methods to improve the abilities of SDTs in replicating,
predicting, and optimizing the behavior of complex systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 2 figures, and 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Structured Matrix Learning under Arbitrary Entrywise Dependence and
  Estimation of Markov Transition Kernel 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinhang Chai, Jianqing Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The problem of structured matrix estimation has been studied mostly under
strong noise dependence assumptions. This paper considers a general framework
of noisy low-rank-plus-sparse matrix recovery, where the noise matrix may come
from any joint distribution with arbitrary dependence across entries. We
propose an incoherent-constrained least-square estimator and prove its
tightness both in the sense of deterministic lower bound and matching minimax
risks under various noise distributions. To attain this, we establish a novel
result asserting that the difference between two arbitrary low-rank incoherent
matrices must spread energy out across its entries, in other words cannot be
too sparse, which sheds light on the structure of incoherent low-rank matrices
and may be of independent interest. We then showcase the applications of our
framework to several important statistical machine learning problems. In the
problem of estimating a structured Markov transition kernel, the proposed
method achieves the minimax optimality and the result can be extended to
estimating the conditional mean operator, a crucial component in reinforcement
learning. The applications to multitask regression and structured covariance
estimation are also presented. We propose an alternating minimization algorithm
to approximately solve the potentially hard optimization problem. Numerical
results corroborate the effectiveness of our method which typically converges
in a few steps.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>55 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gain Scheduling with a Neural Operator for a Transport PDE with
  Nonlinear Recirculation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02511v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02511v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxence Lamarque, Luke Bhan, Rafael Vazquez, Miroslav Krstic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To stabilize PDE models, control laws require space-dependent functional
gains mapped by nonlinear operators from the PDE functional coefficients. When
a PDE is nonlinear and its "pseudo-coefficient" functions are state-dependent,
a gain-scheduling (GS) nonlinear design is the simplest approach to the design
of nonlinear feedback. The GS version of PDE backstepping employs gains
obtained by solving a PDE at each value of the state. Performing such PDE
computations in real time may be prohibitive. The recently introduced neural
operators (NO) can be trained to produce the gain functions, rapidly in real
time, for each state value, without requiring a PDE solution. In this paper we
introduce NOs for GS-PDE backstepping. GS controllers act on the premise that
the state change is slow and, as a result, guarantee only local stability, even
for ODEs. We establish local stabilization of hyperbolic PDEs with nonlinear
recirculation using both a "full-kernel" approach and the "gain-only" approach
to gain operator approximation. Numerical simulations illustrate stabilization
and demonstrate speedup by three orders of magnitude over traditional PDE
gain-scheduling. Code (Github) for the numerical implementation is published to
enable exploration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards an Adaptable and Generalizable Optimization Engine in Decision
  and Control: A Meta Reinforcement Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sungwook Yang, Chaoying Pei, Ran Dai, Chuangchuang Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sampling-based model predictive control (MPC) has found significant success
in optimal control problems with non-smooth system dynamics and cost function.
Many machine learning-based works proposed to improve MPC by a) learning or
fine-tuning the dynamics/ cost function, or b) learning to optimize for the
update of the MPC controllers. For the latter, imitation learning-based
optimizers are trained to update the MPC controller by mimicking the expert
demonstrations, which, however, are expensive or even unavailable. More
significantly, many sequential decision-making problems are in non-stationary
environments, requiring that an optimizer should be adaptable and generalizable
to update the MPC controller for solving different tasks. To address those
issues, we propose to learn an optimizer based on meta-reinforcement learning
(RL) to update the controllers. This optimizer does not need expert
demonstration and can enable fast adaptation (e.g., few-shots) when it is
deployed in unseen control tasks. Experimental results validate the
effectiveness of the learned optimizer regarding fast adaptation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The cell signaling structure function 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Layton Aho, Mark Winter, Marc DeCarlo, Agne Frismantiene, Yannick Blum, Paolo Armando Gagliardi, Olivier Pertz, Andrew R. Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Live cell microscopy captures 5-D $(x,y,z,channel,time)$ movies that display
patterns of cellular motion and signaling dynamics. We present here an approach
to finding spatiotemporal patterns of cell signaling dynamics in 5-D live cell
microscopy movies unique in requiring no \emph{a priori} knowledge of expected
pattern dynamics, and no training data. The proposed cell signaling structure
function (SSF) is a Kolmogorov structure function that optimally measures cell
signaling state as nuclear intensity w.r.t. surrounding cytoplasm, a
significant improvement compared to the current state-of-the-art cytonuclear
ratio. SSF kymographs store at each spatiotemporal cell centroid the SSF value,
or a functional output such as velocity. Patterns of similarity are identified
via the metric normalized compression distance (NCD). The NCD is a reproducing
kernel for a Hilbert space that represents the input SSF kymographs as points
in a low dimensional embedding that optimally captures the pattern similarity
identified by the NCD throughout the space. The only parameter is the expected
cell radii ($\mu m$). A new formulation of the cluster structure function
optimally estimates how meaningful an embedding from the RKHS representation.
Results are presented quantifying the impact of ERK and AKT signaling between
different oncogenic mutations, and by the relation between ERK signaling and
cellular velocity patterns for movies of 2-D monolayers of human breast
epithelial (MCF10A) cells, 3-D MCF10A spheroids under optogenetic manipulation
of ERK, and human induced pluripotent stem cells .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Anatomy-aware and acquisition-agnostic joint registration with
  SynthMorph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11329v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11329v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malte Hoffmann, Andrew Hoopes, Douglas N. Greve, Bruce Fischl, Adrian V. Dalca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Affine image registration is a cornerstone of medical-image analysis. While
classical algorithms can achieve excellent accuracy, they solve a
time-consuming optimization for every image pair. Deep-learning (DL) methods
learn a function that maps an image pair to an output transform. Evaluating the
function is fast, but capturing large transforms can be challenging, and
networks tend to struggle if a test-image characteristic shifts from the
training domain, such as resolution. Most affine methods are agnostic to
anatomy, meaning the registration will be inaccurate if algorithms consider all
structures in the image.
  We address these shortcomings with SynthMorph, an easy-to-use DL tool for
joint affine-deformable registration of any brain image without preprocessing,
right off the MRI scanner. First, we leverage a strategy to train networks with
wildly varying images synthesized from label maps, yielding robust performance
across acquisition specifics unseen at training. Second, we optimize the
spatial overlap of select anatomical labels. This enables networks to
distinguish anatomy of interest from irrelevant structures, removing the need
for preprocessing that excludes content which would impinge on anatomy-specific
registration. Third, we combine the affine model with a deformable hypernetwork
that lets users choose the optimal deformation-field regularity for their
specific data, at registration time, in a fraction of the time required by
classical methods.
  We rigorously analyze how competing architectures learn affine transforms and
compare state-of-the-art registration tools across an extremely diverse set of
neuroimaging data, aiming to truly capture the behavior of methods in the real
world. SynthMorph demonstrates consistent and improved accuracy. It is
available at https://w3id.org/synthmorph, as a single complete end-to-end
solution for registration of brain MRI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 22 figures, 4 tables, affine registration, deformable
  registration, deep learning, hypernetwork, domain shift, neuroimaging</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Controlling Moments with Kernel Stein Discrepancies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.05408v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.05408v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heishiro Kanagawa, Alessandro Barp, Arthur Gretton, Lester Mackey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Kernel Stein discrepancies (KSDs) measure the quality of a distributional
approximation and can be computed even when the target density has an
intractable normalizing constant. Notable applications include the diagnosis of
approximate MCMC samplers and goodness-of-fit tests for unnormalized
statistical models. The present work analyzes the convergence control
properties of KSDs. We first show that standard KSDs used for weak convergence
control fail to control moment convergence. To address this limitation, we next
provide sufficient conditions under which alternative diffusion KSDs control
both moment and weak convergence. As an immediate consequence we develop, for
each $q > 0$, the first KSDs known to exactly characterize $q$-Wasserstein
convergence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>93 pages, 19 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Language-Model Agents on Realistic Autonomous Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11671v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11671v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Megan Kinniment, Lucas Jun Koba Sato, Haoxing Du, Brian Goodrich, Max Hasin, Lawrence Chan, Luke Harold Miles, Tao R. Lin, Hjalmar Wijk, Joel Burget, Aaron Ho, Elizabeth Barnes, Paul Christiano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this report, we explore the ability of language model agents to acquire
resources, create copies of themselves, and adapt to novel challenges they
encounter in the wild. We refer to this cluster of capabilities as "autonomous
replication and adaptation" or ARA. We believe that systems capable of ARA
could have wide-reaching and hard-to-anticipate consequences, and that
measuring and forecasting ARA may be useful for informing measures around
security, monitoring, and alignment. Additionally, once a system is capable of
ARA, placing bounds on a system's capabilities may become significantly more
difficult.
  We construct four simple example agents that combine language models with
tools that allow them to take actions in the world. We then evaluate these
agents on 12 tasks relevant to ARA. We find that these language model agents
can only complete the easiest tasks from this list, although they make some
progress on the more challenging tasks. Unfortunately, these evaluations are
not adequate to rule out the possibility that near-future agents will be
capable of ARA. In particular, we do not think that these evaluations provide
good assurance that the ``next generation'' of language models (e.g. 100x
effective compute scaleup on existing models) will not yield agents capable of
ARA, unless intermediate evaluations are performed during pretraining.
Relatedly, we expect that fine-tuning of the existing models could produce
substantially more competent agents, even if the fine-tuning is not directly
targeted at ARA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Generalize towards Unseen Domains via a Content-Aware Style
  Invariant Model for Disease Detection from Chest X-rays 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.13991v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.13991v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Zunaed, Md. Aynal Haque, Taufiq Hasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Performance degradation due to distribution discrepancy is a longstanding
challenge in intelligent imaging, particularly for chest X-rays (CXRs). Recent
studies have demonstrated that CNNs are biased toward styles (e.g.,
uninformative textures) rather than content (e.g., shape), in stark contrast to
the human vision system. Radiologists tend to learn visual cues from CXRs and
thus perform well across multiple domains. Motivated by this, we employ the
novel on-the-fly style randomization modules at both image (SRM-IL) and feature
(SRM-FL) levels to create rich style perturbed features while keeping the
content intact for robust cross-domain performance. Previous methods simulate
unseen domains by constructing new styles via interpolation or swapping styles
from existing data, limiting them to available source domains during training.
However, SRM-IL samples the style statistics from the possible value range of a
CXR image instead of the training data to achieve more diversified
augmentations. Moreover, we utilize pixel-wise learnable parameters in the
SRM-FL compared to pre-defined channel-wise mean and standard deviations as
style embeddings for capturing more representative style features.
Additionally, we leverage consistency regularizations on global semantic
features and predictive distributions from with and without style-perturbed
versions of the same CXR to tweak the model's sensitivity toward content
markers for accurate predictions. Our proposed method, trained on CheXpert and
MIMIC-CXR datasets, achieves 77.32$\pm$0.35, 88.38$\pm$0.19, 82.63$\pm$0.13
AUCs(%) on the unseen domain test datasets, i.e., BRAX, VinDr-CXR, and NIH
chest X-ray14, respectively, compared to 75.56$\pm$0.80, 87.57$\pm$0.46,
82.07$\pm$0.19 from state-of-the-art models on five-fold cross-validation with
statistically significant results in thoracic disease classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalized Quadratic Embeddings for Nonlinear Dynamics using Deep
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.00357v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.00357v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pawan Goyal, Peter Benner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The engineering design process often relies on mathematical modeling that can
describe the underlying dynamic behavior. In this work, we present a
data-driven methodology for modeling the dynamics of nonlinear systems. To
simplify this task, we aim to identify a coordinate transformation that allows
us to represent the dynamics of nonlinear systems using a common, simple model
structure. The advantage of a common simple model is that customized design
tools developed for it can be applied to study a large variety of nonlinear
systems. The simplest common model -- one can think of -- is linear, but linear
systems often fall short in accurately capturing the complex dynamics of
nonlinear systems. In this work, we propose using quadratic systems as the
common structure, inspired by the lifting principle. According to this
principle, smooth nonlinear systems can be expressed as quadratic systems in
suitable coordinates without approximation errors. However, finding these
coordinates solely from data is challenging. Here, we leverage deep learning to
identify such lifted coordinates using only data, enabling a quadratic
dynamical system to describe the system's dynamics. Additionally, we discuss
the asymptotic stability of these quadratic dynamical systems. We illustrate
the approach using data collected from various numerical examples,
demonstrating its superior performance with the existing well-known techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Generalizable Physics-informed Learning Framework for Risk Probability
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.06432v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.06432v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoyuan Wang, Yorie Nakahira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate estimates of long-term risk probabilities and their gradients are
critical for many stochastic safe control methods. However, computing such risk
probabilities in real-time and in unseen or changing environments is
challenging. Monte Carlo (MC) methods cannot accurately evaluate the
probabilities and their gradients as an infinitesimal devisor can amplify the
sampling noise. In this paper, we develop an efficient method to evaluate the
probabilities of long-term risk and their gradients. The proposed method
exploits the fact that long-term risk probability satisfies certain partial
differential equations (PDEs), which characterize the neighboring relations
between the probabilities, to integrate MC methods and physics-informed neural
networks. We provide theoretical guarantees of the estimation error given
certain choices of training configurations. Numerical results show the proposed
method has better sample efficiency, generalizes well to unseen regions, and
can adapt to systems with changing parameters. The proposed method can also
accurately estimate the gradients of risk probabilities, which enables first-
and second-order techniques on risk probabilities to be used for learning and
control.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 5th Annual Learning for Dynamics & Control (L4DC)
  Conference, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards a Foundation Purchasing Model: <span class="highlight-title">Pretrain</span>ed Generative
  Autoregression on Transaction Sequences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01641v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01641v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piotr Skalski, David Sutton, Stuart Burrell, Iker Perez, Jason Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models underpin many modern financial systems for use cases
such as fraud detection and churn prediction. Most are based on supervised
learning with hand-engineered features, which relies heavily on the
availability of labelled data. Large self-supervised generative models have
shown tremendous success in natural language processing and computer vision,
yet so far they haven't been adapted to multivariate time series of financial
transactions. In this paper, we present a generative pretraining method that
can be used to obtain contextualised embeddings of financial transactions.
Benchmarks on public datasets demonstrate that it outperforms state-of-the-art
self-supervised methods on a range of downstream tasks. We additionally perform
large-scale pretraining of an embedding model using a corpus of data from 180
issuing banks containing 5.1 billion transactions and apply it to the card
fraud detection problem on hold-out datasets. The embedding model significantly
improves value detection rate at high precision thresholds and transfers well
to out-of-domain distributions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarial Data Poisoning for Fake News Detection: How to Make a Model
  Misclassify a Target News without Modifying It 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.15228v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.15228v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Siciliano, Luca Maiano, Lorenzo Papa, Federica Baccini, Irene Amerini, Fabrizio Silvestri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fake news detection models are critical to countering disinformation but can
be manipulated through adversarial attacks. In this position paper, we analyze
how an attacker can compromise the performance of an online learning detector
on specific news content without being able to manipulate the original target
news. In some contexts, such as social networks, where the attacker cannot
exert complete control over all the information, this scenario can indeed be
quite plausible. Therefore, we show how an attacker could potentially introduce
poisoning data into the training data to manipulate the behavior of an online
learning method. Our initial findings reveal varying susceptibility of logistic
regression models based on complexity and attack type.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Brain Tumor Segmentation (BraTS) Challenge 2023: Focus on Pediatrics
  (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17033v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17033v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anahita Fathi Kazerooni, Nastaran Khalili, Xinyang Liu, Debanjan Haldar, Zhifan Jiang, Syed Muhammed Anwar, Jake Albrecht, Maruf Adewole, Udunna Anazodo, Hannah Anderson, Sina Bagheri, Ujjwal Baid, Timothy Bergquist, Austin J. Borja, Evan Calabrese, Verena Chung, Gian-Marco Conte, Farouk Dako, James Eddy, Ivan Ezhov, Ariana Familiar, Keyvan Farahani, Shuvanjan Haldar, Juan Eugenio Iglesias, Anastasia Janas, Elaine Johansen, Blaise V Jones, Florian Kofler, Dominic LaBella, Hollie Anne Lai, Koen Van Leemput, Hongwei Bran Li, Nazanin Maleki, Aaron S McAllister, Zeke Meier, Bjoern Menze, Ahmed W Moawad, Khanak K Nandolia, Julija Pavaine, Marie Piraud, Tina Poussaint, Sanjay P Prabhu, Zachary Reitman, Andres Rodriguez, Jeffrey D Rudie, Ibraheem Salman Shaikh, Lubdha M. Shah, Nakul Sheth, Russel Taki Shinohara, Wenxin Tu, Karthik Viswanathan, Chunhao Wang, Jeffrey B Ware, Benedikt Wiestler, Walter Wiggins, Anna Zapaishchykova, Mariam Aboian, Miriam Bornhorst, Peter de Blank, Michelle Deutsch, Maryam Fouladi, Lindsey Hoffman, Benjamin Kann, Margot Lazow, Leonie Mikael, Ali Nabavizadeh, Roger Packer, Adam Resnick, Brian Rood, Arastoo Vossough, Spyridon Bakas, Marius George Linguraru
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pediatric tumors of the central nervous system are the most common cause of
cancer-related death in children. The five-year survival rate for high-grade
gliomas in children is less than 20\%. Due to their rarity, the diagnosis of
these entities is often delayed, their treatment is mainly based on historic
treatment concepts, and clinical trials require multi-institutional
collaborations. The MICCAI Brain Tumor Segmentation (BraTS) Challenge is a
landmark community benchmark event with a successful history of 12 years of
resource creation for the segmentation and analysis of adult glioma. Here we
present the CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023 challenge, which
represents the first BraTS challenge focused on pediatric brain tumors with
data acquired across multiple international consortia dedicated to pediatric
neuro-oncology and clinical trials. The BraTS-PEDs 2023 challenge focuses on
benchmarking the development of volumentric segmentation algorithms for
pediatric brain glioma through standardized quantitative performance evaluation
metrics utilized across the BraTS 2023 cluster of challenges. Models gaining
knowledge from the BraTS-PEDs multi-parametric structural MRI (mpMRI) training
data will be evaluated on separate validation and unseen test mpMRI dataof
high-grade pediatric glioma. The CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023
challenge brings together clinicians and AI/imaging scientists to lead to
faster development of automated segmentation techniques that could benefit
clinical trials, and ultimately the care of children with brain tumors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Model Compression for Neural Networks: Framework, Algorithm, and
  Convergence Guarantee 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06815v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06815v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyang Li, Jihoon Chung, Biao Cai, Haimin Wang, Xianlian Zhou, Bo Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model compression is a crucial part of deploying neural networks (NNs),
especially when the memory and storage of computing devices are limited in many
applications. This paper focuses on two model compression techniques: low-rank
approximation and weight pruning in neural networks, which are very popular
nowadays. However, training NN with low-rank approximation and weight pruning
always suffers significant accuracy loss and convergence issues. In this paper,
a holistic framework is proposed for model compression from a novel perspective
of nonconvex optimization by designing an appropriate objective function. Then,
we introduce NN-BCD, a block coordinate descent (BCD) algorithm to solve the
nonconvex optimization. One advantage of our algorithm is that an efficient
iteration scheme can be derived with closed-form, which is gradient-free.
Therefore, our algorithm will not suffer from vanishing/exploding gradient
problems. Furthermore, with the Kurdyka-{\L}ojasiewicz (K{\L}) property of our
objective function, we show that our algorithm globally converges to a critical
point at the rate of O(1/k), where k denotes the number of iterations. Lastly,
extensive experiments with tensor train decomposition and weight pruning
demonstrate the efficiency and superior performance of the proposed framework.
Our code implementation is available at https://github.com/ChenyangLi-97/NN-BCD
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Generate Training <span class="highlight-title">Dataset</span>s for Robust Semantic Segmentation <span class="chip">WACV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.02535v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.02535v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marwane Hariat, Olivier Laurent, Rémi Kazmierczak, Shihao Zhang, Andrei Bursuc, Angela Yao, Gianni Franchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation methods have advanced significantly. Still, their
robustness to real-world perturbations and object types not seen during
training remains a challenge, particularly in safety-critical applications. We
propose a novel approach to improve the robustness of semantic segmentation
techniques by leveraging the synergy between label-to-image generators and
image-to-label segmentation models. Specifically, we design Robusta, a novel
robust conditional generative adversarial network to generate realistic and
plausible perturbed images that can be used to train reliable segmentation
models. We conduct in-depth studies of the proposed generative model, assess
the performance and robustness of the downstream segmentation network, and
demonstrate that our approach can significantly enhance the robustness in the
face of real-world perturbations, distribution shifts, and out-of-distribution
samples. Our results suggest that this approach could be valuable in
safety-critical applications, where the reliability of perception modules such
as semantic segmentation is of utmost importance and comes with a limited
computational budget in inference. We release our code at
https://github.com/ENSTA-U2IS/robusta.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at WACV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Approximating the Shapley Value without Marginal Contributions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.00736v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.00736v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Kolpaczki, Viktor Bengs, Maximilian Muschalik, Eyke Hüllermeier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Shapley value, which is arguably the most popular approach for assigning
a meaningful contribution value to players in a cooperative game, has recently
been used intensively in explainable artificial intelligence. Its
meaningfulness is due to axiomatic properties that only the Shapley value
satisfies, which, however, comes at the expense of an exact computation growing
exponentially with the number of agents. Accordingly, a number of works are
devoted to the efficient approximation of the Shapley value, most of them
revolve around the notion of an agent's marginal contribution. In this paper,
we propose with SVARM and Stratified SVARM two parameter-free and
domain-independent approximation algorithms based on a representation of the
Shapley value detached from the notion of marginal contribution. We prove
unmatched theoretical guarantees regarding their approximation quality and
provide empirical results including synthetic games as well as common
explainability use cases comparing ourselves with state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncertainty in GNN Learning Evaluations: A Comparison Between Measures
  for Quantifying Randomness in GNN Community Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.09015v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.09015v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Leeney, Ryan McConville
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  (1) The enhanced capability of Graph Neural Networks (GNNs) in unsupervised
community detection of clustered nodes is attributed to their capacity to
encode both the connectivity and feature information spaces of graphs. The
identification of latent communities holds practical significance in various
domains, from social networks to genomics. Current real-world performance
benchmarks are perplexing due to the multitude of decisions influencing GNN
evaluations for this task. (2) Three metrics are compared to assess the
consistency of algorithm rankings in the presence of randomness. The
consistency and quality of performance between the results under a
hyperparameter optimisation with the default hyperparameters is evaluated. (3)
The results compare hyperparameter optimisation with default hyperparameters,
revealing a significant performance loss when neglecting hyperparameter
investigation. A comparison of metrics indicates that ties in ranks can
substantially alter the quantification of randomness. (4) Ensuring adherence to
the same evaluation criteria may result in notable differences in the reported
performance of methods for this task. The $W$ Randomness coefficient, based on
the Wasserstein distance, is identified as providing the most robust assessment
of randomness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 2 figures, contribution from COMPLEX NETWORKS 2023 selected
  for a possible publication in the special issue of the journal Entropy
  dedicated to the conference. arXiv admin note: substantial text overlap with
  arXiv:2305.06026</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SGFormer: Simplifying and Empowering <span class="highlight-title">Transformer</span>s for Large-Graph
  Representations <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.10759v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.10759v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qitian Wu, Wentao Zhao, Chenxiao Yang, Hengrui Zhang, Fan Nie, Haitian Jiang, Yatao Bian, Junchi Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning representations on large-sized graphs is a long-standing challenge
due to the inter-dependence nature involved in massive data points.
Transformers, as an emerging class of foundation encoders for graph-structured
data, have shown promising performance on small graphs due to its global
attention capable of capturing all-pair influence beyond neighboring nodes.
Even so, existing approaches tend to inherit the spirit of Transformers in
language and vision tasks, and embrace complicated models by stacking deep
multi-head attentions. In this paper, we critically demonstrate that even using
a one-layer attention can bring up surprisingly competitive performance across
node property prediction benchmarks where node numbers range from
thousand-level to billion-level. This encourages us to rethink the design
philosophy for Transformers on large graphs, where the global attention is a
computation overhead hindering the scalability. We frame the proposed scheme as
Simplified Graph Transformers (SGFormer), which is empowered by a simple
attention model that can efficiently propagate information among arbitrary
nodes in one layer. SGFormer requires none of positional encodings,
feature/graph pre-processing or augmented loss. Empirically, SGFormer
successfully scales to the web-scale graph ogbn-papers100M and yields up to
141x inference acceleration over SOTA Transformers on medium-sized graphs.
Beyond current results, we believe the proposed methodology alone enlightens a
new technical path of independent interest for building Transformers on large
graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2023, the codes are available at
  https://github.com/qitianwu/SGFormer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Discretized Neural Networks under Ricci Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.03390v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.03390v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Chen, Hanwen Chen, Mengmeng Wang, Guang Dai, Ivor W. Tsang, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study Discretized Neural Networks (DNNs) composed of
low-precision weights and activations, which suffer from either infinite or
zero gradients due to the non-differentiable discrete function during training.
Most training-based DNNs in such scenarios employ the standard Straight-Through
Estimator (STE) to approximate the gradient w.r.t. discrete values. However,
the use of STE introduces the problem of gradient mismatch, arising from
perturbations in the approximated gradient. To address this problem, this paper
reveals that this mismatch can be interpreted as a metric perturbation in a
Riemannian manifold, viewed through the lens of duality theory. Building on
information geometry, we construct the Linearly Nearly Euclidean (LNE) manifold
for DNNs, providing a background for addressing perturbations. By introducing a
partial differential equation on metrics, i.e., the Ricci flow, we establish
the dynamical stability and convergence of the LNE metric with the $L^2$-norm
perturbation. In contrast to previous perturbation theories with convergence
rates in fractional powers, the metric perturbation under the Ricci flow
exhibits exponential decay in the LNE manifold. Experimental results across
various datasets demonstrate that our method achieves superior and more stable
performance for DNNs compared to other representative training-based methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Smoothing Methods for Automatic Differentiation Across Conditional
  Branches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03585v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03585v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Justin N. Kreikemeyer, Philipp Andelfinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Programs involving discontinuities introduced by control flow constructs such
as conditional branches pose challenges to mathematical optimization methods
that assume a degree of smoothness in the objective function's response
surface. Smooth interpretation (SI) is a form of abstract interpretation that
approximates the convolution of a program's output with a Gaussian kernel, thus
smoothing its output in a principled manner. Here, we combine SI with automatic
differentiation (AD) to efficiently compute gradients of smoothed programs. In
contrast to AD across a regular program execution, these gradients also capture
the effects of alternative control flow paths. The combination of SI with AD
enables the direct gradient-based parameter synthesis for branching programs,
allowing for instance the calibration of simulation models or their combination
with neural network models in machine learning pipelines. We detail the effects
of the approximations made for tractability in SI and propose a novel Monte
Carlo estimator that avoids the underlying assumptions by estimating the
smoothed programs' gradients through a combination of AD and sampling. Using
DiscoGrad, our tool for automatically translating simple C++ programs to a
smooth differentiable form, we perform an extensive evaluation. We compare the
combination of SI with AD and our Monte Carlo estimator to existing
gradient-free and stochastic methods on four non-trivial and originally
discontinuous problems ranging from classical simulation-based optimization to
neural network-driven control. While the optimization progress with the
SI-based estimator depends on the complexity of the program's control flow, our
Monte Carlo estimator is competitive in all problems, exhibiting the fastest
convergence by a substantial margin in our highest-dimensional problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 17 figures, updated content to reflect journal version.
  Published in IEEE Access, available at
  https://ieeexplore.ieee.org/abstract/document/10356054</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attacks in Adversarial Machine Learning: A Systematic <span class="highlight-title">Survey</span> from the
  Life-cycle Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.09457v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.09457v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baoyuan Wu, Zihao Zhu, Li Liu, Qingshan Liu, Zhaofeng He, Siwei Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial machine learning (AML) studies the adversarial phenomenon of
machine learning, which may make inconsistent or unexpected predictions with
humans. Some paradigms have been recently developed to explore this adversarial
phenomenon occurring at different stages of a machine learning system, such as
backdoor attack occurring at the pre-training, in-training and inference stage;
weight attack occurring at the post-training, deployment and inference stage;
adversarial attack occurring at the inference stage. However, although these
adversarial paradigms share a common goal, their developments are almost
independent, and there is still no big picture of AML. In this work, we aim to
provide a unified perspective to the AML community to systematically review the
overall progress of this field. We firstly provide a general definition about
AML, and then propose a unified mathematical framework to covering existing
attack paradigms. According to the proposed unified framework, we build a full
taxonomy to systematically categorize and review existing representative
methods for each paradigm. Besides, using this unified framework, it is easy to
figure out the connections and differences among different attack paradigms,
which may inspire future researchers to develop more advanced attack paradigms.
Finally, to facilitate the viewing of the built taxonomy and the related
literature in adversarial machine learning, we further provide a website, \ie,
\url{http://adversarial-ml.com}, where the taxonomies and literature will be
continuously updated.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 4 figures, 10 tables, 313 reference papers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ STAS: Spatial-Temporal Return Decomposition for Multi-agent
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.07520v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.07520v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sirui Chen, Zhaowei Zhang, Yaodong Yang, Yali Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Centralized Training with Decentralized Execution (CTDE) has been proven to
be an effective paradigm in cooperative multi-agent reinforcement learning
(MARL). One of the major challenges is credit assignment, which aims to credit
agents by their contributions. While prior studies have shown great success,
their methods typically fail to work in episodic reinforcement learning
scenarios where global rewards are revealed only at the end of the episode.
They lack the functionality to model complicated relations of the delayed
global reward in the temporal dimension and suffer from inefficiencies. To
tackle this, we introduce Spatial-Temporal Attention with Shapley (STAS), a
novel method that learns credit assignment in both temporal and spatial
dimensions. It first decomposes the global return back to each time step, then
utilizes the Shapley Value to redistribute the individual payoff from the
decomposed global reward. To mitigate the computational complexity of the
Shapley Value, we introduce an approximation of marginal contribution and
utilize Monte Carlo sampling to estimate it. We evaluate our method on an Alice
& Bob example and MPE environments across different scenarios. Our results
demonstrate that our method effectively assigns spatial-temporal credit,
outperforming all state-of-the-art baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Enhanced Conditional Imputation for Healthcare Time-series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16713v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16713v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linglong Qian, Zina Ibrahim, Hugh Logan Ellis, Ao Zhang, Yuezhou Zhang, Tao Wang, Richard Dobson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents a novel approach to addressing the challenge of missing
data in multivariate time series, with a particular focus on the complexities
of healthcare data. Our Conditional Self-Attention Imputation (CSAI) model,
grounded in a transformer-based framework, introduces a conditional hidden
state initialization tailored to the intricacies of medical time series data.
This methodology diverges from traditional imputation techniques by
specifically targeting the imbalance in missing data distribution, a crucial
aspect often overlooked in healthcare datasets. By integrating advanced
knowledge embedding and a non-uniform masking strategy, CSAI adeptly adjusts to
the distinct patterns of missing data in Electronic Health Records (EHRs).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Covert Channel Attack to Federated Learning Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.10561v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.10561v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriele Costa, Fabio Pinelli, Simone Soderi, Gabriele Tolomei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) goes beyond traditional, centralized machine learning
by distributing model training among a large collection of edge clients. These
clients cooperatively train a global, e.g., cloud-hosted, model without
disclosing their local, private training data. The global model is then shared
among all the participants which use it for local predictions. In this paper,
we put forward a novel attacker model aiming at turning FL systems into covert
channels to implement a stealth communication infrastructure. The main
intuition is that, during federated training, a malicious sender can poison the
global model by submitting purposely crafted examples. Although the effect of
the model poisoning is negligible to other participants, and does not alter the
overall model performance, it can be observed by a malicious receiver and used
to transmit a single bit.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic programming by polymorphic semiring algebraic shortcut fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.01752v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.01752v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max A. Little, Xi He, Ugur Kayas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic programming (DP) is an algorithmic design paradigm for the efficient,
exact solution of otherwise intractable, combinatorial problems. However, DP
algorithm design is often presented in an ad-hoc manner. It is sometimes
difficult to justify algorithm correctness. To address this issue, this paper
presents a rigorous algebraic formalism for systematically deriving DP
algorithms, based on semiring polymorphism. We start with a specification,
construct an algorithm to compute the required solution which is self-evidently
correct because it exhaustively generates and evaluates all possible solutions
meeting the specification. We then derive, through the use of shortcut fusion,
an implementation of this algorithm which is both efficient and correct. We
also demonstrate how, with the use of semiring lifting, the specification can
be augmented with combinatorial constraints, showing how these constraints can
be fused with the algorithm. We furthermore demonstrate how existing DP
algorithms for a given combinatorial problem can be abstracted from their
original context and re-purposed.
  This approach can be applied to the full scope of combinatorial problems
expressible in terms of semirings. This includes, for example: optimal
probability and Viterbi decoding, probabilistic marginalization, logical
inference, fuzzy sets, differentiable softmax, relational and provenance
queries. The approach, building on ideas from the existing literature on
constructive algorithmics, exploits generic properties of polymorphic
functions, tupling and formal sums and algebraic simplifications arising from
constraint algebras. We demonstrate the effectiveness of this formalism for
some example applications arising in signal processing, bioinformatics and
reliability engineering. Python software implementing these algorithms can be
downloaded from: http://www.maxlittle.net/software/dppolyalg.zip.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated v22 with revised text</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Let There Be Sound: Reconstructing High Quality Speech from Silent
  Videos <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15256v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15256v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ji-Hoon Kim, Jaehun Kim, Joon Son Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of this work is to reconstruct high quality speech from lip motions
alone, a task also known as lip-to-speech. A key challenge of lip-to-speech
systems is the one-to-many mapping caused by (1) the existence of homophenes
and (2) multiple speech variations, resulting in a mispronounced and
over-smoothed speech. In this paper, we propose a novel lip-to-speech system
that significantly improves the generation quality by alleviating the
one-to-many mapping problem from multiple perspectives. Specifically, we
incorporate (1) self-supervised speech representations to disambiguate
homophenes, and (2) acoustic variance information to model diverse speech
styles. Additionally, to better solve the aforementioned problem, we employ a
flow based post-net which captures and refines the details of the generated
speech. We perform extensive experiments on two datasets, and demonstrate that
our method achieves the generation quality close to that of real human
utterance, outperforming existing methods in terms of speech naturalness and
intelligibility by a large margin. Synthesised samples are available at our
demo page: https://mm.kaist.ac.kr/projects/LTBS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Provably Powerful Graph Neural Networks for Directed Multigraphs <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.11586v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.11586v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Béni Egressy, Luc von Niederhäusern, Jovan Blanusa, Erik Altman, Roger Wattenhofer, Kubilay Atasu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper analyses a set of simple adaptations that transform standard
message-passing Graph Neural Networks (GNN) into provably powerful directed
multigraph neural networks. The adaptations include multigraph port numbering,
ego IDs, and reverse message passing. We prove that the combination of these
theoretically enables the detection of any directed subgraph pattern. To
validate the effectiveness of our proposed adaptations in practice, we conduct
experiments on synthetic subgraph detection tasks, which demonstrate
outstanding performance with almost perfect results. Moreover, we apply our
proposed adaptations to two financial crime analysis tasks. We observe dramatic
improvements in detecting money laundering transactions, improving the
minority-class F1 score of a standard message-passing GNN by up to 30%, and
closely matching or outperforming tree-based and GNN baselines. Similarly
impressive results are observed on a real-world phishing detection dataset,
boosting three standard GNNs' F1 scores by around 15% and outperforming all
baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SLP-Net:An efficient lightweight network for segmentation of skin
  lesions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12789v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12789v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Yang, Hong Peng, Chenggang Guo, Xiaohui Luo, Jun Wang, Xianzhong Long
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt treatment for melanoma is crucial. To assist physicians in identifying
lesion areas precisely in a quick manner, we propose a novel skin lesion
segmentation technique namely SLP-Net, an ultra-lightweight segmentation
network based on the spiking neural P(SNP) systems type mechanism. Most
existing convolutional neural networks achieve high segmentation accuracy while
neglecting the high hardware cost. SLP-Net, on the contrary, has a very small
number of parameters and a high computation speed. We design a lightweight
multi-scale feature extractor without the usual encoder-decoder structure.
Rather than a decoder, a feature adaptation module is designed to replace it
and implement multi-scale information decoding. Experiments at the ISIC2018
challenge demonstrate that the proposed model has the highest Acc and DSC among
the state-of-the-art methods, while experiments on the PH2 dataset also
demonstrate a favorable generalization ability. Finally, we compare the
computational complexity as well as the computational speed of the models in
experiments, where SLP-Net has the highest overall superiority
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Entropy and the Kullback-Leibler Divergence for Bayesian Networks:
  Computational Complexity and Efficient Implementation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.01520v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.01520v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Scutari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian networks (BNs) are a foundational model in machine learning and
causal inference. Their graphical structure can handle high-dimensional
problems, divide them into a sparse collection of smaller ones, underlies Judea
Pearl's causality, and determines their explainability and interpretability.
Despite their popularity, there are almost no resources in the literature on
how to compute Shannon's entropy and the Kullback-Leibler (KL) divergence for
BNs under their most common distributional assumptions. In this paper, we
provide computationally efficient algorithms for both by leveraging BNs'
graphical structure, and we illustrate them with a complete set of numerical
examples. In the process, we show it is possible to reduce the computational
complexity of KL from cubic to quadratic for Gaussian BNs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Handling Noisy Labels via One-Step Abductive Multi-Target Learning and
  Its Application to Helicobacter Pylori Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2011.14956v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2011.14956v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongquan Yang, Yiming Yang, Jie Chen, Jiayi Zheng, Zhongxi Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning from noisy labels is an important concern in plenty of real-world
scenarios. Various approaches for this concern first make corrections
corresponding to potentially noisy-labeled instances, and then update
predictive model with information of the made corrections. However, in specific
areas, such as medical histopathology whole slide image analysis (MHWSIA), it
is often difficult or impossible for experts to manually achieve the noisy-free
ground-truth labels which leads to labels with complex noise. This situation
raises two more difficult problems: 1) the methodology of approaches making
corrections corresponding to potentially noisy-labeled instances has
limitations due to the complex noise existing in labels; and 2) the appropriate
evaluation strategy for validation/testing is unclear because of the great
difficulty in collecting the noisy-free ground-truth labels. For the problem
1), we present one-step abductive multi-target learning (OSAMTL) that imposes a
one-step logical reasoning upon machine learning via a multi-target learning
procedure to constrain the predictions of the learning model to be subject to
our prior knowledge about the true target. For the problem 2), we propose a
logical assessment formula (LAF) that evaluates the logical rationality of the
outputs of an approach by estimating the consistencies between the predictions
of the learning model and the logical facts narrated from the results of the
one-step logical reasoning of OSAMTL. Based on the Helicobacter pylori (H.
pylori) segmentation task in MHWSIA, we show that OSAMTL enables the machine
learning model achieving logically more rational predictions, which is beyond
various state-of-the-art approaches in handling complex noisy labels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A General Implicit Framework for Fast NeRF Composition and Rendering <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.04669v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.04669v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Gao, Ziyi Yang, Yunlu Zhao, Yuxiang Sun, Xiaogang Jin, Changqing Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A variety of Neural Radiance Fields (NeRF) methods have recently achieved
remarkable success in high render speed. However, current accelerating methods
are specialized and incompatible with various implicit methods, preventing
real-time composition over various types of NeRF works. Because NeRF relies on
sampling along rays, it is possible to provide general guidance for
acceleration. To that end, we propose a general implicit pipeline for composing
NeRF objects quickly. Our method enables the casting of dynamic shadows within
or between objects using analytical light sources while allowing multiple NeRF
objects to be seamlessly placed and rendered together with any arbitrary rigid
transformations. Mainly, our work introduces a new surface representation known
as Neural Depth Fields (NeDF) that quickly determines the spatial relationship
between objects by allowing direct intersection computation between rays and
implicit surfaces. It leverages an intersection neural network to query NeRF
for acceleration instead of depending on an explicit spatial structure.Our
proposed method is the first to enable both the progressive and interactive
composition of NeRF objects. Additionally, it also serves as a previewing
plugin for a range of existing NeRF works.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Approximating Numerical Flux by Fourier Neural Operators for the
  Hyperbolic Conservation Laws 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01783v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01783v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taeyoung Kim, Myungjoo Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classical numerical schemes exist for solving PDEs numerically, and recently,
neural network-based methods have been developed. However, methodologies using
neural networks, such as PINN and neural operators, lack robustness and
generalization power. To compensate for such drawbacks, there are many types of
research combining classical numerical schemes and machine learning methods by
replacing a small portion of the numerical schemes with neural networks. In
this work, we focus on hyperbolic conservation laws and replace numerical
fluxes in the numerical schemes by neural operator. For this, we construct
losses that are motivated by numerical schemes for conservation laws and
approximate numerical flux by FNO. Through experiments, we show that our
methodology has advantages of both numerical schemes and FNO by comparing with
original methods. For instance, we demonstrate our method gains robustness,
resolution invariance property, and feasibility of a data-driven method. Our
method especially has the ability to predict continuously in time and
generalization power on the out-of-distribution samples, which are challenges
to be tackled for existing neural operator methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 28 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Better and Simpler Lower Bounds for Differentially Private Statistical
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06289v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06289v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shyam Narayanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We provide optimal lower bounds for two well-known parameter estimation (also
known as statistical estimation) tasks in high dimensions with approximate
differential privacy. First, we prove that for any $\alpha \le O(1)$,
estimating the covariance of a Gaussian up to spectral error $\alpha$ requires
$\tilde{\Omega}\left(\frac{d^{3/2}}{\alpha \varepsilon} +
\frac{d}{\alpha^2}\right)$ samples, which is tight up to logarithmic factors.
This result improves over previous work which established this for $\alpha \le
O\left(\frac{1}{\sqrt{d}}\right)$, and is also simpler than previous work.
Next, we prove that estimating the mean of a heavy-tailed distribution with
bounded $k$th moments requires $\tilde{\Omega}\left(\frac{d}{\alpha^{k/(k-1)}
\varepsilon} + \frac{d}{\alpha^2}\right)$ samples. Previous work for this
problem was only able to establish this lower bound against pure differential
privacy, or in the special case of $k = 2$.
  Our techniques follow the method of fingerprinting and are generally quite
simple. Our lower bound for heavy-tailed estimation is based on a black-box
reduction from privately estimating identity-covariance Gaussians. Our lower
bound for covariance estimation utilizes a Bayesian approach to show that,
under an Inverse Wishart prior distribution for the covariance matrix, no
private estimator can be accurate even in expectation, without sufficiently
many samples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data Valuation for Vertical Federated Learning: A Model-free and
  Privacy-preserving Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.08364v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.08364v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Han, Leye Wang, Junjie Wu, Xiao Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vertical Federated learning (VFL) is a promising paradigm for predictive
analytics, empowering an organization (i.e., task party) to enhance its
predictive models through collaborations with multiple data suppliers (i.e.,
data parties) in a decentralized and privacy-preserving way. Despite the
fast-growing interest in VFL, the lack of effective and secure tools for
assessing the value of data owned by data parties hinders the application of
VFL in business contexts. In response, we propose FedValue, a
privacy-preserving, task-specific but model-free data valuation method for VFL,
which consists of a data valuation metric and a federated computation method.
Specifically, we first introduce a novel data valuation metric, namely
MShapley-CMI. The metric evaluates a data party's contribution to a predictive
analytics task without the need of executing a machine learning model, making
it well-suited for real-world applications of VFL. Next, we develop an
innovative federated computation method that calculates the MShapley-CMI value
for each data party in a privacy-preserving manner. Extensive experiments
conducted on six public datasets validate the efficacy of FedValue for data
valuation in the context of VFL. In addition, we illustrate the practical
utility of FedValue with a case study involving federated movie
recommendations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preference as Reward, Maximum Preference Optimization with Importance
  Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16430v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16430v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zaifan Jiang, Xing Huang, Chao Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Preference learning is a key technology for aligning language models with
human values. Reinforcement Learning from Human Feedback (RLHF) is a model
based algorithm to optimize preference learning, which first fitting a reward
model for preference score, and then optimizing generating policy with
on-policy PPO algorithm to maximize the reward. The processing of RLHF is
complex, time-consuming and unstable. Direct Preference Optimization (DPO)
algorithm using off-policy algorithm to direct optimize generating policy and
eliminating the need for reward model, which is data efficient and stable. DPO
use Bradley-Terry model and log-loss which leads to over-fitting to the
preference data at the expense of ignoring KL-regularization term when
preference near deterministic. IPO uses a root-finding pairwise MSE loss to
solve the ignoring KL-regularization problem, and learning an optimal policy.
But IPO's pairwise loss still can't s make the KL-regularization to work. In
this paper, we design a simple and intuitive off-policy preferences
optimization algorithm from an importance sampling view, and add an off-policy
KL-regularization term which makes KL-regularization truly effective. To
simplify the learning process and save memory usage, we can generate
regularization data in advance, which eliminate the needs for both reward model
and reference policy in the stage of optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WFTNet: Exploiting Global and Local Periodicity in Long-term Time Series
  Forecasting <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11319v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11319v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiyuan Liu, Beiliang Wu, Naiqi Li, Tao Dai, Fengmao Lei, Jigang Bao, Yong Jiang, Shu-Tao Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent CNN and Transformer-based models tried to utilize frequency and
periodicity information for long-term time series forecasting. However, most
existing work is based on Fourier transform, which cannot capture fine-grained
and local frequency structure. In this paper, we propose a Wavelet-Fourier
Transform Network (WFTNet) for long-term time series forecasting. WFTNet
utilizes both Fourier and wavelet transforms to extract comprehensive
temporal-frequency information from the signal, where Fourier transform
captures the global periodic patterns and wavelet transform captures the local
ones. Furthermore, we introduce a Periodicity-Weighted Coefficient (PWC) to
adaptively balance the importance of global and local frequency patterns.
Extensive experiments on various time series datasets show that WFTNet
consistently outperforms other state-of-the-art baseline. Code is available at
https://github.com/Hank0626/WFTNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Underwater Acoustic Signal Recognition Based on Salient Feature 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.13143v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.13143v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancement of technology, the recognition of underwater
acoustic signals in complex environments has become increasingly crucial.
Currently, mainstream underwater acoustic signal recognition relies primarily
on time-frequency analysis to extract spectral features, finding widespread
applications in the field. However, existing recognition methods heavily depend
on expert systems, facing limitations such as restricted knowledge bases and
challenges in handling complex relationships. These limitations stem from the
complexity and maintenance difficulties associated with rules or inference
engines. Recognizing the potential advantages of deep learning in handling
intricate relationships, this paper proposes a method utilizing neural networks
for underwater acoustic signal recognition. The proposed approach involves
continual learning of features extracted from spectra for the classification of
underwater acoustic signals. Deep learning models can automatically learn
abstract features from data and continually adjust weights during training to
enhance classification performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lookahead: An Inference Acceleration Framework for Large Language Model
  with Lossless Generation Accuracy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12728v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12728v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Zhao, Zhitian Xie, Chenyi Zhuang, Jinjie Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) have made significant advancements across
various tasks, such as question answering, translation, text summarization, and
dialogue systems, the need for accuracy in information becomes crucial,
especially for serious financial products serving billions of users like
Alipay. To address this, Alipay has developed a Retrieval-Augmented Generation
(RAG) system that grounds LLMs on the most accurate and up-to-date information.
However, for a real-world product serving millions of users, the inference
speed of LLMs becomes a critical factor compared to a mere experimental model.
  Hence, this paper presents a generic framework for accelerating the inference
process, resulting in a substantial increase in speed and cost reduction for
our RAG system, with lossless generation accuracy. In the traditional inference
process, each token is generated sequentially by the LLM, leading to a time
consumption proportional to the number of generated tokens. To enhance this
process, our framework, named \textit{lookahead}, introduces a
\textit{multi-branch} strategy. Instead of generating a single token at a time,
we propose a \textit{Trie-based Retrieval} (TR) process that enables the
generation of multiple branches simultaneously, each of which is a sequence of
tokens. Subsequently, for each branch, a \textit{Verification and Accept} (VA)
process is performed to identify the longest correct sub-sequence as the final
output. Our strategy offers two distinct advantages: (1) it guarantees absolute
correctness of the output, avoiding any approximation algorithms, and (2) the
worst-case performance of our approach is equivalent to the conventional
process. We conduct extensive experiments to demonstrate the significant
improvements achieved by applying our inference acceleration framework. Code is
avaliable: https://github.com/alipay/PainlessInferenceAcceleration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual Learning: Forget-free Winning Subnetworks for Video
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11973v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11973v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haeyong Kang, Jaehong Yoon, Sung Ju Hwang, Chang D. Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by the Lottery Ticket Hypothesis (LTH), which highlights the
existence of efficient subnetworks within larger, dense networks, a
high-performing Winning Subnetwork (WSN) in terms of task performance under
appropriate sparsity conditions is considered for various continual learning
tasks. It leverages pre-existing weights from dense networks to achieve
efficient learning in Task Incremental Learning (TIL) scenarios. In Few-Shot
Class Incremental Learning (FSCIL), a variation of WSN referred to as the Soft
subnetwork (SoftNet) is designed to prevent overfitting when the data samples
are scarce. Furthermore, the sparse reuse of WSN weights is considered for
Video Incremental Learning (VIL). The use of Fourier Subneural Operator (FSO)
within WSN is considered. It enables compact encoding of videos and identifies
reusable subnetworks across varying bandwidths. We have integrated FSO into
different architectural frameworks for continual learning, including VIL, TIL,
and FSCIL. Our comprehensive experiments demonstrate FSO's effectiveness,
significantly improving task performance at various convolutional
representational levels. Specifically, FSO enhances higher-layer performance in
TIL and FSCIL and lower-layer performance in VIL
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2303.14962,
  arXiv:2306.11305</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-Time FJ/MAC PDE Solvers via Tensorized, Back-Propagation-Free
  Optical PINN Training <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00413v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00413v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yequan Zhao, Xian Xiao, Xinling Yu, Ziyue Liu, Zhixiong Chen, Geza Kurczveil, Raymond G. Beausoleil, Zheng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Solving partial differential equations (PDEs) numerically often requires huge
computing time, energy cost, and hardware resources in practical applications.
This has limited their applications in many scenarios (e.g., autonomous
systems, supersonic flows) that have a limited energy budget and require near
real-time response. Leveraging optical computing, this paper develops an
on-chip training framework for physics-informed neural networks (PINNs), aiming
to solve high-dimensional PDEs with fJ/MAC photonic power consumption and
ultra-low latency. Despite the ultra-high speed of optical neural networks,
training a PINN on an optical chip is hard due to (1) the large size of
photonic devices, and (2) the lack of scalable optical memory devices to store
the intermediate results of back-propagation (BP). To enable realistic optical
PINN training, this paper presents a scalable method to avoid the BP process.
We also employ a tensor-compressed approach to improve the convergence and
scalability of our optical PINN training. This training framework is designed
with tensorized optical neural networks (TONN) for scalable inference
acceleration and MZI phase-domain tuning for \textit{in-situ} optimization. Our
simulation results of a 20-dim HJB PDE show that our photonic accelerator can
reduce the number of MZIs by a factor of $1.17\times 10^3$, with only $1.36$ J
and $1.15$ s to solve this equation. This is the first real-size optical PINN
training framework that can be applied to solve high-dimensional PDEs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ML with New Compute Paradigms (MLNCP) at NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probabilistic Modeling for Sequences of Sets in Continuous-Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.15045v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.15045v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Chang, Alex Boyd, Padhraic Smyth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural marked temporal point processes have been a valuable addition to the
existing toolbox of statistical parametric models for continuous-time event
data. These models are useful for sequences where each event is associated with
a single item (a single type of event or a "mark") -- but such models are not
suited for the practical situation where each event is associated with a set of
items. In this work, we develop a general framework for modeling set-valued
data in continuous-time, compatible with any intensity-based recurrent neural
point process model. In addition, we develop inference methods that can use
such models to answer probabilistic queries such as "the probability of item
$A$ being observed before item $B$," conditioned on sequence history. Computing
exact answers for such queries is generally intractable for neural models due
to both the continuous-time nature of the problem setting and the
combinatorially-large space of potential outcomes for each event. To address
this, we develop a class of importance sampling methods for querying with
set-based sequences and demonstrate orders-of-magnitude improvements in
efficiency over direct sampling via systematic experiments with four real-world
datasets. We also illustrate how to use this framework to perform model
selection using likelihoods that do not involve one-step-ahead prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Not Only Rewards But Also Constraints: Applications on Legged Robot
  Locomotion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12517v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12517v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunho Kim, Hyunsik Oh, Jeonghyun Lee, Jinhyeok Choi, Gwanghyeon Ji, Moonkyu Jung, Donghoon Youm, Jemin Hwangbo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several earlier studies have shown impressive control performance in complex
robotic systems by designing the controller using a neural network and training
it with model-free reinforcement learning. However, these outstanding
controllers with natural motion style and high task performance are developed
through extensive reward engineering, which is a highly laborious and
time-consuming process of designing numerous reward terms and determining
suitable reward coefficients. In this work, we propose a novel reinforcement
learning framework for training neural network controllers for complex robotic
systems consisting of both rewards and constraints. To let the engineers
appropriately reflect their intent to constraints and handle them with minimal
computation overhead, two constraint types and an efficient policy optimization
algorithm are suggested. The learning framework is applied to train locomotion
controllers for several legged robots with different morphology and physical
attributes to traverse challenging terrains. Extensive simulation and
real-world experiments demonstrate that performant controllers can be trained
with significantly less reward engineering, by tuning only a single reward
coefficient. Furthermore, a more straightforward and intuitive engineering
process can be utilized, thanks to the interpretability and generalizability of
constraints. The summary video is available at https://youtu.be/KAlm3yskhvM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Transactions on Robotics (T-RO)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stochastic Approximation Approaches to Group Distributionally Robust
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.09267v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.09267v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lijun Zhang, Peng Zhao, Zhen-Hua Zhuang, Tianbao Yang, Zhi-Hua Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates group distributionally robust optimization (GDRO),
with the purpose to learn a model that performs well over $m$ different
distributions. First, we formulate GDRO as a stochastic convex-concave
saddle-point problem, and demonstrate that stochastic mirror descent (SMD),
using $m$ samples in each iteration, achieves an $O(m (\log m)/\epsilon^2)$
sample complexity for finding an $\epsilon$-optimal solution, which matches the
$\Omega(m/\epsilon^2)$ lower bound up to a logarithmic factor. Then, we make
use of techniques from online learning to reduce the number of samples required
in each round from $m$ to $1$, keeping the same sample complexity.
Specifically, we cast GDRO as a two-players game where one player simply
performs SMD and the other executes an online algorithm for non-oblivious
multi-armed bandits. Next, we consider a more practical scenario where the
number of samples that can be drawn from each distribution is different, and
propose a novel formulation of weighted GDRO, which allows us to derive
distribution-dependent convergence rates. Denote by $n_i$ the sample budget for
the $i$-th distribution, and assume $n_1 \geq n_2 \geq \cdots \geq n_m$. In the
first approach, we incorporate non-uniform sampling into SMD such that the
sample budget is satisfied in expectation, and prove that the excess risk of
the $i$-th distribution decreases at an $O(\sqrt{n_1 \log m}/n_i)$ rate. In the
second approach, we use mini-batches to meet the budget exactly and also reduce
the variance in stochastic gradients, and then leverage stochastic mirror-prox
algorithm, which can exploit small variances, to optimize a carefully designed
weighted GDRO problem. Under appropriate conditions, it attains an $O((\log
m)/\sqrt{n_i})$ convergence rate, which almost matches the optimal
$O(\sqrt{1/n_i})$ rate of only learning from the $i$-th distribution with $n_i$
samples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CBD: A Certified Backdoor Detector Based on Local Dominant Probability <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17498v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17498v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Xiang, Zidi Xiong, Bo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Backdoor attack is a common threat to deep neural networks. During testing,
samples embedded with a backdoor trigger will be misclassified as an
adversarial target by a backdoored model, while samples without the backdoor
trigger will be correctly classified. In this paper, we present the first
certified backdoor detector (CBD), which is based on a novel, adjustable
conformal prediction scheme based on our proposed statistic local dominant
probability. For any classifier under inspection, CBD provides 1) a detection
inference, 2) the condition under which the attacks are guaranteed to be
detectable for the same classification domain, and 3) a probabilistic upper
bound for the false positive rate. Our theoretical results show that attacks
with triggers that are more resilient to test-time noise and have smaller
perturbation magnitudes are more likely to be detected with guarantees.
Moreover, we conduct extensive experiments on four benchmark datasets
considering various backdoor types, such as BadNet, CB, and Blend. CBD achieves
comparable or even higher detection accuracy than state-of-the-art detectors,
and it in addition provides detection certification. Notably, for backdoor
attacks with random perturbation triggers bounded by $\ell_2\leq0.75$ which
achieves more than 90\% attack success rate, CBD achieves 100\% (98\%), 100\%
(84\%), 98\% (98\%), and 72\% (40\%) empirical (certified) detection true
positive rates on the four benchmark datasets GTSRB, SVHN, CIFAR-10, and
TinyImageNet, respectively, with low false positive rates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Efficiency: A Systematic <span class="highlight-title">Survey</span> of Resource-Efficient Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00625v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00625v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangji Bai, Zheng Chai, Chen Ling, Shiyu Wang, Jiaying Lu, Nan Zhang, Tingwei Shi, Ziyang Yu, Mengdan Zhu, Yifei Zhang, Carl Yang, Yue Cheng, Liang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The burgeoning field of Large Language Models (LLMs), exemplified by
sophisticated models like OpenAI's ChatGPT, represents a significant
advancement in artificial intelligence. These models, however, bring forth
substantial challenges in the high consumption of computational, memory,
energy, and financial resources, especially in environments with limited
resource capabilities. This survey aims to systematically address these
challenges by reviewing a broad spectrum of techniques designed to enhance the
resource efficiency of LLMs. We categorize methods based on their optimization
focus: computational, memory, energy, financial, and network resources and
their applicability across various stages of an LLM's lifecycle, including
architecture design, pretraining, finetuning, and system design. Additionally,
the survey introduces a nuanced categorization of resource efficiency
techniques by their specific resource types, which uncovers the intricate
relationships and mappings between various resources and corresponding
optimization techniques. A standardized set of evaluation metrics and datasets
is also presented to facilitate consistent and fair comparisons across
different models and techniques. By offering a comprehensive overview of the
current sota and identifying open research avenues, this survey serves as a
foundational reference for researchers and practitioners, aiding them in
developing more sustainable and efficient LLMs in a rapidly evolving landscape.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. GitHub repo:
  https://github.com/tiingweii-shii/Awesome-Resource-Efficient-LLM-Papers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Theory of Hallucinations based on Equivariance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14504v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14504v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hisaichi Shibata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study aims to acquire knowledge for creating very large language models
that are immune to hallucinations. Hallucinations in contemporary large
language models are often attributed to a misunderstanding of real-world social
relationships. Therefore, I hypothesize that very large language models capable
of thoroughly grasping all these relationships will be free from
hallucinations. Additionally, I propose that certain types of equivariant
language models are adept at learning and understanding these relationships.
Building on this, I have developed a specialized cross-entropy error function
to create a hallucination scale for language models, which measures their
extent of equivariance acquisition. Utilizing this scale, I tested language
models for their ability to acquire character-level equivariance. In
particular, I introduce and employ a novel technique based on T5 (Text To Text
Transfer Transformer) that efficiently understands permuted input texts without
the need for explicit dictionaries to convert token IDs (integers) to texts
(strings). This T5 model demonstrated a moderate ability to acquire
character-level equivariance. Additionally, I discovered scale laws that can
aid in developing hallucination-free language models at the character level.
This methodology can be extended to assess equivariance acquisition at the word
level, paving the way for very large language models that can comprehensively
understand relationships and, consequently, avoid hallucinations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Multi-Modal Contrastive Diffusion Model for Therapeutic Peptide
  Generation <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.15665v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.15665v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongkang Wang, Xuan Liu, Feng Huang, Zhankun Xiong, Wen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Therapeutic peptides represent a unique class of pharmaceutical agents
crucial for the treatment of human diseases. Recently, deep generative models
have exhibited remarkable potential for generating therapeutic peptides, but
they only utilize sequence or structure information alone, which hinders the
performance in generation. In this study, we propose a Multi-Modal Contrastive
Diffusion model (MMCD), fusing both sequence and structure modalities in a
diffusion framework to co-generate novel peptide sequences and structures.
Specifically, MMCD constructs the sequence-modal and structure-modal diffusion
models, respectively, and devises a multi-modal contrastive learning strategy
with intercontrastive and intra-contrastive in each diffusion timestep, aiming
to capture the consistency between two modalities and boost model performance.
The inter-contrastive aligns sequences and structures of peptides by maximizing
the agreement of their embeddings, while the intra-contrastive differentiates
therapeutic and non-therapeutic peptides by maximizing the disagreement of
their sequence/structure embeddings simultaneously. The extensive experiments
demonstrate that MMCD performs better than other state-of-theart deep
generative methods in generating therapeutic peptides across various metrics,
including antimicrobial/anticancer score, diversity, and peptide-docking.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GIT-Mol: A Multi-modal Large Language Model for Molecular Science with
  Graph, Image, and Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.06911v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.06911v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengfei Liu, Yiming Ren, Zhixiang Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have made significant strides in natural language
processing, enabling innovative applications in molecular science by processing
textual representations of molecules. However, most existing language models
cannot capture the rich information with complex molecular structures or
images. In this paper, we introduce GIT-Mol, a multi-modal large language model
that integrates the Graph, Image, and Text information. To facilitate the
integration of multi-modal molecular data, we propose GIT-Former, a novel
architecture that is capable of aligning all modalities into a unified latent
space. We achieve a 5%-10% accuracy increase in properties prediction and a
20.2% boost in molecule generation validity compared to the baselines. With the
any-to-language molecular translation strategy, our model has the potential to
perform more downstream tasks, such as compound name recognition and chemical
reaction prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantifying Deep Learning Model Uncertainty in Conformal Prediction <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.00876v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.00876v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamed Karimi, Reza Samavi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precise estimation of predictive uncertainty in deep neural networks is a
critical requirement for reliable decision-making in machine learning and
statistical modeling, particularly in the context of medical AI. Conformal
Prediction (CP) has emerged as a promising framework for representing the model
uncertainty by providing well-calibrated confidence levels for individual
predictions. However, the quantification of model uncertainty in conformal
prediction remains an active research area, yet to be fully addressed. In this
paper, we explore state-of-the-art CP methodologies and their theoretical
foundations. We propose a probabilistic approach in quantifying the model
uncertainty derived from the produced prediction sets in conformal prediction
and provide certified boundaries for the computed uncertainty. By doing so, we
allow model uncertainty measured by CP to be compared by other uncertainty
quantification methods such as Bayesian (e.g., MC-Dropout and DeepEnsemble) and
Evidential approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in AAAI Second Symposium on Human Partnership with Medical
  AI: Design, Operationalization, and Ethics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Federated Optimization of Smooth Loss Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.01954v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.01954v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Jadbabaie, Anuran Makur, Devavrat Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we study empirical risk minimization (ERM) within a federated
learning framework, where a central server minimizes an ERM objective function
using training data that is stored across $m$ clients. In this setting, the
Federated Averaging (FedAve) algorithm is the staple for determining
$\epsilon$-approximate solutions to the ERM problem. Similar to standard
optimization algorithms, the convergence analysis of FedAve only relies on
smoothness of the loss function in the optimization parameter. However, loss
functions are often very smooth in the training data too. To exploit this
additional smoothness, we propose the Federated Low Rank Gradient Descent
(FedLRGD) algorithm. Since smoothness in data induces an approximate low rank
structure on the loss function, our method first performs a few rounds of
communication between the server and clients to learn weights that the server
can use to approximate clients' gradients. Then, our method solves the ERM
problem at the server using inexact gradient descent. To show that FedLRGD can
have superior performance to FedAve, we present a notion of federated oracle
complexity as a counterpart to canonical oracle complexity. Under some
assumptions on the loss function, e.g., strong convexity in parameter,
$\eta$-H\"older smoothness in data, etc., we prove that the federated oracle
complexity of FedLRGD scales like $\phi m(p/\epsilon)^{\Theta(d/\eta)}$ and
that of FedAve scales like $\phi m(p/\epsilon)^{3/4}$ (neglecting sub-dominant
factors), where $\phi\gg 1$ is a "communication-to-computation ratio," $p$ is
the parameter dimension, and $d$ is the data dimension. Then, we show that when
$d$ is small and the loss function is sufficiently smooth in the data, FedLRGD
beats FedAve in federated oracle complexity. Finally, in the course of
analyzing FedLRGD, we also establish a result on low rank approximation of
latent variable models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, double column format, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Optimization and Model Selection for Domain Generalization: A
  Mixup-guided Solution <span class="chip">SDM</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.00652v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.00652v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wang Lu, Jindong Wang, Yidong Wang, Xing Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The distribution shifts between training and test data typically undermine
the performance of models. In recent years, lots of work pays attention to
domain generalization (DG) where distribution shifts exist, and target data are
unseen. Despite the progress in algorithm design, two foundational factors have
long been ignored: 1) the optimization for regularization-based objectives, and
2) the model selection for DG since no knowledge about the target domain can be
utilized. In this paper, we propose Mixup guided optimization and selection
techniques for DG. For optimization, we utilize an adapted Mixup to generate an
out-of-distribution dataset that can guide the preference direction and
optimize with Pareto optimization. For model selection, we generate a
validation dataset with a closer distance to the target distribution, and
thereby it can better represent the target data. We also present some
theoretical insights behind our proposals. Comprehensive experiments
demonstrate that our model optimization and selection techniques can largely
improve the performance of existing domain generalization algorithms and even
achieve new state-of-the-art results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIAM International Conference on Data Mining (SDM) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive <span class="highlight-title">Survey</span> on Graph Summarization with Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.06114v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.06114v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nasrin Shabani, Jia Wu, Amin Beheshti, Quan Z. Sheng, Jin Foo, Venus Haghighi, Ambreen Hanif, Maryam Shahabikargar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large-scale graphs become more widespread, more and more computational
challenges with extracting, processing, and interpreting large graph data are
being exposed. It is therefore natural to search for ways to summarize these
expansive graphs while preserving their key characteristics. In the past, most
graph summarization techniques sought to capture the most important part of a
graph statistically. However, today, the high dimensionality and complexity of
modern graph data are making deep learning techniques more popular. Hence, this
paper presents a comprehensive survey of progress in deep learning
summarization techniques that rely on graph neural networks (GNNs). Our
investigation includes a review of the current state-of-the-art approaches,
including recurrent GNNs, convolutional GNNs, graph autoencoders, and graph
attention networks. A new burgeoning line of research is also discussed where
graph reinforcement learning is being used to evaluate and improve the quality
of graph summaries. Additionally, the survey provides details of benchmark
datasets, evaluation metrics, and open-source tools that are often employed in
experimentation settings, along with a detailed comparison, discussion, and
takeaways for the research community focused on graph summarization. Finally,
the survey concludes with a number of open research challenges to motivate
further study in this area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 4 figures, 9 tables, Journal of IEEE Transactions on
  Artificial Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepTaster: Adversarial Perturbation-Based Fingerprinting to Identify
  Proprietary <span class="highlight-title">Dataset</span> Use in Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.13535v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.13535v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seonhye Park, Alsharif Abuadbba, Shuo Wang, Kristen Moore, Yansong Gao, Hyoungshick Kim, Surya Nepal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training deep neural networks (DNNs) requires large datasets and powerful
computing resources, which has led some owners to restrict redistribution
without permission. Watermarking techniques that embed confidential data into
DNNs have been used to protect ownership, but these can degrade model
performance and are vulnerable to watermark removal attacks. Recently,
DeepJudge was introduced as an alternative approach to measuring the similarity
between a suspect and a victim model. While DeepJudge shows promise in
addressing the shortcomings of watermarking, it primarily addresses situations
where the suspect model copies the victim's architecture. In this study, we
introduce DeepTaster, a novel DNN fingerprinting technique, to address
scenarios where a victim's data is unlawfully used to build a suspect model.
DeepTaster can effectively identify such DNN model theft attacks, even when the
suspect model's architecture deviates from the victim's. To accomplish this,
DeepTaster generates adversarial images with perturbations, transforms them
into the Fourier frequency domain, and uses these transformed images to
identify the dataset used in a suspect model. The underlying premise is that
adversarial images can capture the unique characteristics of DNNs built with a
specific dataset. To demonstrate the effectiveness of DeepTaster, we evaluated
the effectiveness of DeepTaster by assessing its detection accuracy on three
datasets (CIFAR10, MNIST, and Tiny-ImageNet) across three model architectures
(ResNet18, VGG16, and DenseNet161). We conducted experiments under various
attack scenarios, including transfer learning, pruning, fine-tuning, and data
augmentation. Specifically, in the Multi-Architecture Attack scenario,
DeepTaster was able to identify all the stolen cases across all datasets, while
DeepJudge failed to detect any of the cases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Local Environment Poisoning Attacks on Federated Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02725v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02725v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evelyn Ma, Praneet Rathi, S. Rasoul Etesami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) has become a popular tool for solving traditional
Reinforcement Learning (RL) tasks. The multi-agent structure addresses the
major concern of data-hungry in traditional RL, while the federated mechanism
protects the data privacy of individual agents. However, the federated
mechanism also exposes the system to poisoning by malicious agents that can
mislead the trained policy. Despite the advantage brought by FL, the
vulnerability of Federated Reinforcement Learning (FRL) has not been
well-studied before. In this work, we propose a general framework to
characterize FRL poisoning as an optimization problem and design a poisoning
protocol that can be applied to policy-based FRL. Our framework can also be
extended to FRL with actor-critic as a local RL algorithm by training a pair of
private and public critics. We provably show that our method can strictly hurt
the global objective. We verify our poisoning effectiveness by conducting
extensive experiments targeting mainstream RL algorithms and over various RL
OpenAI Gym environments covering a wide range of difficulty levels. Within
these experiments, we compare clean and baseline poisoning methods against our
proposed framework. The results show that the proposed framework is successful
in poisoning FRL systems and reducing performance across various environments
and does so more effectively than baseline methods. Our work provides new
insights into the vulnerability of FL in RL training and poses new challenges
for designing robust FRL algorithms
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Homogenization for Elliptic Operators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.12006v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.12006v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaushik Bhattacharya, Nikola Kovachki, Aakila Rajan, Andrew M. Stuart, Margaret Trautner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiscale partial differential equations (PDEs) arise in various
applications, and several schemes have been developed to solve them
efficiently. Homogenization theory is a powerful methodology that eliminates
the small-scale dependence, resulting in simplified equations that are
computationally tractable while accurately predicting the macroscopic response.
In the field of continuum mechanics, homogenization is crucial for deriving
constitutive laws that incorporate microscale physics in order to formulate
balance laws for the macroscopic quantities of interest. However, obtaining
homogenized constitutive laws is often challenging as they do not in general
have an analytic form and can exhibit phenomena not present on the microscale.
In response, data-driven learning of the constitutive law has been proposed as
appropriate for this task. However, a major challenge in data-driven learning
approaches for this problem has remained unexplored: the impact of
discontinuities and corner interfaces in the underlying material. These
discontinuities in the coefficients affect the smoothness of the solutions of
the underlying equations. Given the prevalence of discontinuous materials in
continuum mechanics applications, it is important to address the challenge of
learning in this context; in particular, to develop underpinning theory that
establishes the reliability of data-driven methods in this scientific domain.
The paper addresses this unexplored challenge by investigating the learnability
of homogenized constitutive laws for elliptic operators in the presence of such
complexities. Approximation theory is presented, and numerical experiments are
performed which validate the theory in the context of learning the solution
operator defined by the cell problem arising in homogenization for elliptic
PDEs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A New Frontier of AI: On-Device AI Training and Personalization <span class="chip">ICSE 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.04688v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.04688v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ji Joong Moon, Hyun Suk Lee, Jiho Chu, Donghak Park, Seungbaek Hong, Hyungjun Seo, Donghyeon Jeong, Sungsik Kong, MyungJoo Ham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern consumer electronic devices have started executing deep learning-based
intelligence services on devices, not cloud servers, to keep personal data on
devices and to reduce network and cloud costs. We find such a trend as the
opportunity to personalize intelligence services by updating neural networks
with user data without exposing the data out of devices: on-device training.
However, the limited resources of devices incurs significant difficulties. We
propose a light-weight on-device training framework, NNTrainer, which provides
highly memory-efficient neural network training techniques and proactive
swapping based on fine-grained execution order analysis for neural networks.
Moreover, its optimizations do not sacrifice accuracy and are transparent to
training algorithms; thus, prior algorithmic studies may be implemented on top
of NNTrainer. The evaluations show that NNTrainer can reduce memory consumption
down to 1/20 (saving 95%!) and effectively personalizes intelligence services
on devices. NNTrainer is cross-platform and practical open-source software,
which is being deployed to millions of mobile devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 16 figures, Accepted in ICSE 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM in a flash: Efficient Large Language Model Inference with Limited
  Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11514v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11514v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keivan Alizadeh, Iman Mirzadeh, Dmitry Belenko, Karen Khatamifard, Minsik Cho, Carlo C Del Mundo, Mohammad Rastegari, Mehrdad Farajtabar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are central to modern natural language
processing, delivering exceptional performance in various tasks. However, their
substantial computational and memory requirements present challenges,
especially for devices with limited DRAM capacity. This paper tackles the
challenge of efficiently running LLMs that exceed the available DRAM capacity
by storing the model parameters in flash memory, but bringing them on demand to
DRAM. Our method involves constructing an inference cost model that takes into
account the characteristics of flash memory, guiding us to optimize in two
critical areas: reducing the volume of data transferred from flash and reading
data in larger, more contiguous chunks. Within this hardware-informed
framework, we introduce two principal techniques. First, "windowing"
strategically reduces data transfer by reusing previously activated neurons,
and second, "row-column bundling", tailored to the sequential data access
strengths of flash memory, increases the size of data chunks read from flash
memory. These methods collectively enable running models up to twice the size
of the available DRAM, with a 4-5x and 20-25x increase in inference speed
compared to naive loading approaches in CPU and GPU, respectively. Our
integration of sparsity awareness, context-adaptive loading, and a
hardware-oriented design paves the way for effective inference of LLMs on
devices with limited memory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Supervision by Denoising for Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.02952v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.02952v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sean I. Young, Adrian V. Dalca, Enzo Ferrante, Polina Golland, Christopher A. Metzler, Bruce Fischl, Juan Eugenio Iglesias
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning-based image reconstruction models, such as those based on the U-Net,
require a large set of labeled images if good generalization is to be
guaranteed. In some imaging domains, however, labeled data with pixel- or
voxel-level label accuracy are scarce due to the cost of acquiring them. This
problem is exacerbated further in domains like medical imaging, where there is
no single ground truth label, resulting in large amounts of repeat variability
in the labels. Therefore, training reconstruction networks to generalize better
by learning from both labeled and unlabeled examples (called semi-supervised
learning) is problem of practical and theoretical interest. However,
traditional semi-supervised learning methods for image reconstruction often
necessitate handcrafting a differentiable regularizer specific to some given
imaging problem, which can be extremely time-consuming. In this work, we
propose "supervision by denoising" (SUD), a framework that enables us to
supervise reconstruction models using their own denoised output as soft labels.
SUD unifies stochastic averaging and spatial denoising techniques under a
spatio-temporal denoising framework and alternates denoising and model weight
update steps in an optimization framework for semi-supervision. As example
applications, we apply SUD to two problems arising from biomedical imaging --
anatomical brain reconstruction (3D) and cortical parcellation (2D) -- to
demonstrate a significant improvement in the image reconstructions over
supervised-only and stochastic averaging baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the IEEE Transactions on Pattern Analysis and Machine
  Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Geo2SigMap: High-Fidelity RF Signal Mapping Using Geographic Databases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14303v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14303v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Li, Zeyu Li, Zhihui Gao, Tingjun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radio frequency (RF) signal mapping, which is the process of analyzing and
predicting the RF signal strength and distribution across specific areas, is
crucial for cellular network planning and deployment. Traditional approaches to
RF signal mapping rely on statistical models constructed based on measurement
data, which offer low complexity but often lack accuracy, or ray tracing tools,
which provide enhanced precision for the target area but suffer from increased
computational complexity. Recently, machine learning (ML) has emerged as a
data-driven method for modeling RF signal propagation, which leverages models
trained on synthetic datasets to perform RF signal mapping in "unseen" areas.
  In this paper, we present Geo2SigMap, an ML-based framework for efficient and
high-fidelity RF signal mapping using geographic databases. First, we develop
an automated framework that seamlessly integrates three open-source tools:
OpenStreetMap (geographic databases), Blender (computer graphics), and Sionna
(ray tracing), enabling the efficient generation of large-scale 3D building
maps and ray tracing models. Second, we propose a cascaded U-Net model, which
is pre-trained on synthetic datasets and employed to generate detailed RF
signal maps, leveraging environmental information and sparse measurement data.
Finally, we evaluate the performance of Geo2SigMap via a real-world measurement
campaign, where three types of user equipment (UE) collect over 45,000 data
points related to cellular information from six LTE cells operating in the
citizens broadband radio service (CBRS) band. Our results show that Geo2SigMap
achieves an average root-mean-square-error (RMSE) of 6.04 dB for predicting the
reference signal received power (RSRP) at the UE, representing an average RMSE
improvement of 3.59 dB compared to existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Operators for Accelerating Scientific Simulations and Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15325v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15325v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kamyar Azizzadenesheli, Nikola Kovachki, Zongyi Li, Miguel Liu-Schiaffini, Jean Kossaifi, Anima Anandkumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific discovery and engineering design are currently limited by the time
and cost of physical experiments, selected mostly through trial-and-error and
intuition that require deep domain expertise. Numerical simulations present an
alternative to physical experiments but are usually infeasible for complex
real-world domains due to the computational requirements of existing numerical
methods. Artificial intelligence (AI) presents a potential paradigm shift by
developing fast data-driven surrogate models. In particular, an AI framework,
known as Neural Operators, presents a principled framework for learning
mappings between functions defined on continuous domains, e.g., spatiotemporal
processes and partial differential equations (PDE). They can extrapolate and
predict solutions at new locations unseen during training, i.e., perform
zero-shot super-resolution. Neural Operators can augment or even replace
existing simulators in many applications, such as computational fluid dynamics,
weather forecasting, and material modeling, while being 4-5 orders of magnitude
faster. Further, Neural Operators can be integrated with physics and other
domain constraints enforced at finer resolutions to obtain high-fidelity
solutions and good generalization. Since Neural Operators are differentiable,
they can directly optimize parameters for inverse design and other inverse
problems. We believe that Neural Operators present a transformative approach to
simulation and design, enabling rapid research and development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training Diffusion Models with Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.13301v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.13301v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, Sergey Levine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models are a class of flexible generative models trained with an
approximation to the log-likelihood objective. However, most use cases of
diffusion models are not concerned with likelihoods, but instead with
downstream objectives such as human-perceived image quality or drug
effectiveness. In this paper, we investigate reinforcement learning methods for
directly optimizing diffusion models for such objectives. We describe how
posing denoising as a multi-step decision-making problem enables a class of
policy gradient algorithms, which we refer to as denoising diffusion policy
optimization (DDPO), that are more effective than alternative reward-weighted
likelihood approaches. Empirically, DDPO is able to adapt text-to-image
diffusion models to objectives that are difficult to express via prompting,
such as image compressibility, and those derived from human feedback, such as
aesthetic quality. Finally, we show that DDPO can improve prompt-image
alignment using feedback from a vision-language model without the need for
additional data collection or human annotation. The project's website can be
found at http://rl-diffusion.github.io .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TR-DETR: Task-Reciprocal <span class="highlight-title">Transformer</span> for Joint Moment Retrieval and
  Highlight Detection <span class="chip">AAAI-24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Sun, Mingyao Zhou, Wenjing Chen, Wei Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video moment retrieval (MR) and highlight detection (HD) based on natural
language queries are two highly related tasks, which aim to obtain relevant
moments within videos and highlight scores of each video clip. Recently,
several methods have been devoted to building DETR-based networks to solve both
MR and HD jointly. These methods simply add two separate task heads after
multi-modal feature extraction and feature interaction, achieving good
performance. Nevertheless, these approaches underutilize the reciprocal
relationship between two tasks. In this paper, we propose a task-reciprocal
transformer based on DETR (TR-DETR) that focuses on exploring the inherent
reciprocity between MR and HD. Specifically, a local-global multi-modal
alignment module is first built to align features from diverse modalities into
a shared latent space. Subsequently, a visual feature refinement is designed to
eliminate query-irrelevant information from visual features for modal
interaction. Finally, a task cooperation module is constructed to refine the
retrieval pipeline and the highlight score prediction process by utilizing the
reciprocity between MR and HD. Comprehensive experiments on QVHighlights,
Charades-STA and TVSum datasets demonstrate that TR-DETR outperforms existing
state-of-the-art methods. Codes are available at
\url{https://github.com/mingyao1120/TR-DETR}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI-24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Siamese Residual Neural Network for Musical Shape Evaluation in Piano
  Performance Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoquan Li, Stephan Weiss, Yijun Yan, Yinhe Li, Jinchang Ren, John Soraghan, Ming Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding and identifying musical shape plays an important role in music
education and performance assessment. To simplify the otherwise time- and
cost-intensive musical shape evaluation, in this paper we explore how
artificial intelligence (AI) driven models can be applied. Considering musical
shape evaluation as a classification problem, a light-weight Siamese residual
neural network (S-ResNN) is proposed to automatically identify musical shapes.
To assess the proposed approach in the context of piano musical shape
evaluation, we have generated a new dataset, containing 4116 music pieces
derived by 147 piano preparatory exercises and performed in 28 categories of
musical shapes. The experimental results show that the S-ResNN significantly
outperforms a number of benchmark methods in terms of the precision, recall and
F1 score.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>X.Li, S.Weiss, Y.Yan, Y.Li, J.Ren, J.Soraghan, M.Gong,"Siamese
  residual neural network for musical shape evaluation in piano performance
  assessment" in Proc. of the 31st European Signal Processing Conference,
  Helsinki, Finland</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Aligned Multimodal Learning for NER on Tweet Posts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.08372v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.08372v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peipei Liu, Hong Li, Yimo Ren, Jie Liu, Shuaizong Si, Hongsong Zhu, Limin Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mining structured knowledge from tweets using named entity recognition (NER)
can be beneficial for many down stream applications such as recommendation and
intention understanding. With tweet posts tending to be multimodal, multimodal
named entity recognition (MNER) has attracted more attention. In this paper, we
propose a novel approach, which can dynamically align the image and text
sequence and achieve the multi-level cross-modal learning to augment textual
word representation for MNER improvement. To be specific, our framework can be
split into three main stages: the first stage focuses on intra-modality
representation learning to derive the implicit global and local knowledge of
each modality, the second evaluates the relevance between the text and its
accompanying image and integrates different grained visual information based on
the relevance, the third enforces semantic refinement via iterative cross-modal
interactions and co-attention. We conduct experiments on two open datasets, and
the results and detailed analysis demonstrate the advantage of our model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion Models as Masked Audio-Video Learners <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03937v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03937v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elvis Nunez, Yanzi Jin, Mohammad Rastegari, Sachin Mehta, Maxwell Horton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the past several years, the synchronization between audio and visual
signals has been leveraged to learn richer audio-visual representations. Aided
by the large availability of unlabeled videos, many unsupervised training
frameworks have demonstrated impressive results in various downstream audio and
video tasks. Recently, Masked Audio-Video Learners (MAViL) has emerged as a
state-of-the-art audio-video pre-training framework. MAViL couples contrastive
learning with masked autoencoding to jointly reconstruct audio spectrograms and
video frames by fusing information from both modalities. In this paper, we
study the potential synergy between diffusion models and MAViL, seeking to
derive mutual benefits from these two frameworks. The incorporation of
diffusion into MAViL, combined with various training efficiency methodologies
that include the utilization of a masking ratio curriculum and adaptive batch
sizing, results in a notable 32% reduction in pre-training Floating-Point
Operations (FLOPS) and an 18% decrease in pre-training wall clock time.
Crucially, this enhanced efficiency does not compromise the model's performance
in downstream audio-classification tasks when compared to MAViL's performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version for the Machine Learning for Audio Workshop at
  NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-01-03T00:00:00Z">2024-01-03</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">48</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Theoretical guarantees on the best-of-n alignment policy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01879v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01879v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Beirami, Alekh Agarwal, Jonathan Berant, Alexander D'Amour, Jacob Eisenstein, Chirag Nagpal, Ananda Theertha Suresh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A simple and effective method for the alignment of generative models is the
best-of-$n$ policy, where $n$ samples are drawn from a base policy, and ranked
based on a reward function, and the highest ranking one is selected. A commonly
used analytical expression in the literature claims that the KL divergence
between the best-of-$n$ policy and the base policy is equal to $\log (n) -
(n-1)/n.$ We disprove the validity of this claim, and show that it is an upper
bound on the actual KL divergence. We also explore the tightness of this upper
bound in different regimes. Finally, we propose a new estimator for the KL
divergence and empirically show that it provides a tight approximation through
a few examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Vision Check-up for Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01862v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01862v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pratyusha Sharma, Tamar Rott Shaham, Manel Baradad, Stephanie Fu, Adrian Rodriguez-Munoz, Shivam Duggal, Phillip Isola, Antonio Torralba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What does learning to model relationships between strings teach large
language models (LLMs) about the visual world? We systematically evaluate LLMs'
abilities to generate and recognize an assortment of visual concepts of
increasing complexity and then demonstrate how a preliminary visual
representation learning system can be trained using models of text. As language
models lack the ability to consume or output visual information as pixels, we
use code to represent images in our study. Although LLM-generated images do not
look like natural images, results on image generation and the ability of models
to correct these generated images indicate that precise modeling of strings can
teach language models about numerous aspects of the visual world. Furthermore,
experiments on self-supervised visual representation learning, utilizing images
generated with text models, highlight the potential to train vision models
capable of making semantic assessments of natural images using just LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multilingual Instruction Tuning With Just a Pinch of Multilinguality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Uri Shaham, Jonathan Herzig, Roee Aharoni, Idan Szpektor, Reut Tsarfaty, Matan Eyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As instruction-tuned large language models (LLMs) gain global adoption, their
ability to follow instructions in multiple languages becomes increasingly
crucial. One promising approach is cross-lingual transfer, where a model
acquires specific functionality on some language by finetuning on another
language. In this work, we investigate how multilinguality during instruction
tuning of a multilingual LLM affects instruction-following across languages. We
first show that many languages transfer some instruction-following capabilities
to other languages from even monolingual tuning. Furthermore, we find that only
40 multilingual examples in an English tuning set substantially improve
multilingual instruction-following, both in seen and unseen languages during
tuning. In general, we observe that models tuned on multilingual mixtures
exhibit comparable or superior performance in several languages compared to
monolingually tuned models, despite training on 10x fewer examples in those
languages. Finally, we find that increasing the number of languages in the
instruction tuning set from 1 to only 2, 3, or 4 increases cross-lingual
generalization. Our results suggest that building massively multilingual
instruction-tuned models can be done with only a very small set of multilingual
instruction-responses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating Semi-Supervised Learning Algorithms in Text <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01843v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01843v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Himmet Toprak Kesgin, Mehmet Fatih Amasyali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using large training datasets enhances the generalization capabilities of
neural networks. Semi-supervised learning (SSL) is useful when there are few
labeled data and a lot of unlabeled data. SSL methods that use data
augmentation are most successful for image datasets. In contrast, texts do not
have consistent augmentation methods as images. Consequently, methods that use
augmentation are not as effective in text data as they are in image data. In
this study, we compared SSL algorithms that do not require augmentation; these
are self-training, co-training, tri-training, and tri-training with
disagreement. In the experiments, we used 4 different text datasets for
different tasks. We examined the algorithms from a variety of perspectives by
asking experiment questions and suggested several improvements. Among the
algorithms, tri-training with disagreement showed the closest performance to
the Oracle; however, performance gap shows that new semi-supervised algorithms
or improvements in existing methods are needed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Innovations in Intelligent Systems and Applications Conference (ASYU)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Iterative Mask Filling: An Effective Text Augmentation Method Using
  Masked Language Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Himmet Toprak Kesgin, Mehmet Fatih Amasyali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data augmentation is an effective technique for improving the performance of
machine learning models. However, it has not been explored as extensively in
natural language processing (NLP) as it has in computer vision. In this paper,
we propose a novel text augmentation method that leverages the Fill-Mask
feature of the transformer-based BERT model. Our method involves iteratively
masking words in a sentence and replacing them with language model predictions.
We have tested our proposed method on various NLP tasks and found it to be
effective in many cases. Our results are presented along with a comparison to
existing augmentation methods. Experimental results show that our proposed
method significantly improves performance, especially on topic classification
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in International Conference on Advanced Engineering,
  Technology and Applications (ICAETA 2023). The final version is available
  online at https://link.springer.com/chapter/10.1007/978-3-031-50920-9_35</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physio: An LLM-Based Physiotherapy Advisor <span class="chip">ECIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01825v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01825v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rúben Almeida, Hugo Sousa, Luís F. Cunha, Nuno Guimarães, Ricardo Campos, Alípio Jorge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The capabilities of the most recent language models have increased the
interest in integrating them into real-world applications. However, the fact
that these models generate plausible, yet incorrect text poses a constraint
when considering their use in several domains. Healthcare is a prime example of
a domain where text-generative trustworthiness is a hard requirement to
safeguard patient well-being. In this paper, we present Physio, a chat-based
application for physical rehabilitation. Physio is capable of making an initial
diagnosis while citing reliable health sources to support the information
provided. Furthermore, drawing upon external knowledge databases, Physio can
recommend rehabilitation exercises and over-the-counter medication for symptom
relief. By combining these features, Physio can leverage the power of
generative models for language processing while also conditioning its response
on dependable and verifiable sources. A live demo of Physio is available at
https://physio.inesctec.pt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Demo, ECIR 2024, 3rd Sword AI challenge 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Navigating Uncertainty: Optimizing API Dependency for Hallucination
  Reduction in Closed-Book Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierre Erbacher, Louis Falissar, Vincent Guigue, Laure Soulier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Large Language Models (LLM) are able to accumulate and restore
knowledge, they are still prone to hallucination. Especially when faced with
factual questions, LLM cannot only rely on knowledge stored in parameters to
guarantee truthful and correct answers. Augmenting these models with the
ability to search on external information sources, such as the web, is a
promising approach to ground knowledge to retrieve information. However,
searching in a large collection of documents introduces additional
computational/time costs. An optimal behavior would be to query external
resources only when the LLM is not confident about answers. In this paper, we
propose a new LLM able to self-estimate if it is able to answer directly or
needs to request an external tool. We investigate a supervised approach by
introducing a hallucination masking mechanism in which labels are generated
using a close book question-answering task. In addition, we propose to leverage
parameter-efficient fine-tuning techniques to train our model on a small amount
of data. Our model directly provides answers for $78.2\%$ of the known queries
and opts to search for $77.2\%$ of the unknown ones. This results in the API
being utilized only $62\%$ of the time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-target Stance Detection by Exploiting Target Analytical
  Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01761v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01761v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daijun Ding, Rong Chen, Bowen Zhang, Xu Huang, Li Dong, Xiaowen Zhao, Ge Song, Liwen Jing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-target stance detection (CTSD) is an important task, which infers the
attitude of the destination target by utilizing annotated data derived from the
source target. One important approach in CTSD is to extract domain-invariant
features to bridge the knowledge gap between multiple targets. However, the
analysis of informal and short text structure, and implicit expressions,
complicate the extraction of domain-invariant knowledge. In this paper, we
propose a Multi-Perspective Prompt-Tuning (MPPT) model for CTSD that uses the
analysis perspective as a bridge to transfer knowledge. First, we develop a
two-stage instruct-based chain-of-thought method (TsCoT) to elicit target
analysis perspectives and provide natural language explanations (NLEs) from
multiple viewpoints by formulating instructions based on large language model
(LLM). Second, we propose a multi-perspective prompt-tuning framework
(MultiPLN) to fuse the NLEs into the stance predictor. Extensive experiments
results demonstrate the superiority of MPPT against the state-of-the-art
baseline methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VGA: Vision and Graph Fused Attention Network for Rumor Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Bai, Caiyan Jia, Ziying Song, Chaoqun Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of social media, rumors have been spread broadly on
social media platforms, causing great harm to society. Beside textual
information, many rumors also use manipulated images or conceal textual
information within images to deceive people and avoid being detected, making
multimodal rumor detection be a critical problem. The majority of multimodal
rumor detection methods mainly concentrate on extracting features of source
claims and their corresponding images, while ignoring the comments of rumors
and their propagation structures. These comments and structures imply the
wisdom of crowds and are proved to be crucial to debunk rumors. Moreover, these
methods usually only extract visual features in a basic manner, seldom consider
tampering or textual information in images. Therefore, in this study, we
propose a novel Vision and Graph Fused Attention Network (VGA) for rumor
detection to utilize propagation structures among posts so as to obtain the
crowd opinions and further explore visual tampering features, as well as the
textual information hidden in images. We conduct extensive experiments on three
datasets, demonstrating that VGA can effectively detect multimodal rumors and
outperform state-of-the-art methods significantly.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Large Language Models in Semantic Parsing for Conversational
  Question Answering over Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phillip Schneider, Manuel Klettner, Kristiina Jokinen, Elena Simperl, Florian Matthes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational question answering systems often rely on semantic parsing to
enable interactive information retrieval, which involves the generation of
structured database queries from a natural language input. For
information-seeking conversations about facts stored within a knowledge graph,
dialogue utterances are transformed into graph queries in a process that is
called knowledge-based conversational question answering. This paper evaluates
the performance of large language models that have not been explicitly
pre-trained on this task. Through a series of experiments on an extensive
benchmark dataset, we compare models of varying sizes with different prompting
techniques and identify common issue types in the generated output. Our results
demonstrate that large language models are capable of generating graph queries
from dialogues, with significant improvements achievable through few-shot
prompting and fine-tuning techniques, especially for smaller models that
exhibit lower zero-shot performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICAART 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WordArt Designer API: User-Driven Artistic Typography Synthesis with
  Large Language Models on ModelScope <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun-Yan He, Zhi-Qi Cheng, Chenyang Li, Jingdong Sun, Wangmeng Xiang, Yusen Hu, Xianhui Lin, Xiaoyang Kang, Zengke Jin, Bin Luo, Yifeng Geng, Xuansong Xie, Jingren Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the WordArt Designer API, a novel framework for
user-driven artistic typography synthesis utilizing Large Language Models
(LLMs) on ModelScope. We address the challenge of simplifying artistic
typography for non-professionals by offering a dynamic, adaptive, and
computationally efficient alternative to traditional rigid templates. Our
approach leverages the power of LLMs to understand and interpret user input,
facilitating a more intuitive design process. We demonstrate through various
case studies how users can articulate their aesthetic preferences and
functional requirements, which the system then translates into unique and
creative typographic designs. Our evaluations indicate significant improvements
in user satisfaction, design flexibility, and creative expression over existing
systems. The WordArt Designer API not only democratizes the art of typography
but also opens up new possibilities for personalized digital communication and
design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Spotlight Paper from the Workshop on Machine Learning for Creativity
  and Design at the 37th Conference on Neural Information Processing Systems
  (NeurIPS 2023). 5 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Patterns of Persistence and Diffusibility across World's Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01698v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01698v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyi Chen, Johannes Bjerva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language similarities can be caused by genetic relatedness, areal contact,
universality, or chance. Colexification, i.e.~a type of similarity where a
single lexical form is used to convey multiple meanings, is underexplored. In
our work, we shed light on the linguistic causes of cross-lingual similarity in
colexification and phonology, by exploring genealogical stability (persistence)
and contact-induced change (diffusibility). We construct large-scale graphs
incorporating semantic, genealogical, phonological and geographical data for
1,966 languages. We then show the potential of this resource, by investigating
several established hypotheses from previous work in linguistics, while
proposing new ones. Our results strongly support a previously established
hypothesis in the linguistic literature, while offering contradicting evidence
to another. Our large scale resource opens for further research across
disciplines, e.g.~in multilingual NLP and comparative linguistics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting challenge moments from students' discourse: A comparison of
  <span class="highlight-title">GPT</span>-4 to two traditional natural language processing approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wannapon Suraworachet, Jennifer Seon, Mutlu Cukurova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective collaboration requires groups to strategically regulate themselves
to overcome challenges. Research has shown that groups may fail to regulate due
to differences in members' perceptions of challenges which may benefit from
external support. In this study, we investigated the potential of leveraging
three distinct natural language processing models: an expert knowledge
rule-based model, a supervised machine learning (ML) model and a Large Language
model (LLM), in challenge detection and challenge dimension identification
(cognitive, metacognitive, emotional and technical/other challenges) from
student discourse, was investigated. The results show that the supervised ML
and the LLM approaches performed considerably well in both tasks, in contrast
to the rule-based approach, whose efficacy heavily relies on the engineered
features by experts. The paper provides an extensive discussion of the three
approaches' performance for automated detection and support of students'
challenge moments in collaborative learning activities. It argues that,
although LLMs provide many advantages, they are unlikely to be the panacea to
issues of the detection and feedback provision of socially shared regulation of
learning due to their lack of reliability, as well as issues of validity
evaluation, privacy and confabulation. We conclude the paper with a discussion
on additional considerations, including model transparency to explore feasible
and meaningful analytical feedback for students and educators using LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MLPs Compass: What is learned when MLPs are combined with PLMs? <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01667v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01667v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Zhou, Wenyu Chen, Yong Cao, Dingyi Zeng, Wanlong Liu, Hong Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Transformer-based pre-trained language models and their variants
exhibit strong semantic representation capabilities, the question of
comprehending the information gain derived from the additional components of
PLMs remains an open question in this field. Motivated by recent efforts that
prove Multilayer-Perceptrons (MLPs) modules achieving robust structural capture
capabilities, even outperforming Graph Neural Networks (GNNs), this paper aims
to quantify whether simple MLPs can further enhance the already potent ability
of PLMs to capture linguistic information. Specifically, we design a simple yet
effective probing framework containing MLPs components based on BERT structure
and conduct extensive experiments encompassing 10 probing tasks spanning three
distinct linguistic levels. The experimental results demonstrate that MLPs can
indeed enhance the comprehension of linguistic structure by PLMs. Our research
provides interpretable and valuable insights into crafting variations of PLMs
utilizing MLPs for tasks that emphasize diverse linguistic structures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Social Media Ready Caption Generation for Brands 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Himanshu Maheshwari, Koustava Goswami, Apoorv Saxena, Balaji Vasan Srinivasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social media advertisements are key for brand marketing, aiming to attract
consumers with captivating captions and pictures or logos. While previous
research has focused on generating captions for general images, incorporating
brand personalities into social media captioning remains unexplored. Brand
personalities are shown to be affecting consumers' behaviours and social
interactions and thus are proven to be a key aspect of marketing strategies.
Current open-source multimodal LLMs are not directly suited for this task.
Hence, we propose a pipeline solution to assist brands in creating engaging
social media captions that align with the image and the brand personalities.
Our architecture is based on two parts: a the first part contains an image
captioning model that takes in an image that the brand wants to post online and
gives a plain English caption; b the second part takes in the generated caption
along with the target brand personality and outputs a catchy
personality-aligned social media caption. Along with brand personality, our
system also gives users the flexibility to provide hashtags, Instagram handles,
URLs, and named entities they want the caption to contain, making the captions
more semantically related to the social media handles. Comparative evaluations
against various baselines demonstrate the effectiveness of our approach, both
qualitatively and quantitatively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can AI Be as Creative as Humans? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan Wang, James Zou, Michael Mozer, Linjun Zhang, Anirudh Goyal, Alex Lamb, Zhun Deng, Michael Qizhe Xie, Hannah Brown, Kenji Kawaguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creativity serves as a cornerstone for societal progress and innovation, but
its assessment remains a complex and often subjective endeavor. With the rise
of advanced generative AI models capable of tasks once reserved for human
creativity, the study of AI's creative potential becomes imperative for its
responsible development and application. This paper addresses the complexities
in defining and evaluating creativity by introducing a new concept called
Relative Creativity. Instead of trying to define creativity universally, we
shift the focus to whether AI can match the creative abilities of a
hypothetical human. This perspective draws inspiration from the Turing Test,
expanding upon it to address the challenges and subjectivities inherent in
evaluating creativity. This methodological shift facilitates a statistically
quantifiable evaluation of AI's creativity, which we term Statistical
Creativity. This approach allows for direct comparisons of AI's creative
abilities with those of specific human groups. Building on this foundation, we
discuss the application of statistical creativity in contemporary
prompt-conditioned autoregressive models. In addition to defining and analyzing
a measure of creativity, we introduce an actionable training guideline,
effectively bridging the gap between theoretical quantification of creativity
and practical model training. Through these multifaceted contributions, the
paper establishes a cohesive, continuously evolving, and transformative
framework for assessing and fostering statistical creativity in AI models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper presents "Relative Creativity," comparing AI creativity to
  human creativity, inspired by the Turing Test. It introduces "Statistical
  Creativity" for measurable assessment and provides AI training guidelines to
  foster AI's creative capabilities</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Model Capabilities in Perioperative Risk Prediction and
  Prognostication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01620v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01620v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Chung, Christine T Fong, Andrew M Walters, Nima Aghaeepour, Meliha Yetisgen, Vikas N O'Reilly-Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate whether general-domain large language models such as GPT-4
Turbo can perform risk stratification and predict post-operative outcome
measures using a description of the procedure and a patient's clinical notes
derived from the electronic health record. We examine predictive performance on
8 different tasks: prediction of ASA Physical Status Classification, hospital
admission, ICU admission, unplanned admission, hospital mortality, PACU Phase 1
duration, hospital duration, and ICU duration. Few-shot and chain-of-thought
prompting improves predictive performance for several of the tasks. We achieve
F1 scores of 0.50 for ASA Physical Status Classification, 0.81 for ICU
admission, and 0.86 for hospital mortality. Performance on duration prediction
tasks were universally poor across all prompt strategies. Current generation
large language models can assist clinicians in perioperative risk
stratification on classification tasks and produce high-quality natural
language summaries and explanations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">GPT</span>-4V(ision) is a Generalist Web Agent, if Grounded 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, Yu Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent development on large multimodal models (LMMs), especially
GPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries
of multimodal models beyond traditional tasks like image captioning and visual
question answering. In this work, we explore the potential of LMMs like GPT-4V
as a generalist web agent that can follow natural language instructions to
complete tasks on any given website. We propose SEEACT, a generalist web agent
that harnesses the power of LMMs for integrated visual understanding and acting
on the web. We evaluate on the recent MIND2WEB benchmark. In addition to
standard offline evaluation on cached websites, we enable a new online
evaluation setting by developing a tool that allows running web agents on live
websites. We show that GPT-4V presents a great potential for web agents - it
can successfully complete 50% of the tasks on live websites if we manually
ground its textual plans into actions on the websites. This substantially
outperforms text-only LLMs like GPT-4 or smaller models (FLAN-T5 and BLIP-2)
specifically fine-tuned for web agents. However, grounding still remains a
major challenge. Existing LMM grounding strategies like set-of-mark prompting
turns out not effective for web agents, and the best grounding strategy we
develop in this paper leverages both the HTML text and visuals. Yet, there is
still a substantial gap with oracle grounding, leaving ample room for further
improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PLLaMa: An Open-source Large Language Model for Plant Science 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianjun Yang, Junfeng Gao, Wenxin Xue, Erik Alexandersson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have exhibited remarkable capabilities in
understanding and interacting with natural language across various sectors.
However, their effectiveness is limited in specialized areas requiring high
accuracy, such as plant science, due to a lack of specific expertise in these
fields. This paper introduces PLLaMa, an open-source language model that
evolved from LLaMa-2. It's enhanced with a comprehensive database, comprising
more than 1.5 million scholarly articles in plant science. This development
significantly enriches PLLaMa with extensive knowledge and proficiency in plant
and agricultural sciences. Our initial tests, involving specific datasets
related to plants and agriculture, show that PLLaMa substantially improves its
understanding of plant science-related topics. Moreover, we have formed an
international panel of professionals, including plant scientists, agricultural
engineers, and plant breeders. This team plays a crucial role in verifying the
accuracy of PLLaMa's responses to various academic inquiries, ensuring its
effective and reliable application in the field. To support further research
and development, we have made the model's checkpoints and source codes
accessible to the scientific community. These resources are available for
download at \url{https://github.com/Xianjun-Yang/PLLaMa}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MedSumm: A Multimodal Approach to Summarizing Code-Mixed Hindi-English
  Clinical Queries <span class="chip">ECIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akash Ghosh, Arkadeep Acharya, Prince Jha, Aniket Gaudgaul, Rajdeep Majumdar, Sriparna Saha, Aman Chadha, Raghav Jain, Setu Sinha, Shivani Agarwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the healthcare domain, summarizing medical questions posed by patients is
critical for improving doctor-patient interactions and medical decision-making.
Although medical data has grown in complexity and quantity, the current body of
research in this domain has primarily concentrated on text-based methods,
overlooking the integration of visual cues. Also prior works in the area of
medical question summarisation have been limited to the English language. This
work introduces the task of multimodal medical question summarization for
codemixed input in a low-resource setting. To address this gap, we introduce
the Multimodal Medical Codemixed Question Summarization MMCQS dataset, which
combines Hindi-English codemixed medical queries with visual aids. This
integration enriches the representation of a patient's medical condition,
providing a more comprehensive perspective. We also propose a framework named
MedSumm that leverages the power of LLMs and VLMs for this task. By utilizing
our MMCQS dataset, we demonstrate the value of integrating visual information
from images to improve the creation of medically detailed summaries. This
multimodal strategy not only improves healthcare decision-making but also
promotes a deeper comprehension of patient queries, paving the way for future
exploration in personalized and responsive medical care. Our dataset, code, and
pre-trained models will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hallucinations in Neural Automatic Speech Recognition: Identifying
  Errors and Hallucinatory Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rita Frieske, Bertram E. Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucinations are a type of output error produced by deep neural networks.
While this has been studied in natural language processing, they have not been
researched previously in automatic speech recognition. Here, we define
hallucinations in ASR as transcriptions generated by a model that are
semantically unrelated to the source utterance, yet still fluent and coherent.
The similarity of hallucinations to probable natural language outputs of the
model creates a danger of deception and impacts the credibility of the system.
We show that commonly used metrics, such as word error rates, cannot
differentiate between hallucinatory and non-hallucinatory models. To address
this, we propose a perturbation-based method for assessing the susceptibility
of an automatic speech recognition (ASR) model to hallucination at test time,
which does not require access to the training dataset. We demonstrate that this
method helps to distinguish between hallucinatory and non-hallucinatory models
that have similar baseline word error rates. We further explore the
relationship between the types of ASR errors and the types of dataset noise to
determine what types of noise are most likely to create hallucinatory outputs.
We devise a framework for identifying hallucinations by analysing their
semantic connection with the ground truth and their fluency. Finally, we
discover how to induce hallucinations with a random noise injection to the
utterance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GOAT-Bench: Safety Insights to Large Multimodal Models through
  Meme-Based Social Abuse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01523v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01523v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongzhan Lin, Ziyang Luo, Bo Wang, Ruichao Yang, Jing Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The exponential growth of social media has profoundly transformed how
information is created, disseminated, and absorbed, exceeding any precedent in
the digital age. Regrettably, this explosion has also spawned a significant
increase in the online abuse of memes. Evaluating the negative impact of memes
is notably challenging, owing to their often subtle and implicit meanings,
which are not directly conveyed through the overt text and imagery. In light of
this, large multimodal models (LMMs) have emerged as a focal point of interest
due to their remarkable capabilities in handling diverse multimodal tasks. In
response to this development, our paper aims to thoroughly examine the capacity
of various LMMs (e.g. GPT-4V) to discern and respond to the nuanced aspects of
social abuse manifested in memes. We introduce the comprehensive meme
benchmark, GOAT-Bench, comprising over 6K varied memes encapsulating themes
such as implicit hate speech, sexism, and cyberbullying, etc. Utilizing
GOAT-Bench, we delve into the ability of LMMs to accurately assess hatefulness,
misogyny, offensiveness, sarcasm, and harmful content. Our extensive
experiments across a range of LMMs reveal that current models still exhibit a
deficiency in safety awareness, showing insensitivity to various forms of
implicit abuse. We posit that this shortfall represents a critical impediment
to the realization of safe artificial intelligence. The GOAT-Bench and
accompanying resources are publicly accessible at https://goatlmm.github.io/,
contributing to ongoing research in this vital field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Utilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic
  Token Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01498v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01498v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minchan Kim, Myeonghun Jeong, Byoung Jin Choi, Semin Kim, Joun Yeop Lee, Nam Soo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel text-to-speech (TTS) framework centered around a neural
transducer. Our approach divides the whole TTS pipeline into semantic-level
sequence-to-sequence (seq2seq) modeling and fine-grained acoustic modeling
stages, utilizing discrete semantic tokens obtained from wav2vec2.0 embeddings.
For a robust and efficient alignment modeling, we employ a neural transducer
named token transducer for the semantic token prediction, benefiting from its
hard monotonic alignment constraints. Subsequently, a non-autoregressive (NAR)
speech generator efficiently synthesizes waveforms from these semantic tokens.
Additionally, a reference speech controls temporal dynamics and acoustic
conditions at each stage. This decoupled framework reduces the training
complexity of TTS while allowing each stage to focus on semantic and acoustic
modeling. Our experimental results on zero-shot adaptive TTS demonstrate that
our model surpasses the baseline in terms of speech quality and speaker
similarity, both objectively and subjectively. We also delve into the inference
speed and prosody control capabilities of our approach, highlighting the
potential of neural transducers in TTS frameworks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Two-Stage Multimodal Emotion Recognition Model Based on Graph
  Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Ai, FuChen Zhang, Tao Meng, YunTao Shou, HongEn Shao, Keqin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In terms of human-computer interaction, it is becoming more and more
important to correctly understand the user's emotional state in a conversation,
so the task of multimodal emotion recognition (MER) started to receive more
attention. However, existing emotion classification methods usually perform
classification only once. Sentences are likely to be misclassified in a single
round of classification. Previous work usually ignores the similarities and
differences between different morphological features in the fusion process. To
address the above issues, we propose a two-stage emotion recognition model
based on graph contrastive learning (TS-GCL). First, we encode the original
dataset with different preprocessing modalities. Second, a graph contrastive
learning (GCL) strategy is introduced for these three modal data with other
structures to learn similarities and differences within and between modalities.
Finally, we use MLP twice to achieve the final emotion classification. This
staged classification method can help the model to better focus on different
levels of emotional information, thereby improving the performance of the
model. Extensive experiments show that TS-GCL has superior performance on
IEMOCAP and MELD datasets compared with previous methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Natural Language Processing and Multimodal Stock Price Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01487v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01487v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Taylor, Jerry Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of financial decision-making, predicting stock prices is
pivotal. Artificial intelligence techniques such as long short-term memory
networks (LSTMs), support-vector machines (SVMs), and natural language
processing (NLP) models are commonly employed to predict said prices. This
paper utilizes stock percentage change as training data, in contrast to the
traditional use of raw currency values, with a focus on analyzing publicly
released news articles. The choice of percentage change aims to provide models
with context regarding the significance of price fluctuations and overall price
change impact on a given stock. The study employs specialized BERT natural
language processing models to predict stock price trends, with a particular
emphasis on various data modalities. The results showcase the capabilities of
such strategies with a small natural language processing model to accurately
predict overall stock trends, and highlight the effectiveness of certain data
features and sector-specific data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A First Look at Information Highlighting in Stack Overflow Answers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahla Shaan Ahmed, Shaowei Wang, Yuan Tian,  Tse-Hsun,  Chen, Haoxiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Context: Navigating the knowledge of Stack Overflow (SO) remains challenging.
To make the posts vivid to users, SO allows users to write and edit posts with
Markdown or HTML so that users can leverage various formatting styles (e.g.,
bold, italic, and code) to highlight the important information. Nonetheless,
there have been limited studies on the highlighted information. Objective: We
carried out the first large-scale exploratory study on the information
highlighted in SO answers in our recent study. To extend our previous study, we
develop approaches to automatically recommend highlighted content with
formatting styles using neural network architectures initially designed for the
Named Entity Recognition task. Method: In this paper, we studied 31,169,429
answers of Stack Overflow. For training recommendation models, we choose CNN
and BERT models for each type of formatting (i.e., Bold, Italic, Code, and
Heading) using the information highlighting dataset we collected from SO
answers. Results: Our models based on CNN architecture achieve precision
ranging from 0.71 to 0.82. The trained model for automatic code content
highlighting achieves a recall of 0.73 and an F1 score of 0.71, outperforming
the trained models for other formatting styles. The BERT models have even lower
recalls and F1 scores than the CNN models. Our analysis of failure cases
indicates that the majority of the failure cases are missing identification
(i.e., the model misses the content that is supposed to be highlighted) due to
the models tend to learn the frequently highlighted words while struggling to
learn less frequent words. Conclusion: Our findings suggest that it is possible
to develop recommendation models for highlighting information for answers with
different formatting styles on Stack Overflow.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work is submitted to Information and Software Technology Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Question-Answering Based Summarization of Electronic Health Records
  using Retrieval Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Walid Saba, Suzanne Wendelken, James. Shanahan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Summarization of electronic health records (EHRs) can substantially minimize
'screen time' for both patients as well as medical personnel. In recent years
summarization of EHRs have employed machine learning pipelines using state of
the art neural models. However, these models have produced less than adequate
results that are attributed to the difficulty of obtaining sufficient annotated
data for training. Moreover, the requirement to consider the entire content of
an EHR in summarization has resulted in poor performance due to the fact that
attention mechanisms in modern large language models (LLMs) adds a quadratic
complexity in terms of the size of the input. We propose here a method that
mitigates these shortcomings by combining semantic search, retrieval augmented
generation (RAG) and question-answering using the latest LLMs. In our approach
summarization is the extraction of answers to specific questions that are
deemed important by subject-matter experts (SMEs). Our approach is quite
efficient; requires minimal to no training; does not suffer from the
'hallucination' problem of LLMs; and it ensures diversity, since the summary
will not have repeated content but diverse answers to specific questions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Zero-Shot Abstractive Summarization in the Era of Large
  Language Models from the Perspective of Position Bias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01989v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01989v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anshuman Chhabra, Hadi Askari, Prasant Mohapatra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We characterize and study zero-shot abstractive summarization in Large
Language Models (LLMs) by measuring position bias, which we propose as a
general formulation of the more restrictive lead bias phenomenon studied
previously in the literature. Position bias captures the tendency of a model
unfairly prioritizing information from certain parts of the input text over
others, leading to undesirable behavior. Through numerous experiments on four
diverse real-world datasets, we study position bias in multiple LLM models such
as GPT 3.5-Turbo, Llama-2, and Dolly-v2, as well as state-of-the-art pretrained
encoder-decoder abstractive summarization models such as Pegasus and BART. Our
findings lead to novel insights and discussion on performance and position bias
of models for zero-shot summarization tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO
  and Toxicity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01967v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01967v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Lee, Xiaoyan Bai, Itamar Pres, Martin Wattenberg, Jonathan K. Kummerfeld, Rada Mihalcea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While alignment algorithms are now commonly used to tune pre-trained language
models towards a user's preferences, we lack explanations for the underlying
mechanisms in which models become ``aligned'', thus making it difficult to
explain phenomena like jailbreaks. In this work we study a popular algorithm,
direct preference optimization (DPO), and the mechanisms by which it reduces
toxicity. Namely, we first study how toxicity is represented and elicited in a
pre-trained language model, GPT2-medium. We then apply DPO with a carefully
crafted pairwise dataset to reduce toxicity. We examine how the resulting model
averts toxic outputs, and find that capabilities learned from pre-training are
not removed, but rather bypassed. We use this insight to demonstrate a simple
method to un-align the model, reverting it back to its toxic behavior.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Instruct-Imagen: Image Generation with Multi-modal Instruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01952v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01952v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hexiang Hu, Kelvin C. K. Chan, Yu-Chuan Su, Wenhu Chen, Yandong Li, Kihyuk Sohn, Yang Zhao, Xue Ben, Boqing Gong, William Cohen, Ming-Wei Chang, Xuhui Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents instruct-imagen, a model that tackles heterogeneous image
generation tasks and generalizes across unseen tasks. We introduce *multi-modal
instruction* for image generation, a task representation articulating a range
of generation intents with precision. It uses natural language to amalgamate
disparate modalities (e.g., text, edge, style, subject, etc.), such that
abundant generation intents can be standardized in a uniform format.
  We then build instruct-imagen by fine-tuning a pre-trained text-to-image
diffusion model with a two-stage framework. First, we adapt the model using the
retrieval-augmented training, to enhance model's capabilities to ground its
generation on external multimodal context. Subsequently, we fine-tune the
adapted model on diverse image generation tasks that requires vision-language
understanding (e.g., subject-driven generation, etc.), each paired with a
multi-modal instruction encapsulating the task's essence. Human evaluation on
various image generation datasets reveals that instruct-imagen matches or
surpasses prior task-specific models in-domain and demonstrates promising
generalization to unseen and more complex tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalist embedding models are better at short-context clinical
  semantic search than specialized embedding models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01943v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01943v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean-Baptiste Excoffier, Tom Roehr, Alexei Figueroa, Michalis Papaaioannou, Keno Bressem, Matthieu Ortala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing use of tools and solutions based on Large Language Models
(LLMs) for various tasks in the medical domain has become a prominent trend.
Their use in this highly critical and sensitive domain has thus raised
important questions about their robustness, especially in response to
variations in input, and the reliability of the generated outputs. This study
addresses these questions by constructing a textual dataset based on the
ICD-10-CM code descriptions, widely used in US hospitals and containing many
clinical terms, and their easily reproducible rephrasing. We then benchmarked
existing embedding models, either generalist or specialized in the clinical
domain, in a semantic search task where the goal was to correctly match the
rephrased text to the original description. Our results showed that generalist
models performed better than clinical models, suggesting that existing clinical
specialized models are more sensitive to small changes in input that confuse
them. The highlighted problem of specialized models may be due to the fact that
they have not been trained on sufficient data, and in particular on datasets
that are not diverse enough to have a reliable global language understanding,
which is still necessary for accurate handling of medical documents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 1 figure, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AstroLLaMA-Chat: Scaling AstroLLaMA with Conversational and Diverse
  <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ernest Perkowski, Rui Pan, Tuan Dung Nguyen, Yuan-Sen Ting, Sandor Kruk, Tong Zhang, Charlie O'Neill, Maja Jablonska, Michael J. Smith, Kevin Schawinski, Kartheik Iyer, Ioana Ciucă for UniverseTBD
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore the potential of enhancing LLM performance in astronomy-focused
question-answering through targeted, continual pre-training. By employing a
compact 7B-parameter LLaMA-2 model and focusing exclusively on a curated set of
astronomy corpus -- comprising abstracts, introductions, and conclusions -- we
achieve notable improvements in specialized topic comprehension. While general
LLMs like GPT-4 outperform in broader question-answering scenarios due to
superior reasoning capabilities, our findings suggest that continual
pre-training with limited resources can still enhance model performance on
specialized topics. Additionally, we present an extension of AstroLLaMA: the
fine-tuning of the 7B LLaMA model on a domain-specific conversational dataset,
culminating in the release of the chat-enabled AstroLLaMA for community use.
Comprehensive quantitative benchmarking is currently in progress and will be
detailed in an upcoming full paper. The model, AstroLLaMA-Chat, is now
available at https://huggingface.co/universeTBD, providing the first
open-source conversational AI tool tailored for the astronomy community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 1 figure, model is available at
  https://huggingface.co/universeTBD, submitted to RNAAS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive <span class="highlight-title">Survey</span> of Hallucination Mitigation Techniques in Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01313v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01313v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S. M Towhidul Islam Tonmoy, S M Mehedi Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, Amitava Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) continue to advance in their ability to write
human-like text, a key challenge remains around their tendency to hallucinate
generating content that appears factual but is ungrounded. This issue of
hallucination is arguably the biggest hindrance to safely deploying these
powerful LLMs into real-world production systems that impact people's lives.
The journey toward widespread adoption of LLMs in practical settings heavily
relies on addressing and mitigating hallucinations. Unlike traditional AI
systems focused on limited tasks, LLMs have been exposed to vast amounts of
online text data during training. While this allows them to display impressive
language fluency, it also means they are capable of extrapolating information
from the biases in training data, misinterpreting ambiguous prompts, or
modifying the information to align superficially with the input. This becomes
hugely alarming when we rely on language generation capabilities for sensitive
applications, such as summarizing medical records, financial analysis reports,
etc. This paper presents a comprehensive survey of over 32 techniques developed
to mitigate hallucination in LLMs. Notable among these are Retrieval Augmented
Generation (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023),
CoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023). Furthermore, we
introduce a detailed taxonomy categorizing these methods based on various
parameters, such as dataset utilization, common tasks, feedback mechanisms, and
retriever types. This classification helps distinguish the diverse approaches
specifically designed to tackle hallucination issues in LLMs. Additionally, we
analyze the challenges and limitations inherent in these techniques, providing
a solid foundation for future research in addressing hallucinations and related
phenomena within the realm of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieval-Augmented Generation for Large Language Models: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10997v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10997v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, Haofen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) demonstrate significant capabilities but face
challenges such as hallucination, outdated knowledge, and non-transparent,
untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has
emerged as a promising solution by incorporating knowledge from external
databases. This enhances the accuracy and credibility of the models,
particularly for knowledge-intensive tasks, and allows for continuous knowledge
updates and integration of domain-specific information. RAG synergistically
merges LLMs' intrinsic knowledge with the vast, dynamic repositories of
external databases. This comprehensive review paper offers a detailed
examination of the progression of RAG paradigms, encompassing the Naive RAG,
the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the
tripartite foundation of RAG frameworks, which includes the retrieval , the
generation and the augmentation techniques. The paper highlights the
state-of-the-art technologies embedded in each of these critical components,
providing a profound understanding of the advancements in RAG systems.
Furthermore, this paper introduces the metrics and benchmarks for assessing RAG
models, along with the most up-to-date evaluation framework. In conclusion, the
paper delineates prospective avenues for research, including the identification
of challenges, the expansion of multi-modalities, and the progression of the
RAG infrastructure and its ecosystem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing Work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quality and Quantity of Machine Translation References for Automated
  Metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01283v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01283v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vilém Zouhar, Ondřej Bojar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic machine translation metrics often use human translations to
determine the quality system translations. Common wisdom in the field dictates
that the human references should be of very high quality. However, there are no
cost-benefit analyses that could be used to guide practitioners who plan to
collect references for machine translation evaluation. We find that
higher-quality references lead to better metric correlations with humans at the
segment-level. Having up to 7 references per segment and taking their average
helps all metrics. Interestingly, the references from vendors of different
qualities can be mixed together and improve metric success. Higher quality
references, however, cost more to create and we frame this as an optimization
problem: given a specific budget, what references should be collected to
maximize metric success. These findings can be used by evaluators of shared
tasks when references need to be created under a certain budget.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> of Text Watermarking in the Era of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07913v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07913v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aiwei Liu, Leyi Pan, Yijian Lu, Jingjing Li, Xuming Hu, Lijie Wen, Irwin King, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text watermarking algorithms play a crucial role in the copyright protection
of textual content, yet their capabilities and application scenarios have been
limited historically. The recent developments in large language models (LLMs)
have opened new opportunities for the advancement of text watermarking
techniques. LLMs not only enhance the capabilities of text watermarking
algorithms through their text understanding and generation abilities but also
necessitate the use of text watermarking algorithms for their own copyright
protection. This paper conducts a comprehensive survey of the current state of
text watermarking technology, covering four main aspects: (1) an overview and
comparison of different text watermarking techniques; (2) evaluation methods
for text watermarking algorithms, including their success rates, impact on text
quality, robustness, and unforgeability; (3) potential application scenarios
for text watermarking technology; (4) current challenges and future directions
for development. This survey aims to provide researchers with a thorough
understanding of text watermarking technology, thereby promoting its further
advancement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long
  Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19923v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19923v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Günther, Jackmin Ong, Isabelle Mohr, Alaeddine Abdessalem, Tanguy Abel, Mohammad Kalim Akram, Susana Guzman, Georgios Mastrapas, Saba Sturua, Bo Wang, Maximilian Werk, Nan Wang, Han Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text embedding models have emerged as powerful tools for transforming
sentences into fixed-sized feature vectors that encapsulate semantic
information. While these models are essential for tasks like information
retrieval, semantic clustering, and text re-ranking, most existing open-source
models, especially those built on architectures like BERT, struggle to
represent lengthy documents and often resort to truncation. One common approach
to mitigate this challenge involves splitting documents into smaller paragraphs
for embedding. However, this strategy results in a much larger set of vectors,
consequently leading to increased memory consumption and computationally
intensive vector searches with elevated latency.
  To address these challenges, we introduce Jina Embeddings 2, an open-source
text embedding model capable of accommodating up to 8192 tokens. This model is
designed to transcend the conventional 512-token limit and adeptly process long
documents. Jina Embeddings 2 not only achieves state-of-the-art performance on
a range of embedding-related tasks in the MTEB benchmark but also matches the
performance of OpenAI's proprietary ada-002 model. Additionally, our
experiments indicate that an extended context can enhance performance in tasks
such as NarrativeQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EQ-Bench: An Emotional Intelligence Benchmark for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06281v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06281v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel J. Paech
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce EQ-Bench, a novel benchmark designed to evaluate aspects of
emotional intelligence in Large Language Models (LLMs). We assess the ability
of LLMs to understand complex emotions and social interactions by asking them
to predict the intensity of emotional states of characters in a dialogue. The
benchmark is able to discriminate effectively between a wide range of models.
We find that EQ-Bench correlates strongly with comprehensive multi-domain
benchmarks like MMLU (Hendrycks et al., 2020) (r=0.97), indicating that we may
be capturing similar aspects of broad intelligence. Our benchmark produces
highly repeatable results using a set of 60 English-language questions. We also
provide open-source code for an automated benchmarking pipeline at
https://github.com/EQ-bench/EQ-Bench and a leaderboard at https://eqbench.com
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding the Effects of RLHF on LLM Generalisation and Diversity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06452v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06452v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena Luketina, Eric Hambro, Edward Grefenstette, Roberta Raileanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) fine-tuned with reinforcement learning from
human feedback (RLHF) have been used in some of the most widely deployed AI
models to date, such as OpenAI's ChatGPT or Anthropic's Claude. % , or Meta's
LLaMA-2. While there has been significant work developing these methods, our
understanding of the benefits and downsides of each stage in RLHF is still
limited. To fill this gap, we present an extensive analysis of how each stage
of the process (i.e.~supervised fine-tuning (SFT), reward modelling, and RLHF)
affects two key properties: out-of-distribution (OOD) generalisation and output
diversity. OOD generalisation is crucial given the wide range of real-world
scenarios in which these models are being used, while output diversity refers
to the model's ability to generate varied outputs and is important for a
variety of use cases. We perform our analysis across two base models on both
summarisation and instruction following tasks, the latter being highly relevant
for current LLM use cases. We find that RLHF generalises better than SFT to new
inputs, particularly as the distribution shift between train and test becomes
larger. However, RLHF significantly reduces output diversity compared to SFT
across a variety of measures, implying a tradeoff in current LLM fine-tuning
methods between generalisation and diversity. Our results provide guidance on
which fine-tuning method should be used depending on the application, and show
that more research is needed to improve the tradeoff between generalisation and
diversity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available here: https://github.com/facebookresearch/rlfh-gen-div</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vietnamese Poem Generation & The Prospect Of Cross-Language Poem-To-Poem
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01078v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01078v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Triet Minh Huynh, Quan Le Bao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Poetry generation has been a challenging task in the field of Natural
Language Processing, as it requires the model to understand the nuances of
language, sentiment, and style. In this paper, we propose using Large Language
Models to generate Vietnamese poems from natural language prompts, thereby
facilitating an intuitive process with enhanced content control. Our most
efficacious model, the GPT-3 Babbage variant, achieves a custom evaluation
score of 0.8, specifically tailored to the "luc bat" genre of Vietnamese
poetry. Furthermore, we also explore the idea of paraphrasing poems into normal
text prompts and yield a relatively high score of 0.718 in the "luc bat" genre.
This experiment presents the potential for cross-Language poem-to-poem
translation with translated poems as the inputs while concurrently maintaining
complete control over the generated content.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fairness Certification for Natural Language Processing and Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Freiberger, Erik Buchmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Processing (NLP) plays an important role in our daily lives,
particularly due to the enormous progress of Large Language Models (LLM).
However, NLP has many fairness-critical use cases, e.g., as an expert system in
recruitment or as an LLM-based tutor in education. Since NLP is based on human
language, potentially harmful biases can diffuse into NLP systems and produce
unfair results, discriminate against minorities or generate legal issues.
Hence, it is important to develop a fairness certification for NLP approaches.
We follow a qualitative research approach towards a fairness certification for
NLP. In particular, we have reviewed a large body of literature on algorithmic
fairness, and we have conducted semi-structured expert interviews with a wide
range of experts from that area. We have systematically devised six fairness
criteria for NLP, which can be further refined into 18 sub-categories. Our
criteria offer a foundation for operationalizing and testing processes to
certify fairness, both from the perspective of the auditor and the audited
organization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In depth discussion of our results can be found in the Appendix B</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What's the Magic Word? A Control Theory of LLM <span class="highlight-title">Prompt</span>ing <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04444v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04444v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aman Bhargava, Cameron Witkowski, Manav Shah, Matt Thomson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt engineering is crucial for deploying LLMs but is poorly understood
mathematically. We formalize LLM systems as a class of discrete stochastic
dynamical systems to explore prompt engineering through the lens of control
theory. We investigate the reachable set of output token sequences $R_y(\mathbf
x_0)$ for which there exists a control input sequence $\mathbf u$ for each
$\mathbf y \in R_y(\mathbf x_0)$ that steers the LLM to output $\mathbf y$ from
initial state sequence $\mathbf x_0$. We offer analytic analysis on the
limitations on the controllability of self-attention in terms of reachable set,
where we prove an upper bound on the reachable set of outputs $R_y(\mathbf
x_0)$ as a function of the singular values of the parameter matrices. We
present complementary empirical analysis on the controllability of a panel of
LLMs, including Falcon-7b, Llama-7b, and Falcon-40b. Our results demonstrate a
lower bound on the reachable set of outputs $R_y(\mathbf x_0)$ w.r.t. initial
state sequences $\mathbf x_0$ sampled from the Wikitext dataset. We find that
the correct next Wikitext token following sequence $\mathbf x_0$ is reachable
over 97% of the time with prompts of $k\leq 10$ tokens. We also establish that
the top 75 most likely next tokens, as estimated by the LLM itself, are
reachable at least 85% of the time with prompts of $k\leq 10$ tokens.
Intriguingly, short prompt sequences can dramatically alter the likelihood of
specific outputs, even making the least likely tokens become the most likely
ones. This control-centric analysis of LLMs demonstrates the significant and
poorly understood role of input sequences in steering output probabilities,
offering a foundational perspective for enhancing language model system
capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 8 figures. Under review for ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LMBot: Distilling Graph Knowledge into Language Model for Graph-less
  Deployment in Twitter Bot Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.17408v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.17408v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Cai, Zhaoxuan Tan, Zhenyu Lei, Zifeng Zhu, Hongrui Wang, Qinghua Zheng, Minnan Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As malicious actors employ increasingly advanced and widespread bots to
disseminate misinformation and manipulate public opinion, the detection of
Twitter bots has become a crucial task. Though graph-based Twitter bot
detection methods achieve state-of-the-art performance, we find that their
inference depends on the neighbor users multi-hop away from the targets, and
fetching neighbors is time-consuming and may introduce bias. At the same time,
we find that after finetuning on Twitter bot detection, pretrained language
models achieve competitive performance and do not require a graph structure
during deployment. Inspired by this finding, we propose a novel bot detection
framework LMBot that distills the knowledge of graph neural networks (GNNs)
into language models (LMs) for graph-less deployment in Twitter bot detection
to combat the challenge of data dependency. Moreover, LMBot is compatible with
graph-based and graph-less datasets. Specifically, we first represent each user
as a textual sequence and feed them into the LM for domain adaptation. For
graph-based datasets, the output of LMs provides input features for the GNN,
enabling it to optimize for bot detection and distill knowledge back to the LM
in an iterative, mutually enhancing process. Armed with the LM, we can perform
graph-less inference, which resolves the graph data dependency and sampling
bias issues. For datasets without graph structure, we simply replace the GNN
with an MLP, which has also shown strong performance. Our experiments
demonstrate that LMBot achieves state-of-the-art performance on four Twitter
bot detection benchmarks. Extensive studies also show that LMBot is more
robust, versatile, and efficient compared to graph-based Twitter bot detection
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In the Name of Fairness: Assessing the Bias in Clinical Record
  De-identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.11348v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.11348v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Xiao, Shulammite Lim, Tom Joseph Pollard, Marzyeh Ghassemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data sharing is crucial for open science and reproducible research, but the
legal sharing of clinical data requires the removal of protected health
information from electronic health records. This process, known as
de-identification, is often achieved through the use of machine learning
algorithms by many commercial and open-source systems. While these systems have
shown compelling results on average, the variation in their performance across
different demographic groups has not been thoroughly examined. In this work, we
investigate the bias of de-identification systems on names in clinical notes
via a large-scale empirical analysis. To achieve this, we create 16 name sets
that vary along four demographic dimensions: gender, race, name popularity, and
the decade of popularity. We insert these names into 100 manually curated
clinical templates and evaluate the performance of nine public and private
de-identification methods. Our findings reveal that there are statistically
significant performance gaps along a majority of the demographic dimensions in
most methods. We further illustrate that de-identification quality is affected
by polysemy in names, gender context, and clinical note characteristics. To
mitigate the identified gaps, we propose a simple and method-agnostic solution
by fine-tuning de-identification methods with clinical context and diverse
names. Overall, it is imperative to address the bias in existing methods
immediately so that downstream stakeholders can build high-quality systems to
serve all demographic parties fairly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by FAccT 2023; updated appendix with the de-identification
  performance of GPT-4</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DialCLIP: Empowering CLIP as Multi-Modal Dialog Retriever <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01076v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01076v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichao Yin, Binyuan Hui, Min Yang, Fei Huang, Yongbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, substantial advancements in pre-trained vision-language models have
greatly enhanced the capabilities of multi-modal dialog systems. These models
have demonstrated significant improvements by fine-tuning on downstream tasks.
However, the existing pre-trained models primarily focus on effectively
capturing the alignment between vision and language modalities, often ignoring
the intricate nature of dialog context. In this paper, we propose a
parameter-efficient prompt-tuning method named DialCLIP for multi-modal dialog
retrieval. Specifically, our approach introduces a multi-modal context prompt
generator to learn context features which are subsequently distilled into
prompts within the pre-trained vision-language model CLIP. Besides, we
introduce domain prompt to mitigate the disc repancy from the downstream dialog
data. To facilitate various types of retrieval, we also design multiple experts
to learn mappings from CLIP outputs to multi-modal representation space, with
each expert being responsible to one specific retrieval type. Extensive
experiments show that DialCLIP achieves state-of-the-art performance on two
widely recognized benchmark datasets (i.e., PhotoChat and MMDialog) by tuning a
mere 0.04% of the total parameters. These results highlight the efficacy and
efficiency of our proposed approach, underscoring its potential to advance the
field of multi-modal dialog retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conflicts, Villains, Resolutions: Towards models of Narrative Media
  Framing <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02052v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02052v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lea Frermann, Jiatong Li, Shima Khanehzar, Gosia Mikolajczak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite increasing interest in the automatic detection of media frames in
NLP, the problem is typically simplified as single-label classification and
adopts a topic-like view on frames, evading modelling the broader
document-level narrative. In this work, we revisit a widely used
conceptualization of framing from the communication sciences which explicitly
captures elements of narratives, including conflict and its resolution, and
integrate it with the narrative framing of key entities in the story as heroes,
victims or villains. We adapt an effective annotation paradigm that breaks a
complex annotation task into a series of simpler binary questions, and present
an annotated data set of English news articles, and a case study on the framing
of climate change in articles from news outlets across the political spectrum.
Finally, we explore automatic multi-label prediction of our frames with
supervised and semi-supervised approaches, and present a novel retrieval-based
method which is both effective and transparent in its predictions. We conclude
with a discussion of opportunities and challenges for future work on
document-level models of narrative framing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SPEED: Speculative Pipelined Execution for Efficient Decoding <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12072v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12072v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Hasan Genc, Kurt Keutzer, Amir Gholami, Sophia Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Large Language Models (LLMs) based on the Transformer architecture
have recently emerged as a dominant foundation model for a wide range of
Natural Language Processing tasks. Nevertheless, their application in real-time
scenarios has been highly restricted due to the significant inference latency
associated with these models. This is particularly pronounced due to the
autoregressive nature of generative LLM inference, where tokens are generated
sequentially since each token depends on all previous output tokens. It is
therefore challenging to achieve any token-level parallelism, making inference
extremely memory-bound. In this work, we propose SPEED, which improves
inference efficiency by speculatively executing multiple future tokens in
parallel with the current token using predicted values based on early-layer
hidden states. For Transformer decoders that employ parameter sharing, the
memory operations for the tokens executing in parallel can be amortized, which
allows us to accelerate generative LLM inference. We demonstrate the efficiency
of our method in terms of latency reduction relative to model accuracy and
demonstrate how speculation allows for training deeper decoders with parameter
sharing with minimal runtime overhead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS Workshop on Efficient Natural Language and Speech Processing
  (2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lon-ea at SemEval-2023 Task 11: A Comparison of Activation Functions for
  Soft and Hard Label Prediction <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02468v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02468v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peyman Hosseini, Mehran Hosseini, Sana Sabah Al-Azzawi, Marcus Liwicki, Ignacio Castro, Matthew Purver
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the influence of different activation functions in the output layer
of deep neural network models for soft and hard label prediction in the
learning with disagreement task. In this task, the goal is to quantify the
amount of disagreement via predicting soft labels. To predict the soft labels,
we use BERT-based preprocessors and encoders and vary the activation function
used in the output layer, while keeping other parameters constant. The soft
labels are then used for the hard label prediction. The activation functions
considered are sigmoid as well as a step-function that is added to the model
post-training and a sinusoidal activation function, which is introduced for the
first time in this paper.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ACL 2023 SemEval Workshop as selected task paper</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">106</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LEAP-VO: Long-term Effective Any Point Tracking for Visual Odometry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weirong Chen, Le Chen, Rui Wang, Marc Pollefeys
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual odometry estimates the motion of a moving camera based on visual
input. Existing methods, mostly focusing on two-view point tracking, often
ignore the rich temporal context in the image sequence, thereby overlooking the
global motion patterns and providing no assessment of the full trajectory
reliability. These shortcomings hinder performance in scenarios with occlusion,
dynamic objects, and low-texture areas. To address these challenges, we present
the Long-term Effective Any Point Tracking (LEAP) module. LEAP innovatively
combines visual, inter-track, and temporal cues with mindfully selected anchors
for dynamic track estimation. Moreover, LEAP's temporal probabilistic
formulation integrates distribution updates into a learnable iterative
refinement module to reason about point-wise uncertainty. Based on these
traits, we develop LEAP-VO, a robust visual odometry system adept at handling
occlusions and dynamic scenes. Our mindful integration showcases a novel
practice by employing long-term point tracking as the front-end. Extensive
experiments demonstrate that the proposed pipeline significantly outperforms
existing baselines across various visual odometry benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Audio to Photoreal Embodiment: Synthesizing Humans in Conversations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01885v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01885v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evonne Ng, Javier Romero, Timur Bagautdinov, Shaojie Bai, Trevor Darrell, Angjoo Kanazawa, Alexander Richard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a framework for generating full-bodied photorealistic avatars that
gesture according to the conversational dynamics of a dyadic interaction. Given
speech audio, we output multiple possibilities of gestural motion for an
individual, including face, body, and hands. The key behind our method is in
combining the benefits of sample diversity from vector quantization with the
high-frequency details obtained through diffusion to generate more dynamic,
expressive motion. We visualize the generated motion using highly
photorealistic avatars that can express crucial nuances in gestures (e.g.
sneers and smirks). To facilitate this line of research, we introduce a
first-of-its-kind multi-view conversational dataset that allows for
photorealistic reconstruction. Experiments show our model generates appropriate
and diverse gestures, outperforming both diffusion- and VQ-only methods.
Furthermore, our perceptual evaluation highlights the importance of
photorealism (vs. meshes) in accurately assessing subtle motion details in
conversational gestures. Code and dataset available online.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Step length measurement in the wild using FMCW radar 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01868v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01868v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parthipan Siva, Alexander Wong, Patricia Hewston, George Ioannidis, Dr. Jonathan Adachi, Dr. Alexander Rabinovich, Andrea Lee, Alexandra Papaioannou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With an aging population, numerous assistive and monitoring technologies are
under development to enable older adults to age in place. To facilitate aging
in place predicting risk factors such as falls, and hospitalization and
providing early interventions are important. Much of the work on ambient
monitoring for risk prediction has centered on gait speed analysis, utilizing
privacy-preserving sensors like radar. Despite compelling evidence that
monitoring step length, in addition to gait speed, is crucial for predicting
risk, radar-based methods have not explored step length measurement in the
home. Furthermore, laboratory experiments on step length measurement using
radars are limited to proof of concept studies with few healthy subjects. To
address this gap, a radar-based step length measurement system for the home is
proposed based on detection and tracking using radar point cloud, followed by
Doppler speed profiling of the torso to obtain step lengths in the home. The
proposed method was evaluated in a clinical environment, involving 35 frail
older adults, to establish its validity. Additionally, the method was assessed
in people's homes, with 21 frail older adults who had participated in the
clinical assessment. The proposed radar-based step length measurement method
was compared to the gold standard Zeno Walkway Gait Analysis System, revealing
a 4.5cm/8.3% error in a clinical setting. Furthermore, it exhibited excellent
reliability (ICC(2,k)=0.91, 95% CI 0.82 to 0.96) in uncontrolled home settings.
The method also proved accurate in uncontrolled home settings, as indicated by
a strong agreement (ICC(3,k)=0.81 (95% CI 0.53 to 0.92)) between home
measurements and in-clinic assessments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Vision Check-up for Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01862v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01862v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pratyusha Sharma, Tamar Rott Shaham, Manel Baradad, Stephanie Fu, Adrian Rodriguez-Munoz, Shivam Duggal, Phillip Isola, Antonio Torralba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What does learning to model relationships between strings teach large
language models (LLMs) about the visual world? We systematically evaluate LLMs'
abilities to generate and recognize an assortment of visual concepts of
increasing complexity and then demonstrate how a preliminary visual
representation learning system can be trained using models of text. As language
models lack the ability to consume or output visual information as pixels, we
use code to represent images in our study. Although LLM-generated images do not
look like natural images, results on image generation and the ability of models
to correct these generated images indicate that precise modeling of strings can
teach language models about numerous aspects of the visual world. Furthermore,
experiments on self-supervised visual representation learning, utilizing images
generated with text models, highlight the potential to train vision models
capable of making semantic assessments of natural images using just LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthetic <span class="highlight-title">dataset</span> of ID and Travel Document 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01858v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01858v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Boned, Maxime Talarmain, Nabil Ghanmi, Guillaume Chiron, Sanket Biswas, Ahmad Montaser Awal, Oriol Ramos Terrades
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a new synthetic dataset of ID and travel documents,
called SIDTD. The SIDTD dataset is created to help training and evaluating
forged ID documents detection systems. Such a dataset has become a necessity as
ID documents contain personal information and a public dataset of real
documents can not be released. Moreover, forged documents are scarce, compared
to legit ones, and the way they are generated varies from one fraudster to
another resulting in a class of high intra-variability. In this paper we
trained state-of-the-art models on this dataset and we compare them to the
performance achieved in larger, but private, datasets. The creation of this
dataset will help to document image analysis community to progress in the task
of ID document verification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Frequency Domain Modality-invariant Feature Learning for
  Visible-infrared Person Re-Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01839v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01839v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulin Li, Tianzhu Zhang, Yongdong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visible-infrared person re-identification (VI-ReID) is challenging due to the
significant cross-modality discrepancies between visible and infrared images.
While existing methods have focused on designing complex network architectures
or using metric learning constraints to learn modality-invariant features, they
often overlook which specific component of the image causes the modality
discrepancy problem. In this paper, we first reveal that the difference in the
amplitude component of visible and infrared images is the primary factor that
causes the modality discrepancy and further propose a novel Frequency Domain
modality-invariant feature learning framework (FDMNet) to reduce modality
discrepancy from the frequency domain perspective. Our framework introduces two
novel modules, namely the Instance-Adaptive Amplitude Filter (IAF) module and
the Phrase-Preserving Normalization (PPNorm) module, to enhance the
modality-invariant amplitude component and suppress the modality-specific
component at both the image- and feature-levels. Extensive experimental results
on two standard benchmarks, SYSU-MM01 and RegDB, demonstrate the superior
performance of our FDMNet against state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Moonshot: Towards Controllable Video Generation and Editing with
  Multimodal Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Junhao Zhang, Dongxu Li, Hung Le, Mike Zheng Shou, Caiming Xiong, Doyen Sahoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most existing video diffusion models (VDMs) are limited to mere text
conditions. Thereby, they are usually lacking in control over visual appearance
and geometry structure of the generated videos. This work presents Moonshot, a
new video generation model that conditions simultaneously on multimodal inputs
of image and text. The model builts upon a core module, called multimodal video
block (MVB), which consists of conventional spatialtemporal layers for
representing video features, and a decoupled cross-attention layer to address
image and text inputs for appearance conditioning. In addition, we carefully
design the model architecture such that it can optionally integrate with
pre-trained image ControlNet modules for geometry visual conditions, without
needing of extra training overhead as opposed to prior methods. Experiments
show that with versatile multimodal conditioning mechanisms, Moonshot
demonstrates significant improvement on visual quality and temporal consistency
compared to existing models. In addition, the model can be easily repurposed
for a variety of generative applications, such as personalized video
generation, image animation and video editing, unveiling its potential to serve
as a fundamental architecture for controllable video generation. Models will be
made public on https://github.com/salesforce/LAVIS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page: https://showlab.github.io/Moonshot/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HawkRover: An Autonomous mmWave Vehicular Communication Testbed with
  Multi-sensor Fusion and Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Zhu, Haijian Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Connected and automated vehicles (CAVs) have become a transformative
technology that can change our daily life. Currently, millimeter-wave (mmWave)
bands are identified as the promising CAV connectivity solution. While it can
provide high data rate, their realization faces many challenges such as high
attenuation during mmWave signal propagation and mobility management. Existing
solution has to initiate pilot signal to measure channel information, then
apply signal processing to calculate the best narrow beam towards the receiver
end to guarantee sufficient signal power. This process takes significant
overhead and time, hence not suitable for vehicles. In this study, we propose
an autonomous and low-cost testbed to collect extensive co-located mmWave
signal and other sensors data such as LiDAR (Light Detection and Ranging),
cameras, ultrasonic, etc, traditionally for ``automated'', to facilitate mmWave
vehicular communications. Intuitively, these sensors can build a 3D map around
the vehicle and signal propagation path can be estimated, eliminating iterative
the process via pilot signals. This multimodal data fusion, together with AI,
is expected to bring significant advances in ``connected'' research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE conferences for future publications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detours for Navigating Instructional Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kumar Ashutosh, Zihui Xue, Tushar Nagarajan, Kristen Grauman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the video detours problem for navigating instructional videos.
Given a source video and a natural language query asking to alter the how-to
video's current path of execution in a certain way, the goal is to find a
related ''detour video'' that satisfies the requested alteration. To address
this challenge, we propose VidDetours, a novel video-language approach that
learns to retrieve the targeted temporal segments from a large repository of
how-to's using video-and-text conditioned queries. Furthermore, we devise a
language-based pipeline that exploits how-to video narration text to create
weakly supervised training data. We demonstrate our idea applied to the domain
of how-to cooking videos, where a user can detour from their current recipe to
find steps with alternate ingredients, tools, and techniques. Validating on a
ground truth annotated dataset of 16K samples, we show our model's significant
improvements over best available methods for video retrieval and question
answering, with recall rates exceeding the state of the art by 35%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ aMUSEd: An Open MUSE Reproduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suraj Patil, William Berman, Robin Rombach, Patrick von Platen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present aMUSEd, an open-source, lightweight masked image model (MIM) for
text-to-image generation based on MUSE. With 10 percent of MUSE's parameters,
aMUSEd is focused on fast image generation. We believe MIM is under-explored
compared to latent diffusion, the prevailing approach for text-to-image
generation. Compared to latent diffusion, MIM requires fewer inference steps
and is more interpretable. Additionally, MIM can be fine-tuned to learn
additional styles with only a single image. We hope to encourage further
exploration of MIM by demonstrating its effectiveness on large-scale
text-to-image generation and releasing reproducible training code. We also
release checkpoints for two models which directly produce images at 256x256 and
512x512 resolutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VGA: Vision and Graph Fused Attention Network for Rumor Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Bai, Caiyan Jia, Ziying Song, Chaoqun Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of social media, rumors have been spread broadly on
social media platforms, causing great harm to society. Beside textual
information, many rumors also use manipulated images or conceal textual
information within images to deceive people and avoid being detected, making
multimodal rumor detection be a critical problem. The majority of multimodal
rumor detection methods mainly concentrate on extracting features of source
claims and their corresponding images, while ignoring the comments of rumors
and their propagation structures. These comments and structures imply the
wisdom of crowds and are proved to be crucial to debunk rumors. Moreover, these
methods usually only extract visual features in a basic manner, seldom consider
tampering or textual information in images. Therefore, in this study, we
propose a novel Vision and Graph Fused Attention Network (VGA) for rumor
detection to utilize propagation structures among posts so as to obtain the
crowd opinions and further explore visual tampering features, as well as the
textual information hidden in images. We conduct extensive experiments on three
datasets, demonstrating that VGA can effectively detect multimodal rumors and
outperform state-of-the-art methods significantly.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FullLoRA-AT: Efficiently Boosting the Robustness of <span class="highlight-title">Pretrain</span>ed Vision
  <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Yuan, Jie Zhang, Shiguang Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the Vision Transformer (ViT) model has gradually become
mainstream in various computer vision tasks, and the robustness of the model
has received increasing attention. However, existing large models tend to
prioritize performance during training, potentially neglecting the robustness,
which may lead to serious security concerns. In this paper, we establish a new
challenge: exploring how to use a small number of additional parameters for
adversarial finetuning to quickly and effectively enhance the adversarial
robustness of a standardly trained model. To address this challenge, we develop
the novel LNLoRA module, incorporating a learnable layer normalization before
the conventional LoRA module, which helps mitigate magnitude differences in
parameters between the adversarial and standard training paradigms.
  Furthermore, we propose the FullLoRA-AT framework by integrating the
learnable LNLoRA modules into all key components of ViT-based models while
keeping the pretrained model frozen, which can significantly improve the model
robustness via adversarial finetuning in a parameter-efficient manner.
  Extensive experiments on CIFAR-10, CIFAR-100, and Imagenette demonstrate the
superiority of our proposed FullLoRA-AT framework. It achieves comparable
robustness with full finetuning while only requiring about 5% of the learnable
parameters. This also effectively addresses concerns regarding extra model
storage space and enormous training time caused by adversarial finetuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Robust Semantic Segmentation against Patch-based Attack via
  Attention Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01750v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01750v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Yuan, Jie Zhang, Yude Wang, Shiguang Shan, Xilin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The attention mechanism has been proven effective on various visual tasks in
recent years. In the semantic segmentation task, the attention mechanism is
applied in various methods, including the case of both Convolution Neural
Networks (CNN) and Vision Transformer (ViT) as backbones. However, we observe
that the attention mechanism is vulnerable to patch-based adversarial attacks.
Through the analysis of the effective receptive field, we attribute it to the
fact that the wide receptive field brought by global attention may lead to the
spread of the adversarial patch. To address this issue, in this paper, we
propose a Robust Attention Mechanism (RAM) to improve the robustness of the
semantic segmentation model, which can notably relieve the vulnerability
against patch-based attacks. Compared to the vallina attention mechanism, RAM
introduces two novel modules called Max Attention Suppression and Random
Attention Dropout, both of which aim to refine the attention matrix and limit
the influence of a single adversarial patch on the semantic segmentation
results of other positions. Extensive experiments demonstrate the effectiveness
of our RAM to improve the robustness of semantic segmentation models against
various patch-based attack methods under different attack settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 3 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-shot Image Generation via Information Transfer from the Built
  Geodesic Surface 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuexing Han, Liheng Ruan, Bing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Images generated by most of generative models trained with limited data often
exhibit deficiencies in either fidelity, diversity, or both. One effective
solution to address the limitation is few-shot generative model adaption.
However, the type of approaches typically rely on a large-scale pre-trained
model, serving as a source domain, to facilitate information transfer to the
target domain. In this paper, we propose a method called Information Transfer
from the Built Geodesic Surface (ITBGS), which contains two module: Feature
Augmentation on Geodesic Surface (FAGS); Interpolation and Regularization
(I\&R). With the FAGS module, a pseudo-source domain is created by projecting
image features from the training dataset into the Pre-Shape Space, subsequently
generating new features on the Geodesic surface. Thus, no pre-trained models is
needed for the adaption process during the training of generative models with
FAGS. I\&R module are introduced for supervising the interpolated images and
regularizing their relative distances, respectively, to further enhance the
quality of generated images. Through qualitative and quantitative experiments,
we demonstrate that the proposed method consistently achieves optimal or
comparable results across a diverse range of semantically distinct datasets,
even in extremely few-shot scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-shot Adaptation of Multi-modal Foundation Models: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Liu, Tianshu Zhang, Wenwen Dai, Wenwen Cai Xiaocong Zhou, Delong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal (vision-language) models, such as CLIP, are replacing traditional
supervised pre-training models (e.g., ImageNet-based pre-training) as the new
generation of visual foundation models. These models with robust and aligned
semantic representations learned from billions of internet image-text pairs and
can be applied to various downstream tasks in a zero-shot manner. However, in
some fine-grained domains like medical imaging and remote sensing, the
performance of multi-modal foundation models often leaves much to be desired.
Consequently, many researchers have begun to explore few-shot adaptation
methods for these models, gradually deriving three main technical approaches:
1) prompt-based methods, 2) adapter-based methods, and 3) external
knowledge-based methods. Nevertheless, this rapidly developing field has
produced numerous results without a comprehensive survey to systematically
organize the research progress. Therefore, in this survey, we introduce and
analyze the research advancements in few-shot adaptation methods for
multi-modal models, summarizing commonly used datasets and experimental setups,
and comparing the results of different methods. In addition, due to the lack of
reliable theoretical support for existing methods, we derive the few-shot
adaptation generalization error bound for multi-modal models. The theorem
reveals that the generalization error of multi-modal foundation models is
constrained by three factors: domain gap, model capacity, and sample size.
Based on this, we propose three possible solutions from the following aspects:
1) adaptive domain generalization, 2) adaptive model selection, and 3) adaptive
knowledge utilization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01734v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01734v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Lips, Victor-Louis De Gusseme, Francis wyffels
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assistive robots should be able to wash, fold or iron clothes. However, due
to the variety, deformability and self-occlusions of clothes, creating
general-purpose robot systems for cloth manipulation is challenging. Synthetic
data is a promising direction to improve generalization, though its usability
is often limited by the sim-to-real gap. To advance the use of synthetic data
for cloth manipulation and to enable tasks such as robotic folding, we present
a synthetic data pipeline to train keypoint detectors for almost flattened
cloth items. To test its performance, we have also collected a real-world
dataset. We train detectors for both T-shirts, towels and shorts and obtain an
average precision of 64.3%. Fine-tuning on real-world data improves performance
to 74.2%. Additional insight is provided by discussing various failure modes of
the keypoint detectors and by comparing different approaches to obtain cloth
meshes and materials. We also quantify the remaining sim-to-real gap and argue
that further improvements to the fidelity of cloth assets will be required to
further reduce this gap. The code, dataset and trained models are available
online.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to journal on 20/12</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STAF: 3D Human Mesh Recovery from Video with Spatio-Temporal Alignment
  Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Yao, Hongwen Zhang, Yunlian Sun, Jinhui Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recovery of 3D human mesh from monocular images has significantly been
developed in recent years. However, existing models usually ignore spatial and
temporal information, which might lead to mesh and image misalignment and
temporal discontinuity. For this reason, we propose a novel Spatio-Temporal
Alignment Fusion (STAF) model. As a video-based model, it leverages coherence
clues from human motion by an attention-based Temporal Coherence Fusion Module
(TCFM). As for spatial mesh-alignment evidence, we extract fine-grained local
information through predicted mesh projection on the feature maps. Based on the
spatial features, we further introduce a multi-stage adjacent Spatial Alignment
Fusion Module (SAFM) to enhance the feature representation of the target frame.
In addition to the above, we propose an Average Pooling Module (APM) to allow
the model to focus on the entire input sequence rather than just the target
frame. This method can remarkably improve the smoothness of recovery results
from video. Extensive experiments on 3DPW, MPII3D, and H36M demonstrate the
superiority of STAF. We achieve a state-of-the-art trade-off between precision
and smoothness. Our code and more video results are on the project page
https://yw0208.github.io/staf/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://yw0208.github.io/staf/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lightweight Adaptive Feature De-drifting for Compressed Image
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Peng, Yang Cao, Yuejin Sun, Yang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  JPEG is a widely used compression scheme to efficiently reduce the volume of
transmitted images. The artifacts appear among blocks due to the information
loss, which not only affects the quality of images but also harms the
subsequent high-level tasks in terms of feature drifting. High-level vision
models trained on high-quality images will suffer performance degradation when
dealing with compressed images, especially on mobile devices. Numerous
learning-based JPEG artifact removal methods have been proposed to handle
visual artifacts. However, it is not an ideal choice to use these JPEG artifact
removal methods as a pre-processing for compressed image classification for the
following reasons: 1. These methods are designed for human vision rather than
high-level vision models; 2. These methods are not efficient enough to serve as
pre-processing on resource-constrained devices. To address these issues, this
paper proposes a novel lightweight AFD module to boost the performance of
pre-trained image classification models when facing compressed images. First, a
FDE-Net is devised to generate the spatial-wise FDM in the DCT domain. Next,
the estimated FDM is transmitted to the FE-Net to generate the mapping
relationship between degraded features and corresponding high-quality features.
A simple but effective RepConv block equipped with structural
re-parameterization is utilized in FE-Net, which enriches feature
representation in the training phase while maintaining efficiency in the
deployment phase. After training on limited compressed images, the AFD-Module
can serve as a "plug-and-play" model for pre-trained classification models to
improve their performance on compressed images. Experiments demonstrate that
our proposed AFD module can comprehensively improve the accuracy of the
pre-trained classification models and significantly outperform the existing
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Multimedia 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Local Adaptive Clustering Based Image Matching for Automatic Visual
  Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01720v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01720v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhizhen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monitoring cameras are extensively utilized in industrial production to
monitor equipment running. With advancements in computer vision, device
recognition using image features is viable. This paper presents a
vision-assisted identification system that implements real-time automatic
equipment labeling through image matching in surveillance videos. The system
deploys the ORB algorithm to extract image features and the GMS algorithm to
remove incorrect matching points. According to the principles of clustering and
template locality, a method known as Local Adaptive Clustering (LAC) has been
established to enhance label positioning. This method segments matching
templates using the cluster center, which improves the efficiency and stability
of labels. The experimental results demonstrate that LAC effectively curtails
the label drift.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fact-checking based fake news detection: a <span class="highlight-title">review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzhou Yang, Yangming Zhou, Qichao Ying, Zhenxing Qian, Dan Zeng, Liang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper reviews and summarizes the research results on fact-based fake
news from the perspectives of tasks and problems, algorithm strategies, and
datasets. First, the paper systematically explains the task definition and core
problems of fact-based fake news detection. Second, the paper summarizes the
existing detection methods based on the algorithm principles. Third, the paper
analyzes the classic and newly proposed datasets in the field, and summarizes
the experimental results on each dataset. Finally, the paper summarizes the
advantages and disadvantages of existing methods, proposes several challenges
that methods in this field may face, and looks forward to the next stage of
research. It is hoped that this paper will provide reference for subsequent
work in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Invited short review paper (in Chinese)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WordArt Designer API: User-Driven Artistic Typography Synthesis with
  Large Language Models on ModelScope <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun-Yan He, Zhi-Qi Cheng, Chenyang Li, Jingdong Sun, Wangmeng Xiang, Yusen Hu, Xianhui Lin, Xiaoyang Kang, Zengke Jin, Bin Luo, Yifeng Geng, Xuansong Xie, Jingren Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the WordArt Designer API, a novel framework for
user-driven artistic typography synthesis utilizing Large Language Models
(LLMs) on ModelScope. We address the challenge of simplifying artistic
typography for non-professionals by offering a dynamic, adaptive, and
computationally efficient alternative to traditional rigid templates. Our
approach leverages the power of LLMs to understand and interpret user input,
facilitating a more intuitive design process. We demonstrate through various
case studies how users can articulate their aesthetic preferences and
functional requirements, which the system then translates into unique and
creative typographic designs. Our evaluations indicate significant improvements
in user satisfaction, design flexibility, and creative expression over existing
systems. The WordArt Designer API not only democratizes the art of typography
but also opens up new possibilities for personalized digital communication and
design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Spotlight Paper from the Workshop on Machine Learning for Creativity
  and Design at the 37th Conference on Neural Information Processing Systems
  (NeurIPS 2023). 5 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AID-DTI: Accelerating High-fidelity Diffusion Tensor Imaging with
  Detail-Preserving Model-based Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01693v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01693v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxin Fan, Jian Cheng, Cheng Li, Xinrui Ma, Jing Yang, Juan Zou, Ruoyou Wu, Qiegen Liu, Shanshan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has shown great potential in accelerating diffusion tensor
imaging (DTI). Nevertheless, existing methods tend to suffer from Rician noise
and detail loss in reconstructing the DTI-derived parametric maps especially
when sparsely sampled q-space data are used. This paper proposes a novel
method, AID-DTI (Accelerating hIgh fiDelity Diffusion Tensor Imaging), to
facilitate fast and accurate DTI with only six measurements. AID-DTI is
equipped with a newly designed Singular Value Decomposition (SVD)-based
regularizer, which can effectively capture fine details while suppressing noise
during network training. Experimental results on Human Connectome Project (HCP)
data consistently demonstrate that the proposed method estimates DTI parameter
maps with fine-grained details and outperforms three state-of-the-art methods
both quantitatively and qualitatively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ODTrack: Online Dense Temporal Token Learning for Visual Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaozong Zheng, Bineng Zhong, Qihua Liang, Zhiyi Mo, Shengping Zhang, Xianxian Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online contextual reasoning and association across consecutive video frames
are critical to perceive instances in visual tracking. However, most current
top-performing trackers persistently lean on sparse temporal relationships
between reference and search frames via an offline mode. Consequently, they can
only interact independently within each image-pair and establish limited
temporal correlations. To alleviate the above problem, we propose a simple,
flexible and effective video-level tracking pipeline, named \textbf{ODTrack},
which densely associates the contextual relationships of video frames in an
online token propagation manner. ODTrack receives video frames of arbitrary
length to capture the spatio-temporal trajectory relationships of an instance,
and compresses the discrimination features (localization information) of a
target into a token sequence to achieve frame-to-frame association. This new
solution brings the following benefits: 1) the purified token sequences can
serve as prompts for the inference in the next video frame, whereby past
information is leveraged to guide future inference; 2) the complex online
update strategies are effectively avoided by the iterative propagation of token
sequences, and thus we can achieve more efficient model representation and
computation. ODTrack achieves a new \textit{SOTA} performance on seven
benchmarks, while running at real-time speed. Code and models are available at
\url{https://github.com/GXNU-ZhongLab/ODTrack}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modality Exchange Network for Retinogeniculate Visual Pathway
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01685v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01685v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hua Han, Cheng Li, Lei Xie, Yuanjing Feng, Alou Diakite, Shanshan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate segmentation of the retinogeniculate visual pathway (RGVP) aids in
the diagnosis and treatment of visual disorders by identifying disruptions or
abnormalities within the pathway. However, the complex anatomical structure and
connectivity of RGVP make it challenging to achieve accurate segmentation. In
this study, we propose a novel Modality Exchange Network (ME-Net) that
effectively utilizes multi-modal magnetic resonance (MR) imaging information to
enhance RGVP segmentation. Our ME-Net has two main contributions. Firstly, we
introduce an effective multi-modal soft-exchange technique. Specifically, we
design a channel and spatially mixed attention module to exchange modality
information between T1-weighted and fractional anisotropy MR images. Secondly,
we propose a cross-fusion module that further enhances the fusion of
information between the two modalities. Experimental results demonstrate that
our method outperforms existing state-of-the-art approaches in terms of RGVP
segmentation performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Performance Evaluation of GPS Trajectory Rasterization Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01676v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01676v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Necip Enes Gengec, Ergin Tari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The availability of the Global Positioning System (GPS) trajectory data is
increasing along with the availability of different GPS receivers and with the
increasing use of various mobility services. GPS trajectory is an important
data source which is used in traffic density detection, transport mode
detection, mapping data inferences with the use of different methods such as
image processing and machine learning methods. While the data size increases,
efficient representation of this type of data is becoming difficult to be used
in these methods. A common approach is the representation of GPS trajectory
information such as average speed, bearing, etc. in raster image form and
applying analysis methods. In this study, we evaluate GPS trajectory data
rasterization using the spatial join functions of QGIS, PostGIS+QGIS, and our
iterative spatial structured grid aggregation implementation coded in the
Python programming language. Our implementation is also parallelizable, and
this parallelization is also included as the fourth method. According to the
results of experiment carried out with an example GPS trajectory dataset, QGIS
method and PostGIS+QGIS method showed relatively low performance with respect
to our method using the metric of total processing time. PostGIS+QGIS method
achieved the best results for spatial join though its total performance
decreased quickly while test area size increases. On the other hand, both of
our methods' performances decrease directly proportional to GPS point. And our
methods' performance can be increased proportional to the increase with the
number of processor cores and/or with multiple computing clusters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span> RGBT Tracking with Spatio-Temporal Multimodal Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dengdi Sun, Yajie Pan, Andong Lu, Chenglong Li, Bin Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many RGBT tracking researches primarily focus on modal fusion design, while
overlooking the effective handling of target appearance changes. While some
approaches have introduced historical frames or fuse and replace initial
templates to incorporate temporal information, they have the risk of disrupting
the original target appearance and accumulating errors over time. To alleviate
these limitations, we propose a novel Transformer RGBT tracking approach, which
mixes spatio-temporal multimodal tokens from the static multimodal templates
and multimodal search regions in Transformer to handle target appearance
changes, for robust RGBT tracking. We introduce independent dynamic template
tokens to interact with the search region, embedding temporal information to
address appearance changes, while also retaining the involvement of the initial
static template tokens in the joint feature extraction process to ensure the
preservation of the original reliable target appearance information that
prevent deviations from the target appearance caused by traditional temporal
updates. We also use attention mechanisms to enhance the target features of
multimodal template tokens by incorporating supplementary modal cues, and make
the multimodal search region tokens interact with multimodal dynamic template
tokens via attention mechanisms, which facilitates the conveyance of
multimodal-enhanced target change information. Our module is inserted into the
transformer backbone network and inherits joint feature extraction,
search-template matching, and cross-modal interaction. Extensive experiments on
three RGBT benchmark datasets show that the proposed approach maintains
competitive performance compared to other state-of-the-art tracking algorithms
while running at 39.1 FPS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simultaneous q-Space Sampling Optimization and Reconstruction for Fast
  and High-fidelity Diffusion Magnetic Resonance Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01662v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01662v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Yang, Jian Cheng, Cheng Li, Wenxin Fan, Juan Zou, Ruoyou Wu, Shanshan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion Magnetic Resonance Imaging (dMRI) plays a crucial role in the
noninvasive investigation of tissue microstructural properties and structural
connectivity in the \textit{in vivo} human brain. However, to effectively
capture the intricate characteristics of water diffusion at various directions
and scales, it is important to employ comprehensive q-space sampling.
Unfortunately, this requirement leads to long scan times, limiting the clinical
applicability of dMRI. To address this challenge, we propose SSOR, a
Simultaneous q-Space sampling Optimization and Reconstruction framework. We
jointly optimize a subset of q-space samples using a continuous representation
of spherical harmonic functions and a reconstruction network. Additionally, we
integrate the unique properties of diffusion magnetic resonance imaging (dMRI)
in both the q-space and image domains by applying $l1$-norm and total-variation
regularization. The experiments conducted on HCP data demonstrate that SSOR has
promising strengths both quantitatively and qualitatively and exhibits
robustness to noise.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffYOLO: Object Detection for Anti-Noise via YOLO and Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichen Liu, Huajian Zhang, Daqing Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detection models represented by YOLO series have been widely used and
have achieved great results on the high quality datasets, but not all the
working conditions are ideal. To settle down the problem of locating targets on
low quality datasets, the existing methods either train a new object detection
network, or need a large collection of low-quality datasets to train. However,
we propose a framework in this paper and apply it on the YOLO models called
DiffYOLO. Specifically, we extract feature maps from the denoising diffusion
probabilistic models to enhance the well-trained models, which allows us
fine-tune YOLO on high-quality datasets and test on low-quality datasets. The
results proved this framework can not only prove the performance on noisy
datasets, but also prove the detection results on high-quality test datasets.
We will supplement more experiments later (with various datasets and network
architectures).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AIGCBench: Comprehensive Evaluation of Image-to-Video Content Generated
  by AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01651v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01651v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fanda Fan, Chunjie Luo, Jianfeng Zhan, Wanling Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The burgeoning field of Artificial Intelligence Generated Content (AIGC) is
witnessing rapid advancements, particularly in video generation. This paper
introduces AIGCBench, a pioneering comprehensive and scalable benchmark
designed to evaluate a variety of video generation tasks, with a primary focus
on Image-to-Video (I2V) generation. AIGCBench tackles the limitations of
existing benchmarks, which suffer from a lack of diverse datasets, by including
a varied and open-domain image-text dataset that evaluates different
state-of-the-art algorithms under equivalent conditions. We employ a novel text
combiner and GPT-4 to create rich text prompts, which are then used to generate
images via advanced Text-to-Image models. To establish a unified evaluation
framework for video generation tasks, our benchmark includes 11 metrics
spanning four dimensions to assess algorithm performance. These dimensions are
control-video alignment, motion effects, temporal consistency, and video
quality. These metrics are both reference video-dependent and video-free,
ensuring a comprehensive evaluation strategy. The evaluation standard proposed
correlates well with human judgment, providing insights into the strengths and
weaknesses of current I2V algorithms. The findings from our extensive
experiments aim to stimulate further research and development in the I2V field.
AIGCBench represents a significant step toward creating standardized benchmarks
for the broader AIGC landscape, proposing an adaptable and equitable framework
for future assessments of video generation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ De-Confusing Pseudo-Labels in Source-Free Domain Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Idit Diamant, Idan Achituve, Arnon Netzer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Source-free domain adaptation (SFDA) aims to transfer knowledge learned from
a source domain to an unlabeled target domain, where the source data is
unavailable during adaptation. Existing approaches for SFDA focus on
self-training usually including well-established entropy minimization and
pseudo-labeling techniques. Recent work suggested a co-learning strategy to
improve the quality of the generated target pseudo-labels using robust
pretrained networks such as Swin-B. However, since the generated pseudo-labels
depend on the source model, they may be noisy due to domain shift. In this
paper, we view SFDA from the perspective of label noise learning and learn to
de-confuse the pseudo-labels. More specifically, we learn a noise transition
matrix of the pseudo-labels to capture the label corruption of each class and
learn the underlying true label distribution. Estimating the noise transition
matrix enables a better true class-posterior estimation results with better
prediction accuracy. We demonstrate the effectiveness of our approach applied
with several SFDA methods: SHOT, SHOT++, and AaD. We obtain state-of-the-art
results on three domain adaptation datasets: VisDA, DomainNet, and OfficeHome.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2212.03795</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SIGNeRF: Scene Integrated Generation for Neural Radiance Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01647v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01647v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan-Niklas Dihlmann, Andreas Engelhardt, Hendrik Lensch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in image diffusion models have recently led to notable improvements
in the generation of high-quality images. In combination with Neural Radiance
Fields (NeRFs), they enabled new opportunities in 3D generation. However, most
generative 3D approaches are object-centric and applying them to editing
existing photorealistic scenes is not trivial. We propose SIGNeRF, a novel
approach for fast and controllable NeRF scene editing and scene-integrated
object generation. A new generative update strategy ensures 3D consistency
across the edited images, without requiring iterative optimization. We find
that depth-conditioned diffusion models inherently possess the capability to
generate 3D consistent views by requesting a grid of images instead of single
views. Based on these insights, we introduce a multi-view reference sheet of
modified images. Our method updates an image collection consistently based on
the reference sheet and refines the original NeRF with the newly generated
image set in one go. By exploiting the depth conditioning mechanism of the
image diffusion model, we gain fine control over the spatial location of the
edit and enforce shape guidance by a selected region or an external mesh.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://signerf.jdihlmann.com</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prototypical Information Bottlenecking and Disentangling for Multimodal
  Cancer Survival Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilan Zhang, Yingxue Xu, Jianqi Chen, Fengying Xie, Hao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal learning significantly benefits cancer survival prediction,
especially the integration of pathological images and genomic data. Despite
advantages of multimodal learning for cancer survival prediction, massive
redundancy in multimodal data prevents it from extracting discriminative and
compact information: (1) An extensive amount of intra-modal task-unrelated
information blurs discriminability, especially for gigapixel whole slide images
(WSIs) with many patches in pathology and thousands of pathways in genomic
data, leading to an ``intra-modal redundancy" issue. (2) Duplicated information
among modalities dominates the representation of multimodal data, which makes
modality-specific information prone to being ignored, resulting in an
``inter-modal redundancy" issue. To address these, we propose a new framework,
Prototypical Information Bottlenecking and Disentangling (PIBD), consisting of
Prototypical Information Bottleneck (PIB) module for intra-modal redundancy and
Prototypical Information Disentanglement (PID) module for inter-modal
redundancy. Specifically, a variant of information bottleneck, PIB, is proposed
to model prototypes approximating a bunch of instances for different risk
levels, which can be used for selection of discriminative instances within
modality. PID module decouples entangled multimodal data into compact distinct
components: modality-common and modality-specific knowledge, under the guidance
of the joint prototypical distribution. Extensive experiments on five cancer
benchmark datasets demonstrated our superiority over other methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ S3Net: Innovating Stereo Matching and Semantic Segmentation with a
  Single-Branch Semantic Stereo Network in Satellite Epipolar Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyuan Yang, Guanzhou Chen, Xiaoliang Tan, Tong Wang, Jiaqi Wang, Xiaodong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stereo matching and semantic segmentation are significant tasks in binocular
satellite 3D reconstruction. However, previous studies primarily view these as
independent parallel tasks, lacking an integrated multitask learning framework.
This work introduces a solution, the Single-branch Semantic Stereo Network
(S3Net), which innovatively combines semantic segmentation and stereo matching
using Self-Fuse and Mutual-Fuse modules. Unlike preceding methods that utilize
semantic or disparity information independently, our method dentifies and
leverages the intrinsic link between these two tasks, leading to a more
accurate understanding of semantic information and disparity estimation.
Comparative testing on the US3D dataset proves the effectiveness of our S3Net.
Our model improves the mIoU in semantic segmentation from 61.38 to 67.39, and
reduces the D1-Error and average endpoint error (EPE) in disparity estimation
from 10.051 to 9.579 and 1.439 to 1.403 respectively, surpassing existing
competitive methods. Our codes are available at:https://github.com/CVEO/S3Net.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BLADE: Box-Level Supervised Amodal Segmentation through Directed
  Expansion <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaochen Liu, Zhixuan Li, Tingting Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perceiving the complete shape of occluded objects is essential for human and
machine intelligence. While the amodal segmentation task is to predict the
complete mask of partially occluded objects, it is time-consuming and
labor-intensive to annotate the pixel-level ground truth amodal masks.
Box-level supervised amodal segmentation addresses this challenge by relying
solely on ground truth bounding boxes and instance classes as supervision,
thereby alleviating the need for exhaustive pixel-level annotations.
Nevertheless, current box-level methodologies encounter limitations in
generating low-resolution masks and imprecise boundaries, failing to meet the
demands of practical real-world applications. We present a novel solution to
tackle this problem by introducing a directed expansion approach from visible
masks to corresponding amodal masks. Our approach involves a hybrid end-to-end
network based on the overlapping region - the area where different instances
intersect. Diverse segmentation strategies are applied for overlapping regions
and non-overlapping regions according to distinct characteristics. To guide the
expansion of visible masks, we introduce an elaborately-designed connectivity
loss for overlapping regions, which leverages correlations with visible masks
and facilitates accurate amodal segmentation. Experiments are conducted on
several challenging datasets and the results show that our proposed method can
outperform existing state-of-the-art methods with large margins.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2024;</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context-Aware Interaction Network for RGB-T Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Lv, Zhi Liu, Gongyang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  RGB-T semantic segmentation is a key technique for autonomous driving scenes
understanding. For the existing RGB-T semantic segmentation methods, however,
the effective exploration of the complementary relationship between different
modalities is not implemented in the information interaction between multiple
levels. To address such an issue, the Context-Aware Interaction Network
(CAINet) is proposed for RGB-T semantic segmentation, which constructs
interaction space to exploit auxiliary tasks and global context for explicitly
guided learning. Specifically, we propose a Context-Aware Complementary
Reasoning (CACR) module aimed at establishing the complementary relationship
between multimodal features with the long-term context in both spatial and
channel dimensions. Further, considering the importance of global contextual
and detailed information, we propose the Global Context Modeling (GCM) module
and Detail Aggregation (DA) module, and we introduce specific auxiliary
supervision to explicitly guide the context interaction and refine the
segmentation map. Extensive experiments on two benchmark datasets of MFNet and
PST900 demonstrate that the proposed CAINet achieves state-of-the-art
performance. The code is available at https://github.com/YingLv1106/CAINet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures, Accepted by IEEE Transactions on Multimedia 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">GPT</span>-4V(ision) is a Generalist Web Agent, if Grounded 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, Yu Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent development on large multimodal models (LMMs), especially
GPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries
of multimodal models beyond traditional tasks like image captioning and visual
question answering. In this work, we explore the potential of LMMs like GPT-4V
as a generalist web agent that can follow natural language instructions to
complete tasks on any given website. We propose SEEACT, a generalist web agent
that harnesses the power of LMMs for integrated visual understanding and acting
on the web. We evaluate on the recent MIND2WEB benchmark. In addition to
standard offline evaluation on cached websites, we enable a new online
evaluation setting by developing a tool that allows running web agents on live
websites. We show that GPT-4V presents a great potential for web agents - it
can successfully complete 50% of the tasks on live websites if we manually
ground its textual plans into actions on the websites. This substantially
outperforms text-only LLMs like GPT-4 or smaller models (FLAN-T5 and BLIP-2)
specifically fine-tuned for web agents. However, grounding still remains a
major challenge. Existing LMM grounding strategies like set-of-mark prompting
turns out not effective for web agents, and the best grounding strategy we
develop in this paper leverages both the HTML text and visuals. Yet, there is
still a substantial gap with oracle grounding, leaving ample room for further
improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning <span class="highlight-title">Prompt</span> with Distribution-Based Feature Replay for Few-Shot
  Class-Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zitong Huang, Ze Chen, Zhixing Chen, Erjin Zhou, Xinxing Xu, Rick Siow Mong Goh, Yong Liu, Chunmei Feng, Wangmeng Zuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot Class-Incremental Learning (FSCIL) aims to continuously learn new
classes based on very limited training data without forgetting the old ones
encountered. Existing studies solely relied on pure visual networks, while in
this paper we solved FSCIL by leveraging the Vision-Language model (e.g., CLIP)
and propose a simple yet effective framework, named Learning Prompt with
Distribution-based Feature Replay (LP-DiF). We observe that simply using CLIP
for zero-shot evaluation can substantially outperform the most influential
methods. Then, prompt tuning technique is involved to further improve its
adaptation ability, allowing the model to continually capture specific
knowledge from each session. To prevent the learnable prompt from forgetting
old knowledge in the new session, we propose a pseudo-feature replay approach.
Specifically, we preserve the old knowledge of each class by maintaining a
feature-level Gaussian distribution with a diagonal covariance matrix, which is
estimated by the image features of training images and synthesized features
generated from a VAE. When progressing to a new session, pseudo-features are
sampled from old-class distributions combined with training images of the
current session to optimize the prompt, thus enabling the model to learn new
knowledge while retaining old knowledge. Experiments on three prevalent
benchmarks, i.e., CIFAR100, mini-ImageNet, CUB-200, and two more challenging
benchmarks, i.e., SUN-397 and CUB-200$^*$ proposed in this paper showcase the
superiority of LP-DiF, achieving new state-of-the-art (SOTA) in FSCIL. Code is
publicly available at https://github.com/1170300714/LP-DiF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MLIP: Medical Language-Image <span class="highlight-title">Pre-train</span>ing with Masked Local
  Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarun Liu, Hong-Yu Zhou, Cheng Li, Weijian Huang, Hao Yang, Yong Liang, Shanshan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing contrastive language-image pre-training aims to learn a joint
representation by matching abundant image-text pairs. However, the number of
image-text pairs in medical datasets is usually orders of magnitude smaller
than that in natural datasets. Besides, medical image-text pairs often involve
numerous complex fine-grained correspondences. This paper aims to enhance the
data efficiency by introducing multiple-to-multiple local relationship modeling
to capture denser supervisions. More specifically, we propose a Medical
Language-Image Pre-training (MLIP) framework, which exploits the limited
image-text medical data more efficiently through patch-sentence matching.
Furthermore, we introduce a masked contrastive learning strategy with semantic
integrity estimation to reduce redundancy in images while preserving the
underlying semantics. Our evaluation results show that MLIP outperforms
previous work in zero/few-shot classification and few-shot segmentation tasks
by a large margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-Time Human Fall Detection using a Lightweight Pose Estimation
  Technique 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ekram Alam, Abu Sufian, Paramartha Dutta, Marco Leo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The elderly population is increasing rapidly around the world. There are no
enough caretakers for them. Use of AI-based in-home medical care systems is
gaining momentum due to this. Human fall detection is one of the most important
tasks of medical care system for the aged people. Human fall is a common
problem among elderly people. Detection of a fall and providing medical help as
early as possible is very important to reduce any further complexity. The
chances of death and other medical complications can be reduced by detecting
and providing medical help as early as possible after the fall. There are many
state-of-the-art fall detection techniques available these days, but the
majority of them need very high computing power. In this paper, we proposed a
lightweight and fast human fall detection system using pose estimation. We used
`Movenet' for human joins key-points extraction. Our proposed method can work
in real-time on any low-computing device with any basic camera. All computation
can be processed locally, so there is no problem of privacy of the subject. We
used two datasets `GMDCSA' and `URFD' for the experiment. We got the
sensitivity value of 0.9375 and 0.9167 for the dataset `GMDCSA' and `URFD'
respectively. The source code and the dataset GMDCSA of our work are available
online to access.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing the medical foundation model with multi-scale and
  cross-modality feature learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijian Huang, Cheng Li, Hong-Yu Zhou, Jiarun Liu, Hao Yang, Yong Liang, Shanshan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of multi-modal medical foundation models has attracted
significant attention in the field of medicine and healthcare due to their
promising prospects in various clinical applications. One area of focus in this
research direction is the extractions of features at different scales. While
previous studies have explored feature learning at individual scales,
investigation on integrating the diverse scales and modalities of information
is lacking, which may hinder the potential for mutual reinforcement among these
features. This paper aims to bridge this gap by proposing a method that
effectively exploits multi-scale and cross-modality information to enhance the
performance of medical foundation models. The proposed method simultaneously
exploit features at the local, instance, modality and global aspects,
facilitating comprehensive representation learning within the models. We
evaluate the effectiveness of the proposed method on six open-source datasets
across different clinical tasks, demonstrating its ability to enhance the
performance of medical foundation models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context-Guided Spatio-Temporal Video Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Gu, Heng Fan, Yan Huang, Tiejian Luo, Libo Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatio-temporal video grounding (or STVG) task aims at locating a
spatio-temporal tube for a specific instance given a text query. Despite
advancements, current methods easily suffer the distractors or heavy object
appearance variations in videos due to insufficient object information from the
text, leading to degradation. Addressing this, we propose a novel framework,
context-guided STVG (CG-STVG), which mines discriminative instance context for
object in videos and applies it as a supplementary guidance for target
localization. The key of CG-STVG lies in two specially designed modules,
including instance context generation (ICG), which focuses on discovering
visual context information (in both appearance and motion) of the instance, and
instance context refinement (ICR), which aims to improve the instance context
from ICG by eliminating irrelevant or even harmful information from the
context. During grounding, ICG, together with ICR, are deployed at each
decoding stage of a Transformer architecture for instance context learning.
Particularly, instance context learned from one decoding stage is fed to the
next stage, and leveraged as a guidance containing rich and discriminative
object feature to enhance the target-awareness in decoding feature, which
conversely benefits generating better new instance context for improving
localization finally. Compared to existing methods, CG-STVG enjoys object
information in text query and guidance from mined instance visual context for
more accurate target localization. In our experiments on three benchmarks,
including HCSTVG-v1/-v2 and VidSTG, CG-STVG sets new state-of-the-arts in
m_tIoU and m_vIoU on all of them, showing its efficacy. The code will be
released at https://github.com/HengLan/CGSTVG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Test-Time Personalization with Meta <span class="highlight-title">Prompt</span> for Gaze Estimation <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01577v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01577v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huan Liu, Julia Qi, Zhenhao Li, Mohammad Hassanpour, Yang Wang, Konstantinos Plataniotis, Yuanhao Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the recent remarkable achievement in gaze estimation, efficient and
accurate personalization of gaze estimation without labels is a practical
problem but rarely touched on in the literature. To achieve efficient
personalization, we take inspiration from the recent advances in Natural
Language Processing (NLP) by updating a negligible number of parameters,
"prompts", at the test time. Specifically, the prompt is additionally attached
without perturbing original network and can contain less than 1% of a
ResNet-18's parameters. Our experiments show high efficiency of the prompt
tuning approach. The proposed one can be 10 times faster in terms of adaptation
speed than the methods compared. However, it is non-trivial to update the
prompt for personalized gaze estimation without labels. At the test time, it is
essential to ensure that the minimizing of particular unsupervised loss leads
to the goals of minimizing gaze estimation error. To address this difficulty,
we propose to meta-learn the prompt to ensure that its updates align with the
goal. Our experiments show that the meta-learned prompt can be effectively
adapted even with a simple symmetry loss. In addition, we experiment on four
cross-dataset validations to show the remarkable advantages of the proposed
method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Generalization of Invisible Facial Privacy Cloak via Gradient
  Accumulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01575v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01575v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuannan Liu, Yaoyao Zhong, Weihong Deng, Hongzhi Shi, Xingchen Cui, Yunfeng Yin, Dongchao Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The blooming of social media and face recognition (FR) systems has increased
people's concern about privacy and security. A new type of adversarial privacy
cloak (class-universal) can be applied to all the images of regular users, to
prevent malicious FR systems from acquiring their identity information. In this
work, we discover the optimization dilemma in the existing methods -- the local
optima problem in large-batch optimization and the gradient information
elimination problem in small-batch optimization. To solve these problems, we
propose Gradient Accumulation (GA) to aggregate multiple small-batch gradients
into a one-step iterative gradient to enhance the gradient stability and reduce
the usage of quantization operations. Experiments show that our proposed method
achieves high performance on the Privacy-Commons dataset against black-box face
recognition models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Transformer</span>-Based Adaptive Semantic Aggregation Method for UAV Visual
  Geo-Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shishen Li, Cuiwei Liu, Huaijun Qiu, Zhaokui Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the task of Unmanned Aerial Vehicles (UAV) visual
geo-localization, which aims to match images of the same geographic target
taken by different platforms, i.e., UAVs and satellites. In general, the key to
achieving accurate UAV-satellite image matching lies in extracting visual
features that are robust against viewpoint changes, scale variations, and
rotations. Current works have shown that part matching is crucial for UAV
visual geo-localization since part-level representations can capture image
details and help to understand the semantic information of scenes. However, the
importance of preserving semantic characteristics in part-level representations
is not well discussed. In this paper, we introduce a transformer-based adaptive
semantic aggregation method that regards parts as the most representative
semantics in an image. Correlations of image patches to different parts are
learned in terms of the transformer's feature map. Then our method decomposes
part-level features into an adaptive sum of all patch features. By doing this,
the learned parts are encouraged to focus on patches with typical semantics.
Extensive experiments on the University-1652 dataset have shown the superiority
of our method over the current works.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ View Distribution Alignment with Progressive Adversarial Learning for
  UAV Visual Geo-Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01573v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01573v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cuiwei Liu, Jiahao Liu, Huaijun Qiu, Zhaokui Li, Xiangbin Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unmanned Aerial Vehicle (UAV) visual geo-localization aims to match images of
the same geographic target captured from different views, i.e., the UAV view
and the satellite view. It is very challenging due to the large appearance
differences in UAV-satellite image pairs. Previous works map images captured by
UAVs and satellites to a shared feature space and employ a classification
framework to learn location-dependent features while neglecting the overall
distribution shift between the UAV view and the satellite view. In this paper,
we address these limitations by introducing distribution alignment of the two
views to shorten their distance in a common space. Specifically, we propose an
end-to-end network, called PVDA (Progressive View Distribution Alignment).
During training, feature encoder, location classifier, and view discriminator
are jointly optimized by a novel progressive adversarial learning strategy.
Competition between feature encoder and view discriminator prompts both of them
to be stronger. It turns out that the adversarial learning is progressively
emphasized until UAV-view images are indistinguishable from satellite-view
images. As a result, the proposed PVDA becomes powerful in learning
location-dependent yet view-invariant features with good scalability towards
unseen images of new locations. Compared to the state-of-the-art methods, the
proposed PVDA requires less inference time but has achieved superior
performance on the University-1652 dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AttentionLut: Attention Fusion-based Canonical Polyadic LUT for
  Real-time Image Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kang Fu, Yicong Peng, Zicheng Zhang, Qihang Xu, Xiaohong Liu, Jia Wang, Guangtao Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, many algorithms have employed image-adaptive lookup tables (LUTs)
to achieve real-time image enhancement. Nonetheless, a prevailing trend among
existing methods has been the employment of linear combinations of basic LUTs
to formulate image-adaptive LUTs, which limits the generalization ability of
these methods. To address this limitation, we propose a novel framework named
AttentionLut for real-time image enhancement, which utilizes the attention
mechanism to generate image-adaptive LUTs. Our proposed framework consists of
three lightweight modules. We begin by employing the global image context
feature module to extract image-adaptive features. Subsequently, the attention
fusion module integrates the image feature with the priori attention feature
obtained during training to generate image-adaptive canonical polyadic tensors.
Finally, the canonical polyadic reconstruction module is deployed to
reconstruct image-adaptive residual 3DLUT, which is subsequently utilized for
enhancing input images. Experiments on the benchmark MIT-Adobe FiveK dataset
demonstrate that the proposed method achieves better enhancement performance
quantitatively and qualitatively than the state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One-Step Late Fusion Multi-view Clustering with Compressed Subspace <span class="chip">ICASSP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiyuan Ou, Pei Zhang, Sihang Zhou, En Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Late fusion multi-view clustering (LFMVC) has become a rapidly growing class
of methods in the multi-view clustering (MVC) field, owing to its excellent
computational speed and clustering performance. One bottleneck faced by
existing late fusion methods is that they are usually aligned to the average
kernel function, which makes the clustering performance highly dependent on the
quality of datasets. Another problem is that they require subsequent k-means
clustering after obtaining the consensus partition matrix to get the final
discrete labels, and the resulting separation of the label learning and cluster
structure optimization processes limits the integrity of these models. To
address the above issues, we propose an integrated framework named One-Step
Late Fusion Multi-view Clustering with Compressed Subspace (OS-LFMVC-CS).
Specifically, we use the consensus subspace to align the partition matrix while
optimizing the partition fusion, and utilize the fused partition matrix to
guide the learning of discrete labels. A six-step iterative optimization
approach with verified convergence is proposed. Sufficient experiments on
multiple datasets validate the effectiveness and efficiency of our proposed
method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-modal Learning with Missing Modality in Predicting Axillary Lymph
  Node Metastasis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01553v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01553v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shichuan Zhang, Sunyi Zheng, Zhongyi Shui, Honglin Li, Lin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal Learning has attracted widespread attention in medical image
analysis. Using multi-modal data, whole slide images (WSIs) and clinical
information, can improve the performance of deep learning models in the
diagnosis of axillary lymph node metastasis. However, clinical information is
not easy to collect in clinical practice due to privacy concerns, limited
resources, lack of interoperability, etc. Although patient selection can ensure
the training set to have multi-modal data for model development, missing
modality of clinical information can appear during test. This normally leads to
performance degradation, which limits the use of multi-modal models in the
clinic. To alleviate this problem, we propose a bidirectional distillation
framework consisting of a multi-modal branch and a single-modal branch. The
single-modal branch acquires the complete multi-modal knowledge from the
multi-modal branch, while the multi-modal learns the robust features of WSI
from the single-modal. We conduct experiments on a public dataset of Lymph Node
Metastasis in Early Breast Cancer to validate the method. Our approach not only
achieves state-of-the-art performance with an AUC of 0.861 on the test set
without missing data, but also yields an AUC of 0.842 when the rate of missing
modality is 80\%. This shows the effectiveness of the approach in dealing with
multi-modal data and missing modality. Such a model has the potential to
improve treatment decision-making for early breast cancer patients who have
axillary lymph node metastatic status.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CRA-PCN: Point Cloud Completion with Intra- and Inter-level
  Cross-Resolution <span class="highlight-title">Transformer</span>s <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Rong, Haoran Zhou, Lixin Yuan, Cheng Mei, Jiahao Wang, Tong Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point cloud completion is an indispensable task for recovering complete point
clouds due to incompleteness caused by occlusion, limited sensor resolution,
etc. The family of coarse-to-fine generation architectures has recently
exhibited great success in point cloud completion and gradually became
mainstream. In this work, we unveil one of the key ingredients behind these
methods: meticulously devised feature extraction operations with explicit
cross-resolution aggregation. We present Cross-Resolution Transformer that
efficiently performs cross-resolution aggregation with local attention
mechanisms. With the help of our recursive designs, the proposed operation can
capture more scales of features than common aggregation operations, which is
beneficial for capturing fine geometric characteristics. While prior
methodologies have ventured into various manifestations of inter-level
cross-resolution aggregation, the effectiveness of intra-level one and their
combination has not been analyzed. With unified designs, Cross-Resolution
Transformer can perform intra- or inter-level cross-resolution aggregation by
switching inputs. We integrate two forms of Cross-Resolution Transformers into
one up-sampling block for point generation, and following the coarse-to-fine
manner, we construct CRA-PCN to incrementally predict complete shapes with
stacked up-sampling blocks. Extensive experiments demonstrate that our method
outperforms state-of-the-art methods by a large margin on several widely used
benchmarks. Codes are available at https://github.com/EasyRy/CRA-PCN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting of Implicit Neural Representation-based Image Denoiser <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01548v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01548v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zipei Yan, Zhengji Liu, Jizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit Neural Representation (INR) has emerged as an effective method for
unsupervised image denoising. However, INR models are typically
overparameterized; consequently, these models are prone to overfitting during
learning, resulting in suboptimal results, even noisy ones. To tackle this
problem, we propose a general recipe for regularizing INR models in image
denoising. In detail, we propose to iteratively substitute the supervision
signal with the mean value derived from both the prediction and supervision
signal during the learning process. We theoretically prove that such a simple
iterative substitute can gradually enhance the signal-to-noise ratio of the
supervision signal, thereby benefiting INR models during the learning process.
Our experimental results demonstrate that INR models can be effectively
regularized by the proposed approach, relieving overfitting and boosting image
denoising performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2024, code: https://github.com/TIDS-Lab/ITS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DDN-SLAM: Real-time Dense Dynamic Neural Implicit SLAM with Joint
  Semantic Encoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingrui Li, Jiaming He, Guangan Jiang, Hongyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose DDN-SLAM, a real-time dense neural implicit semantic SLAM system
designed for dynamic scenes. While existing neural implicit SLAM systems
perform well in static scenes, they often encounter challenges in real-world
environments with dynamic interferences, leading to ineffective tracking and
mapping. DDN-SLAM utilizes the priors provided by the deep semantic system,
combined with conditional probability fields, for segmentation.By constructing
depth-guided static masks and employing joint multi-resolution hashing
encoding, we ensure fast hole filling and high-quality mapping while mitigating
the effects of dynamic information interference. To enhance tracking
robustness, we utilize sparse feature points validated with optical flow and
keyframes, enabling loop closure detection and global bundle optimization.
Furthermore, DDN-SLAM supports monocular, stereo, and RGB-D inputs, operating
robustly at a frequency of 20-30Hz. Extensive experiments on 6 virtual/real
datasets demonstrate that our method outperforms state-of-the-art approaches in
both dynamic and static scenes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11pages, 4figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collaborative Perception for Connected and Autonomous Driving:
  Challenges, Possible Solutions and Opportunities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01544v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01544v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Senkang Hu, Zhengru Fang, Yiqin Deng, Xianhao Chen, Yuguang Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving has attracted significant attention from both academia and
industries, which is expected to offer a safer and more efficient driving
system. However, current autonomous driving systems are mostly based on a
single vehicle, which has significant limitations which still poses threats to
driving safety. Collaborative perception with connected and autonomous vehicles
(CAVs) shows a promising solution to overcoming these limitations. In this
article, we first identify the challenges of collaborative perception, such as
data sharing asynchrony, data volume, and pose errors. Then, we discuss the
possible solutions to address these challenges with various technologies, where
the research opportunities are also elaborated. Furthermore, we propose a
scheme to deal with communication efficiency and latency problems, which is a
channel-aware collaborative perception framework to dynamically adjust the
communication graph and minimize latency, thereby improving perception
performance while increasing communication efficiency. Finally, we conduct
experiments to demonstrate the effectiveness of our proposed scheme.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retraining-free Model Quantization via One-Shot Weight-Coupling Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01543v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01543v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Tang, Yuan Meng, Jiacheng Jiang, Shuzhao Xie, Rongwei Lu, Xinzhu Ma, Zhi Wang, Wenwu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantization is of significance for compressing the over-parameterized deep
neural models and deploying them on resource-limited devices. Fixed-precision
quantization suffers from performance drop due to the limited numerical
representation ability. Conversely, mixed-precision quantization (MPQ) is
advocated to compress the model effectively by allocating heterogeneous
bit-width for layers. MPQ is typically organized into a searching-retraining
two-stage process. Previous works only focus on determining the optimal
bit-width configuration in the first stage efficiently, while ignoring the
considerable time costs in the second stage. However, retraining always
consumes hundreds of GPU-hours on the cutting-edge GPUs, thus hindering
deployment efficiency significantly. In this paper, we devise a one-shot
training-searching paradigm for mixed-precision model compression.
Specifically, in the first stage, all potential bit-width configurations are
coupled and thus optimized simultaneously within a set of shared weights.
However, our observations reveal a previously unseen and severe bit-width
interference phenomenon among highly coupled weights during optimization,
leading to considerable performance degradation under a high compression ratio.
To tackle this problem, we first design a bit-width scheduler to dynamically
freeze the most turbulent bit-width of layers during training, to ensure the
rest bit-widths converged properly. Then, taking inspiration from information
theory, we present an information distortion mitigation technique to align the
behaviour of the bad-performing bit-widths to the well-performing ones.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DDPM based X-ray Image Synthesizer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Praveen Mahaulpatha, Thulana Abeywardane, Tomson George
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Access to high-quality datasets in the medical industry limits machine
learning model performance. To address this issue, we propose a Denoising
Diffusion Probabilistic Model (DDPM) combined with a UNet architecture for
X-ray image synthesis. Focused on pneumonia medical condition, our methodology
employs over 3000 pneumonia X-ray images obtained from Kaggle for training.
Results demonstrate the effectiveness of our approach, as the model
successfully generated realistic images with low Mean Squared Error (MSE). The
synthesized images showed distinct differences from non-pneumonia images,
highlighting the model's ability to capture key features of positive cases.
Beyond pneumonia, the applications of this synthesizer extend to various
medical conditions, provided an ample dataset is available. The capability to
produce high-quality images can potentially enhance machine learning models'
performance, aiding in more accurate and efficient medical diagnoses. This
innovative DDPM-based X-ray photo synthesizer presents a promising avenue for
addressing the scarcity of positive medical image datasets, paving the way for
improved medical image analysis and diagnosis in the healthcare industry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Glance and Focus: Memory <span class="highlight-title">Prompt</span>ing for Multi-Event Video Question
  Answering <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01529v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01529v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyi Bai, Ruiping Wang, Xilin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video Question Answering (VideoQA) has emerged as a vital tool to evaluate
agents' ability to understand human daily behaviors. Despite the recent success
of large vision language models in many multi-modal tasks, complex situation
reasoning over videos involving multiple human-object interaction events still
remains challenging. In contrast, humans can easily tackle it by using a series
of episode memories as anchors to quickly locate question-related key moments
for reasoning. To mimic this effective reasoning strategy, we propose the
Glance-Focus model. One simple way is to apply an action detection model to
predict a set of actions as key memories. However, these actions within a
closed set vocabulary are hard to generalize to various video domains. Instead
of that, we train an Encoder-Decoder to generate a set of dynamic event
memories at the glancing stage. Apart from using supervised bipartite matching
to obtain the event memories, we further design an unsupervised memory
generation method to get rid of dependence on event annotations. Next, at the
focusing stage, these event memories act as a bridge to establish the
correlation between the questions with high-level event concepts and low-level
lengthy video content. Given the question, the model first focuses on the
generated key event memory, then focuses on the most relevant moment for
reasoning through our designed multi-level cross-attention mechanism. We
conduct extensive experiments on four Multi-Event VideoQA benchmarks including
STAR, EgoTaskQA, AGQA, and NExT-QA. Our proposed model achieves
state-of-the-art results, surpassing current large models in various
challenging reasoning tasks. The code and models are available at
https://github.com/ByZ0e/Glance-Focus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal <span class="highlight-title">self-supervised</span> learning for lesion localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Yang, Hong-Yu Zhou, Cheng Li, Weijian Huang, Jiarun Liu, Yong Liang, Shanshan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal deep learning utilizing imaging and diagnostic reports has made
impressive progress in the field of medical imaging diagnostics, demonstrating
a particularly strong capability for auxiliary diagnosis in cases where
sufficient annotation information is lacking. Nonetheless, localizing diseases
accurately without detailed positional annotations remains a challenge.
Although existing methods have attempted to utilize local information to
achieve fine-grained semantic alignment, their capability in extracting the
fine-grained semantics of the comprehensive contextual within reports is
limited. To solve this problem, we introduce a new method that takes full
sentences from textual reports as the basic units for local semantic alignment.
Our approach combines chest X-ray images with their corresponding textual
reports, performing contrastive learning at both global and local levels. The
leading results obtained by our method on multiple datasets confirm its
efficacy in the task of lesion localization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LORE++: Logical Location Regression Network for Table Structure
  Recognition with <span class="highlight-title">Pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rujiao Long, Hangdi Xing, Zhibo Yang, Qi Zheng, Zhi Yu, Cong Yao, Fei Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Table structure recognition (TSR) aims at extracting tables in images into
machine-understandable formats. Recent methods solve this problem by predicting
the adjacency relations of detected cell boxes or learning to directly generate
the corresponding markup sequences from the table images. However, existing
approaches either count on additional heuristic rules to recover the table
structures, or face challenges in capturing long-range dependencies within
tables, resulting in increased complexity. In this paper, we propose an
alternative paradigm. We model TSR as a logical location regression problem and
propose a new TSR framework called LORE, standing for LOgical location
REgression network, which for the first time regresses logical location as well
as spatial location of table cells in a unified network. Our proposed LORE is
conceptually simpler, easier to train, and more accurate than other paradigms
of TSR. Moreover, inspired by the persuasive success of pre-trained models on a
number of computer vision and natural language processing tasks, we propose two
pre-training tasks to enrich the spatial and logical representations at the
feature level of LORE, resulting in an upgraded version called LORE++. The
incorporation of pre-training in LORE++ has proven to enjoy significant
advantages, leading to a substantial enhancement in terms of accuracy,
generalization, and few-shot capability compared to its predecessor.
Experiments on standard benchmarks against methods of previous paradigms
demonstrate the superiority of LORE++, which highlights the potential and
promising prospect of the logical location regression paradigm for TSR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2303.03730</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ S$^{2}$-DMs:Skip-Step Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixuan Wang, Shuangyin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have emerged as powerful generative tools, rivaling GANs in
sample quality and mirroring the likelihood scores of autoregressive models. A
subset of these models, exemplified by DDIMs, exhibit an inherent asymmetry:
they are trained over $T$ steps but only sample from a subset of $T$ during
generation. This selective sampling approach, though optimized for speed,
inadvertently misses out on vital information from the unsampled steps, leading
to potential compromises in sample quality. To address this issue, we present
the S$^{2}$-DMs, which is a new training method by using an innovative
$L_{skip}$, meticulously designed to reintegrate the information omitted during
the selective sampling phase. The benefits of this approach are manifold: it
notably enhances sample quality, is exceptionally simple to implement, requires
minimal code modifications, and is flexible enough to be compatible with
various sampling algorithms. On the CIFAR10 dataset, models trained using our
algorithm showed an improvement of 3.27% to 14.06% over models trained with
traditional methods across various sampling algorithms (DDIMs, PNDMs, DEIS) and
different numbers of sampling steps (10, 20, ..., 1000). On the CELEBA dataset,
the improvement ranged from 8.97% to 27.08%. Access to the code and additional
resources is provided in the github.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Answering from Sure to Uncertain: Uncertainty-Aware Curriculum Learning
  for Video Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01510v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01510v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haopeng Li, Qiuhong Ke, Mingming Gong, Tom Drummond
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While significant advancements have been made in video question answering
(VideoQA), the potential benefits of enhancing model generalization through
tailored difficulty scheduling have been largely overlooked in existing
research. This paper seeks to bridge that gap by incorporating VideoQA into a
curriculum learning (CL) framework that progressively trains models from
simpler to more complex data. Recognizing that conventional self-paced CL
methods rely on training loss for difficulty measurement, which might not
accurately reflect the intricacies of video-question pairs, we introduce the
concept of uncertainty-aware CL. Here, uncertainty serves as the guiding
principle for dynamically adjusting the difficulty. Furthermore, we address the
challenge posed by uncertainty by presenting a probabilistic modeling approach
for VideoQA. Specifically, we conceptualize VideoQA as a stochastic computation
graph, where the hidden representations are treated as stochastic variables.
This yields two distinct types of uncertainty: one related to the inherent
uncertainty in the data and another pertaining to the model's confidence. In
practice, we seamlessly integrate the VideoQA model into our framework and
conduct comprehensive experiments. The findings affirm that our approach not
only achieves enhanced performance but also effectively quantifies uncertainty
in the context of VideoQA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex
  and Professional Sports 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01505v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01505v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haopeng Li, Andong Deng, Qiuhong Ke, Jun Liu, Hossein Rahmani, Yulan Guo, Bernt Schiele, Chen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reasoning over sports videos for question answering is an important task with
numerous applications, such as player training and information retrieval.
However, this task has not been explored due to the lack of relevant datasets
and the challenging nature it presents. Most datasets for video question
answering (VideoQA) focus mainly on general and coarse-grained understanding of
daily-life videos, which is not applicable to sports scenarios requiring
professional action understanding and fine-grained motion analysis. In this
paper, we introduce the first dataset, named Sports-QA, specifically designed
for the sports VideoQA task. The Sports-QA dataset includes various types of
questions, such as descriptions, chronologies, causalities, and counterfactual
conditions, covering multiple sports. Furthermore, to address the
characteristics of the sports VideoQA task, we propose a new Auto-Focus
Transformer (AFT) capable of automatically focusing on particular scales of
temporal information for question answering. We conduct extensive experiments
on Sports-QA, including baseline studies and the evaluation of different
methods. The results demonstrate that our AFT achieves state-of-the-art
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Pixel to Slide image: Polarization Modality-based Pathological
  Diagnosis Using Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia Dong, Yao Yao, Yang Dong, Hui Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Thyroid cancer is the most common endocrine malignancy, and accurately
distinguishing between benign and malignant thyroid tumors is crucial for
developing effective treatment plans in clinical practice. Pathologically,
thyroid tumors pose diagnostic challenges due to improper specimen sampling. In
this study, we have designed a three-stage model using representation learning
to integrate pixel-level and slice-level annotations for distinguishing thyroid
tumors. This structure includes a pathology structure recognition method to
predict structures related to thyroid tumors, an encoder-decoder network to
extract pixel-level annotation information by learning the feature
representations of image blocks, and an attention-based learning mechanism for
the final classification task. This mechanism learns the importance of
different image blocks in a pathological region, globally considering the
information from each block. In the third stage, all information from the image
blocks in a region is aggregated using attention mechanisms, followed by
classification to determine the category of the region. Experimental results
demonstrate that our proposed method can predict microscopic structures more
accurately. After color-coding, the method achieves results on unstained
pathology slides that approximate the quality of Hematoxylin and eosin
staining, reducing the need for stained pathology slides. Furthermore, by
leveraging the concept of indirect measurement and extracting polarized
features from structures correlated with lesions, the proposed method can also
classify samples where membrane structures cannot be obtained through sampling,
providing a potential objective and highly accurate indirect diagnostic
technique for thyroid tumors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Incorporating Geo-Diverse Knowledge into <span class="highlight-title">Prompt</span>ing for Increased
  Geographical Robustness in Object Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyle Buettner, Sina Malakouti, Xiang Lorraine Li, Adriana Kovashka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing object recognition models have been shown to lack robustness in
diverse geographical scenarios due to significant domain shifts in design and
context. Class representations need to be adapted to more accurately reflect an
object concept under these shifts. In the absence of training data from target
geographies, we hypothesize that geography-specific descriptive knowledge of
object categories can be leveraged to enhance robustness. For this purpose, we
explore the feasibility of probing a large-language model for
geography-specific object knowledge, and we investigate integrating knowledge
in zero-shot and learnable soft prompting with the CLIP vision-language model.
In particular, we propose a geography knowledge regularization method to ensure
that soft prompts trained on a source set of geographies generalize to an
unseen target set of geographies. Our gains on DollarStreet when generalizing
from a model trained only on data from Europe are as large as +2.8 on countries
from Africa, and +4.6 on the hardest classes. We further show competitive
performance vs. few-shot target training, and provide insights into how
descriptive knowledge captures geographical differences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Token Propagation Controller for Efficient Vision <span class="highlight-title">Transformer</span> <span class="chip">WACV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision transformers (ViTs) have achieved promising results on a variety of
Computer Vision tasks, however their quadratic complexity in the number of
input tokens has limited their application specially in resource-constrained
settings. Previous approaches that employ gradual token reduction to address
this challenge assume that token redundancy in one layer implies redundancy in
all the following layers. We empirically demonstrate that this assumption is
often not correct, i.e., tokens that are redundant in one layer can be useful
in later layers. We employ this key insight to propose a novel token
propagation controller (TPC) that incorporates two different
token-distributions, i.e., pause probability and restart probability to control
the reduction and reuse of tokens respectively, which results in more efficient
token utilization. To improve the estimates of token distributions, we propose
a smoothing mechanism that acts as a regularizer and helps remove noisy
outliers. Furthermore, to improve the training-stability of our proposed TPC,
we introduce a model stabilizer that is able to implicitly encode local image
structures and minimize accuracy fluctuations during model training. We present
extensive experimental results on the ImageNet-1K dataset using DeiT, LV-ViT
and Swin models to demonstrate the effectiveness of our proposed method. For
example, compared to baseline models, our proposed method improves the
inference speed of the DeiT-S by 250% while increasing the classification
accuracy by 1.0%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WACV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GPS-SSL: Guided Positive Sampling to Inject Prior Into <span class="highlight-title">Self-Supervised</span>
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aarash Feizi, Randall Balestriero, Adriana Romero-Soriano, Reihaneh Rabbany
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Guided Positive Sampling Self-Supervised Learning (GPS-SSL), a
general method to inject a priori knowledge into Self-Supervised Learning (SSL)
positive samples selection. Current SSL methods leverage Data-Augmentations
(DA) for generating positive samples and incorporate prior knowledge - an
incorrect, or too weak DA will drastically reduce the quality of the learned
representation. GPS-SSL proposes instead to design a metric space where
Euclidean distances become a meaningful proxy for semantic relationship. In
that space, it is now possible to generate positive samples from nearest
neighbor sampling. Any prior knowledge can now be embedded into that metric
space independently from the employed DA. From its simplicity, GPS-SSL is
applicable to any SSL method, e.g. SimCLR or BYOL. A key benefit of GPS-SSL is
in reducing the pressure in tailoring strong DAs. For example GPS-SSL reaches
85.58% on Cifar10 with weak DA while the baseline only reaches 37.51%. We
therefore move a step forward towards the goal of making SSL less reliant on
DA. We also show that even when using strong DAs, GPS-SSL outperforms the
baselines on under-studied domains. We evaluate GPS-SSL along with multiple
baseline SSL methods on numerous downstream datasets from different domains
when the models use strong or minimal data augmentations. We hope that GPS-SSL
will open new avenues in studying how to inject a priori knowledge into SSL in
a principled manner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AUPIMO: Redefining Visual Anomaly Detection Benchmarks with High Speed
  and Low Tolerance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01984v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01984v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joao P. C. Bertoldo, Dick Ameln, Ashwin Vaidya, Samet Akçay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in visual anomaly detection research have seen AUROC and
AUPRO scores on public benchmark datasets such as MVTec and VisA converge
towards perfect recall, giving the impression that these benchmarks are
near-solved. However, high AUROC and AUPRO scores do not always reflect
qualitative performance, which limits the validity of these metrics in
real-world applications. We argue that the artificial ceiling imposed by the
lack of an adequate evaluation metric restrains progression of the field, and
it is crucial that we revisit the evaluation metrics used to rate our
algorithms. In response, we introduce Per-IMage Overlap (PIMO), a novel metric
that addresses the shortcomings of AUROC and AUPRO. PIMO retains the
recall-based nature of the existing metrics but introduces two distinctions:
the assignment of curves (and respective area under the curve) is per-image,
and its X-axis relies solely on normal images. Measuring recall per image
simplifies instance score indexing and is more robust to noisy annotations. As
we show, it also accelerates computation and enables the usage of statistical
tests to compare models. By imposing low tolerance for false positives on
normal images, PIMO provides an enhanced model validation procedure and
highlights performance variations across datasets. Our experiments demonstrate
that PIMO offers practical advantages and nuanced performance insights that
redefine anomaly detection benchmarks -- notably challenging the perception
that MVTec AD and VisA datasets have been solved by contemporary models.
Available on GitHub: https://github.com/jpcbertoldo/aupimo.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as
  Programmers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksandar Stanić, Sergi Caelles, Michael Tschannen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual reasoning is dominated by end-to-end neural networks scaled to
billions of model parameters and training examples. However, even the largest
models struggle with compositional reasoning, generalization, fine-grained
spatial and temporal reasoning, and counting. Visual reasoning with large
language models (LLMs) as controllers can, in principle, address these
limitations by decomposing the task and solving subtasks by orchestrating a set
of (visual) tools. Recently, these models achieved great performance on tasks
such as compositional visual question answering, visual grounding, and video
temporal reasoning. Nevertheless, in their current form, these models heavily
rely on human engineering of in-context examples in the prompt, which are often
dataset- and task-specific and require significant labor by highly skilled
programmers. In this work, we present a framework that mitigates these issues
by introducing spatially and temporally abstract routines and by leveraging a
small number of labeled examples to automatically generate in-context examples,
thereby avoiding human-created in-context examples. On a number of visual
reasoning tasks, we show that our framework leads to consistent gains in
performance, makes LLMs as controllers setup more robust, and removes the need
for human engineering of in-context examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FMGS: Foundation Model Embedded 3D Gaussian Splatting for Holistic 3D
  Scene Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01970v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01970v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingxing Zuo, Pouya Samangouei, Yunwen Zhou, Yan Di, Mingyang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precisely perceiving the geometric and semantic properties of real-world 3D
objects is crucial for the continued evolution of augmented reality and robotic
applications. To this end, we present \algfull{} (\algname{}), which
incorporates vision-language embeddings of foundation models into 3D Gaussian
Splatting (GS). The key contribution of this work is an efficient method to
reconstruct and represent 3D vision-language models. This is achieved by
distilling feature maps generated from image-based foundation models into those
rendered from our 3D model. To ensure high-quality rendering and fast training,
we introduce a novel scene representation by integrating strengths from both GS
and multi-resolution hash encodings (MHE). Our effective training procedure
also introduces a pixel alignment loss that makes the rendered feature distance
of same semantic entities close, following the pixel-level semantic boundaries.
Our results demonstrate remarkable multi-view semantic consistency,
facilitating diverse downstream tasks, beating state-of-the-art methods by
$\mathbf{10.2}$ percent on open-vocabulary language-based object detection,
despite that we are $\mathbf{851\times}$ faster for inference. This research
explores the intersection of vision, language, and 3D scene representation,
paving the way for enhanced scene understanding in uncontrolled real-world
environments. We plan to release the code upon paper acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, Project page coming soon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Instruct-Imagen: Image Generation with Multi-modal Instruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01952v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01952v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hexiang Hu, Kelvin C. K. Chan, Yu-Chuan Su, Wenhu Chen, Yandong Li, Kihyuk Sohn, Yang Zhao, Xue Ben, Boqing Gong, William Cohen, Ming-Wei Chang, Xuhui Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents instruct-imagen, a model that tackles heterogeneous image
generation tasks and generalizes across unseen tasks. We introduce *multi-modal
instruction* for image generation, a task representation articulating a range
of generation intents with precision. It uses natural language to amalgamate
disparate modalities (e.g., text, edge, style, subject, etc.), such that
abundant generation intents can be standardized in a uniform format.
  We then build instruct-imagen by fine-tuning a pre-trained text-to-image
diffusion model with a two-stage framework. First, we adapt the model using the
retrieval-augmented training, to enhance model's capabilities to ground its
generation on external multimodal context. Subsequently, we fine-tune the
adapted model on diverse image generation tasks that requires vision-language
understanding (e.g., subject-driven generation, etc.), each paired with a
multi-modal instruction encapsulating the task's essence. Human evaluation on
various image generation datasets reveals that instruct-imagen matches or
surpasses prior task-specific models in-domain and demonstrates promising
generalization to unseen and more complex tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can We Generate Realistic Hands Only Using Convolution? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01951v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01951v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehran Hosseini, Peyman Hosseini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The enduring inability of image generative models to recreate intricate
geometric features, such as those present in human hands and fingers has been
an ongoing problem in image generation for nearly a decade. While strides have
been made by increasing model sizes and diversifying training datasets, this
issue remains prevalent across all models, from denoising diffusion models to
Generative Adversarial Networks (GAN), pointing to a fundamental shortcoming in
the underlying architectures. In this paper, we demonstrate how this problem
can be mitigated by augmenting convolution layers geometric capabilities
through providing them with a single input channel incorporating the relative
$n$-dimensional Cartesian coordinate system. We show that this drastically
improves quality of hand and face images generated by GANs and Variational
AutoEncoders (VAE).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Contains 17 pages, 14 figures, and 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Object-Centric Learning from Multiple Unspecified
  Viewpoints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01922v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01922v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyang Yuan, Tonglin Chen, Zhimeng Shen, Bin Li, Xiangyang Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual scenes are extremely diverse, not only because there are infinite
possible combinations of objects and backgrounds but also because the
observations of the same scene may vary greatly with the change of viewpoints.
When observing a multi-object visual scene from multiple viewpoints, humans can
perceive the scene compositionally from each viewpoint while achieving the
so-called ``object constancy'' across different viewpoints, even though the
exact viewpoints are untold. This ability is essential for humans to identify
the same object while moving and to learn from vision efficiently. It is
intriguing to design models that have a similar ability. In this paper, we
consider a novel problem of learning compositional scene representations from
multiple unspecified (i.e., unknown and unrelated) viewpoints without using any
supervision and propose a deep generative model which separates latent
representations into a viewpoint-independent part and a viewpoint-dependent
part to solve this problem. During the inference, latent representations are
randomly initialized and iteratively updated by integrating the information in
different viewpoints with neural networks. Experiments on several specifically
designed synthetic datasets have shown that the proposed method can effectively
learn from multiple unspecified viewpoints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2112.03568</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PressureVision++: Estimating Fingertip Pressure from Diverse RGB Images <span class="chip">WACV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.02310v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.02310v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Grady, Jeremy A. Collins, Chengcheng Tang, Christopher D. Twigg, Kunal Aneja, James Hays, Charles C. Kemp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Touch plays a fundamental role in manipulation for humans; however, machine
perception of contact and pressure typically requires invasive sensors. Recent
research has shown that deep models can estimate hand pressure based on a
single RGB image. However, evaluations have been limited to controlled settings
since collecting diverse data with ground-truth pressure measurements is
difficult. We present a novel approach that enables diverse data to be captured
with only an RGB camera and a cooperative participant. Our key insight is that
people can be prompted to apply pressure in a certain way, and this prompt can
serve as a weak label to supervise models to perform well under varied
conditions. We collect a novel dataset with 51 participants making fingertip
contact with diverse objects. Our network, PressureVision++, outperforms human
annotators and prior work. We also demonstrate an application of
PressureVision++ to mixed reality where pressure estimation allows everyday
surfaces to be used as arbitrary touch-sensitive interfaces. Code, data, and
models are available online.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Brain Tumor Segmentation (BraTS) Challenge 2023: Focus on Pediatrics
  (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17033v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17033v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anahita Fathi Kazerooni, Nastaran Khalili, Xinyang Liu, Debanjan Haldar, Zhifan Jiang, Syed Muhammed Anwar, Jake Albrecht, Maruf Adewole, Udunna Anazodo, Hannah Anderson, Sina Bagheri, Ujjwal Baid, Timothy Bergquist, Austin J. Borja, Evan Calabrese, Verena Chung, Gian-Marco Conte, Farouk Dako, James Eddy, Ivan Ezhov, Ariana Familiar, Keyvan Farahani, Shuvanjan Haldar, Juan Eugenio Iglesias, Anastasia Janas, Elaine Johansen, Blaise V Jones, Florian Kofler, Dominic LaBella, Hollie Anne Lai, Koen Van Leemput, Hongwei Bran Li, Nazanin Maleki, Aaron S McAllister, Zeke Meier, Bjoern Menze, Ahmed W Moawad, Khanak K Nandolia, Julija Pavaine, Marie Piraud, Tina Poussaint, Sanjay P Prabhu, Zachary Reitman, Andres Rodriguez, Jeffrey D Rudie, Ibraheem Salman Shaikh, Lubdha M. Shah, Nakul Sheth, Russel Taki Shinohara, Wenxin Tu, Karthik Viswanathan, Chunhao Wang, Jeffrey B Ware, Benedikt Wiestler, Walter Wiggins, Anna Zapaishchykova, Mariam Aboian, Miriam Bornhorst, Peter de Blank, Michelle Deutsch, Maryam Fouladi, Lindsey Hoffman, Benjamin Kann, Margot Lazow, Leonie Mikael, Ali Nabavizadeh, Roger Packer, Adam Resnick, Brian Rood, Arastoo Vossough, Spyridon Bakas, Marius George Linguraru
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pediatric tumors of the central nervous system are the most common cause of
cancer-related death in children. The five-year survival rate for high-grade
gliomas in children is less than 20\%. Due to their rarity, the diagnosis of
these entities is often delayed, their treatment is mainly based on historic
treatment concepts, and clinical trials require multi-institutional
collaborations. The MICCAI Brain Tumor Segmentation (BraTS) Challenge is a
landmark community benchmark event with a successful history of 12 years of
resource creation for the segmentation and analysis of adult glioma. Here we
present the CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023 challenge, which
represents the first BraTS challenge focused on pediatric brain tumors with
data acquired across multiple international consortia dedicated to pediatric
neuro-oncology and clinical trials. The BraTS-PEDs 2023 challenge focuses on
benchmarking the development of volumentric segmentation algorithms for
pediatric brain glioma through standardized quantitative performance evaluation
metrics utilized across the BraTS 2023 cluster of challenges. Models gaining
knowledge from the BraTS-PEDs multi-parametric structural MRI (mpMRI) training
data will be evaluated on separate validation and unseen test mpMRI dataof
high-grade pediatric glioma. The CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023
challenge brings together clinicians and AI/imaging scientists to lead to
faster development of automated segmentation techniques that could benefit
clinical trials, and ultimately the care of children with brain tumors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ M3D: <span class="highlight-title">Dataset</span> Condensation by Minimizing Maximum Mean Discrepancy <span class="chip">AAAI-24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.15927v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.15927v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hansong Zhang, Shikun Li, Pengju Wang, Dan Zeng, Shiming Ge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training state-of-the-art (SOTA) deep models often requires extensive data,
resulting in substantial training and storage costs. To address these
challenges, dataset condensation has been developed to learn a small synthetic
set that preserves essential information from the original large-scale dataset.
Nowadays, optimization-oriented methods have been the primary method in the
field of dataset condensation for achieving SOTA results. However, the bi-level
optimization process hinders the practical application of such methods to
realistic and larger datasets. To enhance condensation efficiency, previous
works proposed Distribution-Matching (DM) as an alternative, which
significantly reduces the condensation cost. Nonetheless, current DM-based
methods have yielded less comparable results to optimization-oriented methods
due to their focus on aligning only the first moment of the distributions. In
this paper, we present a novel DM-based method named M3D for dataset
condensation by Minimizing the Maximum Mean Discrepancy between feature
representations of the synthetic and real images. By embedding their
distributions in a reproducing kernel Hilbert space, we align all orders of
moments of the distributions of real and synthetic images, resulting in a more
generalized condensed set. Notably, our method even surpasses the SOTA
optimization-oriented method IDC on the high-resolution ImageNet dataset.
Extensive analysis is conducted to verify the effectiveness of the proposed
method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted in AAAI-24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SYNTA: A novel approach for deep learning-based image analysis in muscle
  histopathology using photo-realistic synthetic data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.14650v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.14650v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonid Mill, Oliver Aust, Jochen A. Ackermann, Philipp Burger, Monica Pascual, Katrin Palumbo-Zerr, Gerhard Krönke, Stefan Uderhardt, Georg Schett, Christoph S. Clemen, Rolf Schröder, Christian Holtzhausen, Samir Jabari, Andreas Maier, Anika Grüneboom
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI), machine learning, and deep learning (DL)
methods are becoming increasingly important in the field of biomedical image
analysis. However, to exploit the full potential of such methods, a
representative number of experimentally acquired images containing a
significant number of manually annotated objects is needed as training data.
Here we introduce SYNTA (synthetic data) as a novel approach for the generation
of synthetic, photo-realistic, and highly complex biomedical images as training
data for DL systems. We show the versatility of our approach in the context of
muscle fiber and connective tissue analysis in histological sections. We
demonstrate that it is possible to perform robust and expert-level segmentation
tasks on previously unseen real-world data, without the need for manual
annotations using synthetic training data alone. Being a fully parametric
technique, our approach poses an interpretable and controllable alternative to
Generative Adversarial Networks (GANs) and has the potential to significantly
accelerate quantitative image analysis in a variety of biomedical applications
in microscopy and beyond.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distribution Matching for Multi-Task Learning of Classification Tasks: a
  Large-Scale Study on Faces & Beyond <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01219v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01219v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitrios Kollias, Viktoriia Sharmanska, Stefanos Zafeiriou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Task Learning (MTL) is a framework, where multiple related tasks are
learned jointly and benefit from a shared representation space, or parameter
transfer. To provide sufficient learning support, modern MTL uses annotated
data with full, or sufficiently large overlap across tasks, i.e., each input
sample is annotated for all, or most of the tasks. However, collecting such
annotations is prohibitive in many real applications, and cannot benefit from
datasets available for individual tasks. In this work, we challenge this setup
and show that MTL can be successful with classification tasks with little, or
non-overlapping annotations, or when there is big discrepancy in the size of
labeled data per task. We explore task-relatedness for co-annotation and
co-training, and propose a novel approach, where knowledge exchange is enabled
between the tasks via distribution matching. To demonstrate the general
applicability of our method, we conducted diverse case studies in the domains
of affective computing, face recognition, species recognition, and shopping
item classification using nine datasets. Our large-scale study of affective
tasks for basic expression recognition and facial action unit detection
illustrates that our approach is network agnostic and brings large performance
improvements compared to the state-of-the-art in both tasks and across all
studied databases. In all case studies, we show that co-training via
task-relatedness is advantageous and prevents negative transfer (which occurs
when MT model's performance is worse than that of at least one single-task
model).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at AAAI 2024. arXiv admin note: text overlap with
  arXiv:2105.03790</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SVGDreamer: Text Guided SVG Generation with Diffusion Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16476v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16476v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ximing Xing, Haitao Zhou, Chuang Wang, Jing Zhang, Dong Xu, Qian Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, text-guided scalable vector graphics (SVGs) synthesis has shown
promise in domains such as iconography and sketch. However, existing
text-to-SVG generation methods lack editability and struggle with visual
quality and result diversity. To address these limitations, we propose a novel
text-guided vector graphics synthesis method called SVGDreamer. SVGDreamer
incorporates a semantic-driven image vectorization (SIVE) process that enables
the decomposition of synthesis into foreground objects and background, thereby
enhancing editability. Specifically, the SIVE process introduce attention-based
primitive control and an attention-mask loss function for effective control and
manipulation of individual elements. Additionally, we propose a Vectorized
Particle-based Score Distillation (VPSD) approach to tackle the challenges of
color over-saturation, vector primitives over-smoothing, and limited result
diversity in existing text-to-SVG generation methods. Furthermore, on the basis
of VPSD, we introduce Reward Feedback Learning (ReFL) to accelerate VPSD
convergence and improve aesthetic appeal. Extensive experiments have been
conducted to validate the effectiveness of SVGDreamer, demonstrating its
superiority over baseline methods in terms of editability, visual quality, and
diversity. The code and demo of SVGDreamer can be found at
\href{https://ximinng.github.io/SVGDreamer-project/}{https://ximinng.github.io/SVGDreamer-project/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 15 figures, project link:
  https://ximinng.github.io/SVGDreamer-project/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fading memory as inductive bias in residual recurrent networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.14823v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.14823v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Igor Dubinin, Felix Effenberger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Residual connections have been proposed as an architecture-based inductive
bias to mitigate the problem of exploding and vanishing gradients and increased
task performance in both feed-forward and recurrent networks (RNNs) when
trained with the backpropagation algorithm. Yet, little is known about how
residual connections in RNNs influence their dynamics and fading memory
properties. Here, we introduce weakly coupled residual recurrent networks
(WCRNNs) in which residual connections result in well-defined Lyapunov
exponents and allow for studying properties of fading memory. We investigate
how the residual connections of WCRNNs influence their performance, network
dynamics, and memory properties on a set of benchmark tasks. We show that
several distinct forms of residual connections yield effective inductive biases
that result in increased network expressivity. In particular, those are
residual connections that (i) result in network dynamics at the proximity of
the edge of chaos, (ii) allow networks to capitalize on characteristic spectral
properties of the data, and (iii) result in heterogeneous memory properties. In
addition, we demonstrate how our results can be extended to non-linear
residuals and introduce a weakly coupled residual initialization scheme that
can be used for Elman RNNs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inversion-by-Inversion: Exemplar-based Sketch-to-Photo Synthesis via
  Stochastic Differential Equations without Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.07665v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.07665v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ximing Xing, Chuang Wang, Haitao Zhou, Zhihao Hu, Chongxuan Li, Dong Xu, Qian Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exemplar-based sketch-to-photo synthesis allows users to generate
photo-realistic images based on sketches. Recently, diffusion-based methods
have achieved impressive performance on image generation tasks, enabling
highly-flexible control through text-driven generation or energy functions.
However, generating photo-realistic images with color and texture from sketch
images remains challenging for diffusion models. Sketches typically consist of
only a few strokes, with most regions left blank, making it difficult for
diffusion-based methods to produce photo-realistic images. In this work, we
propose a two-stage method named ``Inversion-by-Inversion" for exemplar-based
sketch-to-photo synthesis. This approach includes shape-enhancing inversion and
full-control inversion. During the shape-enhancing inversion process, an
uncolored photo is generated with the guidance of a shape-energy function. This
step is essential to ensure control over the shape of the generated photo. In
the full-control inversion process, we propose an appearance-energy function to
control the color and texture of the final generated photo.Importantly, our
Inversion-by-Inversion pipeline is training-free and can accept different types
of exemplars for color and texture control. We conducted extensive experiments
to evaluate our proposed method, and the results demonstrate its effectiveness.
The code and project can be found at
https://ximinng.github.io/inversion-by-inversion-project/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HOI4D: A 4D Egocentric <span class="highlight-title">Dataset</span> for Category-Level Human-Object
  Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.01577v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.01577v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunze Liu, Yun Liu, Che Jiang, Kangbo Lyu, Weikang Wan, Hao Shen, Boqiang Liang, Zhoujie Fu, He Wang, Li Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present HOI4D, a large-scale 4D egocentric dataset with rich annotations,
to catalyze the research of category-level human-object interaction. HOI4D
consists of 2.4M RGB-D egocentric video frames over 4000 sequences collected by
4 participants interacting with 800 different object instances from 16
categories over 610 different indoor rooms. Frame-wise annotations for panoptic
segmentation, motion segmentation, 3D hand pose, category-level object pose and
hand action have also been provided, together with reconstructed object meshes
and scene point clouds. With HOI4D, we establish three benchmarking tasks to
promote category-level HOI from 4D visual signals including semantic
segmentation of 4D dynamic point cloud sequences, category-level object pose
tracking, and egocentric action segmentation with diverse interaction targets.
In-depth analysis shows HOI4D poses great challenges to existing methods and
produces great research opportunities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive Study of Object Tracking in Low-Light Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16250v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16250v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anqi Yi, Nantheera Anantrasirichai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate object tracking in low-light environments is crucial, particularly
in surveillance and ethology applications. However, achieving this is
significantly challenging due to the poor quality of captured sequences.
Factors such as noise, color imbalance, and low contrast contribute to these
challenges. This paper presents a comprehensive study examining the impact of
these distortions on automatic object trackers. Additionally, we propose a
solution to enhance tracking performance by integrating denoising and low-light
enhancement methods into the transformer-based object tracking system.
Experimental results show that the proposed tracker, trained with low-light
synthetic datasets, outperforms both the vanilla MixFormer and Siam R-CNN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian posterior approximation with stochastic ensembles <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08123v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08123v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oleksandr Balabanov, Bernhard Mehlig, Hampus Linander
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce ensembles of stochastic neural networks to approximate the
Bayesian posterior, combining stochastic methods such as dropout with deep
ensembles. The stochastic ensembles are formulated as families of distributions
and trained to approximate the Bayesian posterior with variational inference.
We implement stochastic ensembles based on Monte Carlo dropout, DropConnect and
a novel non-parametric version of dropout and evaluate them on a toy problem
and CIFAR image classification. For both tasks, we test the quality of the
posteriors directly against Hamiltonian Monte Carlo simulations. Our results
show that stochastic ensembles provide more accurate posterior estimates than
other popular baselines for Bayesian inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SkateboardAI: The Coolest Video Action Recognition for Skateboarding <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11467v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11467v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanxiao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Impressed by the coolest skateboarding sports program from 2021 Tokyo Olympic
Games, we are the first to curate the original real-world video datasets
"SkateboardAI" in the wild, even self-design and implement diverse uni-modal
and multi-modal video action recognition approaches to recognize different
tricks accurately. For uni-modal methods, we separately apply (1) CNN and LSTM;
(2) CNN and BiLSTM; (3) CNN and BiLSTM with effective attention mechanisms; (4)
Transformer-based action recognition pipeline. Transferred to the multi-modal
conditions, we investigated the two-stream Inflated-3D architecture on
"SkateboardAI" datasets to compare its performance with uni-modal cases. In
sum, our objective is developing an excellent AI sport referee for the coolest
skateboarding competitions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The original first-author work has been accepted and presented by
  CVPR 2022 WiCV Workshop (This is the long-version paper)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lookup Table meets Local Laplacian Filter: Pyramid Reconstruction
  Network for Tone Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17190v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17190v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Zhang, Ming Tian, Zhiqiang Li, Bin Xu, Qingbo Lu, Changxin Gao, Nong Sang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tone mapping aims to convert high dynamic range (HDR) images to low dynamic
range (LDR) representations, a critical task in the camera imaging pipeline. In
recent years, 3-Dimensional LookUp Table (3D LUT) based methods have gained
attention due to their ability to strike a favorable balance between
enhancement performance and computational efficiency. However, these methods
often fail to deliver satisfactory results in local areas since the look-up
table is a global operator for tone mapping, which works based on pixel values
and fails to incorporate crucial local information. To this end, this paper
aims to address this issue by exploring a novel strategy that integrates global
and local operators by utilizing closed-form Laplacian pyramid decomposition
and reconstruction. Specifically, we employ image-adaptive 3D LUTs to
manipulate the tone in the low-frequency image by leveraging the specific
characteristics of the frequency information. Furthermore, we utilize local
Laplacian filters to refine the edge details in the high-frequency components
in an adaptive manner. Local Laplacian filters are widely used to preserve edge
details in photographs, but their conventional usage involves manual tuning and
fixed implementation within camera imaging pipelines or photo editing tools. We
propose to learn parameter value maps progressively for local Laplacian filters
from annotated data using a lightweight network. Our model achieves
simultaneous global tone manipulation and local edge detail preservation in an
end-to-end manner. Extensive experimental results on two benchmark datasets
demonstrate that the proposed method performs favorably against
state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures, accepted by NeurlPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpyroPose: SE(3) Pyramids for Object Pose Distribution Estimation <span class="chip">ICCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05308v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05308v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rasmus Laurvig Haugaard, Frederik Hagelskjær, Thorbjørn Mosekjær Iversen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object pose estimation is a core computer vision problem and often an
essential component in robotics. Pose estimation is usually approached by
seeking the single best estimate of an object's pose, but this approach is
ill-suited for tasks involving visual ambiguity. In such cases it is desirable
to estimate the uncertainty as a pose distribution to allow downstream tasks to
make informed decisions. Pose distributions can have arbitrary complexity which
motivates estimating unparameterized distributions, however, until now they
have only been used for orientation estimation on SO(3) due to the difficulty
in training on and normalizing over SE(3). We propose a novel method for pose
distribution estimation on SE(3). We use a hierarchical grid, a pyramid, which
enables efficient importance sampling during training and sparse evaluation of
the pyramid at inference, allowing real time 6D pose distribution estimation.
Our method outperforms state-of-the-art methods on SO(3), and to the best of
our knowledge, we provide the first quantitative results on pose distribution
estimation on SE(3). Code will be available at spyropose.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCVW 2023 (R6D)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion Model with Perceptual Loss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00110v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00110v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shanchuan Lin, Xiao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models trained with mean squared error loss tend to generate
unrealistic samples. Current state-of-the-art models rely on classifier-free
guidance to improve sample quality, yet its surprising effectiveness is not
fully understood. In this paper, We show that the effectiveness of
classifier-free guidance partly originates from it being a form of implicit
perceptual guidance. As a result, we can directly incorporate perceptual loss
in diffusion training to improve sample quality. Since the score matching
objective used in diffusion training strongly resembles the denoising
autoencoder objective used in unsupervised training of perceptual networks, the
diffusion model itself is a perceptual network and can be used to generate
meaningful perceptual loss. We propose a novel self-perceptual objective that
results in diffusion models capable of generating more realistic samples. For
conditional generation, our method only improves sample quality without
entanglement with the conditional input and therefore does not sacrifice sample
diversity. Our method can also improve sample quality for unconditional
generation, which was not possible with classifier-free guidance before.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.13763v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.13763v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huan Ling, Seung Wook Kim, Antonio Torralba, Sanja Fidler, Karsten Kreis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-guided diffusion models have revolutionized image and video generation
and have also been successfully used for optimization-based 3D object
synthesis. Here, we instead focus on the underexplored text-to-4D setting and
synthesize dynamic, animated 3D objects using score distillation methods with
an additional temporal dimension. Compared to previous work, we pursue a novel
compositional generation-based approach, and combine text-to-image,
text-to-video, and 3D-aware multiview diffusion models to provide feedback
during 4D object optimization, thereby simultaneously enforcing temporal
consistency, high-quality visual appearance and realistic geometry. Our method,
called Align Your Gaussians (AYG), leverages dynamic 3D Gaussian Splatting with
deformation fields as 4D representation. Crucial to AYG is a novel method to
regularize the distribution of the moving 3D Gaussians and thereby stabilize
the optimization and induce motion. We also propose a motion amplification
mechanism as well as a new autoregressive synthesis scheme to generate and
combine multiple 4D sequences for longer generation. These techniques allow us
to synthesize vivid dynamic scenes, outperform previous work qualitatively and
quantitatively and achieve state-of-the-art text-to-4D performance. Due to the
Gaussian 4D representation, different 4D animations can be seamlessly combined,
as we demonstrate. AYG opens up promising avenues for animation, simulation and
digital content creation as well as synthetic data generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page:
  https://research.nvidia.com/labs/toronto-ai/AlignYourGaussians/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regressor-Segmenter Mutual <span class="highlight-title">Prompt</span> Learning for Crowd Counting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.01711v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.01711v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyue Guo, Li Yuan, Zhaoyi Yan, Binghui Chen, Yaowei Wang, Qixiang Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crowd counting has achieved significant progress by training regressors to
predict instance positions. In heavily crowded scenarios, however, regressors
are challenged by uncontrollable annotation variance, which causes density map
bias and context information inaccuracy. In this study, we propose mutual
prompt learning (mPrompt), which leverages a regressor and a segmenter as
guidance for each other, solving bias and inaccuracy caused by annotation
variance while distinguishing foreground from background. In specific, mPrompt
leverages point annotations to tune the segmenter and predict pseudo head masks
in a way of point prompt learning. It then uses the predicted segmentation
masks, which serve as spatial constraint, to rectify biased point annotations
as context prompt learning. mPrompt defines a way of mutual information
maximization from prompt learning, mitigating the impact of annotation variance
while improving model accuracy. Experiments show that mPrompt significantly
reduces the Mean Average Error (MAE), demonstrating the potential to be general
framework for down-stream vision tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>mPrompt defines a way of mutual information maximization from prompt
  learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Follow Your Pose: Pose-Guided Text-to-Video Generation using Pose-Free
  Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.01186v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.01186v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Siran Chen, Ying Shan, Xiu Li, Qifeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating text-editable and pose-controllable character videos have an
imperious demand in creating various digital human. Nevertheless, this task has
been restricted by the absence of a comprehensive dataset featuring paired
video-pose captions and the generative prior models for videos. In this work,
we design a novel two-stage training scheme that can utilize easily obtained
datasets (i.e.,image pose pair and pose-free video) and the pre-trained
text-to-image (T2I) model to obtain the pose-controllable character videos.
Specifically, in the first stage, only the keypoint-image pairs are used only
for a controllable text-to-image generation. We learn a zero-initialized
convolutional encoder to encode the pose information. In the second stage, we
finetune the motion of the above network via a pose-free video dataset by
adding the learnable temporal self-attention and reformed cross-frame
self-attention blocks. Powered by our new designs, our method successfully
generates continuously pose-controllable character videos while keeps the
editing and concept composition ability of the pre-trained T2I model. The code
and models will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://follow-your-pose.github.io/; Github repository:
  https://github.com/mayuelala/FollowYourPose</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accurate and Fast Compressed Video Captioning <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.12867v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.12867v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaojie Shen, Xin Gu, Kai Xu, Heng Fan, Longyin Wen, Libo Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing video captioning approaches typically require to first sample video
frames from a decoded video and then conduct a subsequent process (e.g.,
feature extraction and/or captioning model learning). In this pipeline, manual
frame sampling may ignore key information in videos and thus degrade
performance. Additionally, redundant information in the sampled frames may
result in low efficiency in the inference of video captioning. Addressing this,
we study video captioning from a different perspective in compressed domain,
which brings multi-fold advantages over the existing pipeline: 1) Compared to
raw images from the decoded video, the compressed video, consisting of
I-frames, motion vectors and residuals, is highly distinguishable, which allows
us to leverage the entire video for learning without manual sampling through a
specialized model design; 2) The captioning model is more efficient in
inference as smaller and less redundant information is processed. We propose a
simple yet effective end-to-end transformer in the compressed domain for video
captioning that enables learning from the compressed video for captioning. We
show that even with a simple design, our method can achieve state-of-the-art
performance on different benchmarks while running almost 2x faster than
existing approaches. Code is available at https://github.com/acherstyx/CoCap.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VitalLens: Take A Vital Selfie 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06892v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06892v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp V. Rouast
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This report introduces VitalLens, an app that estimates vital signs such as
heart rate and respiration rate from selfie video in real time. VitalLens uses
a computer vision model trained on a diverse dataset of video and physiological
sensor data. We benchmark performance on several diverse datasets, including
VV-Medium, which consists of 289 unique participants. VitalLens outperforms
several existing methods including POS and MTTS-CAN on all datasets while
maintaining a fast inference speed. On VV-Medium, VitalLens achieves mean
absolute errors of 0.71 bpm for heart rate estimation, and 0.76 bpm for
respiratory rate estimation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GMMFormer: Gaussian-Mixture-Model Based <span class="highlight-title">Transformer</span> for Efficient
  Partially Relevant Video Retrieval <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05195v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05195v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuting Wang, Jinpeng Wang, Bin Chen, Ziyun Zeng, Shu-Tao Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a text query, partially relevant video retrieval (PRVR) seeks to find
untrimmed videos containing pertinent moments in a database. For PRVR, clip
modeling is essential to capture the partial relationship between texts and
videos. Current PRVR methods adopt scanning-based clip construction to achieve
explicit clip modeling, which is information-redundant and requires a large
storage overhead. To solve the efficiency problem of PRVR methods, this paper
proposes GMMFormer, a Gaussian-Mixture-Model based Transformer which models
clip representations implicitly. During frame interactions, we incorporate
Gaussian-Mixture-Model constraints to focus each frame on its adjacent frames
instead of the whole video. Then generated representations will contain
multi-scale clip information, achieving implicit clip modeling. In addition,
PRVR methods ignore semantic differences between text queries relevant to the
same video, leading to a sparse embedding space. We propose a query diverse
loss to distinguish these text queries, making the embedding space more
intensive and contain more semantic information. Extensive experiments on three
large-scale video datasets (i.e., TVR, ActivityNet Captions, and Charades-STA)
demonstrate the superiority and efficiency of GMMFormer. Code is available at
\url{https://github.com/huangmozhi9527/GMMFormer}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2024. Code is released at
  https://github.com/huangmozhi9527/GMMFormer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Improved Baseline for Reasoning Segmentation with Large Language
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17240v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17240v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Senqiao Yang, Tianyuan Qu, Xin Lai, Zhuotao Tian, Bohao Peng, Shu Liu, Jiaya Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While LISA effectively bridges the gap between segmentation and large
language models to enable reasoning segmentation, it poses certain limitations:
unable to distinguish different instances of the target region, and constrained
by the pre-defined textual response formats. In this work, we introduce LISA++,
an update to the existing LISA model, focusing on improving core
functionalities while keeping the base architecture intact. The main
enhancements in LISA++ include: \textbf{1) Enhanced Segmentation}: The instance
segmentation ability has been added, providing a more detailed scene analysis
along with the existing multi-region semantic segmentation. \textbf{2) More
Natural Conversation}: Improved capability for multi-turn dialogue, with the
ability to incorporate segmentation results directly into text responses, i.e.,
Segmentation in Dialogue (SiD). These improvements are achieved by curating the
existing samples of generic segmentation datasets, aimed specifically at
enhancing the segmentation and conversational skills without structural change
and additional data sources. Comparative analysis with the original LISA model
shows significant advancements in these areas, positioning LISA++ as a notable
upgrade in visual understanding and interaction. LISA++'s adaptability and
improved features highlight the versatility of the mask-as-embedding paradigm
proposed by LISA, and the potential as a foundational model for diverse
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech report. The LaTex compilation crash was fixed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EmotionGesture: Audio-Driven Diverse Emotional Co-Speech 3D Gesture
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18891v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18891v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingqun Qi, Chen Liu, Lincheng Li, Jie Hou, Haoran Xin, Xin Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating vivid and diverse 3D co-speech gestures is crucial for various
applications in animating virtual avatars. While most existing methods can
generate gestures from audio directly, they usually overlook that emotion is
one of the key factors of authentic co-speech gesture generation. In this work,
we propose EmotionGesture, a novel framework for synthesizing vivid and diverse
emotional co-speech 3D gestures from audio. Considering emotion is often
entangled with the rhythmic beat in speech audio, we first develop an
Emotion-Beat Mining module (EBM) to extract the emotion and audio beat features
as well as model their correlation via a transcript-based visual-rhythm
alignment. Then, we propose an initial pose based Spatial-Temporal Prompter
(STP) to generate future gestures from the given initial poses. STP effectively
models the spatial-temporal correlations between the initial poses and the
future gestures, thus producing the spatial-temporal coherent pose prompt. Once
we obtain pose prompts, emotion, and audio beat features, we will generate 3D
co-speech gestures through a transformer architecture. However, considering the
poses of existing datasets often contain jittering effects, this would lead to
generating unstable gestures. To address this issue, we propose an effective
objective function, dubbed Motion-Smooth Loss. Specifically, we model motion
offset to compensate for jittering ground-truth by forcing gestures to be
smooth. Last, we present an emotion-conditioned VAE to sample emotion features,
enabling us to generate diverse emotional results. Extensive experiments
demonstrate that our framework outperforms the state-of-the-art, achieving
vivid and diverse emotional co-speech 3D gestures. Our code and dataset will be
released at the project page:
https://xingqunqi-lab.github.io/Emotion-Gesture-Web/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OriCon3D: Effective 3D Object Detection using Orientation and Confidence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.14484v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.14484v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dhyey Manish Rajani, Surya Pratap Singh, Rahul Kashyap Swayampakula
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose an advanced methodology for the detection of 3D
objects and precise estimation of their spatial positions from a single image.
Unlike conventional frameworks that rely solely on center-point and dimension
predictions, our research leverages a deep convolutional neural network-based
3D object weighted orientation regression paradigm. These estimates are then
seamlessly integrated with geometric constraints obtained from a 2D bounding
box, resulting in derivation of a comprehensive 3D bounding box. Our novel
network design encompasses two key outputs. The first output involves the
estimation of 3D object orientation through the utilization of a
discrete-continuous loss function. Simultaneously, the second output predicts
objectivity-based confidence scores with minimal variance. Additionally, we
also introduce enhancements to our methodology through the incorporation of
lightweight residual feature extractors. By combining the derived estimates
with the geometric constraints inherent in the 2D bounding box, our approach
significantly improves the accuracy of 3D object pose determination, surpassing
baseline methodologies. Our method is rigorously evaluated on the KITTI 3D
object detection benchmark, demonstrating superior performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ECCV Caption: Correcting False Negatives by Collecting
  Machine-and-Human-verified Image-Caption Associations for MS-COCO <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.03359v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.03359v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanghyuk Chun, Wonjae Kim, Song Park, Minsuk Chang, Seong Joon Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-Text matching (ITM) is a common task for evaluating the quality of
Vision and Language (VL) models. However, existing ITM benchmarks have a
significant limitation. They have many missing correspondences, originating
from the data construction process itself. For example, a caption is only
matched with one image although the caption can be matched with other similar
images and vice versa. To correct the massive false negatives, we construct the
Extended COCO Validation (ECCV) Caption dataset by supplying the missing
associations with machine and human annotators. We employ five state-of-the-art
ITM models with diverse properties for our annotation process. Our dataset
provides x3.6 positive image-to-caption associations and x8.5 caption-to-image
associations compared to the original MS-COCO. We also propose to use an
informative ranking-based metric mAP@R, rather than the popular Recall@K (R@K).
We re-evaluate the existing 25 VL models on existing and proposed benchmarks.
Our findings are that the existing benchmarks, such as COCO 1K R@K, COCO 5K
R@K, CxC R@1 are highly correlated with each other, while the rankings change
when we shift to the ECCV mAP@R. Lastly, we delve into the effect of the bias
introduced by the choice of machine annotator. Source code and dataset are
available at https://github.com/naver-ai/eccv-caption
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ECCV 2022; 32 pages (2.3MB); Code and dataset:
  https://github.com/naver-ai/eccv-caption; v5 fixes errors in Table 4: the
  COCO 1K R@1 numbers were incorrect. All other tables and figures are correct.
  v5 also adds RSUM scores in Tab 4 and 5: RSUM has a high correlation with
  COCO 1K recalls; v4 fixes errors in v3 -- see the v4 comment for details</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieval-Augmented Egocentric Video Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00789v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00789v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jilan Xu, Yifei Huang, Junlin Hou, Guo Chen, Yuejie Zhang, Rui Feng, Weidi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding human actions from videos of first-person view poses
significant challenges. Most prior approaches explore representation learning
on egocentric videos only, while overlooking the potential benefit of
exploiting existing large-scale third-person videos. In this paper, (1) we
develop EgoInstructor, a retrieval-augmented multimodal captioning model that
automatically retrieves semantically relevant third-person instructional videos
to enhance the video captioning of egocentric videos. (2) For training the
cross-view retrieval module, we devise an automatic pipeline to discover
ego-exo video pairs from distinct large-scale egocentric and exocentric
datasets. (3) We train the cross-view retrieval module with a novel EgoExoNCE
loss that pulls egocentric and exocentric video features closer by aligning
them to shared text features that describe similar actions. (4) Through
extensive experiments, our cross-view retrieval module demonstrates superior
performance across seven benchmarks. Regarding egocentric video captioning,
EgoInstructor exhibits significant improvements by leveraging third-person
videos as references.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SCUNet++: Swin-UNet and CNN Bottleneck Hybrid Architecture with
  Multi-Fusion Dense Skip Connection for Pulmonary Embolism CT Image
  Segmentation <span class="chip">WACV2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14705v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14705v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Chen, Binfeng Zou, Zhaoxin Guo, Yiyu Huang, Yifan Huang, Feiwei Qin, Qinhai Li, Changmiao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pulmonary embolism (PE) is a prevalent lung disease that can lead to right
ventricular hypertrophy and failure in severe cases, ranking second in severity
only to myocardial infarction and sudden death. Pulmonary artery CT angiography
(CTPA) is a widely used diagnostic method for PE. However, PE detection
presents challenges in clinical practice due to limitations in imaging
technology. CTPA can produce noises similar to PE, making confirmation of its
presence time-consuming and prone to overdiagnosis. Nevertheless, the
traditional segmentation method of PE can not fully consider the hierarchical
structure of features, local and global spatial features of PE CT images. In
this paper, we propose an automatic PE segmentation method called SCUNet++
(Swin Conv UNet++). This method incorporates multiple fusion dense skip
connections between the encoder and decoder, utilizing the Swin Transformer as
the encoder. And fuses features of different scales in the decoder subnetwork
to compensate for spatial information loss caused by the inevitable
downsampling in Swin-UNet or other state-of-the-art methods, effectively
solving the above problem. We provide a theoretical analysis of this method in
detail and validate it on publicly available PE CT image datasets FUMPE and
CAD-PE. The experimental results indicate that our proposed method achieved a
Dice similarity coefficient (DSC) of 83.47% and a Hausdorff distance 95th
percentile (HD95) of 3.83 on the FUMPE dataset, as well as a DSC of 83.42% and
an HD95 of 5.10 on the CAD-PE dataset. These findings demonstrate that our
method exhibits strong performance in PE segmentation tasks, potentially
enhancing the accuracy of automatic segmentation of PE and providing a powerful
diagnostic tool for clinical physicians. Our source code and new FUMPE dataset
are available at https://github.com/JustlfC03/SCUNet-plusplus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures, accept WACV2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Credible Teacher for Semi-Supervised Object Detection in Open Scene <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00695v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00695v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyu Zhuang, Kuo Wang, Liang Lin, Guanbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-Supervised Object Detection (SSOD) has achieved resounding success by
leveraging unlabeled data to improve detection performance. However, in Open
Scene Semi-Supervised Object Detection (O-SSOD), unlabeled data may contains
unknown objects not observed in the labeled data, which will increase
uncertainty in the model's predictions for known objects. It is detrimental to
the current methods that mainly rely on self-training, as more uncertainty
leads to the lower localization and classification precision of pseudo labels.
To this end, we propose Credible Teacher, an end-to-end framework. Credible
Teacher adopts an interactive teaching mechanism using flexible labels to
prevent uncertain pseudo labels from misleading the model and gradually reduces
its uncertainty through the guidance of other credible pseudo labels. Empirical
results have demonstrated our method effectively restrains the adverse effect
caused by O-SSOD and significantly outperforms existing counterparts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accpet by ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CityPulse: Fine-Grained Assessment of Urban Change with Street View Time
  Series <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01107v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01107v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyuan Huang, Zejia Wu, Jiajun Wu, Jackelyn Hwang, Ram Rajagopal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Urban transformations have profound societal impact on both individuals and
communities at large. Accurately assessing these shifts is essential for
understanding their underlying causes and ensuring sustainable urban planning.
Traditional measurements often encounter constraints in spatial and temporal
granularity, failing to capture real-time physical changes. While street view
imagery, capturing the heartbeat of urban spaces from a pedestrian point of
view, can add as a high-definition, up-to-date, and on-the-ground visual proxy
of urban change. We curate the largest street view time series dataset to date,
and propose an end-to-end change detection model to effectively capture
physical alterations in the built environment at scale. We demonstrate the
effectiveness of our proposed method by benchmark comparisons with previous
literature and implementing it at the city-wide level. Our approach has the
potential to supplement existing dataset and serve as a fine-grained and
accurate assessment of urban change.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deshadow-Anything: When Segment Anything Model Meets Zero-shot shadow
  removal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11715v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11715v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Feng Zhang, Tian Yi Song, Jia Wei Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segment Anything (SAM), an advanced universal image segmentation model
trained on an expansive visual dataset, has set a new benchmark in image
segmentation and computer vision. However, it faced challenges when it came to
distinguishing between shadows and their backgrounds. To address this, we
developed Deshadow-Anything, considering the generalization of large-scale
datasets, and we performed Fine-tuning on large-scale datasets to achieve image
shadow removal. The diffusion model can diffuse along the edges and textures of
an image, helping to remove shadows while preserving the details of the image.
Furthermore, we design Multi-Self-Attention Guidance (MSAG) and adaptive input
perturbation (DDPM-AIP) to accelerate the iterative training speed of
diffusion. Experiments on shadow removal tasks demonstrate that these methods
can effectively improve image restoration performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>it needs revised</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diff-PCR: Diffusion-Based Correspondence Searching in Doubly Stochastic
  Matrix Space for Point Cloud Registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00436v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00436v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianliang Wu, Haobo Jiang, Yaqing Ding, Lei Luo, Jin Xie, Jian Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficiently finding optimal correspondences between point clouds is crucial
for solving both rigid and non-rigid point cloud registration problems.
Existing methods often rely on geometric or semantic feature embedding to
establish correspondences and estimate transformations or flow fields.
Recently, state-of-the-art methods have employed RAFT-like iterative updates to
refine the solution. However, these methods have certain limitations. Firstly,
their iterative refinement design lacks transparency, and their iterative
updates follow a fixed path during the refinement process, which can lead to
suboptimal results. Secondly, these methods overlook the importance of refining
or optimizing correspondences (or matching matrices) as a precursor to solving
transformations or flow fields. They typically compute candidate
correspondences based on distances in the point feature space. However, they
only project the candidate matching matrix into some matrix space once with
Sinkhorn or dual softmax operations to obtain final correspondences. This
one-shot projected matching matrix may be far from the globally optimal one,
and these approaches do not consider the distribution of the target matching
matrix. In this paper, we propose a novel approach that exploits the Denoising
Diffusion Model to predict a searching gradient for the optimal matching matrix
within the Doubly Stochastic Matrix Space. During the reverse denoising
process, our method iteratively searches for better solutions along this
denoising gradient, which points towards the maximum likelihood direction of
the target matching matrix. Our method offers flexibility by allowing the
search to start from any initial matching matrix provided by the online
backbone or white noise. Experimental evaluations on the 3DMatch/3DLoMatch and
4DMatch/4DLoMatch datasets demonstrate the effectiveness of our newly designed
framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FuRPE: Learning Full-body Reconstruction from Part Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.00731v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.00731v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoxin Fan, Yuqing Pan, Hao Xu, Zhenbo Song, Zhicheng Wang, Kejian Wu, Hongyan Liu, Jun He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of full-body reconstruction, the scarcity of annotated data
often impedes the efficacy of prevailing methods. To address this issue, we
introduce FuRPE, a novel framework that employs part-experts and an ingenious
pseudo ground-truth selection scheme to derive high-quality pseudo labels.
These labels, central to our approach, equip our network with the capability to
efficiently learn from the available data. Integral to FuRPE is a unique
exponential moving average training strategy and expert-derived feature
distillation strategy. These novel elements of FuRPE not only serve to further
refine the model but also to reduce potential biases that may arise from
inaccuracies in pseudo labels, thereby optimizing the network's training
process and enhancing the robustness of the model. We apply FuRPE to train both
two-stage and fully convolutional single-stage full-body reconstruction
networks. Our exhaustive experiments on numerous benchmark datasets illustrate
a substantial performance boost over existing methods, underscoring FuRPE's
potential to reshape the state-of-the-art in full-body reconstruction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SCLIP: Rethinking Self-Attention for Dense Vision-Language Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.01597v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.01597v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Wang, Jieru Mei, Alan Yuille
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in contrastive language-image pretraining (CLIP) have
demonstrated strong capabilities in zero-shot classification by aligning visual
representations with target text embeddings in an image level. However, in
dense prediction tasks, CLIP often struggles to localize visual features within
an image and fails to give accurate pixel-level predictions, which prevents it
from functioning as a generalized visual foundation model. In this work, we aim
to enhance CLIP's potential for semantic segmentation with minimal
modifications to its pretrained models. By rethinking self-attention, we
surprisingly find that CLIP can adapt to dense prediction tasks by simply
introducing a novel Correlative Self-Attention (CSA) mechanism. Specifically,
we replace the traditional self-attention block of CLIP vision encoder's last
layer by our CSA module and reuse its pretrained projection matrices of query,
key, and value, leading to a training-free adaptation approach for CLIP's
zero-shot semantic segmentation. Extensive experiments show the advantage of
CSA: we obtain a 38.2% average zero-shot mIoU across eight semantic
segmentation benchmarks highlighted in this paper, significantly outperforming
the existing SoTA's 33.9% and the vanilla CLIP's 14.1%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Navigating Data Heterogeneity in Federated Learning A Semi-Supervised
  Federated Object Detection <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17097v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17097v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taehyeon Kim, Eric Lin, Junu Lee, Christian Lau, Vaikkunth Mugunthan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) has emerged as a potent framework for training models
across distributed data sources while maintaining data privacy. Nevertheless,
it faces challenges with limited high-quality labels and non-IID client data,
particularly in applications like autonomous driving. To address these hurdles,
we navigate the uncharted waters of Semi-Supervised Federated Object Detection
(SSFOD). We present a pioneering SSFOD framework, designed for scenarios where
labeled data reside only at the server while clients possess unlabeled data.
Notably, our method represents the inaugural implementation of SSFOD for
clients with 0% labeled non-IID data, a stark contrast to previous studies that
maintain some subset of labels at each client. We propose FedSTO, a two-stage
strategy encompassing Selective Training followed by Orthogonally enhanced
full-parameter training, to effectively address data shift (e.g. weather
conditions) between server and clients. Our contributions include selectively
refining the backbone of the detector to avert overfitting, orthogonality
regularization to boost representation divergence, and local EMA-driven pseudo
label assignment to yield high-quality pseudo labels. Extensive validation on
prominent autonomous driving datasets (BDD100K, Cityscapes, and SODA10M)
attests to the efficacy of our approach, demonstrating state-of-the-art
results. Remarkably, FedSTO, using just 20-30% of labels, performs nearly as
well as fully-supervised centralized training methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LinFlo-Net: A two-stage deep learning method to generate simulation
  ready meshes of the heart 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.20065v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.20065v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arjun Narayanan, Fanwei Kong, Shawn Shadden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a deep learning model to automatically generate computer models of
the human heart from patient imaging data with an emphasis on its capability to
generate thin-walled cardiac structures. Our method works by deforming a
template mesh to fit the cardiac structures to the given image. Compared with
prior deep learning methods that adopted this approach, our framework is
designed to minimize mesh self-penetration, which typically arises when
deforming surface meshes separated by small distances. We achieve this by using
a two-stage diffeomorphic deformation process along with a novel loss function
derived from the kinematics of motion that penalizes surface contact and
interpenetration. Our model demonstrates comparable accuracy with
state-of-the-art methods while additionally producing meshes free of
self-intersections. The resultant meshes are readily usable in physics based
simulation, minimizing the need for post-processing and cleanup.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted manuscript in the Journal of Biomechanical Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diabetic Retinopathy Using Gaussian Filter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15216v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15216v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roshan Vasu Muddaluru, Sharvaani Ravikumar Thoguluva, Shruti Prabha, Tanuja Konda Reddy, Dr. Suja P
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The retina is an essential component of the visual system, and maintaining
eyesight depends on the timely and correct detection of disorders. This
research specifically addresses the early-stage detection and severity
classification of diabetic retinopathy (DR), a serious public health hazard. We
compare the results of different deep learning models such as InceptionV3,
DenseNet121 and other CNN based models by using different image filters, such
as Gaussian, grayscale and Gabor. These models could detect subtle pathological
alterations and use that information to estimate the risk of retinal illnesses.
The objective is to improve the diagnostic processes for diabetic retinopathy,
the primary cause of diabetes-related blindness, by utilizing deep learning
models. A comparative analysis between Greyscale, Gaussian and Gabor filters
has been provided after applying these filters on the retinal images. The
Gaussian filter resulted to be the most promising filter giving the best
accuracies for all the models. The best performing model was InceptionV3 which
gave an accuracy of 96% on Gaussian images, therefore Gaussian filter emerged
as our most promising filter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures, conference, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">19</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mining Temporal Attack Patterns from Cy<span class="highlight-title">bert</span>hreat Intelligence Reports 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01883v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01883v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Rayhanur Rahman, Brandon Wroblewski, Quinn Matthews, Brantley Morgan, Tim Menzies, Laurie Williams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Defending from cyberattacks requires practitioners to operate on high-level
adversary behavior. Cyberthreat intelligence (CTI) reports on past cyberattack
incidents describe the chain of malicious actions with respect to time. To
avoid repeating cyberattack incidents, practitioners must proactively identify
and defend against recurring chain of actions - which we refer to as temporal
attack patterns. Automatically mining the patterns among actions provides
structured and actionable information on the adversary behavior of past
cyberattacks. The goal of this paper is to aid security practitioners in
prioritizing and proactive defense against cyberattacks by mining temporal
attack patterns from cyberthreat intelligence reports. To this end, we propose
ChronoCTI, an automated pipeline for mining temporal attack patterns from
cyberthreat intelligence (CTI) reports of past cyberattacks. To construct
ChronoCTI, we build the ground truth dataset of temporal attack patterns and
apply state-of-the-art large language models, natural language processing, and
machine learning techniques. We apply ChronoCTI on a set of 713 CTI reports,
where we identify 124 temporal attack patterns - which we categorize into nine
pattern categories. We identify that the most prevalent pattern category is to
trick victim users into executing malicious code to initiate the attack,
followed by bypassing the anti-malware system in the victim network. Based on
the observed patterns, we advocate organizations to train users about
cybersecurity best practices, introduce immutable operating systems with
limited functionalities, and enforce multi-user authentications. Moreover, we
advocate practitioners to leverage the automated mining capability of ChronoCTI
and design countermeasures against the recurring attack patterns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A modified version of this pre-print is submitted to IEEE
  Transactions on Software Engineering, and is under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Concurrent Brainstorming & Hypothesis Satisfying: An Iterative Framework
  for Enhanced Retrieval-Augmented Generation (R2CBR3H-SR) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01835v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01835v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arash Shahmansoori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Addressing the complexity of comprehensive information retrieval, this study
introduces an innovative, iterative retrieval-augmented generation system. Our
approach uniquely integrates a vector-space driven re-ranking mechanism with
concurrent brainstorming to expedite the retrieval of highly relevant
documents, thereby streamlining the generation of potential queries. This sets
the stage for our novel hybrid process, which synergistically combines
hypothesis formulation with satisfying decision-making strategy to determine
content adequacy, leveraging a chain of thought-based prompting technique. This
unified hypothesize-satisfied phase intelligently distills information to
ascertain whether user queries have been satisfactorily addressed. Upon
reaching this criterion, the system refines its output into a concise
representation, maximizing conceptual density with minimal verbosity. The
iterative nature of the workflow enhances process efficiency and accuracy.
Crucially, the concurrency within the brainstorming phase significantly
accelerates recursive operations, facilitating rapid convergence to solution
satisfaction. Compared to conventional methods, our system demonstrates a
marked improvement in computational time and cost-effectiveness. This research
advances the state-of-the-art in intelligent retrieval systems, setting a new
benchmark for resource-efficient information extraction and abstraction in
knowledge-intensive applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3 pages, 1 table, double column IEEE journal format paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physio: An LLM-Based Physiotherapy Advisor <span class="chip">ECIR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01825v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01825v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rúben Almeida, Hugo Sousa, Luís F. Cunha, Nuno Guimarães, Ricardo Campos, Alípio Jorge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The capabilities of the most recent language models have increased the
interest in integrating them into real-world applications. However, the fact
that these models generate plausible, yet incorrect text poses a constraint
when considering their use in several domains. Healthcare is a prime example of
a domain where text-generative trustworthiness is a hard requirement to
safeguard patient well-being. In this paper, we present Physio, a chat-based
application for physical rehabilitation. Physio is capable of making an initial
diagnosis while citing reliable health sources to support the information
provided. Furthermore, drawing upon external knowledge databases, Physio can
recommend rehabilitation exercises and over-the-counter medication for symptom
relief. By combining these features, Physio can leverage the power of
generative models for language processing while also conditioning its response
on dependable and verifiable sources. A live demo of Physio is available at
https://physio.inesctec.pt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Demo, ECIR 2024, 3rd Sword AI challenge 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Trustworthiness of Online News Publishers via Article
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01781v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01781v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Bianchi, Manuel Pratelli, Marinella Petrocchi, Fabio Pinelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of low-quality online information in today's era has
underscored the need for robust and automatic mechanisms to evaluate the
trustworthiness of online news publishers. In this paper, we analyse the
trustworthiness of online news media outlets by leveraging a dataset of 4033
news stories from 40 different sources. We aim to infer the trustworthiness
level of the source based on the classification of individual articles'
content. The trust labels are obtained from NewsGuard, a journalistic
organization that evaluates news sources using well-established editorial and
publishing criteria. The results indicate that the classification model is
highly effective in classifying the trustworthiness levels of the news
articles. This research has practical applications in alerting readers to
potentially untrustworthy news sources, assisting journalistic organizations in
evaluating new or unfamiliar media outlets and supporting the selection of
articles for their trustworthiness assessment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper will appear in the proceedings of the 2024 ACM/SIGAPP
  Symposium on Applied Computing, Avila, Spain, April 8-12, 2024. The version
  here submitted is the accepted version before publisher typesetting</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Navigating Uncertainty: Optimizing API Dependency for Hallucination
  Reduction in Closed-Book Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierre Erbacher, Louis Falissar, Vincent Guigue, Laure Soulier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Large Language Models (LLM) are able to accumulate and restore
knowledge, they are still prone to hallucination. Especially when faced with
factual questions, LLM cannot only rely on knowledge stored in parameters to
guarantee truthful and correct answers. Augmenting these models with the
ability to search on external information sources, such as the web, is a
promising approach to ground knowledge to retrieve information. However,
searching in a large collection of documents introduces additional
computational/time costs. An optimal behavior would be to query external
resources only when the LLM is not confident about answers. In this paper, we
propose a new LLM able to self-estimate if it is able to answer directly or
needs to request an external tool. We investigate a supervised approach by
introducing a hallucination masking mechanism in which labels are generated
using a close book question-answering task. In addition, we propose to leverage
parameter-efficient fine-tuning techniques to train our model on a small amount
of data. Our model directly provides answers for $78.2\%$ of the known queries
and opts to search for $77.2\%$ of the unknown ones. This results in the API
being utilized only $62\%$ of the time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text mining arXiv: a look through quantitative finance papers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michele Leonardo Bianchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores articles hosted on the arXiv preprint server with the aim
to uncover valuable insights hidden in this vast collection of research.
Employing text mining techniques and through the application of natural
language processing methods, we examine the contents of quantitative finance
papers posted in arXiv from 1997 to 2022. We extract and analyze crucial
information from the entire documents, including the references, to understand
the topics trends over time and to find out the most cited researchers and
journals on this domain. Additionally, we compare numerous algorithms to
perform topic modeling, including state-of-the-art approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Large Language Models in Semantic Parsing for Conversational
  Question Answering over Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phillip Schneider, Manuel Klettner, Kristiina Jokinen, Elena Simperl, Florian Matthes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational question answering systems often rely on semantic parsing to
enable interactive information retrieval, which involves the generation of
structured database queries from a natural language input. For
information-seeking conversations about facts stored within a knowledge graph,
dialogue utterances are transformed into graph queries in a process that is
called knowledge-based conversational question answering. This paper evaluates
the performance of large language models that have not been explicitly
pre-trained on this task. Through a series of experiments on an extensive
benchmark dataset, we compare models of varying sizes with different prompting
techniques and identify common issue types in the generated output. Our results
demonstrate that large language models are capable of generating graph queries
from dialogues, with significant improvements achievable through few-shot
prompting and fine-tuning techniques, especially for smaller models that
exhibit lower zero-shot performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICAART 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">GPT</span>-4V(ision) is a Generalist Web Agent, if Grounded 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, Yu Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent development on large multimodal models (LMMs), especially
GPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries
of multimodal models beyond traditional tasks like image captioning and visual
question answering. In this work, we explore the potential of LMMs like GPT-4V
as a generalist web agent that can follow natural language instructions to
complete tasks on any given website. We propose SEEACT, a generalist web agent
that harnesses the power of LMMs for integrated visual understanding and acting
on the web. We evaluate on the recent MIND2WEB benchmark. In addition to
standard offline evaluation on cached websites, we enable a new online
evaluation setting by developing a tool that allows running web agents on live
websites. We show that GPT-4V presents a great potential for web agents - it
can successfully complete 50% of the tasks on live websites if we manually
ground its textual plans into actions on the websites. This substantially
outperforms text-only LLMs like GPT-4 or smaller models (FLAN-T5 and BLIP-2)
specifically fine-tuned for web agents. However, grounding still remains a
major challenge. Existing LMM grounding strategies like set-of-mark prompting
turns out not effective for web agents, and the best grounding strategy we
develop in this paper leverages both the HTML text and visuals. Yet, there is
still a substantial gap with oracle grounding, leaving ample room for further
improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Team IELAB at TREC Clinical Trial Track 2023: Enhancing Clinical Trial
  Retrieval with Neural Rankers and Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengyao Zhuang, Bevan Koopman, Guido Zuccon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe team ielab from CSIRO and The University of Queensland's approach
to the 2023 TREC Clinical Trials Track. Our approach was to use neural rankers
but to utilise Large Language Models to overcome the issue of lack of training
data for such rankers. Specifically, we employ ChatGPT to generate relevant
patient descriptions for randomly selected clinical trials from the corpus.
This synthetic dataset, combined with human-annotated training data from
previous years, is used to train both dense and sparse retrievers based on
PubmedBERT. Additionally, a cross-encoder re-ranker is integrated into the
system. To further enhance the effectiveness of our approach, we prompting
GPT-4 as a TREC annotator to provide judgments on our run files. These
judgments are subsequently employed to re-rank the results. This architecture
tightly integrates strong PubmedBERT-based rankers with the aid of SOTA Large
Language Models, demonstrating a new approach to clinical trial retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TREC Notebook</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Poisoning Attacks against Recommender Systems: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongwei Wang, Min Gao, Junliang Yu, Hao Ma, Hongzhi Yin, Shazia Sadiq
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern recommender systems have seen substantial success, yet they remain
vulnerable to malicious activities, notably poisoning attacks. These attacks
involve injecting malicious data into the training datasets of RS, thereby
compromising their integrity and manipulating recommendation outcomes for
gaining illicit profits. This survey paper provides a systematic and up-to-date
review of the research landscape on Poisoning Attacks against Recommendation
(PAR). A novel and comprehensive taxonomy is proposed, categorizing existing
PAR methodologies into three distinct categories: Component-Specific,
Goal-Driven, and Capability Probing. For each category, we discuss its
mechanism in detail, along with associated methods. Furthermore, this paper
highlights potential future research avenues in this domain. Additionally, to
facilitate and benchmark the empirical comparison of PAR, we introduce an
open-source library, ARLib, which encompasses a comprehensive collection of PAR
models and common datasets. The library is released at
\url{https://github.com/CoderWZW/ARLib}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages,5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Expected Transaction Value Optimization for Precise Marketing in FinTech
  Platforms <span class="chip">RecSys'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunpeng Weng, Xing Tang, Liang Chen, Dugang Liu, Xiuqiang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  FinTech platforms facilitated by digital payments are watching growth
rapidly, which enable the distribution of mutual funds personalized to
individual investors via mobile Apps. As the important intermediation of
financial products investment, these platforms distribute thousands of mutual
funds obtaining impressions under guaranteed delivery (GD) strategy required by
fund companies. Driven by the profit from fund purchases of users, the platform
aims to maximize each transaction amount of customers by promoting mutual funds
to these investors who will be interested in. Different from the conversions in
traditional advertising or e-commerce recommendations, the investment amount in
each purchase varies greatly even for the same financial product, which
provides a significant challenge for the promotion recommendation of mutual
funds. In addition to predicting the click-through rate (CTR) or the conversion
rate (CVR) as in traditional recommendations, it is essential for FinTech
platforms to estimate the customers' purchase amount for each delivered fund
and achieve an effective allocation of impressions based on the predicted
results to optimize the total expected transaction value (ETV). In this paper,
we propose an ETV optimized customer allocation framework (EOCA) that aims to
maximize the total ETV of recommended funds, under the constraints of GD dealt
with fund companies. To the best of our knowledge, it's the first attempt to
solve the GD problem for financial product promotions based on customer
purchase amount prediction. We conduct extensive experiments on large scale
real-world datasets and online tests based on LiCaiTong, Tencent wealth
management platform, to demonstrate the effectiveness of our proposed EOCA
framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Workshop on Deep Learning Practice for High-Dimensional
  Sparse Data in RecSys'23 (DLP@RecSys), Singapore, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Multilingual Information Retrieval in Mixed Human Resources
  Environments: A RAG Model Implementation for Multicultural Enterprise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01511v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01511v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Syed Rameel Ahmad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of Large Language Models has revolutionized information retrieval,
ushering in a new era of expansive knowledge accessibility. While these models
excel in providing open-world knowledge, effectively extracting answers in
diverse linguistic environments with varying levels of literacy remains a
formidable challenge. Retrieval Augmented Generation (RAG) emerges as a
promising solution, bridging the gap between information availability and
multilingual comprehension. However, deploying RAG models in real-world
scenarios demands careful consideration of various factors. This paper
addresses the critical challenges associated with implementing RAG models in
multicultural environments. We delve into essential considerations, including
data feeding strategies, timely updates, mitigation of hallucinations,
prevention of erroneous responses, and optimization of delivery speed. Our work
involves the integration of a diverse array of tools, meticulously combined to
facilitate the seamless adoption of RAG models across languages and literacy
levels within a multicultural organizational context. Through strategic tweaks
in our approaches, we achieve not only effectiveness but also efficiency,
ensuring the accelerated and accurate delivery of information in a manner that
is tailored to the unique requirements of multilingual and multicultural
settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Pre-train</span>ed Sequential Recommendation Framework: Popularity Dynamics
  for Zero-shot Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junting Wang, Praneet Rathi, Hari Sundaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommenders are crucial to the success of online applications,
\eg e-commerce, video streaming, and social media. While model architectures
continue to improve, for every new application domain, we still have to train a
new model from scratch for high quality recommendations. On the other hand,
pre-trained language and vision models have shown great success in zero-shot or
few-shot adaptation to new application domains. Inspired by the success of
pre-trained models in peer AI fields, we propose a novel pre-trained sequential
recommendation framework: PrepRec. We learn universal item representations by
modeling item popularity dynamics. Through extensive experiments on five
real-world datasets, we show that PrepRec, without any auxiliary information,
can not only zero-shot transfer to a new domain, but achieve competitive
performance compared to state-of-the-art sequential recommender models with
only a fraction of the model size. In addition, with a simple post-hoc
interpolation, PrepRec can improve the performance of existing sequential
recommenders on average by 13.8\% in Recall@10 and 29.5% in NDCG@10. We provide
an anonymized implementation of PrepRec at
https://anonymous.4open.science/r/PrepRec--2F60/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A First Look at Information Highlighting in Stack Overflow Answers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahla Shaan Ahmed, Shaowei Wang, Yuan Tian,  Tse-Hsun,  Chen, Haoxiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Context: Navigating the knowledge of Stack Overflow (SO) remains challenging.
To make the posts vivid to users, SO allows users to write and edit posts with
Markdown or HTML so that users can leverage various formatting styles (e.g.,
bold, italic, and code) to highlight the important information. Nonetheless,
there have been limited studies on the highlighted information. Objective: We
carried out the first large-scale exploratory study on the information
highlighted in SO answers in our recent study. To extend our previous study, we
develop approaches to automatically recommend highlighted content with
formatting styles using neural network architectures initially designed for the
Named Entity Recognition task. Method: In this paper, we studied 31,169,429
answers of Stack Overflow. For training recommendation models, we choose CNN
and BERT models for each type of formatting (i.e., Bold, Italic, Code, and
Heading) using the information highlighting dataset we collected from SO
answers. Results: Our models based on CNN architecture achieve precision
ranging from 0.71 to 0.82. The trained model for automatic code content
highlighting achieves a recall of 0.73 and an F1 score of 0.71, outperforming
the trained models for other formatting styles. The BERT models have even lower
recalls and F1 scores than the CNN models. Our analysis of failure cases
indicates that the majority of the failure cases are missing identification
(i.e., the model misses the content that is supposed to be highlighted) due to
the models tend to learn the frequently highlighted words while struggling to
learn less frequent words. Conclusion: Our findings suggest that it is possible
to develop recommendation models for highlighting information for answers with
different formatting styles on Stack Overflow.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work is submitted to Information and Software Technology Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tailor: Size Recommendations for High-End Fashion Marketplaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01978v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01978v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandre Candeias, Ivo Silva, Vitor Sousa, José Marcelino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the ever-changing and dynamic realm of high-end fashion marketplaces,
providing accurate and personalized size recommendations has become a critical
aspect. Meeting customer expectations in this regard is not only crucial for
ensuring their satisfaction but also plays a pivotal role in driving customer
retention, which is a key metric for the success of any fashion retailer. We
propose a novel sequence classification approach to address this problem,
integrating implicit (Add2Bag) and explicit (ReturnReason) user signals. Our
approach comprises two distinct models: one employs LSTMs to encode the user
signals, while the other leverages an Attention mechanism. Our best model
outperforms SFNet, improving accuracy by 45.7%. By using Add2Bag interactions
we increase the user coverage by 24.5% when compared with only using Orders.
Moreover, we evaluate the models' usability in real-time recommendation
scenarios by conducting experiments to measure their latency performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in FashionXRecsys23 held at the 17th ACM Conference on
  Recommender Systems, 18th-22nd September 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating LLMs on Document-Based QA: Exact Answer Selection and
  Numerical Extraction using Cogtale <span class="highlight-title">dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07878v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07878v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zafaryab Rasool, Stefanus Kurniawan, Sherwin Balugo, Scott Barnett, Rajesh Vasa, Courtney Chesser, Benjamin M. Hampstead, Sylvie Belleville, Kon Mouzakis, Alex Bahar-Fuchs
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document-based Question-Answering (QA) tasks are crucial for precise
information retrieval. While some existing work focus on evaluating large
language models performance on retrieving and answering questions from
documents, assessing the LLMs performance on QA types that require exact answer
selection from predefined options and numerical extraction is yet to be fully
assessed. In this paper, we specifically focus on this underexplored context
and conduct empirical analysis of LLMs (GPT-4 and GPT-3.5) on question types,
including single-choice, yes-no, multiple-choice, and number extraction
questions from documents in zero-shot setting. We use the CogTale dataset for
evaluation, which provide human expert-tagged responses, offering a robust
benchmark for precision and factual grounding. We found that LLMs, particularly
GPT-4, can precisely answer many single-choice and yes-no questions given
relevant context, demonstrating their efficacy in information retrieval tasks.
However, their performance diminishes when confronted with multiple-choice and
number extraction formats, lowering the overall performance of the model on
this task, indicating that these models may not yet be sufficiently reliable
for the task. This limits the applications of LLMs on applications demanding
precise information extraction from documents, such as meta-analysis tasks.
These findings hinge on the assumption that the retrievers furnish pertinent
context necessary for accurate responses, emphasizing the need for further
research. Our work offers a framework for ongoing dataset evaluation, ensuring
that LLM applications for information retrieval and document analysis continue
to meet evolving standards.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unlocking the Potential of Large Language Models for Explainable
  Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.15661v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.15661v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yucong Luo, Mingyue Cheng, Hao Zhang, Junyu Lu, Qi Liu, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating user-friendly explanations regarding why an item is recommended
has become increasingly common, largely due to advances in language generation
technology, which can enhance user trust and facilitate more informed
decision-making when using online services. However, existing explainable
recommendation systems focus on using small-size language models. It remains
uncertain what impact replacing the explanation generator with the recently
emerging large language models (LLMs) would have. Can we expect unprecedented
results?
  In this study, we propose LLMXRec, a simple yet effective two-stage
explainable recommendation framework aimed at further boosting the explanation
quality by employing LLMs. Unlike most existing LLM-based recommendation works,
a key characteristic of LLMXRec is its emphasis on the close collaboration
between previous recommender models and LLM-based explanation generators.
Specifically, by adopting several key fine-tuning techniques, including
parameter-efficient instructing tuning and personalized prompt techniques,
controllable and fluent explanations can be well generated to achieve the goal
of explanation recommendation. Most notably, we provide three different
perspectives to evaluate the effectiveness of the explanations. Finally, we
conduct extensive experiments over several benchmark recommender models and
publicly available datasets. The experimental results not only yield positive
results in terms of effectiveness and efficiency but also uncover some
previously unknown outcomes. To facilitate further explorations in this area,
the full code and detailed original results are open-sourced at
https://github.com/GodFire66666/LLM_rec_explanation/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GMMFormer: Gaussian-Mixture-Model Based <span class="highlight-title">Transformer</span> for Efficient
  Partially Relevant Video Retrieval <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05195v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05195v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuting Wang, Jinpeng Wang, Bin Chen, Ziyun Zeng, Shu-Tao Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a text query, partially relevant video retrieval (PRVR) seeks to find
untrimmed videos containing pertinent moments in a database. For PRVR, clip
modeling is essential to capture the partial relationship between texts and
videos. Current PRVR methods adopt scanning-based clip construction to achieve
explicit clip modeling, which is information-redundant and requires a large
storage overhead. To solve the efficiency problem of PRVR methods, this paper
proposes GMMFormer, a Gaussian-Mixture-Model based Transformer which models
clip representations implicitly. During frame interactions, we incorporate
Gaussian-Mixture-Model constraints to focus each frame on its adjacent frames
instead of the whole video. Then generated representations will contain
multi-scale clip information, achieving implicit clip modeling. In addition,
PRVR methods ignore semantic differences between text queries relevant to the
same video, leading to a sparse embedding space. We propose a query diverse
loss to distinguish these text queries, making the embedding space more
intensive and contain more semantic information. Extensive experiments on three
large-scale video datasets (i.e., TVR, ActivityNet Captions, and Charades-STA)
demonstrate the superiority and efficiency of GMMFormer. Code is available at
\url{https://github.com/huangmozhi9527/GMMFormer}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2024. Code is released at
  https://github.com/huangmozhi9527/GMMFormer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Pre-train</span>ed Recommender Systems: A Causal Debiasing Perspective <span class="chip">WSDM 24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19251v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19251v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqian Lin, Hao Ding, Nghia Hoang, Branislav Kveton, Anoop Deoras, Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies on pre-trained vision/language models have demonstrated the
practical benefit of a new, promising solution-building paradigm in AI where
models can be pre-trained on broad data describing a generic task space and
then adapted successfully to solve a wide range of downstream tasks, even when
training data is severely limited (e.g., in zero- or few-shot learning
scenarios). Inspired by such progress, we investigate in this paper the
possibilities and challenges of adapting such a paradigm to the context of
recommender systems, which is less investigated from the perspective of
pre-trained model. In particular, we propose to develop a generic recommender
that captures universal interaction patterns by training on generic user-item
interaction data extracted from different domains, which can then be fast
adapted to improve few-shot learning performance in unseen new domains (with
limited data).
  However, unlike vision/language data which share strong conformity in the
semantic space, universal patterns underlying recommendation data collected
across different domains (e.g., different countries or different E-commerce
platforms) are often occluded by both in-domain and cross-domain biases
implicitly imposed by the cultural differences in their user and item bases, as
well as their uses of different e-commerce platforms. As shown in our
experiments, such heterogeneous biases in the data tend to hinder the
effectiveness of the pre-trained model. To address this challenge, we further
introduce and formalize a causal debiasing perspective, which is substantiated
via a hierarchical Bayesian deep learning model, named PreRec. Our empirical
studies on real-world data show that the proposed model could significantly
improve the recommendation performance in zero- and few-shot learning settings
under both cross-market and cross-platform scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, WSDM 24</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">111</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mining Temporal Attack Patterns from Cy<span class="highlight-title">bert</span>hreat Intelligence Reports 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01883v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01883v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Rayhanur Rahman, Brandon Wroblewski, Quinn Matthews, Brantley Morgan, Tim Menzies, Laurie Williams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Defending from cyberattacks requires practitioners to operate on high-level
adversary behavior. Cyberthreat intelligence (CTI) reports on past cyberattack
incidents describe the chain of malicious actions with respect to time. To
avoid repeating cyberattack incidents, practitioners must proactively identify
and defend against recurring chain of actions - which we refer to as temporal
attack patterns. Automatically mining the patterns among actions provides
structured and actionable information on the adversary behavior of past
cyberattacks. The goal of this paper is to aid security practitioners in
prioritizing and proactive defense against cyberattacks by mining temporal
attack patterns from cyberthreat intelligence reports. To this end, we propose
ChronoCTI, an automated pipeline for mining temporal attack patterns from
cyberthreat intelligence (CTI) reports of past cyberattacks. To construct
ChronoCTI, we build the ground truth dataset of temporal attack patterns and
apply state-of-the-art large language models, natural language processing, and
machine learning techniques. We apply ChronoCTI on a set of 713 CTI reports,
where we identify 124 temporal attack patterns - which we categorize into nine
pattern categories. We identify that the most prevalent pattern category is to
trick victim users into executing malicious code to initiate the attack,
followed by bypassing the anti-malware system in the victim network. Based on
the observed patterns, we advocate organizations to train users about
cybersecurity best practices, introduce immutable operating systems with
limited functionalities, and enforce multi-user authentications. Moreover, we
advocate practitioners to leverage the automated mining capability of ChronoCTI
and design countermeasures against the recurring attack patterns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A modified version of this pre-print is submitted to IEEE
  Transactions on Software Engineering, and is under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Theoretical guarantees on the best-of-n alignment policy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01879v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01879v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Beirami, Alekh Agarwal, Jonathan Berant, Alexander D'Amour, Jacob Eisenstein, Chirag Nagpal, Ananda Theertha Suresh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A simple and effective method for the alignment of generative models is the
best-of-$n$ policy, where $n$ samples are drawn from a base policy, and ranked
based on a reward function, and the highest ranking one is selected. A commonly
used analytical expression in the literature claims that the KL divergence
between the best-of-$n$ policy and the base policy is equal to $\log (n) -
(n-1)/n.$ We disprove the validity of this claim, and show that it is an upper
bound on the actual KL divergence. We also explore the tightness of this upper
bound in different regimes. Finally, we propose a new estimator for the KL
divergence and empirically show that it provides a tight approximation through
a few examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Neural Networks for Surfactant Multi-Property Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01874v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01874v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christoforos Brozos, Jan G. Rittig, Sandip Bhattacharya, Elie Akanny, Christina Kohlmann, Alexander Mitsos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surfactants are of high importance in different industrial sectors such as
cosmetics, detergents, oil recovery and drug delivery systems. Therefore, many
quantitative structure-property relationship (QSPR) models have been developed
for surfactants. Each predictive model typically focuses on one surfactant
class, mostly nonionics. Graph Neural Networks (GNNs) have exhibited a great
predictive performance for property prediction of ionic liquids, polymers and
drugs in general. Specifically for surfactants, GNNs can successfully predict
critical micelle concentration (CMC), a key surfactant property associated with
micellization. A key factor in the predictive ability of QSPR and GNN models is
the data available for training. Based on extensive literature search, we
create the largest available CMC database with 429 molecules and the first
large data collection for surface excess concentration ($\Gamma$$_{m}$),
another surfactant property associated with foaming, with 164 molecules. Then,
we develop GNN models to predict the CMC and $\Gamma$$_{m}$ and we explore
different learning approaches, i.e., single- and multi-task learning, as well
as different training strategies, namely ensemble and transfer learning. We
find that a multi-task GNN with ensemble learning trained on all $\Gamma$$_{m}$
and CMC data performs best. Finally, we test the ability of our CMC model to
generalize on industrial grade pure component surfactants. The GNN yields
highly accurate predictions for CMC, showing great potential for future
industrial applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the hardness of learning under symmetries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01869v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01869v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bobak T. Kiani, Thien Le, Hannah Lawrence, Stefanie Jegelka, Melanie Weber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of learning equivariant neural networks via gradient
descent. The incorporation of known symmetries ("equivariance") into neural
nets has empirically improved the performance of learning pipelines, in domains
ranging from biology to computer vision. However, a rich yet separate line of
learning theoretic research has demonstrated that actually learning shallow,
fully-connected (i.e. non-symmetric) networks has exponential complexity in the
correlational statistical query (CSQ) model, a framework encompassing gradient
descent. In this work, we ask: are known problem symmetries sufficient to
alleviate the fundamental hardness of learning neural nets with gradient
descent? We answer this question in the negative. In particular, we give lower
bounds for shallow graph neural networks, convolutional networks, invariant
polynomials, and frame-averaged networks for permutation subgroups, which all
scale either superpolynomially or exponentially in the relevant input
dimension. Therefore, in spite of the significant inductive bias imparted via
symmetry, actually learning the complete classes of functions represented by
equivariant neural networks via gradient descent remains hard.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>52 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Dataset</span> Difficulty and the Role of Inductive Bias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01867v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01867v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Devin Kwok, Nikhil Anand, Jonathan Frankle, Gintare Karolina Dziugaite, David Rolnick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by the goals of dataset pruning and defect identification, a
growing body of methods have been developed to score individual examples within
a dataset. These methods, which we call "example difficulty scores", are
typically used to rank or categorize examples, but the consistency of rankings
between different training runs, scoring methods, and model architectures is
generally unknown. To determine how example rankings vary due to these random
and controlled effects, we systematically compare different formulations of
scores over a range of runs and model architectures. We find that scores
largely share the following traits: they are noisy over individual runs of a
model, strongly correlated with a single notion of difficulty, and reveal
examples that range from being highly sensitive to insensitive to the inductive
biases of certain model architectures. Drawing from statistical genetics, we
develop a simple method for fingerprinting model architectures using a few
sensitive examples. These findings guide practitioners in maximizing the
consistency of their scores (e.g. by choosing appropriate scoring methods,
number of runs, and subsets of examples), and establishes comprehensive
baselines for evaluating scores in the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Vision Check-up for Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01862v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01862v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pratyusha Sharma, Tamar Rott Shaham, Manel Baradad, Stephanie Fu, Adrian Rodriguez-Munoz, Shivam Duggal, Phillip Isola, Antonio Torralba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What does learning to model relationships between strings teach large
language models (LLMs) about the visual world? We systematically evaluate LLMs'
abilities to generate and recognize an assortment of visual concepts of
increasing complexity and then demonstrate how a preliminary visual
representation learning system can be trained using models of text. As language
models lack the ability to consume or output visual information as pixels, we
use code to represent images in our study. Although LLM-generated images do not
look like natural images, results on image generation and the ability of models
to correct these generated images indicate that precise modeling of strings can
teach language models about numerous aspects of the visual world. Furthermore,
experiments on self-supervised visual representation learning, utilizing images
generated with text models, highlight the potential to train vision models
capable of making semantic assessments of natural images using just LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal cross-learning for contextual bandits with unknown context
  distributions <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01857v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01857v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jon Schneider, Julian Zimmert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of designing contextual bandit algorithms in the
``cross-learning'' setting of Balseiro et al., where the learner observes the
loss for the action they play in all possible contexts, not just the context of
the current round. We specifically consider the setting where losses are chosen
adversarially and contexts are sampled i.i.d. from an unknown distribution. In
this setting, we resolve an open problem of Balseiro et al. by providing an
efficient algorithm with a nearly tight (up to logarithmic factors) regret
bound of $\widetilde{O}(\sqrt{TK})$, independent of the number of contexts. As
a consequence, we obtain the first nearly tight regret bounds for the problems
of learning to bid in first-price auctions (under unknown value distributions)
and sleeping bandits with a stochastic action set.
  At the core of our algorithm is a novel technique for coordinating the
execution of a learning algorithm over multiple epochs in such a way to remove
correlations between estimation of the unknown distribution and the actions
played by the algorithm. This technique may be of independent interest for
other learning problems involving estimation of an unknown context
distribution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appeared at NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span> Neural Autoregressive Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Massimiliano Patacchiola, Aliaksandra Shysheya, Katja Hofmann, Richard E. Turner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Density estimation, a central problem in machine learning, can be performed
using Normalizing Flows (NFs). NFs comprise a sequence of invertible
transformations, that turn a complex target distribution into a simple one, by
exploiting the change of variables theorem. Neural Autoregressive Flows (NAFs)
and Block Neural Autoregressive Flows (B-NAFs) are arguably the most perfomant
members of the NF family. However, they suffer scalability issues and training
instability due to the constraints imposed on the network structure. In this
paper, we propose a novel solution to these challenges by exploiting
transformers to define a new class of neural flows called Transformer Neural
Autoregressive Flows (T-NAFs). T-NAFs treat each dimension of a random variable
as a separate input token, using attention masking to enforce an autoregressive
constraint. We take an amortization-inspired approach where the transformer
outputs the parameters of an invertible transformation. The experimental
results demonstrate that T-NAFs consistently match or outperform NAFs and
B-NAFs across multiple datasets from the UCI benchmark. Remarkably, T-NAFs
achieve these results using an order of magnitude fewer parameters than
previous approaches, without composing multiple flows.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multilingual Instruction Tuning With Just a Pinch of Multilinguality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Uri Shaham, Jonathan Herzig, Roee Aharoni, Idan Szpektor, Reut Tsarfaty, Matan Eyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As instruction-tuned large language models (LLMs) gain global adoption, their
ability to follow instructions in multiple languages becomes increasingly
crucial. One promising approach is cross-lingual transfer, where a model
acquires specific functionality on some language by finetuning on another
language. In this work, we investigate how multilinguality during instruction
tuning of a multilingual LLM affects instruction-following across languages. We
first show that many languages transfer some instruction-following capabilities
to other languages from even monolingual tuning. Furthermore, we find that only
40 multilingual examples in an English tuning set substantially improve
multilingual instruction-following, both in seen and unseen languages during
tuning. In general, we observe that models tuned on multilingual mixtures
exhibit comparable or superior performance in several languages compared to
monolingually tuned models, despite training on 10x fewer examples in those
languages. Finally, we find that increasing the number of languages in the
instruction tuning set from 1 to only 2, 3, or 4 increases cross-lingual
generalization. Our results suggest that building massively multilingual
instruction-tuned models can be done with only a very small set of multilingual
instruction-responses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Power of Training: How Different Neural Network Setups Influence the
  Energy Demand 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01851v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01851v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Geißler, Bo Zhou, Mengxi Liu, Sungho Suh, Paul Lukowicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work examines the effects of variations in machine learning training
regimes and learning paradigms on the corresponding energy consumption. While
increasing data availability and innovation in high-performance hardware fuels
the training of sophisticated models, it also supports the fading perception of
energy consumption and carbon emission. Therefore, the goal of this work is to
create awareness about the energy impact of general training parameters and
processes, from learning rate over batch size to knowledge transfer. Multiple
setups with different hyperparameter initializations are evaluated on two
different hardware configurations to obtain meaningful results. Experiments on
pretraining and multitask training are conducted on top of the baseline results
to determine their potential towards sustainable machine learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DGDNN: Decoupled Graph Diffusion Neural Network for Stock Movement
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zinuo You, Zijian Shi, Hongbo Bo, John Cartlidge, Li Zhang, Yan Ge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Forecasting future stock trends remains challenging for academia and industry
due to stochastic inter-stock dynamics and hierarchical intra-stock dynamics
influencing stock prices. In recent years, graph neural networks have achieved
remarkable performance in this problem by formulating multiple stocks as
graph-structured data. However, most of these approaches rely on artificially
defined factors to construct static stock graphs, which fail to capture the
intrinsic interdependencies between stocks that rapidly evolve. In addition,
these methods often ignore the hierarchical features of the stocks and lose
distinctive information within. In this work, we propose a novel graph learning
approach implemented without expert knowledge to address these issues. First,
our approach automatically constructs dynamic stock graphs by entropy-driven
edge generation from a signal processing perspective. Then, we further learn
task-optimal dependencies between stocks via a generalized graph diffusion
process on constructed stock graphs. Last, a decoupled representation learning
scheme is adopted to capture distinctive hierarchical intra-stock features.
Experimental results demonstrate substantial improvements over state-of-the-art
baselines on real-world datasets. Moreover, the ablation study and sensitivity
study further illustrate the effectiveness of the proposed method in modeling
the time-evolving inter-stock and intra-stock dynamics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures, author manuscript accepted for ICAART 2024
  (International Conference on Agents and Artificial Intelligence)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating Semi-Supervised Learning Algorithms in Text <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01843v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01843v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Himmet Toprak Kesgin, Mehmet Fatih Amasyali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using large training datasets enhances the generalization capabilities of
neural networks. Semi-supervised learning (SSL) is useful when there are few
labeled data and a lot of unlabeled data. SSL methods that use data
augmentation are most successful for image datasets. In contrast, texts do not
have consistent augmentation methods as images. Consequently, methods that use
augmentation are not as effective in text data as they are in image data. In
this study, we compared SSL algorithms that do not require augmentation; these
are self-training, co-training, tri-training, and tri-training with
disagreement. In the experiments, we used 4 different text datasets for
different tasks. We examined the algorithms from a variety of perspectives by
asking experiment questions and suggested several improvements. Among the
algorithms, tri-training with disagreement showed the closest performance to
the Oracle; however, performance gap shows that new semi-supervised algorithms
or improvements in existing methods are needed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Innovations in Intelligent Systems and Applications Conference (ASYU)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wasserstein Nonnegative Tensor Factorization with Manifold
  Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01842v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01842v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianyu Wang, Linruize Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nonnegative tensor factorization (NTF) has become an important tool for
feature extraction and part-based representation with preserved intrinsic
structure information from nonnegative high-order data. However, the original
NTF methods utilize Euclidean or Kullback-Leibler divergence as the loss
function which treats each feature equally leading to the neglect of the
side-information of features. To utilize correlation information of features
and manifold information of samples, we introduce Wasserstein manifold
nonnegative tensor factorization (WMNTF), which minimizes the Wasserstein
distance between the distribution of input tensorial data and the distribution
of reconstruction. Although some researches about Wasserstein distance have
been proposed in nonnegative matrix factorization (NMF), they ignore the
spatial structure information of higher-order data. We use Wasserstein distance
(a.k.a Earth Mover's distance or Optimal Transport distance) as a metric and
add a graph regularizer to a latent factor. Experimental results demonstrate
the effectiveness of the proposed method compared with other NMF and NTF
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Act as You Learn: Adaptive Decision-Making in Non-Stationary Markov
  Decision Processes <span class="chip">AAMAS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baiting Luo, Yunuo Zhang, Abhishek Dubey, Ayan Mukhopadhyay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental (and largely open) challenge in sequential decision-making is
dealing with non-stationary environments, where exogenous environmental
conditions change over time. Such problems are traditionally modeled as
non-stationary Markov decision processes (NSMDP). However, existing approaches
for decision-making in NSMDPs have two major shortcomings: first, they assume
that the updated environmental dynamics at the current time are known (although
future dynamics can change); and second, planning is largely pessimistic, i.e.,
the agent acts ``safely'' to account for the non-stationary evolution of the
environment. We argue that both these assumptions are invalid in practice --
updated environmental conditions are rarely known, and as the agent interacts
with the environment, it can learn about the updated dynamics and avoid being
pessimistic, at least in states whose dynamics it is confident about. We
present a heuristic search algorithm called \textit{Adaptive Monte Carlo Tree
Search (ADA-MCTS)} that addresses these challenges. We show that the agent can
learn the updated dynamics of the environment over time and then act as it
learns, i.e., if the agent is in a region of the state space about which it has
updated knowledge, it can avoid being pessimistic. To quantify ``updated
knowledge,'' we disintegrate the aleatoric and epistemic uncertainty in the
agent's updated belief and show how the agent can use these estimates for
decision-making. We compare the proposed approach with the multiple
state-of-the-art approaches in decision-making across multiple well-established
open-source problems and empirically show that our approach is faster and
highly adaptive without sacrificing safety.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the International Conference on
  Autonomous Agents and MultiAgent Systems (AAMAS), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Iterative Mask Filling: An Effective Text Augmentation Method Using
  Masked Language Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Himmet Toprak Kesgin, Mehmet Fatih Amasyali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data augmentation is an effective technique for improving the performance of
machine learning models. However, it has not been explored as extensively in
natural language processing (NLP) as it has in computer vision. In this paper,
we propose a novel text augmentation method that leverages the Fill-Mask
feature of the transformer-based BERT model. Our method involves iteratively
masking words in a sentence and replacing them with language model predictions.
We have tested our proposed method on various NLP tasks and found it to be
effective in many cases. Our results are presented along with a comparison to
existing augmentation methods. Experimental results show that our proposed
method significantly improves performance, especially on topic classification
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in International Conference on Advanced Engineering,
  Technology and Applications (ICAETA 2023). The final version is available
  online at https://link.springer.com/chapter/10.1007/978-3-031-50920-9_35</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Signal Processing in the Retina: Interpretable Graph Classifier to
  Predict Ganglion Cell Responses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01813v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01813v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasaman Parhizkar, Gene Cheung, Andrew W. Eckford
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is a popular hypothesis in neuroscience that ganglion cells in the retina
are activated by selectively detecting visual features in an observed scene.
While ganglion cell firings can be predicted via data-trained deep neural nets,
the networks remain indecipherable, thus providing little understanding of the
cells' underlying operations. To extract knowledge from the cell firings, in
this paper we learn an interpretable graph-based classifier from data to
predict the firings of ganglion cells in response to visual stimuli.
Specifically, we learn a positive semi-definite (PSD) metric matrix $\mathbf{M}
\succeq 0$ that defines Mahalanobis distances between graph nodes (visual
events) endowed with pre-computed feature vectors; the computed inter-node
distances lead to edge weights and a combinatorial graph that is amenable to
binary classification. Mathematically, we define the objective of metric matrix
$\mathbf{M}$ optimization using a graph adaptation of large margin nearest
neighbor (LMNN), which is rewritten as a semi-definite programming (SDP)
problem. We solve it efficiently via a fast approximation called Gershgorin
disc perfect alignment (GDPA) linearization. The learned metric matrix
$\mathbf{M}$ provides interpretability: important features are identified along
$\mathbf{M}$'s diagonal, and their mutual relationships are inferred from
off-diagonal terms. Our fast metric learning framework can be applied to other
biological systems with pre-chosen features that require interpretation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A quatum inspired neural network for geometric modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01801v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01801v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weitao Du, Shengchao Liu, Hongyu Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By conceiving physical systems as 3D many-body point clouds, geometric graph
neural networks (GNNs), such as SE(3)/E(3) equivalent GNNs, have showcased
promising performance. In particular, their effective message-passing mechanics
make them adept at modeling molecules and crystalline materials. However,
current geometric GNNs only offer a mean-field approximation of the many-body
system, encapsulated within two-body message passing, thus falling short in
capturing intricate relationships within these geometric graphs. To address
this limitation, tensor networks, widely employed by computational physics to
handle manybody systems using high-order tensors, have been introduced.
Nevertheless, integrating these tensorized networks into the message-passing
framework of GNNs faces scalability and symmetry conservation (e.g.,
permutation and rotation) challenges. In response, we introduce an innovative
equivariant Matrix Product State (MPS)-based message-passing strategy, through
achieving an efficient implementation of the tensor contraction operation. Our
method effectively models complex many-body relationships, suppressing
mean-field approximations, and captures symmetries within geometric graphs.
Importantly, it seamlessly replaces the standard message-passing and
layer-aggregation modules intrinsic to geometric GNNs. We empirically validate
the superior accuracy of our approach on benchmark tasks, including predicting
classical Newton systems and quantum tensor Hamiltonian matrices. To our
knowledge, our approach represents the inaugural utilization of parameterized
geometric tensor networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoMoSVC: Consistency Model-based Singing Voice Conversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01792v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01792v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwen Lu, Zhen Ye, Wei Xue, Xu Tan, Qifeng Liu, Yike Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The diffusion-based Singing Voice Conversion (SVC) methods have achieved
remarkable performances, producing natural audios with high similarity to the
target timbre. However, the iterative sampling process results in slow
inference speed, and acceleration thus becomes crucial. In this paper, we
propose CoMoSVC, a consistency model-based SVC method, which aims to achieve
both high-quality generation and high-speed sampling. A diffusion-based teacher
model is first specially designed for SVC, and a student model is further
distilled under self-consistency properties to achieve one-step sampling.
Experiments on a single NVIDIA GTX4090 GPU reveal that although CoMoSVC has a
significantly faster inference speed than the state-of-the-art (SOTA)
diffusion-based SVC system, it still achieves comparable or superior conversion
performance based on both subjective and objective metrics. Audio samples and
codes are available at https://comosvc.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep learning the Hurst parameter of linear fractional processes and
  assessing its reliability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01789v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01789v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dániel Boros, Bálint Csanády, Iván Ivkovic, Lóránt Nagy, András Lukács, László Márkus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research explores the reliability of deep learning, specifically Long
Short-Term Memory (LSTM) networks, for estimating the Hurst parameter in
fractional stochastic processes. The study focuses on three types of processes:
fractional Brownian motion (fBm), fractional Ornstein-Uhlenbeck (fOU) process,
and linear fractional stable motions (lfsm). The work involves a fast
generation of extensive datasets for fBm and fOU to train the LSTM network on a
large volume of data in a feasible time. The study analyses the accuracy of the
LSTM network's Hurst parameter estimation regarding various performance
measures like RMSE, MAE, MRE, and quantiles of the absolute and relative
errors. It finds that LSTM outperforms the traditional statistical methods in
the case of fBm and fOU processes; however, it has limited accuracy on lfsm
processes. The research also delves into the implications of training length
and valuation sequence length on the LSTM's performance. The methodology is
applied by estimating the Hurst parameter in Li-ion battery degradation data
and obtaining confidence bounds for the estimation. The study concludes that
while deep learning methods show promise in parameter estimation of fractional
processes, their effectiveness is contingent on the process type and the
quality of training data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Applications of machine learning and IoT for Outdoor Air Pollution
  Monitoring and Prediction: A Systematic Literature <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ihsane Gryech, Chaimae Assad, Mounir Ghogho, Abdellatif Kobbane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  According to the World Health Organization (WHO), air pollution kills seven
million people every year. Outdoor air pollution is a major environmental
health problem affecting low, middle, and high-income countries. In the past
few years, the research community has explored IoT-enabled machine learning
applications for outdoor air pollution prediction. The general objective of
this paper is to systematically review applications of machine learning and
Internet of Things (IoT) for outdoor air pollution prediction and the
combination of monitoring sensors and input features used. Two research
questions were formulated for this review. 1086 publications were collected in
the initial PRISMA stage. After the screening and eligibility phases, 37 papers
were selected for inclusion. A cost-based analysis was conducted on the
findings to highlight high-cost monitoring, low-cost IoT and hybrid enabled
prediction. Three methods of prediction were identified: time series,
feature-based and spatio-temporal. This review's findings identify major
limitations in applications found in the literature, namely lack of coverage,
lack of diversity of data and lack of inclusion of context-specific features.
This review proposes directions for future research and underlines practical
implications in healthcare, urban planning, global synergy and smart cities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Approximating Numerical Flux by Fourier Neural Operators for the
  Hyperbolic Conservation Laws 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taeyoung Kim, Myungjoo Sang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classical numerical schemes exist for solving PDEs numerically, and recently,
neural network-based methods have been developed. However, methodologies using
neural networks, such as PINN and neural operators, lack robustness and
generalization power. To compensate for such drawbacks, there are many types of
research combining classical numerical schemes and machine learning methods by
replacing a small portion of the numerical schemes with neural networks. In
this work, we focus on hyperbolic conservation laws and replace numerical
fluxes in the numerical schemes by neural operator. For this, we construct
losses that are motivated by numerical schemes for conservation laws and
approximate numerical flux by FNO. Through experiments, we show that our
methodology has advantages of both numerical schemes and FNO by comparing with
original methods. For instance, we demonstrate our method gains robustness,
resolution invariance property, and feasibility of a data-driven method. Our
method especially has the ability to predict continuously in time and
generalization power on the out-of-distribution samples, which are challenges
to be tackled for existing neural operator methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 28 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating the Suitability of Concept Drift Detection for Detecting
  Leakages in Water Distribution Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valerie Vaquet, Fabian Hinder, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leakages are a major risk in water distribution networks as they cause water
loss and increase contamination risks. Leakage detection is a difficult task
due to the complex dynamics of water distribution networks. In particular,
small leakages are hard to detect. From a machine-learning perspective,
leakages can be modeled as concept drift. Thus, a wide variety of drift
detection schemes seems to be a suitable choice for detecting leakages. In this
work, we explore the potential of model-loss-based and distribution-based drift
detection methods to tackle leakage detection. We additionally discuss the
issue of temporal dependencies in the data and propose a way to cope with it
when applying distribution-based detection. We evaluate different methods
systematically for leakages of different sizes and detection times.
Additionally, we propose a first drift-detection-based technique for localizing
leakages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Task and Explanation Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01732v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01732v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moshe Sipper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainability in deep networks has gained increased importance in recent
years. We argue herein that an AI must be tasked not just with a task but also
with an explanation of why said task was accomplished as such. We present a
basic framework -- Task and Explanation Network (TENet) -- which fully
integrates task completion and its explanation. We believe that the field of AI
as a whole should insist -- quite emphatically -- on explainability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ravnest: Decentralized Asynchronous Training on Heterogeneous Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anirudh Rajiv Menon, Unnikrishnan Menon, Kailash Ahirwar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern deep learning models, growing larger and more complex, have
demonstrated exceptional generalization and accuracy due to training on huge
datasets. This trend is expected to continue. However, the increasing size of
these models poses challenges in training, as traditional centralized methods
are limited by memory constraints at such scales. This paper proposes an
asynchronous decentralized training paradigm for large modern deep learning
models that harnesses the compute power of regular heterogeneous PCs with
limited resources connected across the internet to achieve favourable
performance metrics. Ravnest facilitates decentralized training by efficiently
organizing compute nodes into clusters with similar data transfer rates and
compute capabilities, without necessitating that each node hosts the entire
model. These clusters engage in $\textit{Zero-Bubble Asynchronous Model
Parallel}$ training, and a $\textit{Parallel Multi-Ring All-Reduce}$ method is
employed to effectively execute global parameter averaging across all clusters.
We have framed our asynchronous SGD loss function as a block structured
optimization problem with delayed updates and derived an optimal convergence
rate of $O\left(\frac{1}{\sqrt{K}}\right)$. We further discuss linear speedup
with respect to the number of participating clusters and the bound on the
staleness parameter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EPA: Neural Collapse Inspired Robust Out-of-Distribution Detector <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Zhang, Yufan Chen, Cheng Jin, Lei Zhu, Yuantao Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) detection plays a crucial role in ensuring the
security of neural networks. Existing works have leveraged the fact that
In-distribution (ID) samples form a subspace in the feature space, achieving
state-of-the-art (SOTA) performance. However, the comprehensive characteristics
of the ID subspace still leave under-explored. Recently, the discovery of
Neural Collapse ($\mathcal{NC}$) sheds light on novel properties of the ID
subspace. Leveraging insight from $\mathcal{NC}$, we observe that the Principal
Angle between the features and the ID feature subspace forms a superior
representation for measuring the likelihood of OOD. Building upon this
observation, we propose a novel $\mathcal{NC}$-inspired OOD scoring function,
named Entropy-enhanced Principal Angle (EPA), which integrates both the global
characteristic of the ID subspace and its inner property. We experimentally
compare EPA with various SOTA approaches, validating its superior performance
and robustness across different network architectures and OOD datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-shot Active Learning Using Self Supervised Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhishek Sinha, Shreya Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning algorithms are often said to be data hungry. The performance of
such algorithms generally improve as more and more annotated data is fed into
the model. While collecting unlabelled data is easier (as they can be scraped
easily from the internet), annotating them is a tedious and expensive task.
Given a fixed budget available for data annotation, Active Learning helps
selecting the best subset of data for annotation, such that the deep learning
model when trained over that subset will have maximum generalization
performance under this budget. In this work, we aim to propose a new Active
Learning approach which is model agnostic as well as one doesn't require an
iterative process. We aim to leverage self-supervised learnt features for the
task of Active Learning. The benefit of self-supervised learning, is that one
can get useful feature representation of the input data, without having any
annotation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LESEN: Label-Efficient deep learning for Multi-parametric MRI-based
  Visual Pathway Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alou Diakite, Cheng Li, Lei Xie, Yuanjing Feng, Hua Han, Shanshan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research has shown the potential of deep learning in multi-parametric
MRI-based visual pathway (VP) segmentation. However, obtaining labeled data for
training is laborious and time-consuming. Therefore, it is crucial to develop
effective algorithms in situations with limited labeled samples. In this work,
we propose a label-efficient deep learning method with self-ensembling (LESEN).
LESEN incorporates supervised and unsupervised losses, enabling the student and
teacher models to mutually learn from each other, forming a self-ensembling
mean teacher framework. Additionally, we introduce a reliable unlabeled sample
selection (RUSS) mechanism to further enhance LESEN's effectiveness. Our
experiments on the human connectome project (HCP) dataset demonstrate the
superior performance of our method when compared to state-of-the-art
techniques, advancing multimodal VP segmentation for comprehensive analysis in
clinical and research settings. The implementation code will be available at:
https://github.com/aldiak/Semi-Supervised-Multimodal-Visual-Pathway-
Delineation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Foundation Purchasing Model: <span class="highlight-title">Pretrain</span>ed Generative
  Autoregression on Transaction Sequences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01641v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01641v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piotr Skalski, David Sutton, Stuart Burrell, Iker Perez, Jason Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models underpin many modern financial systems for use cases
such as fraud detection and churn prediction. Most are based on supervised
learning with hand-engineered features, which relies heavily on the
availability of labelled data. Large self-supervised generative models have
shown tremendous success in natural language processing and computer vision,
yet so far they haven't been adapted to multivariate time series of financial
transactions. In this paper, we present a generative pretraining method that
can be used to obtain contextualised embeddings of financial transactions.
Benchmarks on public datasets demonstrate that it outperforms state-of-the-art
self-supervised methods on a range of downstream tasks. We additionally perform
large-scale pretraining of an embedding model using a corpus of data from 180
issuing banks containing 5.1 billion transactions and apply it to the card
fraud detection problem on hold-out datasets. The embedding model significantly
improves value detection rate at high precision thresholds and transfers well
to out-of-domain distributions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Fairness in <span class="highlight-title">Self-supervised</span> and Supervised Models for
  Sequential Data <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sofia Yfantidou, Dimitris Spathis, Marios Constantinides, Athena Vakali, Daniele Quercia, Fahim Kawsar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) has become the de facto training paradigm of
large models where pre-training is followed by supervised fine-tuning using
domain-specific data and labels. Hypothesizing that SSL models would learn more
generic, hence less biased, representations, this study explores the impact of
pre-training and fine-tuning strategies on fairness (i.e., performing equally
on different demographic breakdowns). Motivated by human-centric applications
on real-world timeseries data, we interpret inductive biases on the model,
layer, and metric levels by systematically comparing SSL models to their
supervised counterparts. Our findings demonstrate that SSL has the capacity to
achieve performance on par with supervised methods while significantly
enhancing fairness--exhibiting up to a 27% increase in fairness with a mere 1%
loss in performance through self-supervision. Ultimately, this work underscores
SSL's potential in human-centric computing, particularly high-stakes,
data-scarce application domains like healthcare.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted in Human-Centric Representation Learning workshop at
  AAAI 2024 (https://hcrl-workshop.github.io/2024/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthetic Data in AI: Challenges, Applications, and Ethical Implications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuang Hao, Wenfeng Han, Tao Jiang, Yiping Li, Haonan Wu, Chunlin Zhong, Zhangjun Zhou, He Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the rapidly evolving field of artificial intelligence, the creation and
utilization of synthetic datasets have become increasingly significant. This
report delves into the multifaceted aspects of synthetic data, particularly
emphasizing the challenges and potential biases these datasets may harbor. It
explores the methodologies behind synthetic data generation, spanning
traditional statistical models to advanced deep learning techniques, and
examines their applications across diverse domains. The report also critically
addresses the ethical considerations and legal implications associated with
synthetic datasets, highlighting the urgent need for mechanisms to ensure
fairness, mitigate biases, and uphold ethical standards in AI development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Expressive Power of Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashwin Nalwade, Kelly Marshall, Axel Eladi, Umang Sharma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The study of Graph Neural Networks has received considerable interest in the
past few years. By extending deep learning to graph-structured data, GNNs can
solve a diverse set of tasks in fields including social science, chemistry, and
medicine. The development of GNN architectures has largely been focused on
improving empirical performance on tasks like node or graph classification.
However, a line of recent work has instead sought to find GNN architectures
that have desirable theoretical properties - by studying their expressive power
and designing architectures that maximize this expressiveness.
  While there is no consensus on the best way to define the expressiveness of a
GNN, it can be viewed from several well-motivated perspectives. Perhaps the
most natural approach is to study the universal approximation properties of
GNNs, much in the way that this has been studied extensively for MLPs. Another
direction focuses on the extent to which GNNs can distinguish between different
graph structures, relating this to the graph isomorphism test. Besides, a GNN's
ability to compute graph properties such as graph moments has been suggested as
another form of expressiveness. All of these different definitions are
complementary and have yielded different recommendations for GNN architecture
choices. In this paper, we would like to give an overview of the notion of
"expressive power" of GNNs and provide some valuable insights regarding the
design choices of GNNs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SCALA: Sparsification-based Contrastive Learning for Anomaly Detection
  on Attributed Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01625v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01625v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enbo He, Yitong Hao, Yue Zhang, Guisheng Yin, Lina Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection on attributed networks aims to find the nodes whose
behaviors are significantly different from other majority nodes. Generally,
network data contains information about relationships between entities, and the
anomaly is usually embodied in these relationships. Therefore, how to
comprehensively model complex interaction patterns in networks is still a major
focus. It can be observed that anomalies in networks violate the homophily
assumption. However, most existing studies only considered this phenomenon
obliquely rather than explicitly. Besides, the node representation of normal
entities can be perturbed easily by the noise relationships introduced by
anomalous nodes. To address the above issues, we present a novel contrastive
learning framework for anomaly detection on attributed networks,
\textbf{SCALA}, aiming to improve the embedding quality of the network and
provide a new measurement of qualifying the anomaly score for each node by
introducing sparsification into the conventional method. Extensive experiments
are conducted on five benchmark real-world datasets and the results show that
SCALA consistently outperforms all baseline methods significantly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PLLaMa: An Open-source Large Language Model for Plant Science 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianjun Yang, Junfeng Gao, Wenxin Xue, Erik Alexandersson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have exhibited remarkable capabilities in
understanding and interacting with natural language across various sectors.
However, their effectiveness is limited in specialized areas requiring high
accuracy, such as plant science, due to a lack of specific expertise in these
fields. This paper introduces PLLaMa, an open-source language model that
evolved from LLaMa-2. It's enhanced with a comprehensive database, comprising
more than 1.5 million scholarly articles in plant science. This development
significantly enriches PLLaMa with extensive knowledge and proficiency in plant
and agricultural sciences. Our initial tests, involving specific datasets
related to plants and agriculture, show that PLLaMa substantially improves its
understanding of plant science-related topics. Moreover, we have formed an
international panel of professionals, including plant scientists, agricultural
engineers, and plant breeders. This team plays a crucial role in verifying the
accuracy of PLLaMa's responses to various academic inquiries, ensuring its
effective and reliable application in the field. To support further research
and development, we have made the model's checkpoints and source codes
accessible to the scientific community. These resources are available for
download at \url{https://github.com/Xianjun-Yang/PLLaMa}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalization Error Curves for Analytic Spectral Algorithms under
  Power-law Decay 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01599v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01599v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yicheng Li, Weiye Gan, Zuoqiang Shi, Qian Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The generalization error curve of certain kernel regression method aims at
determining the exact order of generalization error with various source
condition, noise level and choice of the regularization parameter rather than
the minimax rate. In this work, under mild assumptions, we rigorously provide a
full characterization of the generalization error curves of the kernel gradient
descent method (and a large class of analytic spectral algorithms) in kernel
regression. Consequently, we could sharpen the near inconsistency of kernel
interpolation and clarify the saturation effects of kernel regression
algorithms with higher qualification, etc. Thanks to the neural tangent kernel
theory, these results greatly improve our understanding of the generalization
behavior of training the wide neural networks. A novel technical contribution,
the analytic functional argument, might be of independent interest.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Invariant Information Geometric Method for High-Dimensional Online
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengfei Zhang, Yunyue Wei, Yanan Sui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sample efficiency is crucial in optimization, particularly in black-box
scenarios characterized by expensive evaluations and zeroth-order feedback.
When computing resources are plentiful, Bayesian optimization is often favored
over evolution strategies. In this paper, we introduce a full invariance
oriented evolution strategies algorithm, derived from its corresponding
framework, that effectively rivals the leading Bayesian optimization method in
tasks with dimensions at the upper limit of Bayesian capability. Specifically,
we first build the framework InvIGO that fully incorporates historical
information while retaining the full invariant and computational complexity. We
then exemplify InvIGO on multi-dimensional Gaussian, which gives an invariant
and scalable optimizer SynCMA . The theoretical behavior and advantages of our
algorithm over other Gaussian-based evolution strategies are further analyzed.
Finally, We benchmark SynCMA against leading algorithms in Bayesian
optimization and evolution strategies on various high dimension tasks, in
cluding Mujoco locomotion tasks, rover planning task and synthetic functions.
In all scenarios, SynCMA demonstrates great competence, if not dominance, over
other algorithms in sample efficiency, showing the underdeveloped potential of
property oriented evolution strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Modeling Uncertainties of Self-explaining Neural Networks via
  Conformal Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Qian, Chenxu Zhao, Yangyi Li, Fenglong Ma, Chao Zhang, Mengdi Huai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the recent progress in deep neural networks (DNNs), it remains
challenging to explain the predictions made by DNNs. Existing explanation
methods for DNNs mainly focus on post-hoc explanations where another
explanatory model is employed to provide explanations. The fact that post-hoc
methods can fail to reveal the actual original reasoning process of DNNs raises
the need to build DNNs with built-in interpretability. Motivated by this, many
self-explaining neural networks have been proposed to generate not only
accurate predictions but also clear and intuitive insights into why a
particular decision was made. However, existing self-explaining networks are
limited in providing distribution-free uncertainty quantification for the two
simultaneously generated prediction outcomes (i.e., a sample's final prediction
and its corresponding explanations for interpreting that prediction).
Importantly, they also fail to establish a connection between the confidence
values assigned to the generated explanations in the interpretation layer and
those allocated to the final predictions in the ultimate prediction layer. To
tackle the aforementioned challenges, in this paper, we design a novel
uncertainty modeling framework for self-explaining networks, which not only
demonstrates strong distribution-free uncertainty modeling performance for the
generated explanations in the interpretation layer but also excels in producing
efficient and effective prediction sets for the final predictions based on the
informative high-level basis explanations. We perform the theoretical analysis
for the proposed framework. Extensive experimental evaluation demonstrates the
effectiveness of the proposed uncertainty framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Art of Deception: Robust Backdoor Attack using Dynamic Stacking of
  Triggers <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Orson Mengara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The area of Machine Learning as a Service (MLaaS) is experiencing increased
implementation due to recent advancements in the AI (Artificial Intelligence)
industry. However, this spike has prompted concerns regarding AI defense
mechanisms, specifically regarding potential covert attacks from third-party
providers that cannot be entirely trusted. Recent research has uncovered that
auditory backdoors may use certain modifications as their initiating mechanism.
DynamicTrigger is introduced as a methodology for carrying out dynamic backdoor
attacks that use cleverly designed tweaks to ensure that corrupted samples are
indistinguishable from clean. By utilizing fluctuating signal sampling rates
and masking speaker identities through dynamic sound triggers (such as the
clapping of hands), it is possible to deceive speech recognition systems (ASR).
Our empirical testing demonstrates that DynamicTrigger is both potent and
stealthy, achieving impressive success rates during covert attacks while
maintaining exceptional accuracy with non-poisoned datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI Workshop 2024, 8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Will 6G be Semantic Communications? Opportunities and Challenges from
  Task Oriented and Secure Communications to Integrated Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01531v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01531v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yalin E. Sagduyu, Tugba Erpek, Aylin Yener, Sennur Ulukus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores opportunities and challenges of task (goal)-oriented and
semantic communications for next-generation (NextG) communication networks
through the integration of multi-task learning. This approach employs deep
neural networks representing a dedicated encoder at the transmitter and
multiple task-specific decoders at the receiver, collectively trained to handle
diverse tasks including semantic information preservation, source input
reconstruction, and integrated sensing and communications. To extend the
applicability from point-to-point links to multi-receiver settings, we envision
the deployment of decoders at various receivers, where decentralized learning
addresses the challenges of communication load and privacy concerns, leveraging
federated learning techniques that distribute model updates across
decentralized nodes. However, the efficacy of this approach is contingent on
the robustness of the employed deep learning models. We scrutinize potential
vulnerabilities stemming from adversarial attacks during both training and
testing phases. These attacks aim to manipulate both the inputs at the encoder
at the transmitter and the signals received over the air on the receiver side,
highlighting the importance of fortifying semantic communications against
potential multi-domain exploits. Overall, the joint and robust design of
task-oriented communications, semantic communications, and integrated sensing
and communications in a multi-task learning framework emerges as the key
enabler for context-aware, resource-efficient, and secure communications
ultimately needed in NextG network systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Bandits in Many-to-one Matching Markets with Incentive
  Compatibility <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fang Kong, Shuai Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Two-sided matching markets have been widely studied in the literature due to
their rich applications. Since participants are usually uncertain about their
preferences, online algorithms have recently been adopted to learn them through
iterative interactions. \citet{wang2022bandit} initiate the study of this
problem in a many-to-one setting with \textit{responsiveness}. However, their
results are far from optimal and lack guarantees of incentive compatibility. An
extension of \citet{kong2023player} to this more general setting achieves a
near-optimal bound for player-optimal regret. Nevertheless, due to the
substantial requirement for collaboration, a single player's deviation could
lead to a huge increase in its own cumulative rewards and an $O(T)$ regret for
others. In this paper, we aim to enhance the regret bound in many-to-one
markets while ensuring incentive compatibility. We first propose the adaptively
explore-then-deferred-acceptance (AETDA) algorithm for responsiveness setting
and derive an $O(N\min\left\{N,K\right\}C\log T/\Delta^2)$ upper bound for
player-optimal stable regret while demonstrating its guarantee of incentive
compatibility, where $N$ represents the number of players, $K$ is the number of
arms, $T$ denotes the time horizon, $C$ is arms' total capacities and $\Delta$
signifies the minimum preference gap among players. This result is a
significant improvement over \citet{wang2022bandit}. And to the best of our
knowledge, it constitutes the first player-optimal guarantee in matching
markets that offers such robust assurances. We also consider broader
\textit{substitutable} preferences, one of the most general conditions to
ensure the existence of a stable matching and cover responsiveness. We devise
an online DA (ODA) algorithm and establish an $O(NK\log T/\Delta^2)$
player-pessimal stable regret bound for this setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ S$^{2}$-DMs:Skip-Step Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixuan Wang, Shuangyin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have emerged as powerful generative tools, rivaling GANs in
sample quality and mirroring the likelihood scores of autoregressive models. A
subset of these models, exemplified by DDIMs, exhibit an inherent asymmetry:
they are trained over $T$ steps but only sample from a subset of $T$ during
generation. This selective sampling approach, though optimized for speed,
inadvertently misses out on vital information from the unsampled steps, leading
to potential compromises in sample quality. To address this issue, we present
the S$^{2}$-DMs, which is a new training method by using an innovative
$L_{skip}$, meticulously designed to reintegrate the information omitted during
the selective sampling phase. The benefits of this approach are manifold: it
notably enhances sample quality, is exceptionally simple to implement, requires
minimal code modifications, and is flexible enough to be compatible with
various sampling algorithms. On the CIFAR10 dataset, models trained using our
algorithm showed an improvement of 3.27% to 14.06% over models trained with
traditional methods across various sampling algorithms (DDIMs, PNDMs, DEIS) and
different numbers of sampling steps (10, 20, ..., 1000). On the CELEBA dataset,
the improvement ranged from 8.97% to 27.08%. Access to the code and additional
resources is provided in the github.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Frontiers of LLMs in Psychological Applications: A
  Comprehensive <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01519v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01519v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luoma Ke, Song Tong, Peng Chen, Kaiping Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the frontiers of large language models (LLMs) in
psychology applications. Psychology has undergone several theoretical changes,
and the current use of Artificial Intelligence (AI) and Machine Learning,
particularly LLMs, promises to open up new research directions. We provide a
detailed exploration of how LLMs like ChatGPT are transforming psychological
research. It discusses the impact of LLMs across various branches of
psychology, including cognitive and behavioral, clinical and counseling,
educational and developmental, and social and cultural psychology, highlighting
their potential to simulate aspects of human cognition and behavior. The paper
delves into the capabilities of these models to emulate human-like text
generation, offering innovative tools for literature review, hypothesis
generation, experimental design, experimental subjects, data analysis, academic
writing, and peer review in psychology. While LLMs are essential in advancing
research methodologies in psychology, the paper also cautions about their
technical and ethical challenges. There are issues like data privacy, the
ethical implications of using LLMs in psychological research, and the need for
a deeper understanding of these models' limitations. Researchers should
responsibly use LLMs in psychological studies, adhering to ethical standards
and considering the potential consequences of deploying these technologies in
sensitive areas. Overall, the article provides a comprehensive overview of the
current state of LLMs in psychology, exploring potential benefits and
challenges. It serves as a call to action for researchers to leverage LLLs'
advantages responsibly while addressing associated risks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AIRI: Predicting Retention Indices and their Uncertainties using
  Artificial Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lewis Y. Geer, Stephen E. Stein, William Gary Mallard, Douglas J. Slotta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Kov\'ats Retention index (RI) is a quantity measured using gas
chromatography and commonly used in the identification of chemical structures.
Creating libraries of observed RI values is a laborious task, so we explore the
use of a deep neural network for predicting RI values from structure for
standard semipolar columns. This network generated predictions with a mean
absolute error of 15.1 and, in a quantification of the tail of the error
distribution, a 95th percentile absolute error of 46.5. Because of the
Artificial Intelligence Retention Indices (AIRI) network's accuracy, it was
used to predict RI values for the NIST EI-MS spectral libraries. These RI
values are used to improve chemical identification methods and the quality of
the library. Estimating uncertainty is an important practical need when using
prediction models. To quantify the uncertainty of our network for each
individual prediction, we used the outputs of an ensemble of 8 networks to
calculate a predicted standard deviation for each RI value prediction. This
predicted standard deviation was corrected to follow the error between observed
and predicted RI values. The Z scores using these predicted standard deviations
had a standard deviation of 1.52 and a 95th percentile absolute Z score
corresponding to a mean RI value of 42.6.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pontryagin Neural Operator for Solving Parametric General-Sum
  Differential Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Zhang, Mukesh Ghimire, Zhe Xu, Wenlong Zhang, Yi Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The values of two-player general-sum differential games are viscosity
solutions to Hamilton-Jacobi-Isaacs (HJI) equations. Value and policy
approximations for such games suffer from the curse of dimensionality (CoD).
Alleviating CoD through physics-informed neural networks (PINN) encounters
convergence issues when value discontinuity is present due to state
constraints. On top of these challenges, it is often necessary to learn
generalizable values and policies across a parametric space of games, e.g., for
game parameter inference when information is incomplete. To address these
challenges, we propose in this paper a Pontryagin-mode neural operator that
outperforms existing state-of-the-art (SOTA) on safety performance across games
with parametric state constraints. Our key contribution is the introduction of
a costate loss defined on the discrepancy between forward and backward costate
rollouts, which are computationally cheap. We show that the discontinuity of
costate dynamics (in the presence of state constraints) effectively enables the
learning of discontinuous values, without requiring manually supervised data as
suggested by the current SOTA. More importantly, we show that the close
relationship between costates and policies makes the former critical in
learning feedback control policies with generalizable safety performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to L4DC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Utilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic
  Token Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01498v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01498v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minchan Kim, Myeonghun Jeong, Byoung Jin Choi, Semin Kim, Joun Yeop Lee, Nam Soo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel text-to-speech (TTS) framework centered around a neural
transducer. Our approach divides the whole TTS pipeline into semantic-level
sequence-to-sequence (seq2seq) modeling and fine-grained acoustic modeling
stages, utilizing discrete semantic tokens obtained from wav2vec2.0 embeddings.
For a robust and efficient alignment modeling, we employ a neural transducer
named token transducer for the semantic token prediction, benefiting from its
hard monotonic alignment constraints. Subsequently, a non-autoregressive (NAR)
speech generator efficiently synthesizes waveforms from these semantic tokens.
Additionally, a reference speech controls temporal dynamics and acoustic
conditions at each stage. This decoupled framework reduces the training
complexity of TTS while allowing each stage to focus on semantic and acoustic
modeling. Our experimental results on zero-shot adaptive TTS demonstrate that
our model surpasses the baseline in terms of speech quality and speaker
similarity, both objectively and subjectively. We also delve into the inference
speed and prosody control capabilities of our approach, highlighting the
potential of neural transducers in TTS frameworks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Free Lunch for Federated Remote Sensing Target Fine-Grained
  Classification: A Parameter-Efficient Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01493v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01493v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengchao Chen, Ting Shu, Huan Zhao, Jiahao Wang, Sufen Ren, Lina Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Remote Sensing Target Fine-grained Classification (TFGC) is of great
significance in both military and civilian fields. Due to location differences,
growth in data size, and centralized server storage constraints, these data are
usually stored under different databases across regions/countries. However,
privacy laws and national security concerns constrain researchers from
accessing these sensitive remote sensing images for further analysis.
Additionally, low-resource remote sensing devices encounter challenges in terms
of communication overhead and efficiency when dealing with the ever-increasing
data and model scales. To solve the above challenges, this paper proposes a
novel Privacy-Reserving TFGC Framework based on Federated Learning, dubbed
PRFL. The proposed framework allows each client to learn global and local
knowledge to enhance the local representation of private data in environments
with extreme statistical heterogeneity (non. Independent and Identically
Distributed, IID). Thus, it provides highly customized models to clients with
differentiated data distributions. Moreover, the framework minimizes
communication overhead and improves efficiency while ensuring satisfactory
performance, thereby enhancing robustness and practical applicability under
resource-scarce conditions. We demonstrate the effectiveness of the proposed
PRFL on the classical TFGC task by leveraging four public datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review, 23 pages, 3 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Natural Language Processing and Multimodal Stock Price Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01487v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01487v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Taylor, Jerry Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of financial decision-making, predicting stock prices is
pivotal. Artificial intelligence techniques such as long short-term memory
networks (LSTMs), support-vector machines (SVMs), and natural language
processing (NLP) models are commonly employed to predict said prices. This
paper utilizes stock percentage change as training data, in contrast to the
traditional use of raw currency values, with a focus on analyzing publicly
released news articles. The choice of percentage change aims to provide models
with context regarding the significance of price fluctuations and overall price
change impact on a given stock. The study employs specialized BERT natural
language processing models to predict stock price trends, with a particular
emphasis on various data modalities. The results showcase the capabilities of
such strategies with a small natural language processing model to accurately
predict overall stock trends, and highlight the effectiveness of certain data
features and sector-specific data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty Regularized Evidential Regression <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01484v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01484v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Ye, Tiejin Chen, Hua Wei, Liang Zhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Evidential Regression Network (ERN) represents a novel approach that
integrates deep learning with Dempster-Shafer's theory to predict a target and
quantify the associated uncertainty. Guided by the underlying theory, specific
activation functions must be employed to enforce non-negative values, which is
a constraint that compromises model performance by limiting its ability to
learn from all samples. This paper provides a theoretical analysis of this
limitation and introduces an improvement to overcome it. Initially, we define
the region where the models can't effectively learn from the samples. Following
this, we thoroughly analyze the ERN and investigate this constraint. Leveraging
the insights from our analysis, we address the limitation by introducing a
novel regularization term that empowers the ERN to learn from the whole
training set. Our extensive experiments substantiate our theoretical findings
and demonstrate the effectiveness of the proposed solution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2024 main track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Incorporating Geo-Diverse Knowledge into <span class="highlight-title">Prompt</span>ing for Increased
  Geographical Robustness in Object Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyle Buettner, Sina Malakouti, Xiang Lorraine Li, Adriana Kovashka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing object recognition models have been shown to lack robustness in
diverse geographical scenarios due to significant domain shifts in design and
context. Class representations need to be adapted to more accurately reflect an
object concept under these shifts. In the absence of training data from target
geographies, we hypothesize that geography-specific descriptive knowledge of
object categories can be leveraged to enhance robustness. For this purpose, we
explore the feasibility of probing a large-language model for
geography-specific object knowledge, and we investigate integrating knowledge
in zero-shot and learnable soft prompting with the CLIP vision-language model.
In particular, we propose a geography knowledge regularization method to ensure
that soft prompts trained on a source set of geographies generalize to an
unseen target set of geographies. Our gains on DollarStreet when generalizing
from a model trained only on data from Europe are as large as +2.8 on countries
from Africa, and +4.6 on the hardest classes. We further show competitive
performance vs. few-shot target training, and provide insights into how
descriptive knowledge captures geographical differences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kernel-U-Net: Hierarchical and Symmetrical Framework for Multivariate
  Time Series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiang You, Reńe Natowicz, Arben Cela, Jacob Ouanounou, Patrick Siarry
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series forecasting task predicts future trends based on historical
information. Recent U-Net-based methods have demonstrated superior performance
in predicting real-world datasets. However, the performance of these models is
lower than patch-based models or linear models. In this work, we propose a
symmetric and hierarchical framework, Kernel-U-Net, which cuts the input
sequence into slices at each layer of the network and then computes them using
kernels. Furthermore, it generalizes the concept of convolutional kernels in
classic U-Net to accept custom kernels that follow the same design pattern.
Compared to the existing linear or transformer-based solution, our model
contains 3 advantages: 1) A small number of parameters: the parameters size is
$O(log(L)^2)$ where $L$ is the look-back window size, 2) Flexibility: its
kernels can be customized and fitted to the datasets, 3) Computation
efficiency: the computation complexity of transformer modules is reduced to
$O(log(L)^2)$ if they are placed close to the latent vector. Kernel-U-Net
accuracy was greater than or equal to the state-of-the-art model on six (out of
seven) real-world datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A First Look at Information Highlighting in Stack Overflow Answers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahla Shaan Ahmed, Shaowei Wang, Yuan Tian,  Tse-Hsun,  Chen, Haoxiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Context: Navigating the knowledge of Stack Overflow (SO) remains challenging.
To make the posts vivid to users, SO allows users to write and edit posts with
Markdown or HTML so that users can leverage various formatting styles (e.g.,
bold, italic, and code) to highlight the important information. Nonetheless,
there have been limited studies on the highlighted information. Objective: We
carried out the first large-scale exploratory study on the information
highlighted in SO answers in our recent study. To extend our previous study, we
develop approaches to automatically recommend highlighted content with
formatting styles using neural network architectures initially designed for the
Named Entity Recognition task. Method: In this paper, we studied 31,169,429
answers of Stack Overflow. For training recommendation models, we choose CNN
and BERT models for each type of formatting (i.e., Bold, Italic, Code, and
Heading) using the information highlighting dataset we collected from SO
answers. Results: Our models based on CNN architecture achieve precision
ranging from 0.71 to 0.82. The trained model for automatic code content
highlighting achieves a recall of 0.73 and an F1 score of 0.71, outperforming
the trained models for other formatting styles. The BERT models have even lower
recalls and F1 scores than the CNN models. Our analysis of failure cases
indicates that the majority of the failure cases are missing identification
(i.e., the model misses the content that is supposed to be highlighted) due to
the models tend to learn the frequently highlighted words while struggling to
learn less frequent words. Conclusion: Our findings suggest that it is possible
to develop recommendation models for highlighting information for answers with
different formatting styles on Stack Overflow.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work is submitted to Information and Software Technology Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Token Propagation Controller for Efficient Vision <span class="highlight-title">Transformer</span> <span class="chip">WACV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision transformers (ViTs) have achieved promising results on a variety of
Computer Vision tasks, however their quadratic complexity in the number of
input tokens has limited their application specially in resource-constrained
settings. Previous approaches that employ gradual token reduction to address
this challenge assume that token redundancy in one layer implies redundancy in
all the following layers. We empirically demonstrate that this assumption is
often not correct, i.e., tokens that are redundant in one layer can be useful
in later layers. We employ this key insight to propose a novel token
propagation controller (TPC) that incorporates two different
token-distributions, i.e., pause probability and restart probability to control
the reduction and reuse of tokens respectively, which results in more efficient
token utilization. To improve the estimates of token distributions, we propose
a smoothing mechanism that acts as a regularizer and helps remove noisy
outliers. Furthermore, to improve the training-stability of our proposed TPC,
we introduce a model stabilizer that is able to implicitly encode local image
structures and minimize accuracy fluctuations during model training. We present
extensive experimental results on the ImageNet-1K dataset using DeiT, LV-ViT
and Swin models to demonstrate the effectiveness of our proposed method. For
example, compared to baseline models, our proposed method improves the
inference speed of the DeiT-S by 250% while increasing the classification
accuracy by 1.0%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WACV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mean-Field Assisted Deep Boltzmann Learning with Probabilistic Computers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01996v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01996v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuvro Chowdhury, Shaila Niazi, Kerem Y. Camsari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their appeal as physics-inspired, energy-based and generative nature,
general Boltzmann Machines (BM) are considered intractable to train. This
belief led to simplified models of BMs with restricted intralayer connections
or layer-by-layer training of deep BMs. Recent developments in domain-specific
hardware -- specifically probabilistic computers (p-computer) with
probabilistic bits (p-bit) -- may change established wisdom on the tractability
of deep BMs. In this paper, we show that deep and unrestricted BMs can be
trained using p-computers generating hundreds of billions of Markov Chain Monte
Carlo (MCMC) samples per second, on sparse networks developed originally for
use in D-Wave's annealers. To maximize the efficiency of learning the
p-computer, we introduce two families of Mean-Field Theory assisted learning
algorithms, or xMFTs (x = Naive and Hierarchical). The xMFTs are used to
estimate the averages and correlations during the positive phase of the
contrastive divergence (CD) algorithm and our custom-designed p-computer is
used to estimate the averages and correlations in the negative phase. A custom
Field-Programmable-Gate Array (FPGA) emulation of the p-computer architecture
takes up to 45 billion flips per second, allowing the implementation of CD-$n$
where $n$ can be of the order of millions, unlike RBMs where $n$ is typically 1
or 2. Experiments on the full MNIST dataset with the combined algorithm show
that the positive phase can be efficiently computed by xMFTs without much
degradation when the negative phase is computed by the p-computer. Our
algorithm can be used in other scalable Ising machines and its variants can be
used to train BMs, previously thought to be intractable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GPS-SSL: Guided Positive Sampling to Inject Prior Into <span class="highlight-title">Self-Supervised</span>
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aarash Feizi, Randall Balestriero, Adriana Romero-Soriano, Reihaneh Rabbany
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Guided Positive Sampling Self-Supervised Learning (GPS-SSL), a
general method to inject a priori knowledge into Self-Supervised Learning (SSL)
positive samples selection. Current SSL methods leverage Data-Augmentations
(DA) for generating positive samples and incorporate prior knowledge - an
incorrect, or too weak DA will drastically reduce the quality of the learned
representation. GPS-SSL proposes instead to design a metric space where
Euclidean distances become a meaningful proxy for semantic relationship. In
that space, it is now possible to generate positive samples from nearest
neighbor sampling. Any prior knowledge can now be embedded into that metric
space independently from the employed DA. From its simplicity, GPS-SSL is
applicable to any SSL method, e.g. SimCLR or BYOL. A key benefit of GPS-SSL is
in reducing the pressure in tailoring strong DAs. For example GPS-SSL reaches
85.58% on Cifar10 with weak DA while the baseline only reaches 37.51%. We
therefore move a step forward towards the goal of making SSL less reliant on
DA. We also show that even when using strong DAs, GPS-SSL outperforms the
baselines on under-studied domains. We evaluate GPS-SSL along with multiple
baseline SSL methods on numerous downstream datasets from different domains
when the models use strong or minimal data augmentations. We hope that GPS-SSL
will open new avenues in studying how to inject a priori knowledge into SSL in
a principled manner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Representation Learning of Multivariate Time Series using Attention and
  Adversarial Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01987v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01987v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leon Scharwächter, Sebastian Otte
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A critical factor in trustworthy machine learning is to develop robust
representations of the training data. Only under this guarantee methods are
legitimate to artificially generate data, for example, to counteract imbalanced
datasets or provide counterfactual explanations for blackbox decision-making
systems. In recent years, Generative Adversarial Networks (GANs) have shown
considerable results in forming stable representations and generating realistic
data. While many applications focus on generating image data, less effort has
been made in generating time series data, especially multivariate signals. In
this work, a Transformer-based autoencoder is proposed that is regularized
using an adversarial training scheme to generate artificial multivariate time
series signals. The representation is evaluated using t-SNE visualizations,
Dynamic Time Warping (DTW) and Entropy scores. Our results indicate that the
generated signals exhibit higher similarity to an exemplary dataset than using
a convolutional network approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Regrets: Geometric Metrics for Bayesian Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jungtaek Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian optimization is a principled optimization strategy for a black-box
objective function. It shows its effectiveness in a wide variety of real-world
applications such as scientific discovery and experimental design. In general,
the performance of Bayesian optimization is assessed by regret-based metrics
such as instantaneous, simple, and cumulative regrets. These metrics only rely
on function evaluations, so that they do not consider geometric relationships
between query points and global solutions, or query points themselves. Notably,
they cannot discriminate if multiple global solutions are successfully found.
Moreover, they do not evaluate Bayesian optimization's abilities to exploit and
explore a search space given. To tackle these issues, we propose four new
geometric metrics, i.e., precision, recall, average degree, and average
distance. These metrics allow us to compare Bayesian optimization algorithms
considering the geometry of both query points and global optima, or query
points. However, they are accompanied by an extra parameter, which needs to be
carefully determined. We therefore devise the parameter-free forms of the
respective metrics by integrating out the additional parameter. Finally, we
empirically validate that our proposed metrics can provide more convincing
interpretation and understanding of Bayesian optimization algorithms from
distinct perspectives, compared to the conventional metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 25 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tailor: Size Recommendations for High-End Fashion Marketplaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01978v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01978v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandre Candeias, Ivo Silva, Vitor Sousa, José Marcelino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the ever-changing and dynamic realm of high-end fashion marketplaces,
providing accurate and personalized size recommendations has become a critical
aspect. Meeting customer expectations in this regard is not only crucial for
ensuring their satisfaction but also plays a pivotal role in driving customer
retention, which is a key metric for the success of any fashion retailer. We
propose a novel sequence classification approach to address this problem,
integrating implicit (Add2Bag) and explicit (ReturnReason) user signals. Our
approach comprises two distinct models: one employs LSTMs to encode the user
signals, while the other leverages an Attention mechanism. Our best model
outperforms SFNet, improving accuracy by 45.7%. By using Add2Bag interactions
we increase the user coverage by 24.5% when compared with only using Orders.
Moreover, we evaluate the models' usability in real-time recommendation
scenarios by conducting experiments to measure their latency performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in FashionXRecsys23 held at the 17th ACM Conference on
  Recommender Systems, 18th-22nd September 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as
  Programmers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksandar Stanić, Sergi Caelles, Michael Tschannen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual reasoning is dominated by end-to-end neural networks scaled to
billions of model parameters and training examples. However, even the largest
models struggle with compositional reasoning, generalization, fine-grained
spatial and temporal reasoning, and counting. Visual reasoning with large
language models (LLMs) as controllers can, in principle, address these
limitations by decomposing the task and solving subtasks by orchestrating a set
of (visual) tools. Recently, these models achieved great performance on tasks
such as compositional visual question answering, visual grounding, and video
temporal reasoning. Nevertheless, in their current form, these models heavily
rely on human engineering of in-context examples in the prompt, which are often
dataset- and task-specific and require significant labor by highly skilled
programmers. In this work, we present a framework that mitigates these issues
by introducing spatially and temporally abstract routines and by leveraging a
small number of labeled examples to automatically generate in-context examples,
thereby avoiding human-created in-context examples. On a number of visual
reasoning tasks, we show that our framework leads to consistent gains in
performance, makes LLMs as controllers setup more robust, and removes the need
for human engineering of in-context examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can We Generate Realistic Hands Only Using Convolution? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01951v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01951v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehran Hosseini, Peyman Hosseini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The enduring inability of image generative models to recreate intricate
geometric features, such as those present in human hands and fingers has been
an ongoing problem in image generation for nearly a decade. While strides have
been made by increasing model sizes and diversifying training datasets, this
issue remains prevalent across all models, from denoising diffusion models to
Generative Adversarial Networks (GAN), pointing to a fundamental shortcoming in
the underlying architectures. In this paper, we demonstrate how this problem
can be mitigated by augmenting convolution layers geometric capabilities
through providing them with a single input channel incorporating the relative
$n$-dimensional Cartesian coordinate system. We show that this drastically
improves quality of hand and face images generated by GANs and Variational
AutoEncoders (VAE).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Contains 17 pages, 14 figures, and 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IoT in the Era of Generative AI: Vision and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01923v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01923v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Wang, Zhongwei Wan, Arvin Hekmati, Mingyu Zong, Samiul Alam, Mi Zhang, Bhaskar Krishnamachari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Equipped with sensing, networking, and computing capabilities, Internet of
Things (IoT) such as smartphones, wearables, smart speakers, and household
robots have been seamlessly weaved into our daily lives. Recent advancements in
Generative AI exemplified by GPT, LLaMA, DALL-E, and Stable Difussion hold
immense promise to push IoT to the next level. In this article, we share our
vision and views on the benefits that Generative AI brings to IoT, and discuss
some of the most important applications of Generative AI in IoT-related
domains. Fully harnessing Generative AI in IoT is a complex challenge. We
identify some of the most critical challenges including high resource demands
of the Generative AI models, prompt engineering, on-device inference,
offloading, on-device fine-tuning, federated learning, security, as well as
development tools and benchmarks, and discuss current gaps as well as promising
opportunities on enabling Generative AI for IoT. We hope this article can
inspire new research on IoT in the era of Generative AI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Object-Centric Learning from Multiple Unspecified
  Viewpoints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01922v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01922v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyang Yuan, Tonglin Chen, Zhimeng Shen, Bin Li, Xiangyang Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual scenes are extremely diverse, not only because there are infinite
possible combinations of objects and backgrounds but also because the
observations of the same scene may vary greatly with the change of viewpoints.
When observing a multi-object visual scene from multiple viewpoints, humans can
perceive the scene compositionally from each viewpoint while achieving the
so-called ``object constancy'' across different viewpoints, even though the
exact viewpoints are untold. This ability is essential for humans to identify
the same object while moving and to learn from vision efficiently. It is
intriguing to design models that have a similar ability. In this paper, we
consider a novel problem of learning compositional scene representations from
multiple unspecified (i.e., unknown and unrelated) viewpoints without using any
supervision and propose a deep generative model which separates latent
representations into a viewpoint-independent part and a viewpoint-dependent
part to solve this problem. During the inference, latent representations are
randomly initialized and iteratively updated by integrating the information in
different viewpoints with neural networks. Experiments on several specifically
designed synthetic datasets have shown that the proposed method can effectively
learn from multiple unspecified viewpoints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2112.03568</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Brain Tumor Segmentation (BraTS) Challenge 2023: Focus on Pediatrics
  (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17033v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17033v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anahita Fathi Kazerooni, Nastaran Khalili, Xinyang Liu, Debanjan Haldar, Zhifan Jiang, Syed Muhammed Anwar, Jake Albrecht, Maruf Adewole, Udunna Anazodo, Hannah Anderson, Sina Bagheri, Ujjwal Baid, Timothy Bergquist, Austin J. Borja, Evan Calabrese, Verena Chung, Gian-Marco Conte, Farouk Dako, James Eddy, Ivan Ezhov, Ariana Familiar, Keyvan Farahani, Shuvanjan Haldar, Juan Eugenio Iglesias, Anastasia Janas, Elaine Johansen, Blaise V Jones, Florian Kofler, Dominic LaBella, Hollie Anne Lai, Koen Van Leemput, Hongwei Bran Li, Nazanin Maleki, Aaron S McAllister, Zeke Meier, Bjoern Menze, Ahmed W Moawad, Khanak K Nandolia, Julija Pavaine, Marie Piraud, Tina Poussaint, Sanjay P Prabhu, Zachary Reitman, Andres Rodriguez, Jeffrey D Rudie, Ibraheem Salman Shaikh, Lubdha M. Shah, Nakul Sheth, Russel Taki Shinohara, Wenxin Tu, Karthik Viswanathan, Chunhao Wang, Jeffrey B Ware, Benedikt Wiestler, Walter Wiggins, Anna Zapaishchykova, Mariam Aboian, Miriam Bornhorst, Peter de Blank, Michelle Deutsch, Maryam Fouladi, Lindsey Hoffman, Benjamin Kann, Margot Lazow, Leonie Mikael, Ali Nabavizadeh, Roger Packer, Adam Resnick, Brian Rood, Arastoo Vossough, Spyridon Bakas, Marius George Linguraru
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pediatric tumors of the central nervous system are the most common cause of
cancer-related death in children. The five-year survival rate for high-grade
gliomas in children is less than 20\%. Due to their rarity, the diagnosis of
these entities is often delayed, their treatment is mainly based on historic
treatment concepts, and clinical trials require multi-institutional
collaborations. The MICCAI Brain Tumor Segmentation (BraTS) Challenge is a
landmark community benchmark event with a successful history of 12 years of
resource creation for the segmentation and analysis of adult glioma. Here we
present the CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023 challenge, which
represents the first BraTS challenge focused on pediatric brain tumors with
data acquired across multiple international consortia dedicated to pediatric
neuro-oncology and clinical trials. The BraTS-PEDs 2023 challenge focuses on
benchmarking the development of volumentric segmentation algorithms for
pediatric brain glioma through standardized quantitative performance evaluation
metrics utilized across the BraTS 2023 cluster of challenges. Models gaining
knowledge from the BraTS-PEDs multi-parametric structural MRI (mpMRI) training
data will be evaluated on separate validation and unseen test mpMRI dataof
high-grade pediatric glioma. The CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023
challenge brings together clinicians and AI/imaging scientists to lead to
faster development of automated segmentation techniques that could benefit
clinical trials, and ultimately the care of children with brain tumors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A unified recipe for deriving (time-uniform) PAC-Bayes bounds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.03421v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.03421v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ben Chugg, Hongjian Wang, Aaditya Ramdas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a unified framework for deriving PAC-Bayesian generalization
bounds. Unlike most previous literature on this topic, our bounds are
anytime-valid (i.e., time-uniform), meaning that they hold at all stopping
times, not only for a fixed sample size. Our approach combines four tools in
the following order: (a) nonnegative supermartingales or reverse
submartingales, (b) the method of mixtures, (c) the Donsker-Varadhan formula
(or other convex duality principles), and (d) Ville's inequality. Our main
result is a PAC-Bayes theorem which holds for a wide class of discrete
stochastic processes. We show how this result implies time-uniform versions of
well-known classical PAC-Bayes bounds, such as those of Seeger, McAllester,
Maurer, and Catoni, in addition to many recent bounds. We also present several
novel bounds. Our framework also enables us to relax traditional assumptions;
in particular, we consider nonstationary loss functions and non-i.i.d. data. In
sum, we unify the derivation of past bounds and ease the search for future
bounds: one may simply check if our supermartingale or submartingale conditions
are met and, if so, be guaranteed a (time-uniform) PAC-Bayes bound.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>56 pages. Published in the Journal of Machine Learning Research,
  Volume 24 Issue 372</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CardiGraphormer: Unveiling the Power of <span class="highlight-title">Self-Supervised</span> Learning in
  Revolutionizing Drug Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.00859v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.00859v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhijit Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the expansive realm of drug discovery, with approximately 15,000 known
drugs and only around 4,200 approved, the combinatorial nature of the chemical
space presents a formidable challenge. While Artificial Intelligence (AI) has
emerged as a powerful ally, traditional AI frameworks face significant hurdles.
This manuscript introduces CardiGraphormer, a groundbreaking approach that
synergizes self-supervised learning (SSL), Graph Neural Networks (GNNs), and
Cardinality Preserving Attention to revolutionize drug discovery.
CardiGraphormer, a novel combination of Graphormer and Cardinality Preserving
Attention, leverages SSL to learn potent molecular representations and employs
GNNs to extract molecular fingerprints, enhancing predictive performance and
interpretability while reducing computation time. It excels in handling complex
data like molecular structures and performs tasks associated with nodes, pairs
of nodes, subgraphs, or entire graph structures. CardiGraphormer's potential
applications in drug discovery and drug interactions are vast, from identifying
new drug targets to predicting drug-to-drug interactions and enabling novel
drug discovery. This innovative approach provides an AI-enhanced methodology in
drug development, utilizing SSL combined with GNNs to overcome existing
limitations and pave the way for a richer exploration of the vast combinatorial
chemical space in drug discovery.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prediction of good reaction coordinates and future evolution of MD
  trajectories using Regularized Sparse Autoencoders: A novel deep learning
  approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.10962v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.10962v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhijit Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying reaction coordinates(RCs) is an active area of research, given
the crucial role RCs play in determining the progress of a chemical reaction.
The choice of the reaction coordinate is often based on heuristic knowledge.
However, an essential criterion for the choice is that the coordinate should
capture both the reactant and product states unequivocally. Also, the
coordinate should be the slowest one so that all the other degrees of freedom
can easily equilibrate along the reaction coordinate. Also, the coordinate
should be the slowest one so that all the other degrees of freedom can easily
equilibrate along the reaction coordinate. We used a regularised sparse
autoencoder, an energy-based model, to discover a crucial set of reaction
coordinates. Along with discovering reaction coordinates, our model also
predicts the evolution of a molecular dynamics(MD) trajectory. We showcased
that including sparsity enforcing regularisation helps in choosing a small but
important set of reaction coordinates. We used two model systems to demonstrate
our approach: alanine dipeptide system and proflavine and DNA system, which
exhibited intercalation of proflavine into DNA minor groove in an aqueous
environment. We model MD trajectory as a multivariate time series, and our
latent variable model performs the task of multi-step time series prediction.
This idea is inspired by the popular sparse coding approach - to represent each
input sample as a linear combination of few elements taken from a set of
representative patterns.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridging the Gap Between Target Networks and Functional Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.12282v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.12282v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandre Piche, Valentin Thomas, Joseph Marino, Rafael Pardinas, Gian Maria Marconi, Christopher Pal, Mohammad Emtiyaz Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bootstrapping is behind much of the successes of Deep Reinforcement Learning.
However, learning the value function via bootstrapping often leads to unstable
training due to fast-changing target values. Target Networks are employed to
stabilize training by using an additional set of lagging parameters to estimate
the target values. Despite the popularity of Target Networks, their effect on
the optimization is still misunderstood. In this work, we show that they act as
an implicit regularizer. This regularizer has disadvantages such as being
inflexible and non convex. To overcome these issues, we propose an explicit
Functional Regularization that is a convex regularizer in function space and
can easily be tuned. We analyze the convergence of our method theoretically and
empirically demonstrate that replacing Target Networks with the more
theoretically grounded Functional Regularization approach leads to better
sample efficiency and performance improvements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The published version of this paper (TMLR 2023) is available at
  arXiv:2106.02613 and https://openreview.net/forum?id=BFvoemrmqX</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ M3D: <span class="highlight-title">Dataset</span> Condensation by Minimizing Maximum Mean Discrepancy <span class="chip">AAAI-24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.15927v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.15927v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hansong Zhang, Shikun Li, Pengju Wang, Dan Zeng, Shiming Ge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training state-of-the-art (SOTA) deep models often requires extensive data,
resulting in substantial training and storage costs. To address these
challenges, dataset condensation has been developed to learn a small synthetic
set that preserves essential information from the original large-scale dataset.
Nowadays, optimization-oriented methods have been the primary method in the
field of dataset condensation for achieving SOTA results. However, the bi-level
optimization process hinders the practical application of such methods to
realistic and larger datasets. To enhance condensation efficiency, previous
works proposed Distribution-Matching (DM) as an alternative, which
significantly reduces the condensation cost. Nonetheless, current DM-based
methods have yielded less comparable results to optimization-oriented methods
due to their focus on aligning only the first moment of the distributions. In
this paper, we present a novel DM-based method named M3D for dataset
condensation by Minimizing the Maximum Mean Discrepancy between feature
representations of the synthetic and real images. By embedding their
distributions in a reproducing kernel Hilbert space, we align all orders of
moments of the distributions of real and synthetic images, resulting in a more
generalized condensed set. Notably, our method even surpasses the SOTA
optimization-oriented method IDC on the high-resolution ImageNet dataset.
Extensive analysis is conducted to verify the effectiveness of the proposed
method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted in AAAI-24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the hierarchical Bayesian modelling of frequency response functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06263v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06263v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        T. A. Dardeno, K. Worden, N. Dervilis, R. S. Mills, L. A. Bull
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For situations that may benefit from information sharing among datasets,
e.g., population-based SHM of similar structures, the hierarchical Bayesian
approach provides a useful modelling structure. Hierarchical Bayesian models
learn statistical distributions at the population (or parent) and the domain
levels simultaneously, to bolster statistical strength among the parameters. As
a result, variance is reduced among the parameter estimates, particularly when
data are limited. In this paper, a combined probabilistic FRF model is
developed for a small population of nominally-identical helicopter blades,
using a hierarchical Bayesian structure, to support information transfer in the
context of sparse data. The modelling approach is also demonstrated in a
traditional SHM context, for a single helicopter blade exposed to varying
temperatures, to show how the inclusion of physics-based knowledge can improve
generalisation beyond the training data, in the context of scarce data. These
models address critical challenges in SHM, by accommodating benign variations
that present as differences in the underlying dynamics, while also considering
(and utilising), the similarities among the domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Mechanical Systems and Signal Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Validation of Composite Systems by Discrepancy Propagation <span class="chip">UAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.12061v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.12061v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Reeb, Kanil Patel, Karim Barsim, Martin Schiegg, Sebastian Gerwinn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assessing the validity of a real-world system with respect to given quality
criteria is a common yet costly task in industrial applications due to the vast
number of required real-world tests. Validating such systems by means of
simulation offers a promising and less expensive alternative, but requires an
assessment of the simulation accuracy and therefore end-to-end measurements.
Additionally, covariate shifts between simulations and actual usage can cause
difficulties for estimating the reliability of such systems. In this work, we
present a validation method that propagates bounds on distributional
discrepancy measures through a composite system, thereby allowing us to derive
an upper bound on the failure probability of the real system from potentially
inaccurate simulations. Each propagation step entails an optimization problem,
where -- for measures such as maximum mean discrepancy (MMD) -- we develop
tight convex relaxations based on semidefinite programs. We demonstrate that
our propagation method yields valid and useful bounds for composite systems
exhibiting a variety of realistic effects. In particular, we show that the
proposed method can successfully account for data shifts within the
experimental design as well as model inaccuracies within the simulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages incl. 11 pages appendix; camera-ready version at UAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Topological Data Analysis for Neural Network Analysis: A Comprehensive
  <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05840v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05840v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rubén Ballester, Carles Casacuberta, Sergio Escalera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This survey provides a comprehensive exploration of applications of
Topological Data Analysis (TDA) within neural network analysis. Using TDA tools
such as persistent homology and Mapper, we delve into the intricate structures
and behaviors of neural networks and their datasets. We discuss different
strategies to obtain topological information from data and neural networks by
means of TDA. Additionally, we review how topological information can be
leveraged to analyze properties of neural networks, such as their
generalization capacity or expressivity. We explore practical implications of
deep learning, specifically focusing on areas like adversarial detection and
model selection. Our survey organizes the examined works into four broad
domains: 1. Characterization of neural network architectures; 2. Analysis of
decision regions and boundaries; 3. Study of internal representations,
activations, and parameters; 4. Exploration of training dynamics and loss
functions. Within each category, we discuss several articles, offering
background information to aid in understanding the various methodologies. We
conclude with a synthesis of key insights gained from our study, accompanied by
a discussion of challenges and potential advancements in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>70 pages, 7 figures. 4 references added. Minor changes in the text.
  Part of generative models reestructured to improve generality and clarity of
  exposition</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Manipulating Trajectory Prediction with Backdoors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.13863v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.13863v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaouther Messaoud, Kathrin Grosse, Mickael Chen, Matthieu Cord, Patrick Pérez, Alexandre Alahi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous vehicles ought to predict the surrounding agents' trajectories to
allow safe maneuvers in uncertain and complex traffic situations. As companies
increasingly apply trajectory prediction in the real world, security becomes a
relevant concern. In this paper, we focus on backdoors - a security threat
acknowledged in other fields but so far overlooked for trajectory prediction.
To this end, we describe and investigate four triggers that could affect
trajectory prediction. We then show that these triggers (for example, a braking
vehicle), when correlated with a desired output (for example, a curve) during
training, cause the desired output of a state-of-the-art trajectory prediction
model. In other words, the model has good benign performance but is vulnerable
to backdoors. This is the case even if the trigger maneuver is performed by a
non-casual agent behind the target vehicle. As a side-effect, our analysis
reveals interesting limitations within trajectory prediction models. Finally,
we evaluate a range of defenses against backdoors. While some, like simple
offroad checks, do not enable detection for all triggers, clustering is a
promising candidate to support manual inspection to find backdoors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sharper Bounds for $\ell_p$ Sensitivity Sampling <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.00732v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.00732v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David P. Woodruff, Taisuke Yasuda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In large scale machine learning, random sampling is a popular way to
approximate datasets by a small representative subset of examples. In
particular, sensitivity sampling is an intensely studied technique which
provides provable guarantees on the quality of approximation, while reducing
the number of examples to the product of the VC dimension $d$ and the total
sensitivity $\mathfrak S$ in remarkably general settings. However, guarantees
going beyond this general bound of $\mathfrak S d$ are known in perhaps only
one setting, for $\ell_2$ subspace embeddings, despite intense study of
sensitivity sampling in prior work. In this work, we show the first bounds for
sensitivity sampling for $\ell_p$ subspace embeddings for $p > 2$ that improve
over the general $\mathfrak S d$ bound, achieving a bound of roughly $\mathfrak
S^{2-2/p}$ for $2<p<\infty$. Furthermore, our techniques yield further new
results in the study of sampling algorithms, showing that the root leverage
score sampling algorithm achieves a bound of roughly $d$ for $1\leq p<2$, and
that a combination of leverage score and sensitivity sampling achieves an
improved bound of roughly $d^{2/p}\mathfrak S^{2-4/p}$ for $2<p<\infty$. Our
sensitivity sampling results yield the best known sample complexity for a wide
class of structured matrices that have small $\ell_p$ sensitivity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in ICML 2023; added discussion of prior work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How to avoid machine learning pitfalls: a guide for academic researchers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.02497v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.02497v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael A. Lones
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This document outlines some of the common mistakes that occur when using
machine learning, and what can be done to avoid them. Whilst it should be
accessible to anyone with a basic understanding of machine learning techniques,
it was originally written for research students, and focuses on issues that are
of particular concern within academic research, such as the need to do rigorous
comparisons and reach valid conclusions. It covers five stages of the machine
learning process: what to do before model building, how to reliably build
models, how to robustly evaluate models, how to compare models fairly, and how
to report results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do DL models and training environments have an impact on energy
  consumption? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05520v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05520v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Santiago del Rey, Silverio Martínez-Fernández, Luís Cruz, Xavier Franch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current research in the computer vision field mainly focuses on improving
Deep Learning (DL) correctness and inference time performance. However, there
is still little work on the huge carbon footprint that has training DL models.
This study aims to analyze the impact of the model architecture and training
environment when training greener computer vision models. We divide this goal
into two research questions. First, we analyze the effects of model
architecture on achieving greener models while keeping correctness at optimal
levels. Second, we study the influence of the training environment on producing
greener models. To investigate these relationships, we collect multiple metrics
related to energy efficiency and model correctness during the models' training.
Then, we outline the trade-offs between the measured energy efficiency and the
models' correctness regarding model architecture, and their relationship with
the training environment. We conduct this research in the context of a computer
vision system for image classification. In conclusion, we show that selecting
the proper model architecture and training environment can reduce energy
consumption dramatically (up to 81.38%) at the cost of negligible decreases in
correctness. Also, we find evidence that GPUs should scale with the models'
computational complexity for better energy efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>49th Euromicro Conference Series on Software Engineering and Advanced
  Applications (SEAA). 8 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SYNTA: A novel approach for deep learning-based image analysis in muscle
  histopathology using photo-realistic synthetic data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.14650v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.14650v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonid Mill, Oliver Aust, Jochen A. Ackermann, Philipp Burger, Monica Pascual, Katrin Palumbo-Zerr, Gerhard Krönke, Stefan Uderhardt, Georg Schett, Christoph S. Clemen, Rolf Schröder, Christian Holtzhausen, Samir Jabari, Andreas Maier, Anika Grüneboom
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI), machine learning, and deep learning (DL)
methods are becoming increasingly important in the field of biomedical image
analysis. However, to exploit the full potential of such methods, a
representative number of experimentally acquired images containing a
significant number of manually annotated objects is needed as training data.
Here we introduce SYNTA (synthetic data) as a novel approach for the generation
of synthetic, photo-realistic, and highly complex biomedical images as training
data for DL systems. We show the versatility of our approach in the context of
muscle fiber and connective tissue analysis in histological sections. We
demonstrate that it is possible to perform robust and expert-level segmentation
tasks on previously unseen real-world data, without the need for manual
annotations using synthetic training data alone. Being a fully parametric
technique, our approach poses an interpretable and controllable alternative to
Generative Adversarial Networks (GANs) and has the potential to significantly
accelerate quantitative image analysis in a variety of biomedical applications
in microscopy and beyond.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recourse under Model Multiplicity via Argumentative Ensembling
  (Technical Report) <span class="chip">AAMAS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.15097v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.15097v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junqi Jiang, Antonio Rago, Francesco Leofante, Francesca Toni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model Multiplicity (MM) arises when multiple, equally performing machine
learning models can be trained to solve the same prediction task. Recent
studies show that models obtained under MM may produce inconsistent predictions
for the same input. When this occurs, it becomes challenging to provide
counterfactual explanations (CEs), a common means for offering recourse
recommendations to individuals negatively affected by models' predictions. In
this paper, we formalise this problem, which we name recourse-aware ensembling,
and identify several desirable properties which methods for solving it should
satisfy. We show that existing ensembling methods, naturally extended in
different ways to provide CEs, fail to satisfy these properties. We then
introduce argumentative ensembling, deploying computational argumentation to
guarantee robustness of CEs to MM, while also accommodating customisable user
preferences. We show theoretically and experimentally that argumentative
ensembling satisfies properties which the existing methods lack, and that the
trade-offs are minimal wrt accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAMAS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fading memory as inductive bias in residual recurrent networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.14823v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.14823v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Igor Dubinin, Felix Effenberger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Residual connections have been proposed as an architecture-based inductive
bias to mitigate the problem of exploding and vanishing gradients and increased
task performance in both feed-forward and recurrent networks (RNNs) when
trained with the backpropagation algorithm. Yet, little is known about how
residual connections in RNNs influence their dynamics and fading memory
properties. Here, we introduce weakly coupled residual recurrent networks
(WCRNNs) in which residual connections result in well-defined Lyapunov
exponents and allow for studying properties of fading memory. We investigate
how the residual connections of WCRNNs influence their performance, network
dynamics, and memory properties on a set of benchmark tasks. We show that
several distinct forms of residual connections yield effective inductive biases
that result in increased network expressivity. In particular, those are
residual connections that (i) result in network dynamics at the proximity of
the edge of chaos, (ii) allow networks to capitalize on characteristic spectral
properties of the data, and (iii) result in heterogeneous memory properties. In
addition, we demonstrate how our results can be extended to non-linear
residuals and introduce a weakly coupled residual initialization scheme that
can be used for Elman RNNs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Computation and Communication Efficient Method for Distributed
  Nonconvex Problems in the Partial Participation Setting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.15580v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.15580v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Tyurin, Peter Richtárik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new method that includes three key components of distributed
optimization and federated learning: variance reduction of stochastic
gradients, partial participation, and compressed communication. We prove that
the new method has optimal oracle complexity and state-of-the-art communication
complexity in the partial participation setting. Regardless of the
communication compression feature, our method successfully combines variance
reduction and partial participation: we get the optimal oracle complexity,
never need the participation of all nodes, and do not require the bounded
gradients (dissimilarity) assumption.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Disentangled (Un)Controllable Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.00086v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.00086v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob E. Kooi, Mark Hoogendoorn, Vincent François-Lavet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of MDPs with high-dimensional states, downstream tasks are
predominantly applied on a compressed, low-dimensional representation of the
original input space. A variety of learning objectives have therefore been used
to attain useful representations. However, these representations usually lack
interpretability of the different features. We present a novel approach that is
able to disentangle latent features into a controllable and an uncontrollable
partition. We illustrate that the resulting partitioned representations are
easily interpretable on three types of environments and show that, in a
distribution of procedurally generated maze environments, it is feasible to
interpretably employ a planning algorithm in the isolated controllable latent
partition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages (8 main paper pages), 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarial Representation Learning for Robust Privacy Preservation in
  Audio 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.00011v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.00011v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shayan Gharib, Minh Tran, Diep Luong, Konstantinos Drossos, Tuomas Virtanen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sound event detection systems are widely used in various applications such as
surveillance and environmental monitoring where data is automatically
collected, processed, and sent to a cloud for sound recognition. However, this
process may inadvertently reveal sensitive information about users or their
surroundings, hence raising privacy concerns. In this study, we propose a novel
adversarial training method for learning representations of audio recordings
that effectively prevents the detection of speech activity from the latent
features of the recordings. The proposed method trains a model to generate
invariant latent representations of speech-containing audio recordings that
cannot be distinguished from non-speech recordings by a speech classifier. The
novelty of our work is in the optimization algorithm, where the speech
classifier's weights are regularly replaced with the weights of classifiers
trained in a supervised manner. This increases the discrimination power of the
speech classifier constantly during the adversarial training, motivating the
model to generate latent representations in which speech is not
distinguishable, even using new speech classifiers trained outside the
adversarial training loop. The proposed method is evaluated against a baseline
approach with no privacy measures and a prior adversarial training method,
demonstrating a significant reduction in privacy violations compared to the
baseline approach. Additionally, we show that the prior adversarial method is
practically ineffective for this purpose.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in IEEE Open Journal of Signal Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semisupervised Anomaly Detection using Support Vector Regression with
  Quantum Kernel 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.00583v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.00583v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kilian Tscharke, Sebastian Issel, Pascal Debus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection (AD) involves identifying observations or events that
deviate in some way from the rest of the data. Machine learning techniques have
shown success in automating this process by detecting hidden patterns and
deviations in large-scale data. The potential of quantum computing for machine
learning has been widely recognized, leading to extensive research efforts to
develop suitable quantum machine learning (QML) algorithms. In particular, the
search for QML algorithms for near-term NISQ devices is in full swing. However,
NISQ devices pose additional challenges due to their limited qubit coherence
times, low number of qubits, and high error rates. Kernel methods based on
quantum kernel estimation have emerged as a promising approach to QML on NISQ
devices, offering theoretical guarantees, versatility, and compatibility with
NISQ constraints. Especially support vector machines (SVM) utilizing quantum
kernel estimation have shown success in various supervised learning tasks.
However, in the context of AD, semisupervised learning is of great relevance,
and yet there is limited research published in this area. This paper introduces
an approach to semisupervised AD based on the reconstruction loss of a support
vector regression (SVR) with quantum kernel. This novel model is an alternative
to the variational quantum and quantum kernel one-class classifiers, and is
compared to a quantum autoencoder as quantum baseline and a SVR with
radial-basis-function (RBF) kernel as well as a classical autoencoder as
classical baselines. The models are benchmarked extensively on 10 real-world AD
data sets and one toy data set, and it is shown that our SVR model with quantum
kernel performs better than the SVR with RBF kernel as well as all other
models, achieving highest mean AUC over all data sets. In addition, our QSVR
outperforms the quantum autoencoder on 9 out of 11 data sets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE International Conference on Quantum Computing and
  Engineering (QCE) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long
  Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19923v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19923v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Günther, Jackmin Ong, Isabelle Mohr, Alaeddine Abdessalem, Tanguy Abel, Mohammad Kalim Akram, Susana Guzman, Georgios Mastrapas, Saba Sturua, Bo Wang, Maximilian Werk, Nan Wang, Han Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text embedding models have emerged as powerful tools for transforming
sentences into fixed-sized feature vectors that encapsulate semantic
information. While these models are essential for tasks like information
retrieval, semantic clustering, and text re-ranking, most existing open-source
models, especially those built on architectures like BERT, struggle to
represent lengthy documents and often resort to truncation. One common approach
to mitigate this challenge involves splitting documents into smaller paragraphs
for embedding. However, this strategy results in a much larger set of vectors,
consequently leading to increased memory consumption and computationally
intensive vector searches with elevated latency.
  To address these challenges, we introduce Jina Embeddings 2, an open-source
text embedding model capable of accommodating up to 8192 tokens. This model is
designed to transcend the conventional 512-token limit and adeptly process long
documents. Jina Embeddings 2 not only achieves state-of-the-art performance on
a range of embedding-related tasks in the MTEB benchmark but also matches the
performance of OpenAI's proprietary ada-002 model. Additionally, our
experiments indicate that an extended context can enhance performance in tasks
such as NarrativeQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Memorization and Privacy Risks of Sharpness Aware Minimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00488v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00488v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Young In Kim, Pratiksha Agrawal, Johannes O. Royset, Rajiv Khanna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many recent works, there is an increased focus on designing algorithms
that seek flatter optima for neural network loss optimization as there is
empirical evidence that it leads to better generalization performance in many
datasets. In this work, we dissect these performance gains through the lens of
data memorization in overparameterized models. We define a new metric that
helps us identify which data points specifically do algorithms seeking flatter
optima do better when compared to vanilla SGD. We find that the generalization
gains achieved by Sharpness Aware Minimization (SAM) are particularly
pronounced for atypical data points, which necessitate memorization. This
insight helps us unearth higher privacy risks associated with SAM, which we
verify through exhaustive empirical evaluations. Finally, we propose mitigation
strategies to achieve a more desirable accuracy vs privacy tradeoff.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diabetic Retinopathy Using Gaussian Filter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15216v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15216v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roshan Vasu Muddaluru, Sharvaani Ravikumar Thoguluva, Shruti Prabha, Tanuja Konda Reddy, Dr. Suja P
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The retina is an essential component of the visual system, and maintaining
eyesight depends on the timely and correct detection of disorders. This
research specifically addresses the early-stage detection and severity
classification of diabetic retinopathy (DR), a serious public health hazard. We
compare the results of different deep learning models such as InceptionV3,
DenseNet121 and other CNN based models by using different image filters, such
as Gaussian, grayscale and Gabor. These models could detect subtle pathological
alterations and use that information to estimate the risk of retinal illnesses.
The objective is to improve the diagnostic processes for diabetic retinopathy,
the primary cause of diabetes-related blindness, by utilizing deep learning
models. A comparative analysis between Greyscale, Gaussian and Gabor filters
has been provided after applying these filters on the retinal images. The
Gaussian filter resulted to be the most promising filter giving the best
accuracies for all the models. The best performing model was InceptionV3 which
gave an accuracy of 96% on Gaussian images, therefore Gaussian filter emerged
as our most promising filter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures, conference, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Parallel Algorithms Align with Neural Execution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04049v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04049v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valerie Engelmayer, Dobrik Georgiev, Petar Veličković
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural algorithmic reasoners are parallel processors. Teaching them
sequential algorithms contradicts this nature, rendering a significant share of
their computations redundant. Parallel algorithms however may exploit their
full computational power, therefore requiring fewer layers to be executed. This
drastically reduces training times, as we observe when comparing parallel
implementations of searching, sorting and finding strongly connected components
to their sequential counterparts on the CLRS framework. Additionally, parallel
versions achieve (often strongly) superior predictive performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures, Proceedings of the Second Learning on Graphs
  Conference (LoG 2023), PMLR 231</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM4TS: Aligning <span class="highlight-title">Pre-Train</span>ed LLMs as Data-Efficient Time-Series
  Forecasters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.08469v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.08469v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ching Chang, Wei-Yao Wang, Wen-Chih Peng, Tien-Fu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multivariate time-series forecasting is vital in various domains, e.g.,
economic planning and weather prediction. Deep train-from-scratch models have
exhibited effective performance yet require large amounts of data, which limits
real-world applicability. Recently, researchers have explored pre-trained Large
Language Models (LLMs) for limited non-linguistic datasets. However,
incorporating LLMs with time-series data presents challenges of limited
adaptation due to different compositions between time-series and linguistic
data, and the inability to process multi-scale temporal information. To tackle
these challenges, we propose LLM4TS, a framework for time-series forecasting
with pre-trained LLMs. LLM4TS consists of a two-stage fine-tuning strategy: the
time-series alignment stage to align LLMs with the nuances of time-series data,
and the forecasting fine-tuning stage, which is specifically designed for
time-series forecasting tasks. Furthermore, our framework features a novel
two-level aggregation method that integrates multi-scale temporal data within
pre-trained LLMs, enhancing their ability to interpret time-specific
information. In experiments across 7 time-series forecasting datasets, LLM4TS
is superior to existing state-of-the-art methods, including those trained from
scratch, in full-shot scenarios, and also achieves an average improvement of
6.84% in MSE in few-shot scenarios. In addition, evaluations compared with
different self-supervised learning approaches highlight LLM4TS's effectiveness
with representation learning in forecasting scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is currently under review. The code will be made available
  upon acceptance</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian posterior approximation with stochastic ensembles <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08123v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08123v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oleksandr Balabanov, Bernhard Mehlig, Hampus Linander
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce ensembles of stochastic neural networks to approximate the
Bayesian posterior, combining stochastic methods such as dropout with deep
ensembles. The stochastic ensembles are formulated as families of distributions
and trained to approximate the Bayesian posterior with variational inference.
We implement stochastic ensembles based on Monte Carlo dropout, DropConnect and
a novel non-parametric version of dropout and evaluate them on a toy problem
and CIFAR image classification. For both tasks, we test the quality of the
posteriors directly against Hamiltonian Monte Carlo simulations. Our results
show that stochastic ensembles provide more accurate posterior estimates than
other popular baselines for Bayesian inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding the Effects of RLHF on LLM Generalisation and Diversity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06452v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06452v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena Luketina, Eric Hambro, Edward Grefenstette, Roberta Raileanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) fine-tuned with reinforcement learning from
human feedback (RLHF) have been used in some of the most widely deployed AI
models to date, such as OpenAI's ChatGPT or Anthropic's Claude. % , or Meta's
LLaMA-2. While there has been significant work developing these methods, our
understanding of the benefits and downsides of each stage in RLHF is still
limited. To fill this gap, we present an extensive analysis of how each stage
of the process (i.e.~supervised fine-tuning (SFT), reward modelling, and RLHF)
affects two key properties: out-of-distribution (OOD) generalisation and output
diversity. OOD generalisation is crucial given the wide range of real-world
scenarios in which these models are being used, while output diversity refers
to the model's ability to generate varied outputs and is important for a
variety of use cases. We perform our analysis across two base models on both
summarisation and instruction following tasks, the latter being highly relevant
for current LLM use cases. We find that RLHF generalises better than SFT to new
inputs, particularly as the distribution shift between train and test becomes
larger. However, RLHF significantly reduces output diversity compared to SFT
across a variety of measures, implying a tradeoff in current LLM fine-tuning
methods between generalisation and diversity. Our results provide guidance on
which fine-tuning method should be used depending on the application, and show
that more research is needed to improve the tradeoff between generalisation and
diversity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available here: https://github.com/facebookresearch/rlfh-gen-div</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hyperbolic Graph Diffusion Model <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.07618v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.07618v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingfeng Wen, Xuan Tang, Mingjie Ouyang, Xiangxiang Shen, Jian Yang, Daxin Zhu, Mingsong Chen, Xian Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion generative models (DMs) have achieved promising results in image
and graph generation. However, real-world graphs, such as social networks,
molecular graphs, and traffic graphs, generally share non-Euclidean topologies
and hidden hierarchies. For example, the degree distributions of graphs are
mostly power-law distributions. The current latent diffusion model embeds the
hierarchical data in a Euclidean space, which leads to distortions and
interferes with modeling the distribution. Instead, hyperbolic space has been
found to be more suitable for capturing complex hierarchical structures due to
its exponential growth property. In order to simultaneously utilize the data
generation capabilities of diffusion models and the ability of hyperbolic
embeddings to extract latent hierarchical distributions, we propose a novel
graph generation method called, Hyperbolic Graph Diffusion Model (HGDM), which
consists of an auto-encoder to encode nodes into successive hyperbolic
embeddings, and a DM that operates in the hyperbolic latent space. HGDM
captures the crucial graph structure distributions by constructing a hyperbolic
potential node space that incorporates edge information. Extensive experiments
show that HGDM achieves better performance in generic graph and molecule
generation benchmarks, with a $48\%$ improvement in the quality of graph
generation with highly hierarchical structures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion Model with Perceptual Loss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00110v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00110v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shanchuan Lin, Xiao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models trained with mean squared error loss tend to generate
unrealistic samples. Current state-of-the-art models rely on classifier-free
guidance to improve sample quality, yet its surprising effectiveness is not
fully understood. In this paper, We show that the effectiveness of
classifier-free guidance partly originates from it being a form of implicit
perceptual guidance. As a result, we can directly incorporate perceptual loss
in diffusion training to improve sample quality. Since the score matching
objective used in diffusion training strongly resembles the denoising
autoencoder objective used in unsupervised training of perceptual networks, the
diffusion model itself is a perceptual network and can be used to generate
meaningful perceptual loss. We propose a novel self-perceptual objective that
results in diffusion models capable of generating more realistic samples. For
conditional generation, our method only improves sample quality without
entanglement with the conditional input and therefore does not sacrifice sample
diversity. Our method can also improve sample quality for unconditional
generation, which was not possible with classifier-free guidance before.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lower Difficulty and Better Robustness: A Bregman Divergence Perspective
  for Adversarial Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.12511v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.12511v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihui Wu, Haichang Gao, Bingqian Zhou, Xiaoyan Guo, Shudong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate on improving the adversarial robustness
obtained in adversarial training (AT) via reducing the difficulty of
optimization. To better study this problem, we build a novel Bregman divergence
perspective for AT, in which AT can be viewed as the sliding process of the
training data points on the negative entropy curve. Based on this perspective,
we analyze the learning objectives of two typical AT methods, i.e., PGD-AT and
TRADES, and we find that the optimization process of TRADES is easier than
PGD-AT for that TRADES separates PGD-AT. In addition, we discuss the function
of entropy in TRADES, and we find that models with high entropy can be better
robustness learners. Inspired by the above findings, we propose two methods,
i.e., FAIT and MER, which can both not only reduce the difficulty of
optimization under the 10-step PGD adversaries, but also provide better
robustness. Our work suggests that reducing the difficulty of optimization
under the 10-step PGD adversaries is a promising approach for enhancing the
adversarial robustness in AT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Rank-Reduced Kalman Filter: Approximate Dynamical-Low-Rank Filtering
  In High Dimensions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.07774v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.07774v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Schmidt, Philipp Hennig, Jörg Nick, Filip Tronarp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inference and simulation in the context of high-dimensional dynamical systems
remain computationally challenging problems. Some form of dimensionality
reduction is required to make the problem tractable in general. In this paper,
we propose a novel approximate Gaussian filtering and smoothing method which
propagates low-rank approximations of the covariance matrices. This is
accomplished by projecting the Lyapunov equations associated with the
prediction step to a manifold of low-rank matrices, which are then solved by a
recently developed, numerically stable, dynamical low-rank integrator.
Meanwhile, the update steps are made tractable by noting that the covariance
update only transforms the column space of the covariance matrix, which is
low-rank by construction. The algorithm differentiates itself from existing
ensemble-based approaches in that the low-rank approximations of the covariance
matrices are deterministic, rather than stochastic. Crucially, this enables the
method to reproduce the exact Kalman filter as the low-rank dimension
approaches the true dimensionality of the problem. Our method reduces
computational complexity from cubic (for the Kalman filter) to \emph{quadratic}
in the state-space size in the worst-case, and can achieve \emph{linear}
complexity if the state-space model satisfies certain criteria. Through a set
of experiments in classical data-assimilation and spatio-temporal regression,
we show that the proposed method consistently outperforms the ensemble-based
methods in terms of error in the mean and covariance with respect to the exact
Kalman filter. This comes at no additional cost in terms of asymptotic
computational complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages main text (including references) + 9 pages appendix, 6
  figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.13763v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.13763v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huan Ling, Seung Wook Kim, Antonio Torralba, Sanja Fidler, Karsten Kreis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-guided diffusion models have revolutionized image and video generation
and have also been successfully used for optimization-based 3D object
synthesis. Here, we instead focus on the underexplored text-to-4D setting and
synthesize dynamic, animated 3D objects using score distillation methods with
an additional temporal dimension. Compared to previous work, we pursue a novel
compositional generation-based approach, and combine text-to-image,
text-to-video, and 3D-aware multiview diffusion models to provide feedback
during 4D object optimization, thereby simultaneously enforcing temporal
consistency, high-quality visual appearance and realistic geometry. Our method,
called Align Your Gaussians (AYG), leverages dynamic 3D Gaussian Splatting with
deformation fields as 4D representation. Crucial to AYG is a novel method to
regularize the distribution of the moving 3D Gaussians and thereby stabilize
the optimization and induce motion. We also propose a motion amplification
mechanism as well as a new autoregressive synthesis scheme to generate and
combine multiple 4D sequences for longer generation. These techniques allow us
to synthesize vivid dynamic scenes, outperform previous work qualitatively and
quantitatively and achieve state-of-the-art text-to-4D performance. Due to the
Gaussian 4D representation, different 4D animations can be seamlessly combined,
as we demonstrate. AYG opens up promising avenues for animation, simulation and
digital content creation as well as synthetic data generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page:
  https://research.nvidia.com/labs/toronto-ai/AlignYourGaussians/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A New Frontier of AI: On-Device AI Training and Personalization <span class="chip">ICSE 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.04688v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.04688v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ji Joong Moon, Hyeonseok Lee, Jiho Chu, Donghak Park, Seungbaek Hong, Hyungjun Seo, Donghyeon Jeong, Sungsik Kong, MyungJoo Ham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern consumer electronic devices have started executing deep learning-based
intelligence services on devices, not cloud servers, to keep personal data on
devices and to reduce network and cloud costs. We find such a trend as the
opportunity to personalize intelligence services by updating neural networks
with user data without exposing the data out of devices: on-device training.
However, the limited resources of devices incurs significant difficulties. We
propose a light-weight on-device training framework, NNTrainer, which provides
highly memory-efficient neural network training techniques and proactive
swapping based on fine-grained execution order analysis for neural networks.
Moreover, its optimizations do not sacrifice accuracy and are transparent to
training algorithms; thus, prior algorithmic studies may be implemented on top
of NNTrainer. The evaluations show that NNTrainer can reduce memory consumption
down to 1/20 (saving 95%!) and effectively personalizes intelligence services
on devices. NNTrainer is cross-platform and practical open-source software,
which is being deployed to millions of mobile devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 16 figures, Accepted in ICSE 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fairness Certification for Natural Language Processing and Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Freiberger, Erik Buchmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Processing (NLP) plays an important role in our daily lives,
particularly due to the enormous progress of Large Language Models (LLM).
However, NLP has many fairness-critical use cases, e.g., as an expert system in
recruitment or as an LLM-based tutor in education. Since NLP is based on human
language, potentially harmful biases can diffuse into NLP systems and produce
unfair results, discriminate against minorities or generate legal issues.
Hence, it is important to develop a fairness certification for NLP approaches.
We follow a qualitative research approach towards a fairness certification for
NLP. In particular, we have reviewed a large body of literature on algorithmic
fairness, and we have conducted semi-structured expert interviews with a wide
range of experts from that area. We have systematically devised six fairness
criteria for NLP, which can be further refined into 18 sub-categories. Our
criteria offer a foundation for operationalizing and testing processes to
certify fairness, both from the perspective of the auditor and the audited
organization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In depth discussion of our results can be found in the Appendix B</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Memory Gym: Towards Endless Tasks to Benchmark Memory Capabilities of
  Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.17207v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.17207v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Pleines, Matthias Pallasch, Frank Zimmer, Mike Preuss
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Memory Gym presents a suite of 2D partially observable environments, namely
Mortar Mayhem, Mystery Path, and Searing Spotlights, designed to benchmark
memory capabilities in decision-making agents. These environments, originally
with finite tasks, are expanded into innovative, endless formats, mirroring the
escalating challenges of cumulative memory games such as ``I packed my bag''.
This progression in task design shifts the focus from merely assessing sample
efficiency to also probing the levels of memory effectiveness in dynamic,
prolonged scenarios. To address the gap in available memory-based Deep
Reinforcement Learning baselines, we introduce an implementation that
integrates Transformer-XL (TrXL) with Proximal Policy Optimization. This
approach utilizes TrXL as a form of episodic memory, employing a sliding window
technique. Our comparative study between the Gated Recurrent Unit (GRU) and
TrXL reveals varied performances across different settings. TrXL, on the finite
environments, demonstrates superior sample efficiency in Mystery Path and
outperforms in Mortar Mayhem. However, GRU is more efficient on Searing
Spotlights. Most notably, in all endless tasks, GRU makes a remarkable
resurgence, consistently outperforming TrXL by significant margins. Website and
Source Code: https://github.com/MarcoMeter/endless-memory-gym/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, 17 figures, 5 tables, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Relation-Attentive Graph Neural Networks for Fraud Detection <span class="chip">ICDM 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04171v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04171v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heehyeon Kim, Jinhyeok Choi, Joyce Jiyoung Whang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fraud detection aims to discover fraudsters deceiving other users by, for
example, leaving fake reviews or making abnormal transactions. Graph-based
fraud detection methods consider this task as a classification problem with two
classes: frauds or normal. We address this problem using Graph Neural Networks
(GNNs) by proposing a dynamic relation-attentive aggregation mechanism. Based
on the observation that many real-world graphs include different types of
relations, we propose to learn a node representation per relation and aggregate
the node representations using a learnable attention function that assigns a
different attention coefficient to each relation. Furthermore, we combine the
node representations from different layers to consider both the local and
global structures of a target node, which is beneficial to improving the
performance of fraud detection on graphs with heterophily. By employing dynamic
graph attention in all the aggregation processes, our method adaptively
computes the attention coefficients for each node. Experimental results show
that our method, DRAG, outperforms state-of-the-art fraud detection methods on
real-world benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, 3 tables. Machine Learning on Graphs (MLoG)
  Workshop at the 23rd IEEE International Conference on Data Mining (ICDM 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What's the Magic Word? A Control Theory of LLM <span class="highlight-title">Prompt</span>ing <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04444v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04444v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aman Bhargava, Cameron Witkowski, Manav Shah, Matt Thomson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt engineering is crucial for deploying LLMs but is poorly understood
mathematically. We formalize LLM systems as a class of discrete stochastic
dynamical systems to explore prompt engineering through the lens of control
theory. We investigate the reachable set of output token sequences $R_y(\mathbf
x_0)$ for which there exists a control input sequence $\mathbf u$ for each
$\mathbf y \in R_y(\mathbf x_0)$ that steers the LLM to output $\mathbf y$ from
initial state sequence $\mathbf x_0$. We offer analytic analysis on the
limitations on the controllability of self-attention in terms of reachable set,
where we prove an upper bound on the reachable set of outputs $R_y(\mathbf
x_0)$ as a function of the singular values of the parameter matrices. We
present complementary empirical analysis on the controllability of a panel of
LLMs, including Falcon-7b, Llama-7b, and Falcon-40b. Our results demonstrate a
lower bound on the reachable set of outputs $R_y(\mathbf x_0)$ w.r.t. initial
state sequences $\mathbf x_0$ sampled from the Wikitext dataset. We find that
the correct next Wikitext token following sequence $\mathbf x_0$ is reachable
over 97% of the time with prompts of $k\leq 10$ tokens. We also establish that
the top 75 most likely next tokens, as estimated by the LLM itself, are
reachable at least 85% of the time with prompts of $k\leq 10$ tokens.
Intriguingly, short prompt sequences can dramatically alter the likelihood of
specific outputs, even making the least likely tokens become the most likely
ones. This control-centric analysis of LLMs demonstrates the significant and
poorly understood role of input sequences in steering output probabilities,
offering a foundational perspective for enhancing language model system
capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 8 figures. Under review for ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Out-of-Distribution Detection by Restoring Lossy Inputs
  with Variational Autoencoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.02084v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.02084v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zezhen Zeng, Bin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep generative models have been demonstrated as problematic in the
unsupervised out-of-distribution (OOD) detection task, where they tend to
assign higher likelihoods to OOD samples. Previous studies on this issue are
usually not applicable to the Variational Autoencoder (VAE). As a popular
subclass of generative models, the VAE can be effective with a relatively
smaller model size and be more stable and faster in training and inference,
which can be more advantageous in real-world applications. In this paper, We
propose a novel VAE-based score called Error Reduction (ER) for OOD detection,
which is based on a VAE that takes a lossy version of the training set as
inputs and the original set as targets. Experiments are carried out on various
datasets to show the effectiveness of our method, we also present the effect of
design choices with ablation experiments. Our code is available at:
https://github.com/ZJLAB-AMMI/VAE4OOD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Summary of the DISPLACE Challenge 2023 -- DIarization of SPeaker and
  LAnguage in Conversational Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12564v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12564v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shikha Baghel, Shreyas Ramoji, Somil Jain, Pratik Roy Chowdhuri, Prachi Singh, Deepu Vijayasenan, Sriram Ganapathy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In multi-lingual societies, where multiple languages are spoken in a small
geographic vicinity, informal conversations often involve mix of languages.
Existing speech technologies may be inefficient in extracting information from
such conversations, where the speech data is rich in diversity with multiple
languages and speakers. The DISPLACE (DIarization of SPeaker and LAnguage in
Conversational Environments) challenge constitutes an open-call for evaluating
and bench-marking the speaker and language diarization technologies on this
challenging condition. The challenge entailed two tracks: Track-1 focused on
speaker diarization (SD) in multilingual situations while, Track-2 addressed
the language diarization (LD) in a multi-speaker scenario. Both the tracks were
evaluated using the same underlying audio data. To facilitate this evaluation,
a real-world dataset featuring multilingual, multi-speaker conversational
far-field speech was recorded and distributed. Furthermore, a baseline system
was made available for both SD and LD task which mimicked the state-of-art in
these tasks. The challenge garnered a total of $42$ world-wide registrations
and received a total of $19$ combined submissions for Track-1 and Track-2. This
paper describes the challenge, details of the datasets, tasks, and the baseline
system. Additionally, the paper provides a concise overview of the submitted
systems in both tracks, with an emphasis given to the top performing systems.
The paper also presents insights and future perspectives for SD and LD tasks,
focusing on the key challenges that the systems need to overcome before
wide-spread commercial deployment on such conversations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient selective attention LSTM for well log curve synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.10253v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.10253v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuankai Zhou, Huanyu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-core drilling has gradually become the primary exploration method in
geological exploration engineering, and well logging curves have increasingly
gained importance as the main carriers of geological information. However,
factors such as geological environment, logging equipment, borehole quality,
and unexpected events can all impact the quality of well logging curves.
Previous methods of re-logging or manual corrections have been associated with
high costs and low efficiency. This paper proposes a machine learning method
that utilizes existing data to predict missing data, and its effectiveness and
feasibility have been validated through field experiments. The proposed method
builds on the traditional Long Short-Term Memory (LSTM) neural network by
incorporating a self-attention mechanism to analyze the sequential dependencies
of the data. It selects the dominant computational results in the LSTM,
reducing the computational complexity from O(n^2) to O(nlogn) and improving
model efficiency. Experimental results demonstrate that the proposed method
achieves higher accuracy compared to traditional curve synthesis methods based
on Fully Connected Neural Networks (FCNN) and vanilla LSTM. This accurate,
efficient, and cost-effective prediction method holds a practical value in
engineering applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SCUNet++: Swin-UNet and CNN Bottleneck Hybrid Architecture with
  Multi-Fusion Dense Skip Connection for Pulmonary Embolism CT Image
  Segmentation <span class="chip">WACV2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14705v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14705v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Chen, Binfeng Zou, Zhaoxin Guo, Yiyu Huang, Yifan Huang, Feiwei Qin, Qinhai Li, Changmiao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pulmonary embolism (PE) is a prevalent lung disease that can lead to right
ventricular hypertrophy and failure in severe cases, ranking second in severity
only to myocardial infarction and sudden death. Pulmonary artery CT angiography
(CTPA) is a widely used diagnostic method for PE. However, PE detection
presents challenges in clinical practice due to limitations in imaging
technology. CTPA can produce noises similar to PE, making confirmation of its
presence time-consuming and prone to overdiagnosis. Nevertheless, the
traditional segmentation method of PE can not fully consider the hierarchical
structure of features, local and global spatial features of PE CT images. In
this paper, we propose an automatic PE segmentation method called SCUNet++
(Swin Conv UNet++). This method incorporates multiple fusion dense skip
connections between the encoder and decoder, utilizing the Swin Transformer as
the encoder. And fuses features of different scales in the decoder subnetwork
to compensate for spatial information loss caused by the inevitable
downsampling in Swin-UNet or other state-of-the-art methods, effectively
solving the above problem. We provide a theoretical analysis of this method in
detail and validate it on publicly available PE CT image datasets FUMPE and
CAD-PE. The experimental results indicate that our proposed method achieved a
Dice similarity coefficient (DSC) of 83.47% and a Hausdorff distance 95th
percentile (HD95) of 3.83 on the FUMPE dataset, as well as a DSC of 83.42% and
an HD95 of 5.10 on the CAD-PE dataset. These findings demonstrate that our
method exhibits strong performance in PE segmentation tasks, potentially
enhancing the accuracy of automatic segmentation of PE and providing a powerful
diagnostic tool for clinical physicians. Our source code and new FUMPE dataset
are available at https://github.com/JustlfC03/SCUNet-plusplus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures, accept WACV2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In the Name of Fairness: Assessing the Bias in Clinical Record
  De-identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.11348v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.11348v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Xiao, Shulammite Lim, Tom Joseph Pollard, Marzyeh Ghassemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data sharing is crucial for open science and reproducible research, but the
legal sharing of clinical data requires the removal of protected health
information from electronic health records. This process, known as
de-identification, is often achieved through the use of machine learning
algorithms by many commercial and open-source systems. While these systems have
shown compelling results on average, the variation in their performance across
different demographic groups has not been thoroughly examined. In this work, we
investigate the bias of de-identification systems on names in clinical notes
via a large-scale empirical analysis. To achieve this, we create 16 name sets
that vary along four demographic dimensions: gender, race, name popularity, and
the decade of popularity. We insert these names into 100 manually curated
clinical templates and evaluate the performance of nine public and private
de-identification methods. Our findings reveal that there are statistically
significant performance gaps along a majority of the demographic dimensions in
most methods. We further illustrate that de-identification quality is affected
by polysemy in names, gender context, and clinical note characteristics. To
mitigate the identified gaps, we propose a simple and method-agnostic solution
by fine-tuning de-identification methods with clinical context and diverse
names. Overall, it is imperative to address the bias in existing methods
immediately so that downstream stakeholders can build high-quality systems to
serve all demographic parties fairly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by FAccT 2023; updated appendix with the de-identification
  performance of GPT-4</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Human Sequential Decision-Making with Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.08454v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.08454v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamsa Bastani, Osbert Bastani, Wichinpong Park Sinchaisri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Workers spend a significant amount of time learning how to make good
decisions. Evaluating the efficacy of a given decision, however, can be
complicated -- e.g., decision outcomes are often long-term and relate to the
original decision in complex ways. Surprisingly, even though learning good
decision-making strategies is difficult, they can often be expressed in simple
and concise forms. Focusing on sequential decision-making, we design a novel
machine learning algorithm that is capable of extracting "best practices" from
trace data and conveying its insights to humans in the form of interpretable
"tips". Our algorithm selects the tip that best bridges the gap between the
actions taken by human workers and those taken by the optimal policy in a way
that accounts for which actions are consequential for achieving higher
performance. We evaluate our approach through a series of randomized controlled
experiments where participants manage a virtual kitchen. Our experiments show
that the tips generated by our algorithm can significantly improve human
performance relative to intuitive baselines. In addition, we discuss a number
of empirical insights that can help inform the design of algorithms intended
for human-AI interfaces. For instance, we find evidence that participants do
not simply blindly follow our tips; instead, they combine them with their own
experience to discover additional strategies for improving performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harmonizing Covariance and Expressiveness for Deep Hamiltonian
  Regression in Crystalline Material Research: a Hybrid Cascaded Regression
  Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00744v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00744v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shi Yin, Xudong Zhu, Tianyu Gao, Haochong Zhang, Feng Wu, Lixin He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning for Hamiltonian regression of quantum systems in material
research necessitates satisfying the covariance laws, among which achieving
SO(3)-equivariance without sacrificing the expressiveness of networks remains
an elusive challenge due to the restriction to non-linear mappings on
guaranteeing theoretical equivariance. To alleviate the
covariance-expressiveness dilemma, we propose a hybrid framework with two
cascaded regression stages. The first stage, with a theoretically-guaranteed
covariant neural network modeling symmetry properties of 3D atom systems,
yields theoretically covariant features and baseline Hamiltonian predictions,
assisting the second stage in learning covariance. Meanwhile, the second stage,
powered by a non-linear 3D graph Transformer network we propose for structural
modeling of 3D atomic systems, refines the first stage's output as a
fine-grained prediction of Hamiltonians with better expressiveness capability.
The combination of a theoretically covariant yet inevitably less expressive
model with a highly expressive non-linear network enables precise,
generalizable predictions while maintaining robust covariance under coordinate
transformations. Our method achieves state-of-the-art performance in
Hamiltonian prediction for electronic structure calculations, confirmed through
experiments on five crystalline material databases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DIRA: Dynamic Domain Incremental Regularised Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.00147v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.00147v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abanoub Ghobrial, Xuan Zheng, Darryl Hond, Hamid Asgari, Kerstin Eder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous systems (AS) often use Deep Neural Network (DNN) classifiers to
allow them to operate in complex, high-dimensional, non-linear, and dynamically
changing environments. Due to the complexity of these environments, DNN
classifiers may output misclassifications during operation when they face
domains not identified during development. Removing a system from operation for
retraining becomes impractical as the number of such AS increases. To increase
AS reliability and overcome this limitation, DNN classifiers need to have the
ability to adapt during operation when faced with different operational domains
using a few samples (e.g. 2 to 100 samples). However, retraining DNNs on a few
samples is known to cause catastrophic forgetting and poor generalisation. In
this paper, we introduce Dynamic Incremental Regularised Adaptation (DIRA), an
approach for dynamic operational domain adaption of DNNs using regularisation
techniques. We show that DIRA improves on the problem of forgetting and
achieves strong gains in performance when retraining using a few samples from
the target domain. Our approach shows improvements on different image
classification benchmarks aimed at evaluating robustness to distribution shifts
(e.g.CIFAR-10C/100C, ImageNet-C), and produces state-of-the-art performance in
comparison with other methods from the literature.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ULF: Unsupervised Labeling Function Correction using Cross-Validation
  for Weak Supervision <span class="chip">EMNLP'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.06863v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.06863v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anastasiia Sedova, Benjamin Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A cost-effective alternative to manual data labeling is weak supervision
(WS), where data samples are automatically annotated using a predefined set of
labeling functions (LFs), rule-based mechanisms that generate artificial labels
for the associated classes. In this work, we investigate noise reduction
techniques for WS based on the principle of k-fold cross-validation. We
introduce a new algorithm ULF for Unsupervised Labeling Function correction,
which denoises WS data by leveraging models trained on all but some LFs to
identify and correct biases specific to the held-out LFs. Specifically, ULF
refines the allocation of LFs to classes by re-estimating this assignment on
highly reliable cross-validated samples. Evaluation on multiple datasets
confirms ULF's effectiveness in enhancing WS learning without the need for
manual labeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning with Noisy Labels by Adaptive Gradient-Based Outlier Removal <span class="chip">ECML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04502v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04502v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anastasiia Sedova, Lena Zellinger, Benjamin Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An accurate and substantial dataset is essential for training a reliable and
well-performing model. However, even manually annotated datasets contain label
errors, not to mention automatically labeled ones. Previous methods for label
denoising have primarily focused on detecting outliers and their permanent
removal - a process that is likely to over- or underfilter the dataset. In this
work, we propose AGRA: a new method for learning with noisy labels by using
Adaptive GRAdient-based outlier removal. Instead of cleaning the dataset prior
to model training, the dataset is dynamically adjusted during the training
process. By comparing the aggregated gradient of a batch of samples and an
individual example gradient, our method dynamically decides whether a
corresponding example is helpful for the model at this point or is
counter-productive and should be left out for the current update. Extensive
evaluation on several datasets demonstrates AGRA's effectiveness, while a
comprehensive results analysis supports our initial hypothesis: permanent hard
outlier removal is not always what model benefits the most from.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECML PKDD 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Computational Discovery of Microstructured Composites with Optimal
  Stiffness-Toughness Trade-Offs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01078v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01078v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beichen Li, Bolei Deng, Wan Shou, Tae-Hyun Oh, Yuanming Hu, Yiyue Luo, Liang Shi, Wojciech Matusik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The conflict between stiffness and toughness is a fundamental problem in
engineering materials design. However, the systematic discovery of
microstructured composites with optimal stiffness-toughness trade-offs has
never been demonstrated, hindered by the discrepancies between simulation and
reality and the lack of data-efficient exploration of the entire Pareto front.
We introduce a generalizable pipeline that integrates physical experiments,
numerical simulations, and artificial neural networks to address both
challenges. Without any prescribed expert knowledge of material design, our
approach implements a nested-loop proposal-validation workflow to bridge the
simulation-to-reality gap and discover microstructured composites that are
stiff and tough with high sample efficiency. Further analysis of Pareto-optimal
designs allows us to automatically identify existing toughness enhancement
mechanisms, which were previously discovered through trial-and-error or
biomimicry. On a broader scale, our method provides a blueprint for
computational design in various research areas beyond solid mechanics, such as
polymer chemistry, fluid dynamics, meteorology, and robotics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LinFlo-Net: A two-stage deep learning method to generate simulation
  ready meshes of the heart 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.20065v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.20065v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arjun Narayanan, Fanwei Kong, Shawn Shadden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a deep learning model to automatically generate computer models of
the human heart from patient imaging data with an emphasis on its capability to
generate thin-walled cardiac structures. Our method works by deforming a
template mesh to fit the cardiac structures to the given image. Compared with
prior deep learning methods that adopted this approach, our framework is
designed to minimize mesh self-penetration, which typically arises when
deforming surface meshes separated by small distances. We achieve this by using
a two-stage diffeomorphic deformation process along with a novel loss function
derived from the kinematics of motion that penalizes surface contact and
interpenetration. Our model demonstrates comparable accuracy with
state-of-the-art methods while additionally producing meshes free of
self-intersections. The resultant meshes are readily usable in physics based
simulation, minimizing the need for post-processing and cleanup.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted manuscript in the Journal of Biomechanical Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improved uncertainty quantification for neural networks with Bayesian
  last layer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10975v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10975v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Fiedler, Sergio Lucia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncertainty quantification is an important task in machine learning - a task
in which standardneural networks (NNs) have traditionally not excelled. This
can be a limitation for safety-critical applications, where uncertainty-aware
methods like Gaussian processes or Bayesian linear regression are often
preferred. Bayesian neural networks are an approach to address this limitation.
They assume probability distributions for all parameters and yield distributed
predictions. However, training and inference are typically intractable and
approximations must be employed. A promising approximation is NNs with Bayesian
last layer (BLL). They assume distributed weights only in the linear output
layer and yield a normally distributed prediction. To approximate the
intractable Bayesian neural network, point estimates of the distributed weights
in all but the last layer should be obtained by maximizing the marginal
likelihood. This has previously been challenging, as the marginal likelihood is
expensive to evaluate in this setting. We present a reformulation of the
log-marginal likelihood of a NN with BLL which allows for efficient training
using backpropagation. Furthermore, we address the challenge of uncertainty
quantification for extrapolation points. We provide a metric to quantify the
degree of extrapolation and derive a method to improve the uncertainty
quantification for these points. Our methods are derived for the multivariate
case and demonstrated in a simulation study. In comparison to Bayesian linear
regression with fixed features, and a Bayesian neural network trained with
variational inference, our proposed method achieves the highest log-predictive
density on test data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been published at IEEE Access with Digital Object
  Identifier 10.1109/ACCESS.2023.3329685 under a Creative Commons Attribution
  4.0 License</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lon-ea at SemEval-2023 Task 11: A Comparison of Activation Functions for
  Soft and Hard Label Prediction <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02468v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02468v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peyman Hosseini, Mehran Hosseini, Sana Sabah Al-Azzawi, Marcus Liwicki, Ignacio Castro, Matthew Purver
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the influence of different activation functions in the output layer
of deep neural network models for soft and hard label prediction in the
learning with disagreement task. In this task, the goal is to quantify the
amount of disagreement via predicting soft labels. To predict the soft labels,
we use BERT-based preprocessors and encoders and vary the activation function
used in the output layer, while keeping other parameters constant. The soft
labels are then used for the hard label prediction. The activation functions
considered are sigmoid as well as a step-function that is added to the model
post-training and a sinusoidal activation function, which is introduced for the
first time in this paper.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ACL 2023 SemEval Workshop as selected task paper</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VGA: Vision and Graph Fused Attention Network for Rumor Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Bai, Caiyan Jia, Ziying Song, Chaoqun Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of social media, rumors have been spread broadly on
social media platforms, causing great harm to society. Beside textual
information, many rumors also use manipulated images or conceal textual
information within images to deceive people and avoid being detected, making
multimodal rumor detection be a critical problem. The majority of multimodal
rumor detection methods mainly concentrate on extracting features of source
claims and their corresponding images, while ignoring the comments of rumors
and their propagation structures. These comments and structures imply the
wisdom of crowds and are proved to be crucial to debunk rumors. Moreover, these
methods usually only extract visual features in a basic manner, seldom consider
tampering or textual information in images. Therefore, in this study, we
propose a novel Vision and Graph Fused Attention Network (VGA) for rumor
detection to utilize propagation structures among posts so as to obtain the
crowd opinions and further explore visual tampering features, as well as the
textual information hidden in images. We conduct extensive experiments on three
datasets, demonstrating that VGA can effectively detect multimodal rumors and
outperform state-of-the-art methods significantly.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WordArt Designer API: User-Driven Artistic Typography Synthesis with
  Large Language Models on ModelScope <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun-Yan He, Zhi-Qi Cheng, Chenyang Li, Jingdong Sun, Wangmeng Xiang, Yusen Hu, Xianhui Lin, Xiaoyang Kang, Zengke Jin, Bin Luo, Yifeng Geng, Xuansong Xie, Jingren Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the WordArt Designer API, a novel framework for
user-driven artistic typography synthesis utilizing Large Language Models
(LLMs) on ModelScope. We address the challenge of simplifying artistic
typography for non-professionals by offering a dynamic, adaptive, and
computationally efficient alternative to traditional rigid templates. Our
approach leverages the power of LLMs to understand and interpret user input,
facilitating a more intuitive design process. We demonstrate through various
case studies how users can articulate their aesthetic preferences and
functional requirements, which the system then translates into unique and
creative typographic designs. Our evaluations indicate significant improvements
in user satisfaction, design flexibility, and creative expression over existing
systems. The WordArt Designer API not only democratizes the art of typography
but also opens up new possibilities for personalized digital communication and
design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Spotlight Paper from the Workshop on Machine Learning for Creativity
  and Design at the 37th Conference on Neural Information Processing Systems
  (NeurIPS 2023). 5 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Token Propagation Controller for Efficient Vision <span class="highlight-title">Transformer</span> <span class="chip">WACV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision transformers (ViTs) have achieved promising results on a variety of
Computer Vision tasks, however their quadratic complexity in the number of
input tokens has limited their application specially in resource-constrained
settings. Previous approaches that employ gradual token reduction to address
this challenge assume that token redundancy in one layer implies redundancy in
all the following layers. We empirically demonstrate that this assumption is
often not correct, i.e., tokens that are redundant in one layer can be useful
in later layers. We employ this key insight to propose a novel token
propagation controller (TPC) that incorporates two different
token-distributions, i.e., pause probability and restart probability to control
the reduction and reuse of tokens respectively, which results in more efficient
token utilization. To improve the estimates of token distributions, we propose
a smoothing mechanism that acts as a regularizer and helps remove noisy
outliers. Furthermore, to improve the training-stability of our proposed TPC,
we introduce a model stabilizer that is able to implicitly encode local image
structures and minimize accuracy fluctuations during model training. We present
extensive experimental results on the ImageNet-1K dataset using DeiT, LV-ViT
and Swin models to demonstrate the effectiveness of our proposed method. For
example, compared to baseline models, our proposed method improves the
inference speed of the DeiT-S by 250% while increasing the classification
accuracy by 1.0%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WACV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MULTI-CASE: A <span class="highlight-title">Transformer</span>-based Ethics-aware Multimodal Investigative
  Intelligence Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01955v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01955v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian T. Fischer, Yannick Metz, Lucas Joos, Matthias Miller, Daniel A. Keim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI-driven models are increasingly deployed in operational analytics
solutions, for instance, in investigative journalism or the intelligence
community. Current approaches face two primary challenges: ethical and privacy
concerns, as well as difficulties in efficiently combining heterogeneous data
sources for multimodal analytics. To tackle the challenge of multimodal
analytics, we present MULTI-CASE, a holistic visual analytics framework
tailored towards ethics-aware and multimodal intelligence exploration, designed
in collaboration with domain experts. It leverages an equal joint agency
between human and AI to explore and assess heterogeneous information spaces,
checking and balancing automation through Visual Analytics. MULTI-CASE operates
on a fully-integrated data model and features type-specific analysis with
multiple linked components, including a combined search, annotated text view,
and graph-based analysis. Parts of the underlying entity detection are based on
a RoBERTa-based language model, which we tailored towards user requirements
through fine-tuning. An overarching knowledge exploration graph combines all
information streams, provides in-situ explanations, transparent source
attribution, and facilitates effective exploration. To assess our approach, we
conducted a comprehensive set of evaluations: We benchmarked the underlying
language model on relevant NER tasks, achieving state-of-the-art performance.
The demonstrator was assessed according to intelligence capability assessments,
while the methodology was evaluated according to ethics design guidelines. As a
case study, we present our framework in an investigative journalism setting,
supporting war crime investigations. Finally, we conduct a formative user
evaluation with domain experts in law enforcement. Our evaluations confirm that
our framework facilitates human agency and steering in security-sensitive
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GMMFormer: Gaussian-Mixture-Model Based <span class="highlight-title">Transformer</span> for Efficient
  Partially Relevant Video Retrieval <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05195v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05195v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuting Wang, Jinpeng Wang, Bin Chen, Ziyun Zeng, Shu-Tao Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a text query, partially relevant video retrieval (PRVR) seeks to find
untrimmed videos containing pertinent moments in a database. For PRVR, clip
modeling is essential to capture the partial relationship between texts and
videos. Current PRVR methods adopt scanning-based clip construction to achieve
explicit clip modeling, which is information-redundant and requires a large
storage overhead. To solve the efficiency problem of PRVR methods, this paper
proposes GMMFormer, a Gaussian-Mixture-Model based Transformer which models
clip representations implicitly. During frame interactions, we incorporate
Gaussian-Mixture-Model constraints to focus each frame on its adjacent frames
instead of the whole video. Then generated representations will contain
multi-scale clip information, achieving implicit clip modeling. In addition,
PRVR methods ignore semantic differences between text queries relevant to the
same video, leading to a sparse embedding space. We propose a query diverse
loss to distinguish these text queries, making the embedding space more
intensive and contain more semantic information. Extensive experiments on three
large-scale video datasets (i.e., TVR, ActivityNet Captions, and Charades-STA)
demonstrate the superiority and efficiency of GMMFormer. Code is available at
\url{https://github.com/huangmozhi9527/GMMFormer}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2024. Code is released at
  https://github.com/huangmozhi9527/GMMFormer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EmotionGesture: Audio-Driven Diverse Emotional Co-Speech 3D Gesture
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18891v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18891v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingqun Qi, Chen Liu, Lincheng Li, Jie Hou, Haoran Xin, Xin Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating vivid and diverse 3D co-speech gestures is crucial for various
applications in animating virtual avatars. While most existing methods can
generate gestures from audio directly, they usually overlook that emotion is
one of the key factors of authentic co-speech gesture generation. In this work,
we propose EmotionGesture, a novel framework for synthesizing vivid and diverse
emotional co-speech 3D gestures from audio. Considering emotion is often
entangled with the rhythmic beat in speech audio, we first develop an
Emotion-Beat Mining module (EBM) to extract the emotion and audio beat features
as well as model their correlation via a transcript-based visual-rhythm
alignment. Then, we propose an initial pose based Spatial-Temporal Prompter
(STP) to generate future gestures from the given initial poses. STP effectively
models the spatial-temporal correlations between the initial poses and the
future gestures, thus producing the spatial-temporal coherent pose prompt. Once
we obtain pose prompts, emotion, and audio beat features, we will generate 3D
co-speech gestures through a transformer architecture. However, considering the
poses of existing datasets often contain jittering effects, this would lead to
generating unstable gestures. To address this issue, we propose an effective
objective function, dubbed Motion-Smooth Loss. Specifically, we model motion
offset to compensate for jittering ground-truth by forcing gestures to be
smooth. Last, we present an emotion-conditioned VAE to sample emotion features,
enabling us to generate diverse emotional results. Extensive experiments
demonstrate that our framework outperforms the state-of-the-art, achieving
vivid and diverse emotional co-speech 3D gestures. Our code and dataset will be
released at the project page:
https://xingqunqi-lab.github.io/Emotion-Gesture-Web/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-01-02T00:00:00Z">2024-01-02</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">40</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Play Fine-Tuning Converts Weak Language Models to Strong Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, Quanquan Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Harnessing the power of human-annotated data through Supervised Fine-Tuning
(SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we
delve into the prospect of growing a strong LLM out of a weak one without the
need for acquiring additional human-annotated data. We propose a new
fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a
supervised fine-tuned model. At the heart of SPIN lies a self-play mechanism,
where the LLM refines its capability by playing against instances of itself.
More specifically, the LLM generates its own training data from its previous
iterations, refining its policy by discerning these self-generated responses
from those obtained from human-annotated data. Our method progressively
elevates the LLM from a nascent model to a formidable one, unlocking the full
potential of human-annotated demonstration data for SFT. Theoretically, we
prove that the global optimum to the training objective function of our method
is achieved only when the LLM policy aligns with the target data distribution.
Empirically, we evaluate our method on several benchmark datasets including the
HuggingFace Open LLM Leaderboard, MT-Bench, and datasets from Big-Bench. Our
results show that SPIN can significantly improve the LLM's performance across a
variety of benchmarks and even outperform models trained through direct
preference optimization (DPO) supplemented with extra GPT-4 preference data.
This sheds light on the promise of self-play, enabling the achievement of
human-level performance in LLMs without the need for expert opponents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 6 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TREC iKAT 2023: The Interactive Knowledge Assistance Track <span class="highlight-title">Overview</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Aliannejadi, Zahra Abbasiantaeb, Shubham Chatterjee, Jeffery Dalton, Leif Azzopardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational Information Seeking stands as a pivotal research area with
significant contributions from previous works. The TREC Interactive Knowledge
Assistance Track (iKAT) builds on the foundational work of the TREC
Conversational Assistance Track (CAsT). However, iKAT distinctively emphasizes
the creation and research of conversational search agents that adapt responses
based on user's prior interactions and present context. The challenge lies in
enabling Conversational Search Agents (CSA) to incorporate this personalized
context to efficiency and effectively guide users through the relevant
information to them. iKAT also emphasizes decisional search tasks, where users
sift through data and information to weigh up options in order to reach a
conclusion or perform an action. These tasks, prevalent in everyday
information-seeking decisions -- be it related to travel, health, or shopping
-- often revolve around a subset of high-level information operators where
queries or questions about the information space include: finding options,
comparing options, identifying the pros and cons of options, etc. Given the
different personas and their information need (expressed through the sequence
of questions), diverse conversation trajectories will arise -- because the
answers to these similar queries will be very different. In this paper, we
report on the first year of TREC iKAT, describing the task, topics, data
collection, and evaluation framework. We further review the submissions and
summarize the findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TREC 2023 Overview Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Autoregressive Text-to-Graph Framework for Joint Entity and Relation
  Extraction <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zaratiana Urchade, Nadi Tomeh, Pierre Holat, Thierry Charnois
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel method for joint entity and relation
extraction from unstructured text by framing it as a conditional sequence
generation problem. In contrast to conventional generative information
extraction models that are left-to-right token-level generators, our approach
is \textit{span-based}. It generates a linearized graph where nodes represent
text spans and edges represent relation triplets. Our method employs a
transformer encoder-decoder architecture with pointing mechanism on a dynamic
vocabulary of spans and relation types. Our model can capture the structural
characteristics and boundaries of entities and relations through span
representations while simultaneously grounding the generated output in the
original text thanks to the pointing mechanism. Evaluation on benchmark
datasets validates the effectiveness of our approach, demonstrating competitive
results. Code is available at https://github.com/urchade/ATG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01325v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01325v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen, Xia Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work elicits LLMs' inherent ability to handle long contexts without
fine-tuning. The limited length of the training sequence during training may
limit the application of Large Language Models (LLMs) on long input sequences
for inference. In this work, we argue that existing LLMs themselves have
inherent capabilities for handling long contexts. Based on this argument, we
suggest extending LLMs' context window by themselves to fully utilize the
inherent ability.We propose Self-Extend to stimulate LLMs' long context
handling potential. The basic idea is to construct bi-level attention
information: the group level and the neighbor level. The two levels are
computed by the original model's self-attention, which means the proposed does
not require any training. With only four lines of code modification, the
proposed method can effortlessly extend existing LLMs' context window without
any fine-tuning. We conduct comprehensive experiments and the results show that
the proposed method can effectively extend existing LLMs' context window's
length.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive <span class="highlight-title">Survey</span> of Hallucination Mitigation Techniques in Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S. M Towhidul Islam Tonmoy, S M Mehedi Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, Amitava Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) continue to advance in their ability to write
human-like text, a key challenge remains around their tendency to hallucinate
generating content that appears factual but is ungrounded. This issue of
hallucination is arguably the biggest hindrance to safely deploying these
powerful LLMs into real-world production systems that impact people's lives.
The journey toward widespread adoption of LLMs in practical settings heavily
relies on addressing and mitigating hallucinations. Unlike traditional AI
systems focused on limited tasks, LLMs have been exposed to vast amounts of
online text data during training. While this allows them to display impressive
language fluency, it also means they are capable of extrapolating information
from the biases in training data, misinterpreting ambiguous prompts, or
modifying the information to align superficially with the input. This becomes
hugely alarming when we rely on language generation capabilities for sensitive
applications, such as summarizing medical records, financial analysis reports,
etc. This paper presents a comprehensive survey of over 32 techniques developed
to mitigate hallucination in LLMs. Notable among these are Retrieval Augmented
Generation (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023),
CoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023). Furthermore, we
introduce a detailed taxonomy categorizing these methods based on various
parameters, such as dataset utilization, common tasks, feedback mechanisms, and
retriever types. This classification helps distinguish the diverse approaches
specifically designed to tackle hallucination issues in LLMs. Additionally, we
analyze the challenges and limitations inherent in these techniques, providing
a solid foundation for future research in addressing hallucinations and related
phenomena within the realm of LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2311.09677,
  arXiv:2308.11764 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Legal Fictions: Profiling Legal Hallucinations in Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Dahl, Varun Magesh, Mirac Suzgun, Daniel E. Ho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have the potential to transform the practice of
law, but this potential is threatened by the presence of legal hallucinations
-- responses from these models that are not consistent with legal facts. We
investigate the extent of these hallucinations using an original suite of legal
queries, comparing LLMs' responses to structured legal metadata and examining
their consistency. Our work makes four key contributions: (1) We develop a
typology of legal hallucinations, providing a conceptual framework for future
research in this area. (2) We find that legal hallucinations are alarmingly
prevalent, occurring between 69% of the time with ChatGPT 3.5 and 88% with
Llama 2, when these models are asked specific, verifiable questions about
random federal court cases. (3) We illustrate that LLMs often fail to correct a
user's incorrect legal assumptions in a contra-factual question setup. (4) We
provide evidence that LLMs cannot always predict, or do not always know, when
they are producing legal hallucinations. Taken together, these findings caution
against the rapid and unsupervised integration of popular LLMs into legal
tasks. Even experienced lawyers must remain wary of legal hallucinations, and
the risks are highest for those who stand to benefit from LLMs the most -- pro
se litigants or those without access to traditional legal resources.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive Study of Knowledge Editing for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, Siyuan Cheng, Ziwen Xu, Xin Xu, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Lei Liang, Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown extraordinary capabilities in
understanding and generating text that closely mirrors human communication.
However, a primary limitation lies in the significant computational demands
during training, arising from their extensive parameterization. This challenge
is further intensified by the dynamic nature of the world, necessitating
frequent updates to LLMs to correct outdated information or integrate new
knowledge, thereby ensuring their continued relevance. Note that many
applications demand continual model adjustments post-training to address
deficiencies or undesirable behaviors. There is an increasing interest in
efficient, lightweight methods for on-the-fly model modifications. To this end,
recent years have seen a burgeoning in the techniques of knowledge editing for
LLMs, which aim to efficiently modify LLMs' behaviors within specific domains
while preserving overall performance across various inputs. In this paper, we
first define the knowledge editing problem and then provide a comprehensive
review of cutting-edge approaches. Drawing inspiration from educational and
cognitive research theories, we propose a unified categorization criterion that
classifies knowledge editing methods into three groups: resorting to external
knowledge, merging knowledge into the model, and editing intrinsic knowledge.
Furthermore, we introduce a new benchmark, KnowEdit, for a comprehensive
empirical evaluation of representative knowledge editing approaches.
Additionally, we provide an in-depth analysis of knowledge location, which can
provide a deeper understanding of the knowledge structures inherent within
LLMs. Finally, we discuss several potential applications of knowledge editing,
outlining its broad and impactful implications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing work; 50 pages, 265 citations; benchmark is available at
  https://huggingface.co/datasets/zjunlp/KnowEdit; code is available at
  https://github.com/zjunlp/EasyEdit; paper list is available at
  https://github.com/zjunlp/KnowledgeEditingPapers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quality and Quantity of Machine Translation References for Automated
  Metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01283v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01283v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vilém Zouhar, Ondřej Bojar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic machine translation metrics often use human translations to
determine the quality system translations. Common wisdom in the field dictates
that the human references should be of very high quality. However, there are no
cost-benefit analyses that could be used to guide practitioners who plan to
collect references for machine translation evaluation. We find that
higher-quality references lead to better metric correlations with humans at the
segment-level. Having up to 7 references per segment and taking their average
helps all metrics. Interestingly, the references from vendors of different
qualities can be mixed together and improve metric success. Higher quality
references, however, cost more to create and we frame this as an optimization
problem: given a specific budget, what references should be collected to
maximize metric success. These findings can be used by evaluators of shared
tasks when references need to be created under a certain budget.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent
  Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quan Tu, Shilong Fan, Zihang Tian, Rui Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the advent of large language models (LLMs) has revolutionized
generative agents. Among them, Role-Playing Conversational Agents (RPCAs)
attract considerable attention due to their ability to emotionally engage
users. However, the absence of a comprehensive benchmark impedes progress in
this field. To bridge this gap, we introduce CharacterEval, a Chinese benchmark
for comprehensive RPCA assessment, complemented by a tailored high-quality
dataset. The dataset comprises 1,785 multi-turn role-playing dialogues,
encompassing 23,020 examples and featuring 77 characters derived from Chinese
novels and scripts. It was carefully constructed, beginning with initial
dialogue extraction via GPT-4, followed by rigorous human-led quality control,
and enhanced with in-depth character profiles sourced from Baidu Baike.
CharacterEval employs a multifaceted evaluation approach, encompassing thirteen
targeted metrics on four dimensions. Comprehensive experiments on CharacterEval
demonstrate that Chinese LLMs exhibit more promising capabilities than GPT-4 in
Chinese role-playing conversation. Source code, data source and reward model
will be publicly accessible at https://github.com/morecry/CharacterEval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fairness Certification for Natural Language Processing and Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Freiberger, Erik Buchmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Processing (NLP) plays an important role in our daily lives,
particularly due to the enormous progress of Large Language Models (LLM).
However, NLP has many fairness-critical use cases, e.g., as an expert system in
recruitment or as an LLM-based tutor in education. Since NLP is based on human
language, potentially harmful biases can diffuse into NLP systems and produce
unfair results, discriminate against minorities or generate legal issues.
Hence, it is important to develop a fairness certification for NLP approaches.
We follow a qualitative research approach towards a fairness certification for
NLP. In particular, we have reviewed a large body of literature on algorithmic
fairness, and we have conducted semi-structured expert interviews with a wide
range of experts from that area. We have systematically devised six fairness
criteria for NLP, which can be further refined into 18 sub-categories. Our
criteria offer a foundation for operationalizing and testing processes to
certify fairness, both from the perspective of the auditor and the audited
organization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In depth discussion of our results can be found in the Appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VideoDrafter: Content-Consistent Multi-Scene Video Generation with LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuchen Long, Zhaofan Qiu, Ting Yao, Tao Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent innovations and breakthroughs in diffusion models have
significantly expanded the possibilities of generating high-quality videos for
the given prompts. Most existing works tackle the single-scene scenario with
only one video event occurring in a single background. Extending to generate
multi-scene videos nevertheless is not trivial and necessitates to nicely
manage the logic in between while preserving the consistent visual appearance
of key content across video scenes. In this paper, we propose a novel
framework, namely VideoDrafter, for content-consistent multi-scene video
generation. Technically, VideoDrafter leverages Large Language Models (LLM) to
convert the input prompt into comprehensive multi-scene script that benefits
from the logical knowledge learnt by LLM. The script for each scene includes a
prompt describing the event, the foreground/background entities, as well as
camera movement. VideoDrafter identifies the common entities throughout the
script and asks LLM to detail each entity. The resultant entity description is
then fed into a text-to-image model to generate a reference image for each
entity. Finally, VideoDrafter outputs a multi-scene video by generating each
scene video via a diffusion process that takes the reference images, the
descriptive prompt of the event and camera movement into account. The diffusion
model incorporates the reference images as the condition and alignment to
strengthen the content consistency of multi-scene videos. Extensive experiments
demonstrate that VideoDrafter outperforms the SOTA video generation models in
terms of visual quality, content consistency, and user preference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://videodrafter.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-Shot Position Debiasing for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01218v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01218v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongkun Liu, Zheng Chen, Mengqi Zhang, Zhaochun Ren, Zhumin Chen, Pengjie Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning has been demonstrated to be an effective method to improve the
domain performance of large language models (LLMs). However, LLMs might fit the
dataset bias and shortcuts for prediction, leading to poor generation
performance. Experimental result shows that LLMs are prone to exhibit position
bias, i.e., leveraging information positioned at the beginning or end, or
specific positional cues within the input. Existing works on mitigating
position bias require external bias knowledge or annotated non-biased samples,
which is unpractical in reality. In this work, we propose a zero-shot position
debiasing (ZOE) framework to mitigate position bias for LLMs. ZOE leverages
unsupervised responses from pre-trained LLMs for debiasing, thus without any
external knowledge or datasets. To improve the quality of unsupervised
responses, we propose a master-slave alignment (MSA) module to prune these
responses. Experiments on eight datasets and five tasks show that ZOE
consistently outperforms existing methods in mitigating four types of position
biases. Besides, ZOE achieves this by sacrificing only a small performance on
biased samples, which is simple and effective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 22 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty Resolution in Misinformation Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01197v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01197v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yury Orlovskiy, Camille Thibault, Anne Imouza, Jean-François Godbout, Reihaneh Rabbany, Kellin Pelrine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Misinformation poses a variety of risks, such as undermining public trust and
distorting factual discourse. Large Language Models (LLMs) like GPT-4 have been
shown effective in mitigating misinformation, particularly in handling
statements where enough context is provided. However, they struggle to assess
ambiguous or context-deficient statements accurately. This work introduces a
new method to resolve uncertainty in such statements. We propose a framework to
categorize missing information and publish category labels for the LIAR-New
dataset, which is adaptable to cross-domain content with missing information.
We then leverage this framework to generate effective user queries for missing
context. Compared to baselines, our method improves the rate at which generated
questions are answerable by the user by 38 percentage points and classification
performance by over 10 percentage points macro F1. Thus, this approach may
provide a valuable component for future misinformation mitigation pipelines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unifying Structured Data as Graph for Data-to-Text <span class="highlight-title">Pre-Train</span>ing <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shujie Li, Liang Li, Ruiying Geng, Min Yang, Binhua Li, Guanghu Yuan, Wanwei He, Shao Yuan, Can Ma, Fei Huang, Yongbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-to-text (D2T) generation aims to transform structured data into natural
language text. Data-to-text pre-training has proved to be powerful in enhancing
D2T generation and yields impressive performances. However, previous
pre-training methods either oversimplified structured data into a sequence
without considering input structures or designed training objectives tailored
for a specific data structure (e.g., table or knowledge graph). In this paper,
we unify different types of structured data (i.e., table, key-value data,
knowledge graph) into the graph format and cast different data-to-text
generation tasks as graph-to-text generation. To effectively exploit the
structural information of the input graph, we propose a structure-enhanced
pre-training method for D2T generation by designing a structure-enhanced
Transformer. Concretely, we devise a position matrix for the Transformer,
encoding relative positional information of connected nodes in the input graph.
In addition, we propose a new attention matrix to incorporate graph structures
into the original Transformer by taking the available explicit connectivity
structure into account. Extensive experiments on six benchmark datasets show
the effectiveness of our model. Our source codes are available at
https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/unid2t.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for TACL. Pre-MIT Press publication version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling Comparative Sentiments in Vietnamese Product <span class="highlight-title">Review</span>s: A
  Sequential Classification Framework <span class="chip">SP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01108v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01108v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ha Le, Bao Tran, Phuong Le, Tan Nguyen, Dac Nguyen, Ngoan Pham, Dang Huynh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Comparative opinion mining is a specialized field of sentiment analysis that
aims to identify and extract sentiments expressed comparatively. To address
this task, we propose an approach that consists of solving three sequential
sub-tasks: (i) identifying comparative sentence, i.e., if a sentence has a
comparative meaning, (ii) extracting comparative elements, i.e., what are
comparison subjects, objects, aspects, predicates, and (iii) classifying
comparison types which contribute to a deeper comprehension of user sentiments
in Vietnamese product reviews. Our method is ranked fifth at the Vietnamese
Language and Speech Processing (VLSP) 2023 challenge on Comparative Opinion
Mining (ComOM) from Vietnamese Product Reviews.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted manuscript at VLSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quokka: An Open-source Large Language Model ChatBot for Material Science 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01089v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01089v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianjun Yang, Stephen D. Wilson, Linda Petzold
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the development of a specialized chatbot for materials
science, leveraging the Llama-2 language model, and continuing pre-training on
the expansive research articles in the materials science domain from the S2ORC
dataset. The methodology involves an initial pretraining phase on over one
million domain-specific papers, followed by an instruction-tuning process to
refine the chatbot's capabilities. The chatbot is designed to assist
researchers, educators, and students by providing instant, context-aware
responses to queries in the field of materials science. We make the four
trained checkpoints (7B, 13B, with or without chat ability) freely available to
the research community at https://github.com/Xianjun-Yang/Quokka.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vietnamese Poem Generation & The Prospect Of Cross-Language Poem-To-Poem
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01078v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01078v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Triet Huynh Minh, Quan Le Bao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Poetry generation has been a challenging task in the field of Natural
Language Processing, as it requires the model to understand the nuances of
language, sentiment, and style. In this paper, we propose using Large Language
Models to generate Vietnamese poems from natural language prompts, thereby
facilitating an intuitive process with enhanced content control. Our most
efficacious model, the GPT-3 Babbage variant, achieves a custom evaluation
score of 0.8, specifically tailored to the "luc bat" genre of Vietnamese
poetry. Furthermore, we also explore the idea of paraphrasing poems into normal
text prompts and yield a relatively high score of 0.718 in the "luc bat" genre.
This experiment presents the potential for cross-Language poem-to-poem
translation with translated poems as the inputs while concurrently maintaining
complete control over the generated content.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DialCLIP: Empowering CLIP as Multi-Modal Dialog Retriever 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01076v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01076v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichao Yin, Binyuan Hui, Min Yang, Fei Huang, Yongbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, substantial advancements in pre-trained vision-language models have
greatly enhanced the capabilities of multi-modal dialog systems. These models
have demonstrated significant improvements by fine-tuning on downstream tasks.
However, the existing pre-trained models primarily focus on effectively
capturing the alignment between vision and language modalities, often ignoring
the intricate nature of dialog context. In this paper, we propose a
parameter-efficient prompt-tuning method named DialCLIP for multi-modal dialog
retrieval. Specifically, our approach introduces a multi-modal context prompt
generator to learn context features which are subsequently distilled into
prompts within the pre-trained vision-language model CLIP. Besides, we
introduce domain prompt to mitigate the disc repancy from the downstream dialog
data. To facilitate various types of retrieval, we also design multiple experts
to learn mappings from CLIP outputs to multi-modal representation space, with
each expert being responsible to one specific retrieval type. Extensive
experiments show that DialCLIP achieves state-of-the-art performance on two
widely recognized benchmark datasets (i.e., PhotoChat and MMDialog) by tuning a
mere 0.04% of the total parameters. These results highlight the efficacy and
efficiency of our proposed approach, underscoring its potential to advance the
field of multi-modal dialog retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arxiv: first version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Discovering Significant Topics from Legal Decisions with Selective
  Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01068v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01068v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jerrold Soh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose and evaluate an automated pipeline for discovering significant
topics from legal decision texts by passing features synthesized with topic
models through penalised regressions and post-selection significance tests. The
method identifies case topics significantly correlated with outcomes,
topic-word distributions which can be manually-interpreted to gain insights
about significant topics, and case-topic weights which can be used to identify
representative cases for each topic. We demonstrate the method on a new dataset
of domain name disputes and a canonical dataset of European Court of Human
Rights violation cases. Topic models based on latent semantic analysis as well
as language model embeddings are evaluated. We show that topics derived by the
pipeline are consistent with legal doctrines in both areas and can be useful in
other related legal analysis tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is an accepted manuscript of work forthcoming in PhilTrans A.
  Please cite the publisher's version only</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLaMA Beyond English: An Empirical Study on Language Capability Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Zhao, Zhihao Zhang, Qi Zhang, Tao Gui, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent times, substantial advancements have been witnessed in large
language models (LLMs), exemplified by ChatGPT, showcasing remarkable
proficiency across a range of complex tasks. However, many mainstream LLMs
(e.g. LLaMA) are pretrained on English-dominant corpus, which limits their
performance in other non-English languages. In this paper, we focus on how to
effectively transfer the capabilities of language generation and following
instructions to a non-English language. To answer this question, we conduct an
extensive empirical investigation based on LLaMA, accumulating over 1440 GPU
hours. We analyze the impact of key factors such as vocabulary extension,
further pretraining, and instruction tuning on transfer. To accurately assess
the model's level of knowledge, we employ four widely used standardized testing
benchmarks: C-Eval, MMLU, AGI-Eval, and GAOKAO-Bench. Furthermore, a
comprehensive evaluation of the model's response quality is conducted,
considering aspects such as accuracy, fluency, informativeness, logical
coherence, and harmlessness, based on LLM-Eval, a benchmarks consisting
instruction tasks from 17 diverse categories. Our evaluation results
demonstrate that comparable performance to state-of-the-art transfer models can
be achieved with less than 1% of the pretraining data, both in terms of
knowledge alignment and response quality. Furthermore, the experimental
outcomes across the thirteen low-resource languages also exhibit similar
trends. We anticipate that the conclusions revealed by the experiments will aid
the community in developing non-English LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cheetah: Natural Language Generation for 517 African Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01053v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01053v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ife Adebara, AbdelRahim Elmadany, Muhammad Abdul-Mageed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-resource African languages pose unique challenges for natural language
processing (NLP) tasks, including natural language generation (NLG). In this
paper, we develop Cheetah, a massively multilingual NLG language model for
African languages. Cheetah supports 517 African languages and language
varieties, allowing us to address the scarcity of NLG resources and provide a
solution to foster linguistic diversity. We demonstrate the effectiveness of
Cheetah through comprehensive evaluations across seven generation downstream
tasks. In five of the seven tasks, Cheetah significantly outperforms other
models, showcasing its remarkable performance for generating coherent and
contextually appropriate text in a wide range of African languages. We
additionally conduct a detailed human evaluation to delve deeper into the
linguistic capabilities of Cheetah. The introduction of Cheetah has
far-reaching benefits for linguistic diversity. By leveraging pretrained models
and adapting them to specific languages, our approach facilitates the
development of practical NLG applications for African communities. The findings
of this study contribute to advancing NLP research in low-resource settings,
enabling greater accessibility and inclusion for African languages in a rapidly
expanding digital landscape. We will publicly release our models for research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Auffusion: Leveraging the Power of Diffusion and Large Language Models
  for Text-to-Audio Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinlong Xue, Yayue Deng, Yingming Gao, Ya Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in diffusion models and large language models (LLMs) have
significantly propelled the field of AIGC. Text-to-Audio (TTA), a burgeoning
AIGC application designed to generate audio from natural language prompts, is
attracting increasing attention. However, existing TTA studies often struggle
with generation quality and text-audio alignment, especially for complex
textual inputs. Drawing inspiration from state-of-the-art Text-to-Image (T2I)
diffusion models, we introduce Auffusion, a TTA system adapting T2I model
frameworks to TTA task, by effectively leveraging their inherent generative
strengths and precise cross-modal alignment. Our objective and subjective
evaluations demonstrate that Auffusion surpasses previous TTA approaches using
limited data and computational resource. Furthermore, previous studies in T2I
recognizes the significant impact of encoder choice on cross-modal alignment,
like fine-grained details and object bindings, while similar evaluation is
lacking in prior TTA works. Through comprehensive ablation studies and
innovative cross-attention map visualizations, we provide insightful
assessments of text-audio alignment in TTA. Our findings reveal Auffusion's
superior capability in generating audios that accurately match textual
descriptions, which further demonstrated in several related tasks, such as
audio style transfer, inpainting and other manipulations. Our implementation
and demos are available at https://auffusion.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Demo and implementation at https://auffusion.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine
  Translation vs Human Translation <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01419v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01419v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaming Luo, Colin Cherry, George Foster
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We conduct a large-scale fine-grained comparative analysis of machine
translations (MT) against human translations (HT) through the lens of
morphosyntactic divergence. Across three language pairs and two types of
divergence defined as the structural difference between the source and the
target, MT is consistently more conservative than HT, with less morphosyntactic
diversity, more convergent patterns, and more one-to-one alignments. Through
analysis on different decoding algorithms, we attribute this discrepancy to the
use of beam search that biases MT towards more convergent patterns. This bias
is most amplified when the convergent pattern appears around 50% of the time in
training data. Lastly, we show that for a majority of morphosyntactic
divergences, their presence in HT is correlated with decreased MT performance,
presenting a greater challenge for MT systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TACL, pre-MIT Press publication version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantifying the Uniqueness of Donald Trump in Presidential Discourse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karen Zhou, Alexander A. Meitus, Milo Chase, Grace Wang, Anne Mykland, William Howell, Chenhao Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Does Donald Trump speak differently from other presidents? If so, in what
ways? Are these differences confined to any single medium of communication? To
investigate these questions, this paper introduces a novel metric of uniqueness
based on large language models, develops a new lexicon for divisive speech, and
presents a framework for comparing the lexical features of political opponents.
Applying these tools to a variety of corpora of presidential speeches, we find
considerable evidence that Trump's speech patterns diverge from those of all
major party nominees for the presidency in recent history. Some notable
findings include Trump's employment of particularly divisive and antagonistic
language targeting of his political opponents and his patterns of repetition
for emphasis. Furthermore, Trump is significantly more distinctive than his
fellow Republicans, whose uniqueness values are comparably closer to those of
the Democrats. These differences hold across a variety of measurement
strategies, arise on both the campaign trail and in official presidential
addresses, and do not appear to be an artifact of secular time trends.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodality and Attention Increase Alignment in Natural Language
  Prediction Between Humans and Computational Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.06035v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.06035v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viktor Kewenig, Andrew Lampinen, Samuel A. Nastase, Christopher Edwards, Quitterie Lacome DEstalenx, Akilles Rechardt, Jeremy I Skipper, Gabriella Vigliocco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The potential of multimodal generative artificial intelligence (mAI) to
replicate human grounded language understanding, including the pragmatic,
context-rich aspects of communication, remains to be clarified. Humans are
known to use salient multimodal features, such as visual cues, to facilitate
the processing of upcoming words. Correspondingly, multimodal computational
models can integrate visual and linguistic data using a visual attention
mechanism to assign next-word probabilities. To test whether these processes
align, we tasked both human participants (N = 200) as well as several
state-of-the-art computational models with evaluating the predictability of
forthcoming words after viewing short audio-only or audio-visual clips with
speech. During the task, the model's attention weights were recorded and human
attention was indexed via eye tracking. Results show that predictability
estimates from humans aligned more closely with scores generated from
multimodal models vs. their unimodal counterparts. Furthermore, including an
attention mechanism doubled alignment with human judgments when visual and
linguistic context facilitated predictions. In these cases, the model's
attention patches and human eye tracking significantly overlapped. Our results
indicate that improved modeling of naturalistic language processing in mAI does
not merely depend on training diet but can be driven by multimodality in
combination with attention-based architectures. Humans and computational models
alike can leverage the predictive constraints of multimodal information by
attending to relevant features in the input.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 4 figures, submitted to Nature Human Behaviour</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Art<span class="highlight-title">GPT</span>-4: Towards Artistic-understanding Large Vision-Language Models
  with Enhanced Adapter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.07490v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.07490v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengqing Yuan, Xinyi Wang, Kun Wang, Lichao Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, advancements in large language models have been remarkable,
with models such as ChatGPT demonstrating exceptional proficiency in diverse
linguistic tasks. The pre-training of large models with billions of parameters,
poses a formidable challenge, primarily due to the scarcity of datasets of a
commensurate scale for effective training. Nevertheless, innovative strategies
have emerged, including methods to fine-tune these pre-trained models using
fewer parameters set, as evidenced by models like MiniGPT-4 and LLaVA. Despite
their potential in various domains, these models remain limited in their
understanding of artistic imagery. They have yet to fully grasp the intricate
nuances of art images or to provide an objective articulation of the emotions
they evoke, in a manner akin to human perception. This work introduces
ArtGPT-4, a pioneering large vision-language model tailored to address the
deficiencies of contemporary models in artistic comprehension. ArtGPT-4
underwent training on image-text pairs utilizing a Tesla A100 device in a mere
2 hours, with a dataset comprising approximately 0.52M entries. Impressively,
the model can render images with an artistic-understanding and convey the
emotions they inspire, mirroring human interpretation. Additionally, this work
presents a unique dataset designed to evaluate the efficacy of vision-language
models. In subsequent evaluations, ArtGPT-4 not only achieved state-of-the-art
performance on the ArtEmis and ArtEmis-v2.0 datasets but also exceeded the
established benchmarks introduced in This study, lagging behind professional
artists' descriptions by a negligible 0.15 points on a 6-point scale. The code
and the pre-trained model are accessible in
https://huggingface.co/Tyrannosaurus/ArtGPT-4.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-Time Online Stock Forecasting Utilizing Integrated Quantitative and
  Qualitative Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15218v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15218v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Akash Bathini, Dagli Cihan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The application of Machine learning to finance has become a familiar
approach, even more so in stock market forecasting. The stock market is highly
volatile, and huge amounts of data are generated every minute globally. The
extraction of effective intelligence from this data is of critical importance.
However, a collaboration of numerical stock data with qualitative text data can
be a challenging task. In this work, we accomplish this by providing an
unprecedented, publicly available dataset with technical and fundamental data
and sentiment that we gathered from news archives, TV news captions, radio
transcripts, tweets, daily financial newspapers, etc. The text data entries
used for sentiment extraction total more than 1.4 Million. The dataset consists
of daily entries from January 2018 to December 2022 for eight companies
representing diverse industrial sectors and the Dow Jones Industrial Average
(DJIA) as a whole. Holistic Fundamental and Technical data is provided training
ready for Model learning and deployment. Most importantly, the data generated
could be used for incremental online learning with real-time data points
retrieved daily since no stagnant data was utilized. All the data was retired
from APIs or self-designed robust information retrieval technologies with
extremely low latency and zero monetary cost. These adaptable technologies
facilitate data extraction for any stock. Moreover, the utilization of
Spearman's rank correlation over real-time data, linking stock returns with
sentiment analysis has produced noteworthy results for the DJIA and the eight
other stocks, achieving accuracy levels surpassing 60%. The dataset is made
available at https://github.com/batking24/Huge-Stock-Dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structured Packing in LLM Training Improves Long Context Utilization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17296v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17296v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konrad Staniszewski, Szymon Tworkowski, Sebastian Jaszczur, Henryk Michalewski, Łukasz Kuciński, Piotr Miłoś
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in long-context Large Language Models (LCLMs) have generated
significant interest, especially in applications such as querying scientific
research papers. However, their potential is often limited by inadequate
context utilization. We identify the absence of long-range semantic
dependencies in typical training data as a primary hindrance. To address this,
we delve into the benefits of frequently incorporating related documents into
training inputs. Using the inherent directory structure of code data as a
source of training examples, we demonstrate improvements in perplexity, even
for tasks unrelated to coding. Building on these findings, but with a broader
focus, we introduce Structured Packing for Long Context (SPLiCe). SPLiCe is an
innovative method for creating training examples by using a retrieval method to
collate the most mutually relevant documents into a single training context.
Our results indicate that \method{} enhances model performance and can be used
to train large models to utilize long contexts better. We validate our results
by training a large $3$B model, showing both perplexity improvements and better
long-context performance on downstream tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RS5M and GeoRSCLIP: A Large Scale Vision-Language <span class="highlight-title">Dataset</span> and A Large
  Vision-Language Model for Remote Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.11300v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.11300v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zilun Zhang, Tiancheng Zhao, Yulong Guo, Jianwei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained Vision-Language Models (VLMs) utilizing extensive image-text
paired data have demonstrated unprecedented image-text association
capabilities, achieving remarkable results across various downstream tasks. A
critical challenge is how to make use of existing large-scale pre-trained VLMs,
which are trained on common objects, to perform the domain-specific transfer
for accomplishing domain-related downstream tasks. A critical challenge is how
to make use of existing large-scale pre-trained VLMs, which are trained on
common objects, to perform the domain-specific transfer for accomplishing
domain-related downstream tasks. In this paper, we propose a new framework that
includes the Domain pre-trained Vision-Language Model (DVLM), bridging the gap
between the General Vision-Language Model (GVLM) and domain-specific downstream
tasks. Moreover, we present an image-text paired dataset in the field of remote
sensing (RS), RS5M, which has 5 million RS images with English descriptions.
The dataset is obtained from filtering publicly available image-text paired
datasets and captioning label-only RS datasets with pre-trained VLM. These
constitute the first large-scale RS image-text paired dataset. Additionally, we
fine-tuned the CLIP model and tried several Parameter-Efficient Fine-Tuning
methods on RS5M to implement the DVLM. Experimental results show that our
proposed dataset is highly effective for various tasks, and our model GeoRSCLIP
improves upon the baseline or previous state-of-the-art model by $3\%\sim20\%$
in Zero-shot Classification (ZSC), $3\%\sim6\%$ in Remote Sensing Cross-Modal
Text-Image Retrieval (RSCTIR) and $4\%\sim5\%$ in Semantic Localization (SeLo)
tasks. Dataset and models have been released in:
\url{https://github.com/om-ai-lab/RS5M}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>RS5M dataset v5</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StyleSinger: Style Transfer for Out-of-Domain Singing Voice Synthesis <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10741v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10741v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Zhang, Rongjie Huang, Ruiqi Li, JinZheng He, Yan Xia, Feiyang Chen, Xinyu Duan, Baoxing Huai, Zhou Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Style transfer for out-of-domain (OOD) singing voice synthesis (SVS) focuses
on generating high-quality singing voices with unseen styles (such as timbre,
emotion, pronunciation, and articulation skills) derived from reference singing
voice samples. However, the endeavor to model the intricate nuances of singing
voice styles is an arduous task, as singing voices possess a remarkable degree
of expressiveness. Moreover, existing SVS methods encounter a decline in the
quality of synthesized singing voices in OOD scenarios, as they rest upon the
assumption that the target vocal attributes are discernible during the training
phase. To overcome these challenges, we propose StyleSinger, the first singing
voice synthesis model for zero-shot style transfer of out-of-domain reference
singing voice samples. StyleSinger incorporates two critical approaches for
enhanced effectiveness: 1) the Residual Style Adaptor (RSA) which employs a
residual quantization module to capture diverse style characteristics in
singing voices, and 2) the Uncertainty Modeling Layer Normalization (UMLN) to
perturb the style attributes within the content representation during the
training phase and thus improve the model generalization. Our extensive
evaluations in zero-shot style transfer undeniably establish that StyleSinger
outperforms baseline models in both audio quality and similarity to the
reference singing voice samples. Access to singing voice samples can be found
at https://stylesinger.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long
  Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19923v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19923v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Günther, Jackmin Ong, Isabelle Mohr, Alaeddine Abdessalem, Tanguy Abel, Mohammad Kalim Akram, Susana Guzman, Georgios Mastrapas, Saba Sturua, Bo Wang, Maximilian Werk, Nan Wang, Han Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text embedding models have emerged as powerful tools for transforming
sentences into fixed-sized feature vectors that encapsulate semantic
information. While these models are essential for tasks like information
retrieval, semantic clustering, and text re-ranking, most existing open-source
models, especially those built on architectures like BERT, struggle to
represent lengthy documents and often resort to truncation. One common approach
to mitigate this challenge involves splitting documents into smaller paragraphs
for embedding. However, this strategy results in a much larger set of vectors,
consequently leading to increased memory consumption and computationally
intensive vector searches with elevated latency.
  To address these challenges, we introduce Jina Embeddings 2, an open-source
text embedding model capable of accommodating up to 8192 tokens. This model is
designed to transcend the conventional 512-token limit and adeptly process long
documents. Jina Embeddings 2 not only achieves state-of-the-art performance on
a range of embedding-related tasks in the MTEB benchmark but also matches the
performance of OpenAI's proprietary ada-002 model. Additionally, our
experiments indicate that an extended context can enhance performance in tasks
such as NarrativeQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Stochastic Analysis of the Linguistic Provenance of English Place
  Names 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12850v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12850v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Dalvean
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In English place name analysis, meanings are often derived from the
resemblance of roots in place names to topographical features, proper names
and/or habitation terms in one of the languages that have had an influence on
English place names. The problem here is that it is sometimes difficult to
determine the base language to use to interpret the roots. The purpose of this
paper is to stochastically determine the resemblance between 18799 English
place names and 84687 place names from Ireland, Scotland, Wales, Denmark,
Norway, Sweden, France, Germany, the Netherlands and Ancient Rome. Each English
place name is ranked according to the extent to which it resembles place names
from the other countries, and this provides a basis for determining the likely
language to use to interpret the place name. A number of observations can be
made using the ranking provided. In particular, it is found that `Harlington'
is the most archetypically English place name in the English sample, and `Anna'
is the least. Furthermore, it is found that the place names in the non-English
datasets are most similar to Norwegian place names and least similar to Welsh
place names.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-depth analysis of music structure as a text network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13631v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13631v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ping-Rui Tsai, Yen-Ting Chou, Nathan-Christopher Wang, Hui-Ling Chen, Hong-Yue Huang, Zih-Jia Luo, Tzay-Ming Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Music, enchanting and poetic, permeates every corner of human civilization.
Although music is not unfamiliar to people, our understanding of its essence
remains limited, and there is still no universally accepted scientific
description. This is primarily due to music being regarded as a product of both
reason and emotion, making it difficult to define. In this article, we focus on
the fundamental elements of music and construct an evolutionary network from
the perspective of music as a natural language, aligning with the statistical
characteristics of texts. Through this approach, we aim to comprehend the
structural differences in music across different periods, enabling a more
scientific exploration of music. Relying on the advantages of structuralism, we
can concentrate on the relationships and order between the physical elements of
music, rather than getting entangled in the blurred boundaries of science and
philosophy. The scientific framework we present not only conforms to past
conclusions in music, but also serves as a bridge that connects music to
natural language processing and knowledge graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10477v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10477v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Chen, Chunwei Wang, Kuo Yang, Jianhua Han, Lanqing Hong, Fei Mi, Hang Xu, Zhengying Liu, Wenyong Huang, Zhenguo Li, Dit-Yan Yeung, Lifeng Shang, Xin Jiang, Qun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of large language models (LLMs) has not only provided
numerous opportunities but also presented significant challenges. This becomes
particularly evident when LLMs inadvertently generate harmful or toxic content,
either unintentionally or because of intentional inducement. Existing alignment
methods usually direct LLMs toward the favorable outcomes by utilizing
human-annotated, flawless instruction-response pairs. Conversely, this study
proposes a novel alignment technique based on mistake analysis, which
deliberately exposes LLMs to erroneous content to learn the reasons for
mistakes and how to avoid them. In this case, mistakes are repurposed into
valuable data for alignment, effectively helping to avoid the production of
erroneous responses. Without external models or human annotations, our method
leverages a model's intrinsic ability to discern undesirable mistakes and
improves the safety of its generated responses. Experimental results reveal
that our method outperforms existing alignment approaches in enhancing model
safety while maintaining the overall utility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Search Model: Redefining Search Stack in the Era of LLMs <span class="chip">SIGIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14587v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14587v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern search engines are built on a stack of different components, including
query understanding, retrieval, multi-stage ranking, and question answering,
among others. These components are often optimized and deployed independently.
In this paper, we introduce a novel conceptual framework called large search
model, which redefines the conventional search stack by unifying search tasks
with one large language model (LLM). All tasks are formulated as autoregressive
text generation problems, allowing for the customization of tasks through the
use of natural language prompts. This proposed framework capitalizes on the
strong language understanding and reasoning capabilities of LLMs, offering the
potential to enhance search result quality while simultaneously simplifying the
existing cumbersome search stack. To substantiate the feasibility of this
framework, we present a series of proof-of-concept experiments and discuss the
potential challenges associated with implementing this approach within
real-world search systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGIR Forum, Vol. 57 No. 2 - December 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Study on the Calibration of In-context Learning <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04021v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04021v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanlin Zhang, Yi-Fan Zhang, Yaodong Yu, Dhruv Madeka, Dean Foster, Eric Xing, Himabindu Lakkaraju, Sham Kakade
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate uncertainty quantification is crucial for the safe deployment of
language models (LMs), and prior research has demonstrated improvements in the
calibration of modern LMs. Our study focuses on in-context learning (ICL), a
prevalent method for adapting static LMs through tailored prompts, and examines
the balance between performance and calibration across a broad spectrum of
natural language understanding and reasoning tasks. Through comprehensive
experiments, we observe that, with an increasing number of ICL examples, models
initially exhibit increased miscalibration before achieving better calibration
and miscalibration tends to arise in low-shot settings. Moreover, we find that
methods aimed at improving usability, such as fine-tuning and chain-of-thought
(CoT) prompting, can lead to miscalibration and unreliable natural language
explanations, suggesting that new methods may be required for scenarios where
models are expected to be reliable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Spotlight Talk at NeurIPS 2023 Workshop on Failure Modes in the Age
  of Foundation Models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reward-Augmented Decoding: Efficient Controlled Text Generation With a
  Unidirectional Reward Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09520v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09520v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haikang Deng, Colin Raffel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models have proven effective in a huge range of
downstream applications, they often generate text that is problematic or lacks
a desired attribute. In this paper, we introduce Reward-Augmented Decoding
(RAD), a text generation procedure that uses a small unidirectional reward
model to encourage a language model to generate text that has certain
properties. Specifically, RAD uses the reward model to score generations as
they are produced and rescales sampling probabilities to favor high-reward
tokens. By using a unidirectional reward model, RAD can cache activations from
prior generation steps to decrease computational overhead. Through experiments
on generating non-toxic and sentiment-controlled text, we demonstrate that RAD
performs best among methods that change only the generation procedure and
matches the performance of state-of-the-art methods that involve re-training
the language model. We further validate that RAD is effective on very large
language models while incurring a minimal computational overhead.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Assessment Tests are Unreliable Measures of LLM Personality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08163v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08163v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshat Gupta, Xiaoyang Song, Gopala Anumanchipalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLM) evolve in their capabilities, various recent
studies have tried to quantify their behavior using psychological tools created
to study human behavior. One such example is the measurement of "personality"
of LLMs using self-assessment personality tests developed to measure human
personality. Yet almost none of these works verify the applicability of these
tests on LLMs. In this paper, we analyze the reliability of LLM personality
scores obtained from self-assessment personality tests using two simple
experiments. We first introduce the property of prompt sensitivity, where three
semantically equivalent prompts representing three intuitive ways of
administering self-assessment tests on LLMs are used to measure the personality
of the same LLM. We find that all three prompts lead to very different
personality scores, a difference that is statistically significant for all
traits in a large majority of scenarios. We then introduce the property of
option-order symmetry for personality measurement of LLMs. Since most of the
self-assessment tests exist in the form of multiple choice question (MCQ)
questions, we argue that the scores should also be robust to not just the
prompt template but also the order in which the options are presented. This
test unsurprisingly reveals that the self-assessment test scores are not robust
to the order of the options. These simple tests, done on ChatGPT and three
Llama2 models of different sizes, show that self-assessment personality tests
created for humans are unreliable measures of personality in LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models Are Not Strong Abstract Reasoners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.19555v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.19555v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaël Gendron, Qiming Bao, Michael Witbrock, Gillian Dobbie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models have shown tremendous performance on a large variety of
natural language processing tasks, ranging from text comprehension to common
sense reasoning. However, the mechanisms responsible for this success remain
opaque, and it is unclear whether LLMs can achieve human-like cognitive
capabilities or whether these models are still fundamentally circumscribed.
Abstract reasoning is a fundamental task for cognition, consisting of finding
and applying a general pattern from few data. Evaluating deep neural
architectures on this task could give insight into their potential limitations
regarding reasoning and their broad generalisation abilities, yet this is
currently an under-explored area. In this paper, we introduce a new benchmark
for evaluating language models beyond memorization on abstract reasoning tasks.
We perform extensive evaluations of state-of-the-art LLMs, showing that they
currently achieve very limited performance in contrast with other natural
language tasks, even when applying techniques that have been shown to improve
performance on other NLP tasks. We argue that guiding LLM generation to follow
causal paths could help improve the generalisation and reasoning abilities of
LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>50 pages, 14 pages for the main paper and 36 pages for the
  supplement, 35 figures, 17 tables. V3: performed additional experiments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05134v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05134v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Grimal, Hervé Le Borgne, Olivier Ferret, Julien Tourille
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The progress in the generation of synthetic images has made it crucial to
assess their quality. While several metrics have been proposed to assess the
rendering of images, it is crucial for Text-to-Image (T2I) models, which
generate images based on a prompt, to consider additional aspects such as to
which extent the generated image matches the important content of the prompt.
Moreover, although the generated images usually result from a random starting
point, the influence of this one is generally not considered. In this article,
we propose a new metric based on prompt templates to study the alignment
between the content specified in the prompt and the corresponding generated
images. It allows us to better characterize the alignment in terms of the type
of the specified objects, their number, and their color. We conducted a study
on several recent T2I models about various aspects. An additional interesting
result we obtained with our approach is that image quality can vary drastically
depending on the noise used as a seed for the images. We also quantify the
influence of the number of concepts in the prompt, their order as well as their
(color) attributes. Finally, our method allows us to identify some seeds that
produce better images than others, opening novel directions of research on this
understudied topic.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">90</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Street Gaussians for Modeling Dynamic Urban Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunzhi Yan, Haotong Lin, Chenxu Zhou, Weijie Wang, Haiyang Sun, Kun Zhan, Xianpeng Lang, Xiaowei Zhou, Sida Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper aims to tackle the problem of modeling dynamic urban street scenes
from monocular videos. Recent methods extend NeRF by incorporating tracked
vehicle poses to animate vehicles, enabling photo-realistic view synthesis of
dynamic urban street scenes. However, significant limitations are their slow
training and rendering speed, coupled with the critical need for high precision
in tracked vehicle poses. We introduce Street Gaussians, a new explicit scene
representation that tackles all these limitations. Specifically, the dynamic
urban street is represented as a set of point clouds equipped with semantic
logits and 3D Gaussians, each associated with either a foreground vehicle or
the background. To model the dynamics of foreground object vehicles, each
object point cloud is optimized with optimizable tracked poses, along with a
dynamic spherical harmonics model for the dynamic appearance. The explicit
representation allows easy composition of object vehicles and background, which
in turn allows for scene editing operations and rendering at 133 FPS
(1066$\times$1600 resolution) within half an hour of training. The proposed
method is evaluated on multiple challenging benchmarks, including KITTI and
Waymo Open datasets. Experiments show that the proposed method consistently
outperforms state-of-the-art methods across all datasets. Furthermore, the
proposed representation delivers performance on par with that achieved using
precise ground-truth poses, despite relying only on poses from an off-the-shelf
tracker. The code is available at https://zju3dv.github.io/street_gaussians/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://zju3dv.github.io/street_gaussians/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrating Edges into U-Net Models with Explainable Activation Maps for
  Brain Tumor Segmentation using MR Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01303v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01303v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subin Sahayam, Umarani Jayaraman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manual delineation of tumor regions from magnetic resonance (MR) images is
time-consuming, requires an expert, and is prone to human error. In recent
years, deep learning models have been the go-to approach for the segmentation
of brain tumors. U-Net and its' variants for semantic segmentation of medical
images have achieved good results in the literature. However, U-Net and its'
variants tend to over-segment tumor regions and may not accurately segment the
tumor edges. The edges of the tumor are as important as the tumor regions for
accurate diagnosis, surgical precision, and treatment planning. In the proposed
work, the authors aim to extract edges from the ground truth using a
derivative-like filter followed by edge reconstruction to obtain an edge ground
truth in addition to the brain tumor ground truth. Utilizing both ground
truths, the author studies several U-Net and its' variant architectures with
and without tumor edges ground truth as a target along with the tumor ground
truth for brain tumor segmentation. The author used the BraTS2020 benchmark
dataset to perform the study and the results are tabulated for the dice and
Hausdorff95 metrics. The mean and median metrics are calculated for the whole
tumor (WT), tumor core (TC), and enhancing tumor (ET) regions. Compared to the
baseline U-Net and its variants, the models that learned edges along with the
tumor regions performed well in core tumor regions in both training and
validation datasets. The improved performance of edge-trained models trained on
baseline models like U-Net and V-Net achieved performance similar to baseline
state-of-the-art models like Swin U-Net and hybrid MR-U-Net. The edge-target
trained models are capable of generating edge maps that can be useful for
treatment planning. Additionally, for further explainability of the results,
the activation map generated by the hybrid MR-U-Net has been studied.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-informed Generalizable Wireless Channel Modeling with
  Segmentation and Deep Learning: Fundamentals, Methodologies, and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01288v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01288v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Zhu, Haijian Sun, Mingyue Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Channel modeling is fundamental in advancing wireless systems and has thus
attracted considerable research focus. Recent trends have seen a growing
reliance on data-driven techniques to facilitate the modeling process and yield
accurate channel predictions. In this work, we first provide a concise overview
of data-driven channel modeling methods, highlighting their limitations.
Subsequently, we introduce the concept and advantages of physics-informed
neural network (PINN)-based modeling and a summary of recent contributions in
this area. Our findings demonstrate that PINN-based approaches in channel
modeling exhibit promising attributes such as generalizability,
interpretability, and robustness. We offer a comprehensive architecture for
PINN methodology, designed to inform and inspire future model development. A
case-study of our recent work on precise indoor channel prediction with
semantic segmentation and deep learning is presented. The study concludes by
addressing the challenges faced and suggesting potential research directions in
this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Magazine for potential future publications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive Study of Knowledge Editing for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, Siyuan Cheng, Ziwen Xu, Xin Xu, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Lei Liang, Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown extraordinary capabilities in
understanding and generating text that closely mirrors human communication.
However, a primary limitation lies in the significant computational demands
during training, arising from their extensive parameterization. This challenge
is further intensified by the dynamic nature of the world, necessitating
frequent updates to LLMs to correct outdated information or integrate new
knowledge, thereby ensuring their continued relevance. Note that many
applications demand continual model adjustments post-training to address
deficiencies or undesirable behaviors. There is an increasing interest in
efficient, lightweight methods for on-the-fly model modifications. To this end,
recent years have seen a burgeoning in the techniques of knowledge editing for
LLMs, which aim to efficiently modify LLMs' behaviors within specific domains
while preserving overall performance across various inputs. In this paper, we
first define the knowledge editing problem and then provide a comprehensive
review of cutting-edge approaches. Drawing inspiration from educational and
cognitive research theories, we propose a unified categorization criterion that
classifies knowledge editing methods into three groups: resorting to external
knowledge, merging knowledge into the model, and editing intrinsic knowledge.
Furthermore, we introduce a new benchmark, KnowEdit, for a comprehensive
empirical evaluation of representative knowledge editing approaches.
Additionally, we provide an in-depth analysis of knowledge location, which can
provide a deeper understanding of the knowledge structures inherent within
LLMs. Finally, we discuss several potential applications of knowledge editing,
outlining its broad and impactful implications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing work; 50 pages, 265 citations; benchmark is available at
  https://huggingface.co/datasets/zjunlp/KnowEdit; code is available at
  https://github.com/zjunlp/EasyEdit; paper list is available at
  https://github.com/zjunlp/KnowledgeEditingPapers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MOC-RVQ: Multilevel Codebook-assisted Digital Generative Semantic
  Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01272v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01272v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingbin Zhou, Yaping Sun, Guanying Chen, Xiaodong Xu, Hao Chen, Binhong Huang, Shuguang Cui, Ping Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vector quantization-based image semantic communication systems have
successfully boosted transmission efficiency, but face a challenge with
conflicting requirements between codebook design and digital constellation
modulation. Traditional codebooks need a wide index range, while modulation
favors few discrete states. To address this, we propose a multilevel generative
semantic communication system with a two-stage training framework. In the first
stage, we train a high-quality codebook, using a multi-head octonary codebook
(MOC) to compress the index range. We also integrate a residual vector
quantization (RVQ) mechanism for effective multilevel communication. In the
second stage, a noise reduction block (NRB) based on Swin Transformer is
introduced, coupled with the multilevel codebook from the first stage, serving
as a high-quality semantic knowledge base (SKB) for generative feature
restoration. Experimental results highlight MOC-RVQ's superior performance over
methods like BPG or JPEG, even without channel error correction coding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VideoDrafter: Content-Consistent Multi-Scene Video Generation with LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuchen Long, Zhaofan Qiu, Ting Yao, Tao Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent innovations and breakthroughs in diffusion models have
significantly expanded the possibilities of generating high-quality videos for
the given prompts. Most existing works tackle the single-scene scenario with
only one video event occurring in a single background. Extending to generate
multi-scene videos nevertheless is not trivial and necessitates to nicely
manage the logic in between while preserving the consistent visual appearance
of key content across video scenes. In this paper, we propose a novel
framework, namely VideoDrafter, for content-consistent multi-scene video
generation. Technically, VideoDrafter leverages Large Language Models (LLM) to
convert the input prompt into comprehensive multi-scene script that benefits
from the logical knowledge learnt by LLM. The script for each scene includes a
prompt describing the event, the foreground/background entities, as well as
camera movement. VideoDrafter identifies the common entities throughout the
script and asks LLM to detail each entity. The resultant entity description is
then fed into a text-to-image model to generate a reference image for each
entity. Finally, VideoDrafter outputs a multi-scene video by generating each
scene video via a diffusion process that takes the reference images, the
descriptive prompt of the event and camera movement into account. The diffusion
model incorporates the reference images as the condition and alignment to
strengthen the content consistency of multi-scene videos. Extensive experiments
demonstrate that VideoDrafter outperforms the SOTA video generation models in
terms of visual quality, content consistency, and user preference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://videodrafter.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning-Based Computational Model for Disease Identification in
  Cocoa Pods (Theobroma cacao L.) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Darlyn Buenaño Vera, Byron Oviedo, Washington Chiriboga Casanova, Cristian Zambrano-Vega
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The early identification of diseases in cocoa pods is an important task to
guarantee the production of high-quality cocoa. The use of artificial
intelligence techniques such as machine learning, computer vision and deep
learning are promising solutions to help identify and classify diseases in
cocoa pods. In this paper we introduce the development and evaluation of a deep
learning computational model applied to the identification of diseases in cocoa
pods, focusing on "monilia" and "black pod" diseases. An exhaustive review of
state-of-the-art of computational models was carried out, based on scientific
articles related to the identification of plant diseases using computer vision
and deep learning techniques. As a result of the search, EfficientDet-Lite4, an
efficient and lightweight model for object detection, was selected. A dataset,
including images of both healthy and diseased cocoa pods, has been utilized to
train the model to detect and pinpoint disease manifestations with considerable
accuracy. Significant enhancements in the model training and evaluation
demonstrate the capability of recognizing and classifying diseases through
image analysis. Furthermore, the functionalities of the model were integrated
into an Android native mobile with an user-friendly interface, allowing to
younger or inexperienced farmers a fast and accuracy identification of health
status of cocoa pods
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal Adaptive RGBT Tracking with Modality <span class="highlight-title">Prompt</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01244v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01244v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyu Wang, Xiaotao Liu, Yifan Li, Meng Sun, Dian Yuan, Jing Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  RGBT tracking has been widely used in various fields such as robotics,
surveillance processing, and autonomous driving. Existing RGBT trackers fully
explore the spatial information between the template and the search region and
locate the target based on the appearance matching results. However, these RGBT
trackers have very limited exploitation of temporal information, either
ignoring temporal information or exploiting it through online sampling and
training. The former struggles to cope with the object state changes, while the
latter neglects the correlation between spatial and temporal information. To
alleviate these limitations, we propose a novel Temporal Adaptive RGBT Tracking
framework, named as TATrack. TATrack has a spatio-temporal two-stream structure
and captures temporal information by an online updated template, where the
two-stream structure refers to the multi-modal feature extraction and
cross-modal interaction for the initial template and the online update template
respectively. TATrack contributes to comprehensively exploit spatio-temporal
information and multi-modal information for target localization. In addition,
we design a spatio-temporal interaction (STI) mechanism that bridges two
branches and enables cross-modal interaction to span longer time scales.
Extensive experiments on three popular RGBT tracking benchmarks show that our
method achieves state-of-the-art performance, while running at real-time speed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IdentiFace : A VGG Based Multimodal Facial Biometric System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01227v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01227v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahmoud Rabea, Hanya Ahmed, Sohaila Mahmoud, Nourhan Sayed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of facial biometric systems has contributed greatly to the
development of the computer vision field. Nowadays, there's always a need to
develop a multimodal system that combines multiple biometric traits in an
efficient, meaningful way. In this paper, we introduce "IdentiFace" which is a
multimodal facial biometric system that combines the core of facial recognition
with some of the most important soft biometric traits such as gender, face
shape, and emotion. We also focused on developing the system using only VGG-16
inspired architecture with minor changes across different subsystems. This
unification allows for simpler integration across modalities. It makes it
easier to interpret the learned features between the tasks which gives a good
indication about the decision-making process across the facial modalities and
potential connection. For the recognition problem, we acquired a 99.2% test
accuracy for five classes with high intra-class variations using data collected
from the FERET database[1]. We achieved 99.4% on our dataset and 95.15% on the
public dataset[2] in the gender recognition problem. We were also able to
achieve a testing accuracy of 88.03% in the face-shape problem using the
celebrity face-shape dataset[3]. Finally, we achieved a decent testing accuracy
of 66.13% in the emotion task which is considered a very acceptable accuracy
compared to related work on the FER2013 dataset[4].
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 22 figures and 9 images</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distribution Matching for Multi-Task Learning of Classification Tasks: a
  Large-Scale Study on Faces & Beyond <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitrios Kollias, Viktoriia Sharmanska, Stefanos Zafeiriou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Task Learning (MTL) is a framework, where multiple related tasks are
learned jointly and benefit from a shared representation space, or parameter
transfer. To provide sufficient learning support, modern MTL uses annotated
data with full, or sufficiently large overlap across tasks, i.e., each input
sample is annotated for all, or most of the tasks. However, collecting such
annotations is prohibitive in many real applications, and cannot benefit from
datasets available for individual tasks. In this work, we challenge this setup
and show that MTL can be successful with classification tasks with little, or
non-overlapping annotations, or when there is big discrepancy in the size of
labeled data per task. We explore task-relatedness for co-annotation and
co-training, and propose a novel approach, where knowledge exchange is enabled
between the tasks via distribution matching. To demonstrate the general
applicability of our method, we conducted diverse case studies in the domains
of affective computing, face recognition, species recognition, and shopping
item classification using nine datasets. Our large-scale study of affective
tasks for basic expression recognition and facial action unit detection
illustrates that our approach is network agnostic and brings large performance
improvements compared to the state-of-the-art in both tasks and across all
studied databases. In all case studies, we show that co-training via
task-relatedness is advantageous and prevents negative transfer (which occurs
when MT model's performance is worse than that of at least one single-task
model).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at AAAI 2024. arXiv admin note: text overlap with
  arXiv:2105.03790</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Noise-NeRF: Hide Information in Neural Radiance Fields using Trainable
  Noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01216v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01216v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinglong Huang, Yong Liao, Yanbin Hao, Pengyuan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural radiance fields (NeRF) have been proposed as an innovative 3D
representation method. While attracting lots of attention, NeRF faces critical
issues such as information confidentiality and security. Steganography is a
technique used to embed information in another object as a means of protecting
information security. Currently, there are few related studies on NeRF
steganography, facing challenges in low steganography quality, model weight
damage, and a limited amount of steganographic information. This paper proposes
a novel NeRF steganography method based on trainable noise: Noise-NeRF.
Furthermore, we propose the Adaptive Pixel Selection strategy and Pixel
Perturbation strategy to improve the steganography quality and efficiency. The
extensive experiments on open-source datasets show that Noise-NeRF provides
state-of-the-art performances in both steganography quality and rendering
quality, as well as effectiveness in super-resolution image steganography.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ YOLO algorithm with hybrid attention feature pyramid network for solder
  joint defect detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Ang, Siti Khatijah Nor Abdul Rahim, Raseeda Hamzah, Raihah Aminuddin, Gao Yousheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional manual detection for solder joint defect is no longer applied
during industrial production due to low efficiency, inconsistent evaluation,
high cost and lack of real-time data. A new approach has been proposed to
address the issues of low accuracy, high false detection rates and
computational cost of solder joint defect detection in surface mount technology
of industrial scenarios. The proposed solution is a hybrid attention mechanism
designed specifically for the solder joint defect detection algorithm to
improve quality control in the manufacturing process by increasing the accuracy
while reducing the computational cost. The hybrid attention mechanism comprises
a proposed enhanced multi-head self-attention and coordinate attention
mechanisms increase the ability of attention networks to perceive contextual
information and enhances the utilization range of network features. The
coordinate attention mechanism enhances the connection between different
channels and reduces location information loss. The hybrid attention mechanism
enhances the capability of the network to perceive long-distance position
information and learn local features. The improved algorithm model has good
detection ability for solder joint defect detection, with mAP reaching 91.5%,
4.3% higher than the You Only Look Once version 5 algorithm and better than
other comparative algorithms. Compared to other versions, mean Average
Precision, Precision, Recall, and Frame per Seconds indicators have also
improved. The improvement of detection accuracy can be achieved while meeting
real-time detection requirements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FGENet: Fine-Grained Extraction Network for Congested Crowd Counting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao-Yuan Ma, Li Zhang, Xiang-Yi Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crowd counting has gained significant popularity due to its practical
applications. However, mainstream counting methods ignore precise individual
localization and suffer from annotation noise because of counting from
estimating density maps. Additionally, they also struggle with high-density
images.To address these issues, we propose an end-to-end model called
Fine-Grained Extraction Network (FGENet). Different from methods estimating
density maps, FGENet directly learns the original coordinate points that
represent the precise localization of individuals.This study designs a fusion
module, named Fine-Grained Feature Pyramid(FGFP), that is used to fuse feature
maps extracted by the backbone of FGENet. The fused features are then passed to
both regression and classification heads, where the former provides predicted
point coordinates for a given image, and the latter determines the confidence
level for each predicted point being an individual. At the end, FGENet
establishes correspondences between prediction points and ground truth points
by employing the Hungarian algorithm. For training FGENet, we design a robust
loss function, named Three-Task Combination (TTC), to mitigate the impact of
annotation noise. Extensive experiments are conducted on four widely used crowd
counting datasets. Experimental results demonstrate the effectiveness of
FGENet. Notably, our method achieves a remarkable improvement of 3.14 points in
Mean Absolute Error (MAE) on the ShanghaiTech Part A dataset, showcasing its
superiority over the existing state-of-the-art methods. Even more impressively,
FGENet surpasses previous benchmarks on the UCF\_CC\_50 dataset with an
astounding enhancement of 30.16 points in MAE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 30th International Conference on MultiMedia Modeling</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Simultaneous and Granular Identity-Expression Control in
  Personalized Face Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01207v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01207v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renshuai Liu, Bowen Ma, Wei Zhang, Zhipeng Hu, Changjie Fan, Tangjie Lv, Yu Ding, Xuan Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In human-centric content generation, the pre-trained text-to-image models
struggle to produce user-wanted portrait images, which retain the identity of
individuals while exhibiting diverse expressions. This paper introduces our
efforts towards personalized face generation. To this end, we propose a novel
multi-modal face generation framework, capable of simultaneous
identity-expression control and more fine-grained expression synthesis. Our
expression control is so sophisticated that it can be specialized by the
fine-grained emotional vocabulary. We devise a novel diffusion model that can
undertake the task of simultaneously face swapping and reenactment. Due to the
entanglement of identity and expression, it's nontrivial to separately and
precisely control them in one framework, thus has not been explored yet. To
overcome this, we propose several innovative designs in the conditional
diffusion model, including balancing identity and expression encoder, improved
midpoint sampling, and explicitly background conditioning. Extensive
experiments have demonstrated the controllability and scalability of the
proposed framework, in comparison with state-of-the-art text-to-image, face
swapping, and face reenactment methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Whole-examination AI estimation of fetal biometrics from 20-week
  ultrasound scans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01201v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01201v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Venturini, Samuel Budd, Alfonso Farruggia, Robert Wright, Jacqueline Matthew, Thomas G. Day, Bernhard Kainz, Reza Razavi, Jo V. Hajnal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current approach to fetal anomaly screening is based on biometric
measurements derived from individually selected ultrasound images. In this
paper, we introduce a paradigm shift that attains human-level performance in
biometric measurement by aggregating automatically extracted biometrics from
every frame across an entire scan, with no need for operator intervention. We
use a convolutional neural network to classify each frame of an ultrasound
video recording. We then measure fetal biometrics in every frame where
appropriate anatomy is visible. We use a Bayesian method to estimate the true
value of each biometric from a large number of measurements and
probabilistically reject outliers. We performed a retrospective experiment on
1457 recordings (comprising 48 million frames) of 20-week ultrasound scans,
estimated fetal biometrics in those scans and compared our estimates to the
measurements sonographers took during the scan. Our method achieves human-level
performance in estimating fetal biometrics and estimates well-calibrated
credible intervals in which the true biometric value is expected to lie.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 16 figures. Submitted to NPJ digital medicine. For
  associated video file, see
  http://wp.doc.ic.ac.uk/ifind/wp-content/uploads/sites/79/2023/12/realtime.gif</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Skin cancer diagnosis using NIR spectroscopy data of skin lesions in
  vivo using machine learning algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Flavio P. Loss, Pedro H. da Cunha, Matheus B. Rocha, Madson Poltronieri Zanoni, Leandro M. de Lima, Isadora Tavares Nascimento, Isabella Rezende, Tania R. P. Canuto, Luciana de Paula Vieira, Renan Rossoni, Maria C. S. Santos, Patricia Lyra Frasson, Wanderson Romão, Paulo R. Filgueiras, Renato A. Krohling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skin lesions are classified in benign or malignant. Among the malignant,
melanoma is a very aggressive cancer and the major cause of deaths. So, early
diagnosis of skin cancer is very desired. In the last few years, there is a
growing interest in computer aided diagnostic (CAD) using most image and
clinical data of the lesion. These sources of information present limitations
due to their inability to provide information of the molecular structure of the
lesion. NIR spectroscopy may provide an alternative source of information to
automated CAD of skin lesions. The most commonly used techniques and
classification algorithms used in spectroscopy are Principal Component Analysis
(PCA), Partial Least Squares - Discriminant Analysis (PLS-DA), and Support
Vector Machines (SVM). Nonetheless, there is a growing interest in applying the
modern techniques of machine and deep learning (MDL) to spectroscopy. One of
the main limitations to apply MDL to spectroscopy is the lack of public
datasets. Since there is no public dataset of NIR spectral data to skin
lesions, as far as we know, an effort has been made and a new dataset named
NIR-SC-UFES, has been collected, annotated and analyzed generating the
gold-standard for classification of NIR spectral data to skin cancer. Next, the
machine learning algorithms XGBoost, CatBoost, LightGBM, 1D-convolutional
neural network (1D-CNN) were investigated to classify cancer and non-cancer
skin lesions. Experimental results indicate the best performance obtained by
LightGBM with pre-processing using standard normal variate (SNV), feature
extraction providing values of 0.839 for balanced accuracy, 0.851 for recall,
0.852 for precision, and 0.850 for F-score. The obtained results indicate the
first steps in CAD of skin lesions aiming the automated triage of patients with
skin lesions in vivo using NIR spectral data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JMA: a General Algorithm to Craft Nearly Optimal Targeted Adversarial
  Example 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01199v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01199v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benedetta Tondi, Wei Guo, Mauro Barni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most of the approaches proposed so far to craft targeted adversarial examples
against Deep Learning classifiers are highly suboptimal and typically rely on
increasing the likelihood of the target class, thus implicitly focusing on
one-hot encoding settings. In this paper, we propose a more general,
theoretically sound, targeted attack that resorts to the minimization of a
Jacobian-induced MAhalanobis distance (JMA) term, taking into account the
effort (in the input space) required to move the latent space representation of
the input sample in a given direction. The minimization is solved by exploiting
the Wolfe duality theorem, reducing the problem to the solution of a
Non-Negative Least Square (NNLS) problem. The proposed algorithm provides an
optimal solution to a linearized version of the adversarial example problem
originally introduced by Szegedy et al. \cite{szegedy2013intriguing}. The
experiments we carried out confirm the generality of the proposed attack which
is proven to be effective under a wide variety of output encoding schemes.
Noticeably, the JMA attack is also effective in a multi-label classification
scenario, being capable to induce a targeted modification of up to half the
labels in a complex multilabel classification scenario with 20 labels, a
capability that is out of reach of all the attacks proposed so far. As a
further advantage, the JMA attack usually requires very few iterations, thus
resulting more efficient than existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Query-Based Knowledge Sharing for Open-Vocabulary Multi-Label
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01181v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01181v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuelin Zhu, Jian Liu, Dongqi Tang, Jiawei Ge, Weijia Liu, Bo Liu, Jiuxin Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying labels that did not appear during training, known as multi-label
zero-shot learning, is a non-trivial task in computer vision. To this end,
recent studies have attempted to explore the multi-modal knowledge of
vision-language pre-training (VLP) models by knowledge distillation, allowing
to recognize unseen labels in an open-vocabulary manner. However, experimental
evidence shows that knowledge distillation is suboptimal and provides limited
performance gain in unseen label prediction. In this paper, a novel query-based
knowledge sharing paradigm is proposed to explore the multi-modal knowledge
from the pretrained VLP model for open-vocabulary multi-label classification.
Specifically, a set of learnable label-agnostic query tokens is trained to
extract critical vision knowledge from the input image, and further shared
across all labels, allowing them to select tokens of interest as visual clues
for recognition. Besides, we propose an effective prompt pool for robust label
embedding, and reformulate the standard ranking learning into a form of
classification to allow the magnitude of feature vectors for matching, which
both significantly benefit label recognition. Experimental results show that
our framework significantly outperforms state-of-the-art methods on zero-shot
task by 5.9% and 4.5% in mAP on the NUS-WIDE and Open Images, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accurate and Efficient Urban Street Tree Inventory with Deep Learning on
  Mobile Phone Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01180v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01180v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Asim Khan, Umair Nawaz, Anwaar Ulhaq, Iqbal Gondal, Sajid Javed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deforestation, a major contributor to climate change, poses detrimental
consequences such as agricultural sector disruption, global warming, flash
floods, and landslides. Conventional approaches to urban street tree inventory
suffer from inaccuracies and necessitate specialised equipment. To overcome
these challenges, this paper proposes an innovative method that leverages deep
learning techniques and mobile phone imaging for urban street tree inventory.
Our approach utilises a pair of images captured by smartphone cameras to
accurately segment tree trunks and compute the diameter at breast height (DBH).
Compared to traditional methods, our approach exhibits several advantages,
including superior accuracy, reduced dependency on specialised equipment, and
applicability in hard-to-reach areas. We evaluated our method on a
comprehensive dataset of 400 trees and achieved a DBH estimation accuracy with
an error rate of less than 2.5%. Our method holds significant potential for
substantially improving forest management practices. By enhancing the accuracy
and efficiency of tree inventory, our model empowers urban management to
mitigate the adverse effects of deforestation and climate change.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 Pages, 7 figures and 5 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Freeze the backbones: A Parameter-Efficient Contrastive Approach to
  Robust Medical Vision-Language <span class="highlight-title">Pre-train</span>ing <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiuming Qin, Che Liu, Sibo Cheng, Yike Guo, Rossella Arcucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern healthcare often utilises radiographic images alongside textual
reports for diagnostics, encouraging the use of Vision-Language Self-Supervised
Learning (VL-SSL) with large pre-trained models to learn versatile medical
vision representations. However, most existing VL-SSL frameworks are trained
end-to-end, which is computation-heavy and can lose vital prior information
embedded in pre-trained encoders. To address both issues, we introduce the
backbone-agnostic Adaptor framework, which preserves medical knowledge in
pre-trained image and text encoders by keeping them frozen, and employs a
lightweight Adaptor module for cross-modal learning. Experiments on medical
image classification and segmentation tasks across three datasets reveal that
our framework delivers competitive performance while cutting trainable
parameters by over 90% compared to current pre-training approaches. Notably,
when fine-tuned with just 1% of data, Adaptor outperforms several
Transformer-based methods trained on full datasets in medical image
segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GBSS:a global building semantic segmentation <span class="highlight-title">dataset</span> for large-scale
  remote sensing building extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuping Hu, Xin Huang, Jiayi Li, Zhen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation techniques for extracting building footprints from
high-resolution remote sensing images have been widely used in many fields such
as urban planning. However, large-scale building extraction demands higher
diversity in training samples. In this paper, we construct a Global Building
Semantic Segmentation (GBSS) dataset (The dataset will be released), which
comprises 116.9k pairs of samples (about 742k buildings) from six continents.
There are significant variations of building samples in terms of size and
style, so the dataset can be a more challenging benchmark for evaluating the
generalization and robustness of building semantic segmentation models. We
validated through quantitative and qualitative comparisons between different
datasets, and further confirmed the potential application in the field of
transfer learning by conducting experiments on subsets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages,6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Surface Scattering Parameters From SAR Images Using
  Differentiable Ray Tracing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01175v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01175v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiangtao Wei, Yixiang Luomei, Xu Zhang, Feng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulating high-resolution Synthetic Aperture Radar (SAR) images in complex
scenes has consistently presented a significant research challenge. The
development of a microwave-domain surface scattering model and its
reversibility are poised to play a pivotal role in enhancing the authenticity
of SAR image simulations and facilitating the reconstruction of target
parameters. Drawing inspiration from the field of computer graphics, this paper
proposes a surface microwave rendering model that comprehensively considers
both Specular and Diffuse contributions. The model is analytically represented
by the coherent spatially varying bidirectional scattering distribution
function (CSVBSDF) based on the Kirchhoff approximation (KA) and the
perturbation method (SPM). And SAR imaging is achieved through the synergistic
combination of ray tracing and fast mapping projection techniques. Furthermore,
a differentiable ray tracing (DRT) engine based on SAR images was constructed
for CSVBSDF surface scattering parameter learning. Within this SAR image
simulation engine, the use of differentiable reverse ray tracing enables the
rapid estimation of parameter gradients from SAR images. The effectiveness of
this approach has been validated through simulations and comparisons with real
SAR images. By learning the surface scattering parameters, substantial
enhancements in SAR image simulation performance under various observation
conditions have been demonstrated.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ En3D: An Enhanced Generative Model for Sculpting 3D Humans from 2D
  Synthetic Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01173v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01173v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifang Men, Biwen Lei, Yuan Yao, Miaomiao Cui, Zhouhui Lian, Xuansong Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present En3D, an enhanced generative scheme for sculpting high-quality 3D
human avatars. Unlike previous works that rely on scarce 3D datasets or limited
2D collections with imbalanced viewing angles and imprecise pose priors, our
approach aims to develop a zero-shot 3D generative scheme capable of producing
visually realistic, geometrically accurate and content-wise diverse 3D humans
without relying on pre-existing 3D or 2D assets. To address this challenge, we
introduce a meticulously crafted workflow that implements accurate physical
modeling to learn the enhanced 3D generative model from synthetic 2D data.
During inference, we integrate optimization modules to bridge the gap between
realistic appearances and coarse 3D shapes. Specifically, En3D comprises three
modules: a 3D generator that accurately models generalizable 3D humans with
realistic appearance from synthesized balanced, diverse, and structured human
images; a geometry sculptor that enhances shape quality using multi-view normal
constraints for intricate human anatomy; and a texturing module that
disentangles explicit texture maps with fidelity and editability, leveraging
semantical UV partitioning and a differentiable rasterizer. Experimental
results show that our approach significantly outperforms prior works in terms
of image quality, geometry accuracy and content diversity. We also showcase the
applicability of our generated avatars for animation and editing, as well as
the scalability of our approach for content-style free adaptation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://menyifang.github.io/projects/En3D/index.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distilling Local Texture Features for Colorectal Tissue Classification
  in Low Data Regimes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dmitry Demidov, Roba Al Majzoub, Amandeep Kumar, Fahad Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-class colorectal tissue classification is a challenging problem that is
typically addressed in a setting, where it is assumed that ample amounts of
training data is available. However, manual annotation of fine-grained
colorectal tissue samples of multiple classes, especially the rare ones like
stromal tumor and anal cancer is laborious and expensive. To address this, we
propose a knowledge distillation-based approach, named KD-CTCNet, that
effectively captures local texture information from few tissue samples, through
a distillation loss, to improve the standard CNN features. The resulting
enriched feature representation achieves improved classification performance
specifically in low data regimes. Extensive experiments on two public datasets
of colorectal tissues reveal the merits of the proposed contributions, with a
consistent gain achieved over different approaches across low data settings.
The code and models are publicly available on GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NU-Class Net: A Novel Deep Learning-based Approach for Video Quality
  Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parham Zilouchian Moghaddam, Mehdi Modarressi, MohammadAmin Sadeghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video content has experienced a surge in popularity, asserting its dominance
over internet traffic and Internet of Things (IoT) networks. Video compression
has long been regarded as the primary means of efficiently managing the
substantial multimedia traffic generated by video-capturing devices.
Nevertheless, video compression algorithms entail significant computational
demands in order to achieve substantial compression ratios. This complexity
presents a formidable challenge when implementing efficient video coding
standards in resource-constrained embedded systems, such as IoT edge node
cameras. To tackle this challenge, this paper introduces NU-Class Net, an
innovative deep-learning model designed to mitigate compression artifacts
stemming from lossy compression codecs. This enhancement significantly elevates
the perceptible quality of low-bit-rate videos. By employing the NU-Class Net,
the video encoder within the video-capturing node can reduce output quality,
thereby generating low-bit-rate videos and effectively curtailing both
computation and bandwidth requirements at the edge. On the decoder side, which
is typically less encumbered by resource limitations, NU-Class Net is applied
after the video decoder to compensate for artifacts and approximate the quality
of the original video. Experimental results affirm the efficacy of the proposed
model in enhancing the perceptible quality of videos, especially those streamed
at low bit rates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Train-Free Segmentation in MRI with Cubical Persistent Homology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anton François, Raphaël Tinarrage
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe a new general method for segmentation in MRI scans using
Topological Data Analysis (TDA), offering several advantages over traditional
machine learning approaches. It works in three steps, first identifying the
whole object to segment via automatic thresholding, then detecting a
distinctive subset whose topology is known in advance, and finally deducing the
various components of the segmentation. Although convoking classical ideas of
TDA, such an algorithm has never been proposed separately from deep learning
methods. To achieve this, our approach takes into account, in addition to the
homology of the image, the localization of representative cycles, a piece of
information that seems never to have been exploited in this context. In
particular, it offers the ability to perform segmentation without the need for
large annotated data sets. TDA also provides a more interpretable and stable
framework for segmentation by explicitly mapping topological features to
segmentation components. By adapting the geometric object to be detected, the
algorithm can be adjusted to a wide range of data segmentation challenges. We
carefully study the examples of glioblastoma segmentation in brain MRI, where a
sphere is to be detected, as well as myocardium in cardiac MRI, involving a
cylinder, and cortical plate detection in fetal brain MRI, whose 2D slices are
circles. We compare our method to state-of-the-art algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint, 17 pages, 19 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid Pooling and Convolutional Network for Improving Accuracy and
  Training Convergence Speed in Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01134v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01134v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiwen Zhao, Wei Wang, Junhui Hou, Hai Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces HPC-Net, a high-precision and rapidly convergent object
detection network.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages,5 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Generative Modeling of Scene Graphs and Images via Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bicheng Xu, Qi Yan, Renjie Liao, Lele Wang, Leonid Sigal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a novel generative task: joint scene graph - image
generation. While previous works have explored image generation conditioned on
scene graphs or layouts, our task is distinctive and important as it involves
generating scene graphs themselves unconditionally from noise, enabling
efficient and interpretable control for image generation. Our task is
challenging, requiring the generation of plausible scene graphs with
heterogeneous attributes for nodes (objects) and edges (relations among
objects), including continuous object bounding boxes and discrete object and
relation categories. We introduce a novel diffusion model, DiffuseSG, that
jointly models the adjacency matrix along with heterogeneous node and edge
attributes. We explore various types of encodings for the categorical data,
relaxing it into a continuous space. With a graph transformer being the
denoiser, DiffuseSG successively denoises the scene graph representation in a
continuous space and discretizes the final representation to generate the clean
scene graph. Additionally, we introduce an IoU regularization to enhance the
empirical performance. Our model significantly outperforms existing methods in
scene graph generation on the Visual Genome and COCO-Stuff datasets, both on
standard and newly introduced metrics that better capture the problem
complexity. Moreover, we demonstrate the additional benefits of our model in
two downstream applications: 1) excelling in a series of scene graph completion
tasks, and 2) improving scene graph detection models by using extra training
samples generated from DiffuseSG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SSP: A Simple and Safe automatic <span class="highlight-title">Prompt</span> engineering method towards
  realistic image synthesis on LVM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01128v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01128v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijin Cheng, Jianzhi Liu, Jiawen Deng, Fuji Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, text-to-image (T2I) synthesis has undergone significant
advancements, particularly with the emergence of Large Language Models (LLM)
and their enhancement in Large Vision Models (LVM), greatly enhancing the
instruction-following capabilities of traditional T2I models. Nevertheless,
previous methods focus on improving generation quality but introduce unsafe
factors into prompts. We explore that appending specific camera descriptions to
prompts can enhance safety performance. Consequently, we propose a simple and
safe prompt engineering method (SSP) to improve image generation quality by
providing optimal camera descriptions. Specifically, we create a dataset from
multi-datasets as original prompts. To select the optimal camera, we design an
optimal camera matching approach and implement a classifier for original
prompts capable of automatically matching. Appending camera descriptions to
original prompts generates optimized prompts for further LVM image generation.
Experiments demonstrate that SSP improves semantic consistency by an average of
16% compared to others and safety metrics by 48.9%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Q-Refine: A Perceptual Quality Refiner for AI-Generated Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunyi Li, Haoning Wu, Zicheng Zhang, Hongkun Hao, Kaiwei Zhang, Lei Bai, Xiaohong Liu, Xiongkuo Min, Weisi Lin, Guangtao Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid evolution of the Text-to-Image (T2I) model in recent years,
their unsatisfactory generation result has become a challenge. However,
uniformly refining AI-Generated Images (AIGIs) of different qualities not only
limited optimization capabilities for low-quality AIGIs but also brought
negative optimization to high-quality AIGIs. To address this issue, a
quality-award refiner named Q-Refine is proposed. Based on the preference of
the Human Visual System (HVS), Q-Refine uses the Image Quality Assessment (IQA)
metric to guide the refining process for the first time, and modify images of
different qualities through three adaptive pipelines. Experimental shows that
for mainstream T2I models, Q-Refine can perform effective optimization to AIGIs
of different qualities. It can be a general refiner to optimize AIGIs from both
fidelity and aesthetic quality levels, thus expanding the application of the
T2I generation models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CityPulse: Fine-Grained Assessment of Urban Change with Street View Time
  Series <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01107v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01107v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyuan Huang, Zejia Wu, Jiajun Wu, Jackelyn Hwang, Ram Rajagopal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Urban transformations have profound societal impact on both individuals and
communities at large. Accurately assessing these shifts is essential for
understanding their underlying causes and ensuring sustainable urban planning.
Traditional measurements often encounter constraints in spatial and temporal
granularity, failing to capture real-time physical changes. While street view
imagery, capturing the heartbeat of urban spaces from a pedestrian point of
view, can add as a high-definition, up-to-date, and on-the-ground visual proxy
of urban change. We curate the largest street view time series dataset to date,
and propose an end-to-end change detection model to effectively capture
physical alterations in the built environment at scale. We demonstrate the
effectiveness of our proposed method by benchmark comparisons with previous
literature and implementing it at the city-wide level. Our approach has the
potential to supplement existing dataset and serve as a fine-grained and
accurate assessment of urban change.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual Teacher Knowledge Distillation with Domain Alignment for Face
  Anti-spoofing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01102v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01102v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Kong, Wentian Zhang, Tao Wang, Kaihao Zhang, Yuexiang Li, Xiaoying Tang, Wenhan Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face recognition systems have raised concerns due to their vulnerability to
different presentation attacks, and system security has become an increasingly
critical concern. Although many face anti-spoofing (FAS) methods perform well
in intra-dataset scenarios, their generalization remains a challenge. To
address this issue, some methods adopt domain adversarial training (DAT) to
extract domain-invariant features. However, the competition between the encoder
and the domain discriminator can cause the network to be difficult to train and
converge. In this paper, we propose a domain adversarial attack (DAA) method to
mitigate the training instability problem by adding perturbations to the input
images, which makes them indistinguishable across domains and enables domain
alignment. Moreover, since models trained on limited data and types of attacks
cannot generalize well to unknown attacks, we propose a dual perceptual and
generative knowledge distillation framework for face anti-spoofing that
utilizes pre-trained face-related models containing rich face priors.
Specifically, we adopt two different face-related models as teachers to
transfer knowledge to the target student model. The pre-trained teacher models
are not from the task of face anti-spoofing but from perceptual and generative
tasks, respectively, which implicitly augment the data. By combining both DAA
and dual-teacher knowledge distillation, we develop a dual teacher knowledge
distillation with domain alignment framework (DTDA) for face anti-spoofing. The
advantage of our proposed method has been verified through extensive ablation
studies and comparison with state-of-the-art methods on public datasets across
multiple protocols.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust single-particle cryo-EM image denoising and restoration <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01097v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01097v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Zhang, Tengfei Zhao, ShiYu Hu, Xin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cryo-electron microscopy (cryo-EM) has achieved near-atomic level resolution
of biomolecules by reconstructing 2D micrographs. However, the resolution and
accuracy of the reconstructed particles are significantly reduced due to the
extremely low signal-to-noise ratio (SNR) and complex noise structure of
cryo-EM images. In this paper, we introduce a diffusion model with
post-processing framework to effectively denoise and restore single particle
cryo-EM images. Our method outperforms the state-of-the-art (SOTA) denoising
methods by effectively removing structural noise that has not been addressed
before. Additionally, more accurate and high-resolution three-dimensional
reconstruction structures can be obtained from denoised cryo-EM images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted to ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Hyperspectral Anomaly Detection with Human Vision: A Small
  Target Aware Detector 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jitao Ma, Weiying Xie, Yunsong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral anomaly detection (HAD) aims to localize pixel points whose
spectral features differ from the background. HAD is essential in scenarios of
unknown or camouflaged target features, such as water quality monitoring, crop
growth monitoring and camouflaged target detection, where prior information of
targets is difficult to obtain. Existing HAD methods aim to objectively detect
and distinguish background and anomalous spectra, which can be achieved almost
effortlessly by human perception. However, the underlying processes of human
visual perception are thought to be quite complex. In this paper, we analyze
hyperspectral image (HSI) features under human visual perception, and transfer
the solution process of HAD to the more robust feature space for the first
time. Specifically, we propose a small target aware detector (STAD), which
introduces saliency maps to capture HSI features closer to human visual
perception. STAD not only extracts more anomalous representations, but also
reduces the impact of low-confidence regions through a proposed small target
filter (STF). Furthermore, considering the possibility of HAD algorithms being
applied to edge devices, we propose a full connected network to convolutional
network knowledge distillation strategy. It can learn the spectral and spatial
features of the HSI while lightening the network. We train the network on the
HAD100 training set and validate the proposed method on the HAD100 test set.
Our method provides a new solution space for HAD that is closer to human visual
perception with high confidence. Sufficient experiments on real HSI with
multiple method comparisons demonstrate the excellent performance and unique
potential of the proposed method. The code is available at
https://github.com/majitao-xd/STAD-HAD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Depth-discriminative Metric Learning for Monocular 3D Object Detection <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01075v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01075v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonhyeok Choi, Mingyu Shin, Sunghoon Im
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular 3D object detection poses a significant challenge due to the lack
of depth information in RGB images. Many existing methods strive to enhance the
object depth estimation performance by allocating additional parameters for
object depth estimation, utilizing extra modules or data. In contrast, we
introduce a novel metric learning scheme that encourages the model to extract
depth-discriminative features regardless of the visual attributes without
increasing inference time and model size. Our method employs the
distance-preserving function to organize the feature space manifold in relation
to ground-truth object depth. The proposed (K, B, eps)-quasi-isometric loss
leverages predetermined pairwise distance restriction as guidance for adjusting
the distance among object descriptors without disrupting the non-linearity of
the natural feature manifold. Moreover, we introduce an auxiliary head for
object-wise depth estimation, which enhances depth quality while maintaining
the inference time. The broad applicability of our method is demonstrated
through experiments that show improvements in overall performance when
integrated into various baselines. The results show that our method
consistently improves the performance of various baselines by 23.51% and 5.78%
on average across KITTI and Waymo, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AliFuse: Aligning and Fusing Multi-modal Medical Data for Computer-Aided
  Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiuhui Chen, Xinyue Hu, Zirui Wang, Yi Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical data collected for making a diagnostic decision are typically
multi-modal and provide complementary perspectives of a subject. A
computer-aided diagnosis system welcomes multi-modal inputs; however, how to
effectively fuse such multi-modal data is a challenging task and attracts a lot
of attention in the medical research field. In this paper, we propose a
transformer-based framework, called Alifuse, for aligning and fusing
multi-modal medical data. Specifically, we convert images and unstructured and
structured texts into vision and language tokens, and use intramodal and
intermodal attention mechanisms to learn holistic representations of all
imaging and non-imaging data for classification. We apply Alifuse to classify
Alzheimer's disease and obtain state-of-the-art performance on five public
datasets, by outperforming eight baselines. The source code will be available
online later.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DTBS: Dual-Teacher Bi-directional Self-training for Domain Adaptation in
  Nighttime Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01066v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01066v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fanding Huang, Zihao Yao, Wenhui Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the poor illumination and the difficulty in annotating, nighttime
conditions pose a significant challenge for autonomous vehicle perception
systems. Unsupervised domain adaptation (UDA) has been widely applied to
semantic segmentation on such images to adapt models from normal conditions to
target nighttime-condition domains. Self-training (ST) is a paradigm in UDA,
where a momentum teacher is utilized for pseudo-label prediction, but a
confirmation bias issue exists. Because the one-directional knowledge transfer
from a single teacher is insufficient to adapt to a large domain shift. To
mitigate this issue, we propose to alleviate domain gap by incrementally
considering style influence and illumination change. Therefore, we introduce a
one-stage Dual-Teacher Bi-directional Self-training (DTBS) framework for smooth
knowledge transfer and feedback. Based on two teacher models, we present a
novel pipeline to respectively decouple style and illumination shift. In
addition, we propose a new Re-weight exponential moving average (EMA) to merge
the knowledge of style and illumination factors, and provide feedback to the
student model. In this way, our method can be embedded in other UDA methods to
enhance their performance. For example, the Cityscapes to ACDC night task
yielded 53.8 mIoU (\%), which corresponds to an improvement of +5\% over the
previous state-of-the-art. The code is available at
\url{https://github.com/hf618/DTBS}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BEV-CLIP: Multi-modal BEV Retrieval Methodology for Complex Scene in
  Autonomous Driving <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01065v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01065v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dafeng Wei, Tian Gao, Zhengyu Jia, Changwei Cai, Chengkai Hou, Peng Jia, Fu Liu, Kun Zhan, Jingchen Fan, Yixing Zhao, Yang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The demand for the retrieval of complex scene data in autonomous driving is
increasing, especially as passenger vehicles have been equipped with the
ability to navigate urban settings, with the imperative to address long-tail
scenarios. Meanwhile, under the pre-existing two dimensional image retrieval
method, some problems may arise with scene retrieval, such as lack of global
feature representation and subpar text retrieval ability. To address these
issues, we have proposed \textbf{BEV-CLIP}, the first multimodal Bird's-Eye
View(BEV) retrieval methodology that utilizes descriptive text as an input to
retrieve corresponding scenes. This methodology applies the semantic feature
extraction abilities of a large language model (LLM) to facilitate zero-shot
retrieval of extensive text descriptions, and incorporates semi-structured
information from a knowledge graph to improve the semantic richness and variety
of the language embedding. Our experiments result in 87.66% accuracy on
NuScenes dataset in text-to-BEV feature retrieval. The demonstrated cases in
our paper support that our retrieval method is also indicated to be effective
in identifying certain long-tail corner scenes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review of CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Relating Events and Frames Based on <span class="highlight-title">Self-Supervised</span> Learning and
  Uncorrelated Conditioning for Unsupervised Domain Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01042v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01042v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Rostami, Dayuan Jian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event-based cameras provide accurate and high temporal resolution
measurements for performing computer vision tasks in challenging scenarios,
such as high-dynamic range environments and fast-motion maneuvers. Despite
their advantages, utilizing deep learning for event-based vision encounters a
significant obstacle due to the scarcity of annotated data caused by the
relatively recent emergence of event-based cameras. To overcome this
limitation, leveraging the knowledge available from annotated data obtained
with conventional frame-based cameras presents an effective solution based on
unsupervised domain adaptation. We propose a new algorithm tailored for
adapting a deep neural network trained on annotated frame-based data to
generalize well on event-based unannotated data. Our approach incorporates
uncorrelated conditioning and self-supervised learning in an adversarial
learning scheme to close the gap between the two source and target domains. By
applying self-supervised learning, the algorithm learns to align the
representations of event-based data with those from frame-based camera data,
thereby facilitating knowledge transfer.Furthermore, the inclusion of
uncorrelated conditioning ensures that the adapted model effectively
distinguishes between event-based and conventional data, enhancing its ability
to classify event-based images accurately.Through empirical experimentation and
evaluation, we demonstrate that our algorithm surpasses existing approaches
designed for the same purpose using two benchmarks. The superior performance of
our solution is attributed to its ability to effectively utilize annotated data
from frame-based cameras and transfer the acquired knowledge to the event-based
vision domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online Continual Domain Adaptation for Semantic Image Segmentation Using
  Internal Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01035v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01035v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Serban Stan, Mohammad Rostami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation models trained on annotated data fail to generalize
well when the input data distribution changes over extended time period,
leading to requiring re-training to maintain performance. Classic Unsupervised
domain adaptation (UDA) attempts to address a similar problem when there is
target domain with no annotated data points through transferring knowledge from
a source domain with annotated data. We develop an online UDA algorithm for
semantic segmentation of images that improves model generalization on
unannotated domains in scenarios where source data access is restricted during
adaptation. We perform model adaptation is by minimizing the distributional
distance between the source latent features and the target features in a shared
embedding space. Our solution promotes a shared domain-agnostic latent feature
space between the two domains, which allows for classifier generalization on
the target dataset. To alleviate the need of access to source samples during
adaptation, we approximate the source latent feature distribution via an
appropriate surrogate distribution, in this case a Gassian mixture model (GMM).
We evaluate our approach on well established semantic segmentation datasets and
demonstrate it compares favorably against state-of-the-art (SOTA) UDA semantic
segmentation methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Continual Anomaly Detection with Contrastively-learned
  <span class="highlight-title">Prompt</span> <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Liu, Kai Wu, Qiang Nie, Ying Chen, Bin-Bin Gao, Yong Liu, Jinbao Wang, Chengjie Wang, Feng Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised Anomaly Detection (UAD) with incremental training is crucial in
industrial manufacturing, as unpredictable defects make obtaining sufficient
labeled data infeasible. However, continual learning methods primarily rely on
supervised annotations, while the application in UAD is limited due to the
absence of supervision. Current UAD methods train separate models for different
classes sequentially, leading to catastrophic forgetting and a heavy
computational burden. To address this issue, we introduce a novel Unsupervised
Continual Anomaly Detection framework called UCAD, which equips the UAD with
continual learning capability through contrastively-learned prompts. In the
proposed UCAD, we design a Continual Prompting Module (CPM) by utilizing a
concise key-prompt-knowledge memory bank to guide task-invariant `anomaly'
model predictions using task-specific `normal' knowledge. Moreover,
Structure-based Contrastive Learning (SCL) is designed with the Segment
Anything Model (SAM) to improve prompt learning and anomaly segmentation
results. Specifically, by treating SAM's masks as structure, we draw features
within the same mask closer and push others apart for general feature
representations. We conduct comprehensive experiments and set the benchmark on
unsupervised continual anomaly detection and segmentation, demonstrating that
our method is significantly better than anomaly detection methods, even with
rehearsal training. The code will be available at
https://github.com/shirowalker/UCAD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diversity-aware Buffer for Coping with Temporally Correlated Data
  Streams in Online Test-time Adaptation <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00989v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00989v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mario Döbler, Florian Marencke, Robert A. Marsden, Bin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since distribution shifts are likely to occur after a model's deployment and
can drastically decrease the model's performance, online test-time adaptation
(TTA) continues to update the model during test-time, leveraging the current
test data. In real-world scenarios, test data streams are not always
independent and identically distributed (i.i.d.). Instead, they are frequently
temporally correlated, making them non-i.i.d. Many existing methods struggle to
cope with this scenario. In response, we propose a diversity-aware and
category-balanced buffer that can simulate an i.i.d. data stream, even in
non-i.i.d. scenarios. Combined with a diversity and entropy-weighted entropy
loss, we show that a stable adaptation is possible on a wide range of
corruptions and natural domain shifts, based on ImageNet. We achieve
state-of-the-art results on most considered benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICASSP 2024. arXiv admin note: text overlap with
  arXiv:2306.00650</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Holistic Autonomous Driving Understanding by Bird's-Eye-View Injected
  Multi-Modal Large Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinpeng Ding, Jinahua Han, Hang Xu, Xiaodan Liang, Wei Zhang, Xiaomeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of multimodal large language models (MLLMs) has spurred interest in
language-based driving tasks. However, existing research typically focuses on
limited tasks and often omits key multi-view and temporal information which is
crucial for robust autonomous driving. To bridge these gaps, we introduce
NuInstruct, a novel dataset with 91K multi-view video-QA pairs across 17
subtasks, where each task demands holistic information (e.g., temporal,
multi-view, and spatial), significantly elevating the challenge level. To
obtain NuInstruct, we propose a novel SQL-based method to generate
instruction-response pairs automatically, which is inspired by the driving
logical progression of humans. We further present BEV-InMLLM, an end-to-end
method for efficiently deriving instruction-aware Bird's-Eye-View (BEV)
features, language-aligned for large language models. BEV-InMLLM integrates
multi-view, spatial awareness, and temporal semantics to enhance MLLMs'
capabilities on NuInstruct tasks. Moreover, our proposed BEV injection module
is a plug-and-play method for existing MLLMs. Our experiments on NuInstruct
demonstrate that BEV-InMLLM significantly outperforms existing MLLMs, e.g.
around 9% improvement on various tasks. We plan to release our NuInstruct for
future research development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-Time Object Detection in Occluded Environment with Background
  Cluttering Effects Using Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00986v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00986v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Syed Muhammad Aamir, Hongbin Ma, Malak Abid Ali Khan, Muhammad Aaqib
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detection of small, undetermined moving objects or objects in an occluded
environment with a cluttered background is the main problem of computer vision.
This greatly affects the detection accuracy of deep learning models. To
overcome these problems, we concentrate on deep learning models for real-time
detection of cars and tanks in an occluded environment with a cluttered
background employing SSD and YOLO algorithms and improved precision of
detection and reduce problems faced by these models. The developed method makes
the custom dataset and employs a preprocessing technique to clean the noisy
dataset. For training the developed model we apply the data augmentation
technique to balance and diversify the data. We fine-tuned, trained, and
evaluated these models on the established dataset by applying these techniques
and highlighting the results we got more accurately than without applying these
techniques. The accuracy and frame per second of the SSD-Mobilenet v2 model are
higher than YOLO V3 and YOLO V4. Furthermore, by employing various techniques
like data enhancement, noise reduction, parameter optimization, and model
fusion we improve the effectiveness of detection and recognition. We further
added a counting algorithm, and target attributes experimental comparison, and
made a graphical user interface system for the developed model with features of
object counting, alerts, status, resolution, and frame per second.
Subsequently, to justify the importance of the developed method analysis of
YOLO V3, V4, and SSD were incorporated. Which resulted in the overall
completion of the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Visibility-aware Generalizable Neural Radiance Fields for Interacting
  Hands <span class="chip">AAAI-24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00979v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00979v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan Huang, Hanhui Li, Zejun Yang, Zhisheng Wang, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural radiance fields (NeRFs) are promising 3D representations for scenes,
objects, and humans. However, most existing methods require multi-view inputs
and per-scene training, which limits their real-life applications. Moreover,
current methods focus on single-subject cases, leaving scenes of interacting
hands that involve severe inter-hand occlusions and challenging view variations
remain unsolved. To tackle these issues, this paper proposes a generalizable
visibility-aware NeRF (VA-NeRF) framework for interacting hands. Specifically,
given an image of interacting hands as input, our VA-NeRF first obtains a
mesh-based representation of hands and extracts their corresponding geometric
and textural features. Subsequently, a feature fusion module that exploits the
visibility of query points and mesh vertices is introduced to adaptively merge
features of both hands, enabling the recovery of features in unseen areas.
Additionally, our VA-NeRF is optimized together with a novel discriminator
within an adversarial learning paradigm. In contrast to conventional
discriminators that predict a single real/fake label for the synthesized image,
the proposed discriminator generates a pixel-wise visibility map, providing
fine-grained supervision for unseen areas and encouraging the VA-NeRF to
improve the visual quality of synthesized images. Experiments on the
Interhand2.6M dataset demonstrate that our proposed VA-NeRF outperforms
conventional NeRFs significantly. Project Page:
\url{https://github.com/XuanHuang0/VANeRF}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI-24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Hybrid Zoom using Camera Fusion on Mobile Phones <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01461v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01461v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaotong Wu, Wei-Sheng Lai, YiChang Shih, Charles Herrmann, Michael Krainin, Deqing Sun, Chia-Kai Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  DSLR cameras can achieve multiple zoom levels via shifting lens distances or
swapping lens types. However, these techniques are not possible on smartphone
devices due to space constraints. Most smartphone manufacturers adopt a hybrid
zoom system: commonly a Wide (W) camera at a low zoom level and a Telephoto (T)
camera at a high zoom level. To simulate zoom levels between W and T, these
systems crop and digitally upsample images from W, leading to significant
detail loss. In this paper, we propose an efficient system for hybrid zoom
super-resolution on mobile devices, which captures a synchronous pair of W and
T shots and leverages machine learning models to align and transfer details
from T to W. We further develop an adaptive blending method that accounts for
depth-of-field mismatches, scene occlusion, flow uncertainty, and alignment
errors. To minimize the domain gap, we design a dual-phone camera rig to
capture real-world inputs and ground-truths for supervised training. Our method
generates a 12-megapixel image in 500ms on a mobile platform and compares
favorably against state-of-the-art methods under extensive evaluation on
real-world scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to SIGGRAPH Asia 2023 (ACM TOG). Project website:
  https://www.wslai.net/publications/fusion_zoom</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ColorizeDiffusion: Adjustable Sketch Colorization with Reference Image
  and Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01456v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01456v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingkun Yan, Liang Yuan, Yuma Nishioka, Issei Fujishiro, Suguru Saito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, diffusion models have demonstrated their effectiveness in
generating extremely high-quality images and have found wide-ranging
applications, including automatic sketch colorization. However, most existing
models use text to guide the conditional generation, with fewer attempts
exploring the potential advantages of using image tokens as conditional inputs
for networks. As such, this paper exhaustively investigates image-guided
models, specifically targeting reference-based sketch colorization, which aims
to colorize sketch images using reference color images. We investigate three
critical aspects of reference-based diffusion models: the shortcomings compared
to text-based counterparts, the training strategies, and the capability in
zero-shot, sequential text-based manipulation. We introduce two variations of
an image-guided latent diffusion model using different image tokens from the
pre-trained CLIP image encoder, and we propose corresponding manipulation
methods to adjust their results sequentially using weighted text inputs. We
conduct comprehensive evaluations of our models through qualitative and
quantitative experiments, as well as a user study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Autonomous Driving <span class="highlight-title">Dataset</span>s: Data Statistic, Annotation, and
  Outlook 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01454v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01454v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyu Liu, Ekim Yurtsever, Xingcheng Zhou, Jonathan Fossaert, Yuning Cui, Bare Luka Zagar, Alois C. Knoll
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving has rapidly developed and shown promising performance with
recent advances in hardware and deep learning methods. High-quality datasets
are fundamental for developing reliable autonomous driving algorithms. Previous
dataset surveys tried to review the datasets but either focused on a limited
number or lacked detailed investigation of the characters of datasets. To this
end, we present an exhaustive study of over 200 autonomous driving datasets
from multiple perspectives, including sensor modalities, data size, tasks, and
contextual conditions. We introduce a novel metric to evaluate the impact of
each dataset, which can also be a guide for establishing new datasets. We
further analyze the annotation process and quality of datasets. Additionally,
we conduct an in-depth analysis of the data distribution of several vital
datasets. Finally, we discuss the development trend of the future autonomous
driving datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProbMCL: Simple Probabilistic Contrastive Learning for Multi-label
  Visual Classification <span class="chip">ICASSP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Sajedi, Samir Khaki, Yuri A. Lawryshyn, Konstantinos N. Plataniotis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-label image classification presents a challenging task in many domains,
including computer vision and medical imaging. Recent advancements have
introduced graph-based and transformer-based methods to improve performance and
capture label dependencies. However, these methods often include complex
modules that entail heavy computation and lack interpretability. In this paper,
we propose Probabilistic Multi-label Contrastive Learning (ProbMCL), a novel
framework to address these challenges in multi-label image classification
tasks. Our simple yet effective approach employs supervised contrastive
learning, in which samples that share enough labels with an anchor image based
on a decision threshold are introduced as a positive set. This structure
captures label dependencies by pulling positive pair embeddings together and
pushing away negative samples that fall below the threshold. We enhance
representation learning by incorporating a mixture density network into
contrastive learning and generating Gaussian mixture distributions to explore
the epistemic uncertainty of the feature encoder. We validate the effectiveness
of our framework through experimentation with datasets from the computer vision
and medical imaging domains. Our method outperforms the existing
state-of-the-art methods while achieving a low computational footprint on both
datasets. Visualization analyses also demonstrate that ProbMCL-learned
classifiers maintain a meaningful semantic topology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for the IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Indoor Obstacle Discovery on Reflective Ground via Monocular Camera 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Xue, Yicong Chang, Tianxi Wang, Yu Zhou, Anlong Ming
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual obstacle discovery is a key step towards autonomous navigation of
indoor mobile robots. Successful solutions have many applications in multiple
scenes. One of the exceptions is the reflective ground. In this case, the
reflections on the floor resemble the true world, which confuses the obstacle
discovery and leaves navigation unsuccessful. We argue that the key to this
problem lies in obtaining discriminative features for reflections and
obstacles. Note that obstacle and reflection can be separated by the ground
plane in 3D space. With this observation, we firstly introduce a
pre-calibration based ground detection scheme that uses robot motion to predict
the ground plane. Due to the immunity of robot motion to reflection, this
scheme avoids failed ground detection caused by reflection. Given the detected
ground, we design a ground-pixel parallax to describe the location of a pixel
relative to the ground. Based on this, a unified appearance-geometry feature
representation is proposed to describe objects inside rectangular boxes.
Eventually, based on segmenting by detection framework, an appearance-geometry
fusion regressor is designed to utilize the proposed feature to discover the
obstacles. It also prevents our model from concentrating too much on parts of
obstacles instead of whole obstacles. For evaluation, we introduce a new
dataset for Obstacle on Reflective Ground (ORG), which comprises 15 scenes with
various ground reflections, a total of more than 200 image sequences and 3400
RGB images. The pixel-wise annotations of ground and obstacle provide a
comparison to our method and other methods. By reducing the misdetection of the
reflection, the proposed approach outperforms others. The source code and the
dataset will be available at
https://github.com/XuefengBUPT/IndoorObstacleDiscovery-RG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Journal of Computer Vision (IJCV) 2023. Project Page:
  https://xuefeng-cvr.github.io/IODRG</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Off-Road LiDAR Intensity Based Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kasi Viswanath, Peng Jiang, Sujit PB, Srikanth Saripalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR is used in autonomous driving to provide 3D spatial information and
enable accurate perception in off-road environments, aiding in obstacle
detection, mapping, and path planning. Learning-based LiDAR semantic
segmentation utilizes machine learning techniques to automatically classify
objects and regions in LiDAR point clouds. Learning-based models struggle in
off-road environments due to the presence of diverse objects with varying
colors, textures, and undefined boundaries, which can lead to difficulties in
accurately classifying and segmenting objects using traditional geometric-based
features. In this paper, we address this problem by harnessing the LiDAR
intensity parameter to enhance object segmentation in off-road environments.
Our approach was evaluated in the RELLIS-3D data set and yielded promising
results as a preliminary analysis with improved mIoU for classes "puddle" and
"grass" compared to more complex deep learning-based benchmarks. The
methodology was evaluated for compatibility across both Velodyne and Ouster
LiDAR systems, assuring its cross-platform applicability. This analysis
advocates for the incorporation of calibrated intensity as a supplementary
input, aiming to enhance the prediction accuracy of learning based semantic
segmentation frameworks.
https://github.com/MOONLABIISERB/lidar-intensity-predictor/tree/main
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ISER 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Swap<span class="highlight-title">Transformer</span>: highway overtaking tactical planner model via imitation
  learning on OSHA <span class="highlight-title">dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Shamsoshoara, Safin B Salih, Pedram Aghazadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the high-level decision-making problem in highway
scenarios regarding lane changing and over-taking other slower vehicles. In
particular, this paper aims to improve the Travel Assist feature for automatic
overtaking and lane changes on highways. About 9 million samples including lane
images and other dynamic objects are collected in simulation. This data;
Overtaking on Simulated HighwAys (OSHA) dataset is released to tackle this
challenge. To solve this problem, an architecture called SwapTransformer is
designed and implemented as an imitation learning approach on the OSHA dataset.
Moreover, auxiliary tasks such as future points and car distance network
predictions are proposed to aid the model in better understanding the
surrounding environment. The performance of the proposed solution is compared
with a multi-layer perceptron (MLP) and multi-head self-attention networks as
baselines in a simulation environment. We also demonstrate the performance of
the model with and without auxiliary tasks. All models are evaluated based on
different metrics such as time to finish each lap, number of overtakes, and
speed difference with speed limit. The evaluation shows that the
SwapTransformer model outperforms other models in different traffic densities
in the inference phase.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 12 Figures, 1 Algorithm, 2 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image Sculpting: Precise Object Editing with 3D Geometry Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiraphon Yenphraphai, Xichen Pan, Sainan Liu, Daniele Panozzo, Saining Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Image Sculpting, a new framework for editing 2D images by
incorporating tools from 3D geometry and graphics. This approach differs
markedly from existing methods, which are confined to 2D spaces and typically
rely on textual instructions, leading to ambiguity and limited control. Image
Sculpting converts 2D objects into 3D, enabling direct interaction with their
3D geometry. Post-editing, these objects are re-rendered into 2D, merging into
the original image to produce high-fidelity results through a coarse-to-fine
enhancement process. The framework supports precise, quantifiable, and
physically-plausible editing options such as pose editing, rotation,
translation, 3D composition, carving, and serial addition. It marks an initial
step towards combining the creative freedom of generative models with the
precision of graphics pipelines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and project page: https://image-sculpting.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep autoregressive modeling for land use land cover 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01395v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01395v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Krapu, Mark Borsuk, Ryan Calder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Land use / land cover (LULC) modeling is a challenging task due to long-range
dependencies between geographic features and distinct spatial patterns related
to topography, ecology, and human development. We identify a close connection
between modeling of spatial patterns of land use and the task of image
inpainting from computer vision and conduct a study of a modified PixelCNN
architecture with approximately 19 million parameters for modeling LULC. In
comparison with a benchmark spatial statistical model, we find that the former
is capable of capturing much richer spatial correlation patterns such as roads
and water bodies but does not produce a calibrated predictive distribution,
suggesting the need for additional tuning. We find evidence of predictive
underdispersion with regard to important ecologically-relevant land use
statistics such as patch count and adjacency which can be ameliorated to some
extent by manipulating sampling variability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KeDuSR: Real-World Dual-Lens Super-Resolution via Kernel-Free Matching <span class="chip">AAAI-2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17050v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17050v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanjing Yue, Zifan Cui, Kun Li, Jingyu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dual-lens super-resolution (SR) is a practical scenario for reference (Ref)
based SR by utilizing the telephoto image (Ref) to assist the super-resolution
of the low-resolution wide-angle image (LR input). Different from general
RefSR, the Ref in dual-lens SR only covers the overlapped field of view (FoV)
area. However, current dual-lens SR methods rarely utilize these specific
characteristics and directly perform dense matching between the LR input and
Ref. Due to the resolution gap between LR and Ref, the matching may miss the
best-matched candidate and destroy the consistent structures in the overlapped
FoV area. Different from them, we propose to first align the Ref with the
center region (namely the overlapped FoV area) of the LR input by combining
global warping and local warping to make the aligned Ref be sharp and
consistent. Then, we formulate the aligned Ref and LR center as value-key
pairs, and the corner region of the LR is formulated as queries. In this way,
we propose a kernel-free matching strategy by matching between the LR-corner
(query) and LR-center (key) regions, and the corresponding aligned Ref (value)
can be warped to the corner region of the target. Our kernel-free matching
strategy avoids the resolution gap between LR and Ref, which makes our network
have better generalization ability. In addition, we construct a DuSR-Real
dataset with (LR, Ref, HR) triples, where the LR and HR are well aligned.
Experiments on three datasets demonstrate that our method outperforms the
second-best method by a large margin. Our code and dataset are available at
https://github.com/ZifanCui/KeDuSR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 10 figures. Accepted by AAAI-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tuned Compositional Feature Replays for Efficient Stream Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.02206v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.02206v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Morgan B. Talbot, Rushikesh Zawar, Rohil Badkundri, Mengmi Zhang, Gabriel Kreiman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our brains extract durable, generalizable knowledge from transient
experiences of the world. Artificial neural networks come nowhere close to this
ability. When tasked with learning to classify objects by training on
non-repeating video frames in temporal order (online stream learning), models
that learn well from shuffled datasets catastrophically forget old knowledge
upon learning new stimuli. We propose a new continual learning algorithm,
Compositional Replay Using Memory Blocks (CRUMB), which mitigates forgetting by
replaying feature maps reconstructed by combining generic parts. CRUMB
concatenates trainable and re-usable "memory block" vectors to compositionally
reconstruct feature map tensors in convolutional neural networks. Storing the
indices of memory blocks used to reconstruct new stimuli enables memories of
the stimuli to be replayed during later tasks. This reconstruction mechanism
also primes the neural network to minimize catastrophic forgetting by biasing
it towards attending to information about object shapes more than information
about image textures, and stabilizes the network during stream learning by
providing a shared feature-level basis for all training examples. These
properties allow CRUMB to outperform an otherwise identical algorithm that
stores and replays raw images, while occupying only 3.6% as much memory. We
stress-tested CRUMB alongside 13 competing methods on 7 challenging datasets.
To address the limited number of existing online stream learning datasets, we
introduce 2 new benchmarks by adapting existing datasets for stream learning.
With only 3.7-4.1% as much memory and 15-43% as much runtime, CRUMB mitigates
catastrophic forgetting more effectively than the state-of-the-art. Our code is
available at https://github.com/MorganBDT/crumb.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Copyright 2023 IEEE. The journal version of this article is hosted at
  https://ieeexplore.ieee.org/document/10373937 and
  https://klab.tch.harvard.edu/publications/PDFs/gk8019.pdf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recovering 3D Human Mesh from Monocular Images: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.01923v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.01923v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yating Tian, Hongwen Zhang, Yebin Liu, Limin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating human pose and shape from monocular images is a long-standing
problem in computer vision. Since the release of statistical body models, 3D
human mesh recovery has been drawing broader attention. With the same goal of
obtaining well-aligned and physically plausible mesh results, two paradigms
have been developed to overcome challenges in the 2D-to-3D lifting process: i)
an optimization-based paradigm, where different data terms and regularization
terms are exploited as optimization objectives; and ii) a regression-based
paradigm, where deep learning techniques are embraced to solve the problem in
an end-to-end fashion. Meanwhile, continuous efforts are devoted to improving
the quality of 3D mesh labels for a wide range of datasets. Though remarkable
progress has been achieved in the past decade, the task is still challenging
due to flexible body motions, diverse appearances, complex environments, and
insufficient in-the-wild annotations. To the best of our knowledge, this is the
first survey that focuses on the task of monocular 3D human mesh recovery. We
start with the introduction of body models and then elaborate recovery
frameworks and training objectives by providing in-depth analyses of their
strengths and weaknesses. We also summarize datasets, evaluation metrics, and
benchmark results. Open issues and future directions are discussed in the end,
hoping to motivate researchers and facilitate their research in this area. A
regularly updated project page can be found at
https://github.com/tinatiansjz/hmr-survey.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in IEEE TPAMI, Survey on monocular 3D human mesh recovery,
  Project page: https://github.com/tinatiansjz/hmr-survey</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Art<span class="highlight-title">GPT</span>-4: Towards Artistic-understanding Large Vision-Language Models
  with Enhanced Adapter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.07490v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.07490v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengqing Yuan, Xinyi Wang, Kun Wang, Lichao Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, advancements in large language models have been remarkable,
with models such as ChatGPT demonstrating exceptional proficiency in diverse
linguistic tasks. The pre-training of large models with billions of parameters,
poses a formidable challenge, primarily due to the scarcity of datasets of a
commensurate scale for effective training. Nevertheless, innovative strategies
have emerged, including methods to fine-tune these pre-trained models using
fewer parameters set, as evidenced by models like MiniGPT-4 and LLaVA. Despite
their potential in various domains, these models remain limited in their
understanding of artistic imagery. They have yet to fully grasp the intricate
nuances of art images or to provide an objective articulation of the emotions
they evoke, in a manner akin to human perception. This work introduces
ArtGPT-4, a pioneering large vision-language model tailored to address the
deficiencies of contemporary models in artistic comprehension. ArtGPT-4
underwent training on image-text pairs utilizing a Tesla A100 device in a mere
2 hours, with a dataset comprising approximately 0.52M entries. Impressively,
the model can render images with an artistic-understanding and convey the
emotions they inspire, mirroring human interpretation. Additionally, this work
presents a unique dataset designed to evaluate the efficacy of vision-language
models. In subsequent evaluations, ArtGPT-4 not only achieved state-of-the-art
performance on the ArtEmis and ArtEmis-v2.0 datasets but also exceeded the
established benchmarks introduced in This study, lagging behind professional
artists' descriptions by a negligible 0.15 points on a 6-point scale. The code
and the pre-trained model are accessible in
https://huggingface.co/Tyrannosaurus/ArtGPT-4.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-Efficient Multimodal Fusion on a Single GPU 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10144v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10144v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noël Vouitsis, Zhaoyan Liu, Satya Krishna Gorti, Valentin Villecroze, Jesse C. Cresswell, Guangwei Yu, Gabriel Loaiza-Ganem, Maksims Volkovs
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of multimodal alignment is to learn a single latent space that is
shared between multimodal inputs. The most powerful models in this space have
been trained using massive datasets of paired inputs and large-scale
computational resources, making them prohibitively expensive to train in many
practical scenarios. We surmise that existing unimodal encoders pre-trained on
large amounts of unimodal data should provide an effective bootstrap to create
multimodal models from unimodal ones at much lower costs. We therefore propose
FuseMix, a multimodal augmentation scheme that operates on the latent spaces of
arbitrary pre-trained unimodal encoders. Using FuseMix for multimodal
alignment, we achieve competitive performance -- and in certain cases
outperform state-of-the art methods -- in both image-text and audio-text
retrieval, with orders of magnitude less compute and data: for example, we
outperform CLIP on the Flickr30K text-to-image retrieval task with $\sim \!
600\times$ fewer GPU days and $\sim \! 80\times$ fewer image-text pairs.
Additionally, we show how our method can be applied to convert pre-trained
text-to-image generative models into audio-to-image ones. Code is available at:
https://github.com/layer6ai-labs/fusemix.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lossy Image Compression with Conditional Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.06950v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.06950v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruihan Yang, Stephan Mandt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper outlines an end-to-end optimized lossy image compression framework
using diffusion generative models. The approach relies on the transform coding
paradigm, where an image is mapped into a latent space for entropy coding and,
from there, mapped back to the data space for reconstruction. In contrast to
VAE-based neural compression, where the (mean) decoder is a deterministic
neural network, our decoder is a conditional diffusion model. Our approach thus
introduces an additional ``content'' latent variable on which the reverse
diffusion process is conditioned and uses this variable to store information
about the image. The remaining ``texture'' variables characterizing the
diffusion process are synthesized at decoding time. We show that the model's
performance can be tuned toward perceptual metrics of interest. Our extensive
experiments involving multiple datasets and image quality assessment metrics
show that our approach yields stronger reported FID scores than the GAN-based
model, while also yielding competitive performance with VAE-based models in
several distortion metrics. Furthermore, training the diffusion with
$\mathcal{X}$-parameterization enables high-quality reconstructions in only a
handful of decoding steps, greatly affecting the model's practicality. Our code
is available at: \url{https://github.com/buggyyang/CDC_compression}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PointDC:Unsupervised Semantic Segmentation of 3D Point Clouds via
  Cross-modal Distillation and Super-Voxel Clustering <span class="chip">ICCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.08965v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.08965v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zisheng Chen, Hongbin Xu, Weitao Chen, Zhipeng Zhou, Haihong Xiao, Baigui Sun, Xuansong Xie, Wenxiong Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation of point clouds usually requires exhausting efforts of
human annotations, hence it attracts wide attention to the challenging topic of
learning from unlabeled or weaker forms of annotations. In this paper, we take
the first attempt for fully unsupervised semantic segmentation of point clouds,
which aims to delineate semantically meaningful objects without any form of
annotations. Previous works of unsupervised pipeline on 2D images fails in this
task of point clouds, due to: 1) Clustering Ambiguity caused by limited
magnitude of data and imbalanced class distribution; 2) Irregularity Ambiguity
caused by the irregular sparsity of point cloud. Therefore, we propose a novel
framework, PointDC, which is comprised of two steps that handle the
aforementioned problems respectively: Cross-Modal Distillation (CMD) and
Super-Voxel Clustering (SVC). In the first stage of CMD, multi-view visual
features are back-projected to the 3D space and aggregated to a unified point
feature to distill the training of the point representation. In the second
stage of SVC, the point features are aggregated to super-voxels and then fed to
the iterative clustering process for excavating semantic classes. PointDC
yields a significant improvement over the prior state-of-the-art unsupervised
methods, on both the ScanNet-v2 (+18.4 mIoU) and S3DIS (+11.5 mIoU) semantic
segmentation benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by International Conference on Computer Vision (ICCV) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploiting Causality Signals in Medical Images: A Pilot Study with
  Empirical Results 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10399v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10399v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gianluca Carloni, Sara Colantonio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel technique to discover and exploit weak causal signals
directly from images via neural networks for classification purposes. This way,
we model how the presence of a feature in one part of the image affects the
appearance of another feature in a different part of the image. Our method
consists of a convolutional neural network backbone and a causality-factors
extractor module, which computes weights to enhance each feature map according
to its causal influence in the scene. We develop different architecture
variants and empirically evaluate all the models on two public datasets of
prostate MRI images and breast histopathology slides for cancer diagnosis. We
study the effectiveness of our module both in fully-supervised and few-shot
learning, we assess its addition to existing attention-based solutions, we
conduct ablation studies, and investigate the explainability of our models via
class activation maps. Our findings show that our lightweight block extracts
meaningful information and improves the overall classification, together with
producing more robust predictions that focus on relevant parts of the image.
That is crucial in medical imaging, where accurate and reliable classifications
are essential for effective diagnosis and treatment planning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Added experiments in which we integrate our Mulcat module to existing
  models using Bottleneck Attention Modules, and added experiments in Few-Shot
  Learning; 19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RS5M and GeoRSCLIP: A Large Scale Vision-Language <span class="highlight-title">Dataset</span> and A Large
  Vision-Language Model for Remote Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.11300v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.11300v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zilun Zhang, Tiancheng Zhao, Yulong Guo, Jianwei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained Vision-Language Models (VLMs) utilizing extensive image-text
paired data have demonstrated unprecedented image-text association
capabilities, achieving remarkable results across various downstream tasks. A
critical challenge is how to make use of existing large-scale pre-trained VLMs,
which are trained on common objects, to perform the domain-specific transfer
for accomplishing domain-related downstream tasks. A critical challenge is how
to make use of existing large-scale pre-trained VLMs, which are trained on
common objects, to perform the domain-specific transfer for accomplishing
domain-related downstream tasks. In this paper, we propose a new framework that
includes the Domain pre-trained Vision-Language Model (DVLM), bridging the gap
between the General Vision-Language Model (GVLM) and domain-specific downstream
tasks. Moreover, we present an image-text paired dataset in the field of remote
sensing (RS), RS5M, which has 5 million RS images with English descriptions.
The dataset is obtained from filtering publicly available image-text paired
datasets and captioning label-only RS datasets with pre-trained VLM. These
constitute the first large-scale RS image-text paired dataset. Additionally, we
fine-tuned the CLIP model and tried several Parameter-Efficient Fine-Tuning
methods on RS5M to implement the DVLM. Experimental results show that our
proposed dataset is highly effective for various tasks, and our model GeoRSCLIP
improves upon the baseline or previous state-of-the-art model by $3\%\sim20\%$
in Zero-shot Classification (ZSC), $3\%\sim6\%$ in Remote Sensing Cross-Modal
Text-Image Retrieval (RSCTIR) and $4\%\sim5\%$ in Semantic Localization (SeLo)
tasks. Dataset and models have been released in:
\url{https://github.com/om-ai-lab/RS5M}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>RS5M dataset v5</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Application of Efficient Neural Mapping to Real-Time Indoor
  Localisation for Unmanned Ground Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.04718v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.04718v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher J. Holder, Muhammad Shafique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Global localisation from visual data is a challenging problem applicable to
many robotics domains. Prior works have shown that neural networks can be
trained to map images of an environment to absolute camera pose within that
environment, learning an implicit neural mapping in the process. In this work
we evaluate the applicability of such an approach to real-world robotics
scenarios, demonstrating that by constraining the problem to 2-dimensions and
significantly increasing the quantity of training data, a compact model capable
of real-time inference on embedded platforms can be used to achieve
localisation accuracy of several centimetres. We deploy our trained model
onboard a UGV platform, demonstrating its effectiveness in a waypoint
navigation task, wherein it is able to localise with a mean accuracy of 9cm at
a rate of 6fps running on the UGV onboard CPU, 35fps on an embedded GPU, or
220fps on a desktop GPU. Along with this work we will release a novel
localisation dataset comprising simulated and real environments, each with
training samples numbering in the tens of thousands.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>\c{opyright} 2024 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GD^2-NeRF: Generative Detail Compensation via GAN and Diffusion for
  One-shot Generalizable Neural Radiance Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00616v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00616v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Pan, Zongxin Yang, Shuai Bai, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we focus on the One-shot Novel View Synthesis (O-NVS) task
which targets synthesizing photo-realistic novel views given only one reference
image per scene. Previous One-shot Generalizable Neural Radiance Fields
(OG-NeRF) methods solve this task in an inference-time finetuning-free manner,
yet suffer the blurry issue due to the encoder-only architecture that highly
relies on the limited reference image. On the other hand, recent
diffusion-based image-to-3d methods show vivid plausible results via distilling
pre-trained 2D diffusion models into a 3D representation, yet require tedious
per-scene optimization. Targeting these issues, we propose the GD$^2$-NeRF, a
Generative Detail compensation framework via GAN and Diffusion that is both
inference-time finetuning-free and with vivid plausible details. In detail,
following a coarse-to-fine strategy, GD$^2$-NeRF is mainly composed of a
One-stage Parallel Pipeline (OPP) and a 3D-consistent Detail Enhancer
(Diff3DE). At the coarse stage, OPP first efficiently inserts the GAN model
into the existing OG-NeRF pipeline for primarily relieving the blurry issue
with in-distribution priors captured from the training dataset, achieving a
good balance between sharpness (LPIPS, FID) and fidelity (PSNR, SSIM). Then, at
the fine stage, Diff3DE further leverages the pre-trained image diffusion
models to complement rich out-distribution details while maintaining decent 3D
consistency. Extensive experiments on both the synthetic and real-world
datasets show that GD$^2$-NeRF noticeably improves the details while without
per-scene finetuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Reading with Macbook Preview is recommended for best quality;
  Submitted to Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ImFace++: A Sophisticated Nonlinear 3D Morphable Face Model with
  Implicit Neural Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04028v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04028v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingwu Zheng, Haiyu Zhang, Hongyu Yang, Liming Chen, Di Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate representations of 3D faces are of paramount importance in various
computer vision and graphics applications. However, the challenges persist due
to the limitations imposed by data discretization and model linearity, which
hinder the precise capture of identity and expression clues in current studies.
This paper presents a novel 3D morphable face model, named ImFace++, to learn a
sophisticated and continuous space with implicit neural representations.
ImFace++ first constructs two explicitly disentangled deformation fields to
model complex shapes associated with identities and expressions, respectively,
which simultaneously facilitate the automatic learning of correspondences
across diverse facial shapes. To capture more sophisticated facial details, a
refinement displacement field within the template space is further
incorporated, enabling a fine-grained learning of individual-specific facial
details. Furthermore, a Neural Blend-Field is designed to reinforce the
representation capabilities through adaptive blending of an array of local
fields. In addition to ImFace++, we have devised an improved learning strategy
to extend expression embeddings, allowing for a broader range of expression
variations. Comprehensive qualitative and quantitative evaluations demonstrate
that ImFace++ significantly advances the state-of-the-art in terms of both face
reconstruction fidelity and correspondence accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page:
  https://github.com/MingwuZheng/ImFace/tree/imface%2B%2B. arXiv admin note:
  text overlap with arXiv:2203.14510</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tomato Maturity Recognition with Convolutional <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.01530v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.01530v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Asim Khan, Taimur Hassan, Muhammad Shafay, Israa Fahmy, Naoufel Werghi, Lakmal Seneviratne, Irfan Hussain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tomatoes are a major crop worldwide, and accurately classifying their
maturity is important for many agricultural applications, such as harvesting,
grading, and quality control. In this paper, the authors propose a novel method
for tomato maturity classification using a convolutional transformer. The
convolutional transformer is a hybrid architecture that combines the strengths
of convolutional neural networks (CNNs) and transformers. Additionally, this
study introduces a new tomato dataset named KUTomaData, explicitly designed to
train deep-learning models for tomato segmentation and classification.
KUTomaData is a compilation of images sourced from a greenhouse in the UAE,
with approximately 700 images available for training and testing. The dataset
is prepared under various lighting conditions and viewing perspectives and
employs different mobile camera sensors, distinguishing it from existing
datasets. The contributions of this paper are threefold:Firstly, the authors
propose a novel method for tomato maturity classification using a modular
convolutional transformer. Secondly, the authors introduce a new tomato image
dataset that contains images of tomatoes at different maturity levels. Lastly,
the authors show that the convolutional transformer outperforms
state-of-the-art methods for tomato maturity classification. The effectiveness
of the proposed framework in handling cluttered and occluded tomato instances
was evaluated using two additional public datasets, Laboro Tomato and Rob2Pheno
Annotated Tomato, as benchmarks. The evaluation results across these three
datasets demonstrate the exceptional performance of our proposed framework,
surpassing the state-of-the-art by 58.14%, 65.42%, and 66.39% in terms of mean
average precision scores for KUTomaData, Laboro Tomato, and Rob2Pheno Annotated
Tomato, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 6 figures and 8 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tensor PCA from basis in tensor space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.02803v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.02803v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Claudio Turchetti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The aim of this paper is to present a mathematical framework for tensor PCA.
The proposed approach is able to overcome the limitations of previous methods
that extract a low dimensional subspace by iteratively solving an optimization
problem. The core of the proposed approach is the derivation of a basis in
tensor space from a real self-adjoint tensor operator, thus reducing the
problem of deriving a basis to an eigenvalue problem. Three different cases
have been studied to derive: i) a basis from a self-adjoint tensor operator;
ii) a rank-1 basis; iii) a basis in a subspace. In particular, the equivalence
between eigenvalue equation for a real self-adjoint tensor operator and
standard matrix eigenvalue equation has been proven. For all the three cases
considered, a subspace approach has been adopted to derive a tensor PCA.
Experiments on image datasets validate the proposed mathematical framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Long- and Short-Range Temporal Information for Learned Video
  Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.03754v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.03754v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huairui Wang, Zhenzhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learned video compression methods have gained a variety of interest in the
video coding community since they have matched or even exceeded the
rate-distortion (RD) performance of traditional video codecs. However, many
current learning-based methods are dedicated to utilizing short-range temporal
information, thus limiting their performance. In this paper, we focus on
exploiting the unique characteristics of video content and further exploring
temporal information to enhance compression performance. Specifically, for
long-range temporal information exploitation, we propose temporal prior that
can update continuously within the group of pictures (GOP) during inference. In
that case temporal prior contains valuable temporal information of all decoded
images within the current GOP. As for short-range temporal information, we
propose a progressive guided motion compensation to achieve robust and
effective compensation. In detail, we design a hierarchical structure to
achieve multi-scale compensation. More importantly, we use optical flow
guidance to generate pixel offsets between feature maps at each scale, and the
compensation results at each scale will be used to guide the following scale's
compensation. Sufficient experimental results demonstrate that our method can
obtain better RD performance than state-of-the-art video compression
approaches. The code is publicly available on:
https://github.com/Huairui/LSTVC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2207.04589</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attention Based Encoder Decoder Model for Video Captioning in Nepali
  (2023) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07418v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07418v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kabita Parajuli, Shashidhar Ram Joshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video captioning in Nepali, a language written in the Devanagari script,
presents a unique challenge due to the lack of existing academic work in this
domain. This work develops a novel encoder-decoder paradigm for Nepali video
captioning to tackle this difficulty. LSTM and GRU sequence-to-sequence models
are used in the model to produce related textual descriptions based on features
retrieved from video frames using CNNs. Using Google Translate and manual
post-editing, a Nepali video captioning dataset is generated from the Microsoft
Research Video Description Corpus (MSVD) dataset created using Google
Translate, and manual post-editing work. The efficacy of the model for
Devanagari-scripted video captioning is demonstrated by BLEU, METOR, and ROUGE
measures, which are used to assess its performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Result are wrong and took some time</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ YOLO and Mask R-CNN for Vehicle Number Plate Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.13165v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.13165v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Ganjoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  License plate scanners have grown in popularity in parking lots during the
past few years. In order to quickly identify license plates, traditional plate
recognition devices used in parking lots employ a fixed source of light and
shooting angles. For skewed angles, such as license plate images taken with
ultra-wide angle or fisheye lenses, deformation of the license plate
recognition plate can also be quite severe, impairing the ability of standard
license plate recognition systems to identify the plate. Mask RCNN gadget that
may be utilised for oblique pictures and various shooting angles. The results
of the experiments show that the suggested design will be capable of
classifying license plates with bevel angles larger than 0/60. Character
recognition using the suggested Mask R-CNN approach has advanced significantly
as well. The proposed Mask R-CNN method has also achieved significant progress
in character recognition, which is tilted more than 45 degrees as compared to
the strategy of employing the YOLOv2 model. Experiment results also suggest
that the methodology presented in the open data plate collecting is better than
other techniques (known as the AOLP dataset).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>changes to be done</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 6D-Diff: A Keypoint Diffusion Framework for 6D Object Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00029v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00029v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Xu, Haoxuan Qu, Yujun Cai, Jun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating the 6D object pose from a single RGB image often involves noise
and indeterminacy due to challenges such as occlusions and cluttered
backgrounds. Meanwhile, diffusion models have shown appealing performance in
generating high-quality images from random noise with high indeterminacy
through step-by-step denoising. Inspired by their denoising capability, we
propose a novel diffusion-based framework (6D-Diff) to handle the noise and
indeterminacy in object pose estimation for better performance. In our
framework, to establish accurate 2D-3D correspondence, we formulate 2D
keypoints detection as a reverse diffusion (denoising) process. To facilitate
such a denoising process, we design a Mixture-of-Cauchy-based forward diffusion
process and condition the reverse process on the object features. Extensive
experiments on the LM-O and YCB-V datasets demonstrate the effectiveness of our
framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Fix typo</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Content Bias in Deep Learning Image Age Approximation: A new Approach
  Towards better Explainability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02067v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02067v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Jöchl, Andreas Uhl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of temporal image forensics, it is not evident that a neural
network, trained on images from different time-slots (classes), exploits solely
image age related features. Usually, images taken in close temporal proximity
(e.g., belonging to the same age class) share some common content properties.
Such content bias can be exploited by a neural network. In this work, a novel
approach is proposed that evaluates the influence of image content. This
approach is verified using synthetic images (where content bias can be ruled
out) with an age signal embedded. Based on the proposed approach, it is shown
that a deep learning approach proposed in the context of age classification is
most likely highly dependent on the image content. As a possible
countermeasure, two different models from the field of image steganalysis,
along with three different preprocessing techniques to increase the
signal-to-noise ratio (age signal to image content), are evaluated using the
proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is a preprint, the paper is currently under consideration at
  Pattern Recognition Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VGDiffZero: Text-to-image Diffusion Models Can Be Zero-shot Visual
  Grounders <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.01141v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.01141v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuyang Liu, Siteng Huang, Yachen Kang, Honggang Chen, Donglin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale text-to-image diffusion models have shown impressive capabilities
for generative tasks by leveraging strong vision-language alignment from
pre-training. However, most vision-language discriminative tasks require
extensive fine-tuning on carefully-labeled datasets to acquire such alignment,
with great cost in time and computing resources. In this work, we explore
directly applying a pre-trained generative diffusion model to the challenging
discriminative task of visual grounding without any fine-tuning and additional
training dataset. Specifically, we propose VGDiffZero, a simple yet effective
zero-shot visual grounding framework based on text-to-image diffusion models.
We also design a comprehensive region-scoring method considering both global
and local contexts of each isolated proposal. Extensive experiments on RefCOCO,
RefCOCO+, and RefCOCOg show that VGDiffZero achieves strong performance on
zero-shot visual grounding. Our code is available at
https://github.com/xuyang-liu16/VGDiffZero.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual Validation versus Visual Estimation: A Study on the Average Value
  in Scatterplots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09330v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09330v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Braun, Ashley Suh, Remco Chang, Michael Gleicher, Tatiana von Landesberger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the ability of individuals to visually validate statistical
models in terms of their fit to the data. While visual model estimation has
been studied extensively, visual model validation remains under-investigated.
It is unknown how well people are able to visually validate models, and how
their performance compares to visual and computational estimation. As a
starting point, we conducted a study across two populations (crowdsourced and
volunteers). Participants had to both visually estimate (i.e, draw) and
visually validate (i.e., accept or reject) the frequently studied model of
averages. Across both populations, the level of accuracy of the models that
were considered valid was lower than the accuracy of the estimated models. We
find that participants' validation and estimation were unbiased. Moreover,
their natural critical point between accepting and rejecting a given mean value
is close to the boundary of its 95% confidence interval, indicating that the
visually perceived confidence interval corresponds to a common statistical
standard. Our work contributes to the understanding of visual model validation
and opens new research opportunities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint and Author Version of a Short Paper, accepted to the 2023
  IEEE Visualization Conference (VIS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EMAGE: Towards Unified Holistic Co-Speech Gesture Generation via Masked
  Audio Gesture Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00374v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00374v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiyang Liu, Zihao Zhu, Giorgio Becherini, Yichen Peng, Mingyang Su, You Zhou, Naoya Iwamoto, Bo Zheng, Michael J. Black
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose EMAGE, a framework to generate full-body human gestures from audio
and masked gestures, encompassing facial, local body, hands, and global
movements. To achieve this, we first introduce BEATX (BEAT-SMPLX-FLAME), a new
mesh-level holistic co-speech dataset. BEATX combines MoShed SMPLX body with
FLAME head parameters and further refines the modeling of head, neck, and
finger movements, offering a community-standardized, high-quality 3D motion
captured dataset. EMAGE leverages masked body gesture priors during training to
boost inference performance. It involves a Masked Audio Gesture Transformer,
facilitating joint training on audio-to-gesture generation and masked gesture
reconstruction to effectively encode audio and body gesture hints. Encoded body
hints from masked gestures are then separately employed to generate facial and
body movements. Moreover, EMAGE adaptively merges speech features from the
audio's rhythm and content and utilizes four compositional VQ-VAEs to enhance
the results' fidelity and diversity. Experiments demonstrate that EMAGE
generates holistic gestures with state-of-the-art performance and is flexible
in accepting predefined spatial-temporal gesture inputs, generating complete,
audio-synchronized results. Our code and dataset are available at
https://pantomatrix.github.io/EMAGE/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://pantomatrix.github.io/EMAGE/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Moving Camera Pedestrian Tracking with a New <span class="highlight-title">Dataset</span> and Global
  Link Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11035v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11035v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanting Zhang, Shuanghong Wang, Qingxiang Wang, Cairong Yan, Rui Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring driving safety for autonomous vehicles has become increasingly
crucial, highlighting the need for systematic tracking of pedestrians on the
road. Most vehicles are equipped with visual sensors, however, the large-scale
visual dataset from different agents has not been well studied yet. Basically,
most of the multi-target multi-camera (MTMC) tracking systems are composed of
two modules: single camera tracking (SCT) and inter-camera tracking (ICT). To
reliably coordinate between them, MTMC tracking has been a very complicated
task, while tracking across multi-moving cameras makes it even more
challenging. In this paper, we focus on multi-target multi-moving camera
(MTMMC) tracking, which is attracting increasing attention from the research
community. Observing there are few datasets for MTMMC tracking, we collect a
new dataset, called Multi-Moving Camera Track (MMCT), which contains sequences
under various driving scenarios. To address the common problems of identity
switch easily faced by most existing SCT trackers, especially for moving
cameras due to ego-motion between the camera and targets, a lightweight
appearance-free global link model, called Linker, is proposed to mitigate the
identity switch by associating two disjoint tracklets of the same target into a
complete trajectory within the same camera. Incorporated with Linker, existing
SCT trackers generally obtain a significant improvement. Moreover, a strong
baseline approach of re-identification (Re-ID) is effectively incorporated to
extract robust appearance features under varying surroundings for pedestrian
association across moving cameras for ICT, resulting in a much improved MTMMC
tracking system, which can constitute a step further towards coordinated mining
of multiple moving cameras. The dataset is available at
https://github.com/dhu-mmct/DHU-MMCT}{https://github.com/dhu-mmct/DHU-MMCT .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLIP-based Synergistic Knowledge Transfer for Text-based Person
  Retrieval <span class="chip">ICASSP2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09496v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09496v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yating Liu, Yaowei Li, Zimo Liu, Wenming Yang, Yaowei Wang, Qingmin Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-based Person Retrieval (TPR) aims to retrieve the target person images
given a textual query. The primary challenge lies in bridging the substantial
gap between vision and language modalities, especially when dealing with
limited large-scale datasets. In this paper, we introduce a CLIP-based
Synergistic Knowledge Transfer (CSKT) approach for TPR. Specifically, to
explore the CLIP's knowledge on input side, we first propose a Bidirectional
Prompts Transferring (BPT) module constructed by text-to-image and
image-to-text bidirectional prompts and coupling projections. Secondly, Dual
Adapters Transferring (DAT) is designed to transfer knowledge on output side of
Multi-Head Attention (MHA) in vision and language. This synergistic two-way
collaborative mechanism promotes the early-stage feature fusion and efficiently
exploits the existing knowledge of CLIP. CSKT outperforms the state-of-the-art
approaches across three benchmark datasets when the training parameters merely
account for 7.4% of the entire model, demonstrating its remarkable efficiency,
effectiveness and generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP2024(accepted). minor typos revision compared to version 1 in
  arxiv</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Scale Memory Comparison for Zero-/Few-Shot Anomaly Detection <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.04789v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.04789v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoqin Huang, Aofan Jiang, Ya Zhang, Yanfeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection has gained considerable attention due to its broad range of
applications, particularly in industrial defect detection. To address the
challenges of data collection, researchers have introduced zero-/few-shot
anomaly detection techniques that require minimal normal images for each
category. However, complex industrial scenarios often involve multiple objects,
presenting a significant challenge. In light of this, we propose a
straightforward yet powerful multi-scale memory comparison framework for
zero-/few-shot anomaly detection. Our approach employs a global memory bank to
capture features across the entire image, while an individual memory bank
focuses on simplified scenes containing a single object. The efficacy of our
method is validated by its remarkable achievement of 4th place in the zero-shot
track and 2nd place in the few-shot track of the Visual Anomaly and Novelty
Detection (VAND) competition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>VAND Runner-up Winner in CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Risk-optimized Outlier Removal for Robust 3D Point Cloud Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.10875v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.10875v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinke Li, Junchi Lu, Henghui Ding, Changsheng Sun, Joey Tianyi Zhou, Chee Yeow Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growth of 3D sensing technology, deep learning system for 3D point
clouds has become increasingly important, especially in applications like
autonomous vehicles where safety is a primary concern. However, there are also
growing concerns about the reliability of these systems when they encounter
noisy point clouds, whether occurring naturally or introduced with malicious
intent. This paper highlights the challenges of point cloud classification
posed by various forms of noise, from simple background noise to malicious
backdoor attacks that can intentionally skew model predictions. While there's
an urgent need for optimized point cloud denoising, current point outlier
removal approaches, an essential step for denoising, rely heavily on
handcrafted strategies and are not adapted for higher-level tasks, such as
classification. To address this issue, we introduce an innovative point outlier
cleansing method that harnesses the power of downstream classification models.
By employing gradient-based attribution analysis, we define a novel concept:
point risk. Drawing inspiration from tail risk minimization in finance, we
recast the outlier removal process as an optimization problem, named PointCVaR.
Extensive experiments show that our proposed technique not only robustly
filters diverse point cloud outliers but also consistently and significantly
enhances existing robust methods for point cloud classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hybrid Internal Model: Learning Agile Legged Locomotion with Simulated
  Robot Response 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11460v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11460v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junfeng Long, Zirui Wang, Quanyi Li, Jiawei Gao, Liu Cao, Jiangmiao Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust locomotion control depends on accurate state estimations. However, the
sensors of most legged robots can only provide partial and noisy observations,
making the estimation particularly challenging, especially for external states
like terrain frictions and elevation maps. Inspired by the classical Internal
Model Control principle, we consider these external states as disturbances and
introduce Hybrid Internal Model (HIM) to estimate them according to the
response of the robot. The response, which we refer to as the hybrid internal
embedding, contains the robot's explicit velocity and implicit stability
representation, corresponding to two primary goals for locomotion tasks:
explicitly tracking velocity and implicitly maintaining stability. We use
contrastive learning to optimize the embedding to be close to the robot's
successor state, in which the response is naturally embedded. HIM has several
appealing benefits: It only needs the robot's proprioceptions, i.e., those from
joint encoders and IMU as observations. It innovatively maintains consistent
observations between simulation reference and reality that avoids information
loss in mimicking learning. It exploits batch-level information that is more
robust to noises and keeps better sample efficiency. It only requires 1 hour of
training on an RTX 4090 to enable a quadruped robot to traverse any terrain
under any disturbances. A wealth of real-world experiments demonstrates its
agility, even in high-difficulty tasks and cases never occurred during the
training process, revealing remarkable open-world generalizability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Use 1 hour to train a quadruped robot capable of traversing any
  terrain under any disturbances in the open world, Project Page:
  https://github.com/OpenRobotLab/HIMLoco</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Global Feature Pyramid Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11231v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11231v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weilin Xiao, Ming Xu, Yonggui Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The visual feature pyramid has proven its effectiveness and efficiency in
target detection tasks. Yet, current methodologies tend to overly emphasize
inter-layer feature interaction, neglecting the crucial aspect of intra-layer
feature adjustment. Experience underscores the significant advantages of
intra-layer feature interaction in enhancing target detection tasks. While some
approaches endeavor to learn condensed intra-layer feature representations
using attention mechanisms or visual transformers, they overlook the
incorporation of global information interaction. This oversight results in
increased false detections and missed targets.To address this critical issue,
this paper introduces the Global Feature Pyramid Network (GFPNet), an augmented
version of PAFPN that integrates global information for enhanced target
detection. Specifically, we leverage a lightweight MLP to capture global
feature information, utilize the VNC encoder to process these features, and
employ a parallel learnable mechanism to extract intra-layer features from the
input image. Building on this foundation, we retain the PAFPN method to
facilitate inter-layer feature interaction, extracting rich feature details
across various levels.Compared to conventional feature pyramids, GFPN not only
effectively focuses on inter-layer feature information but also captures global
feature details, fostering intra-layer feature interaction and generating a
more comprehensive and impactful feature representation. GFPN consistently
demonstrates performance improvements over object detection baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>dataset not open</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reliable Joint Segmentation of Retinal Edema Lesions in OCT Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.00330v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.00330v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Wang, Kai Yu, Chun-Mei Feng, Ke Zou, Yanyu Xu, Qingquan Meng, Rick Siow Mong Goh, Yong Liu, Huazhu Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Focusing on the complicated pathological features, such as blurred
boundaries, severe scale differences between symptoms, background noise
interference, etc., in the task of retinal edema lesions joint segmentation
from OCT images and enabling the segmentation results more reliable. In this
paper, we propose a novel reliable multi-scale wavelet-enhanced transformer
network, which can provide accurate segmentation results with reliability
assessment. Specifically, aiming at improving the model's ability to learn the
complex pathological features of retinal edema lesions in OCT images, we
develop a novel segmentation backbone that integrates a wavelet-enhanced
feature extractor network and a multi-scale transformer module of our newly
designed. Meanwhile, to make the segmentation results more reliable, a novel
uncertainty segmentation head based on the subjective logical evidential theory
is introduced to generate the final segmentation results with a corresponding
overall uncertainty evaluation score map. We conduct comprehensive experiments
on the public database of AI-Challenge 2018 for retinal edema lesions
segmentation, and the results show that our proposed method achieves better
segmentation accuracy with a high degree of reliability as compared to other
state-of-the-art segmentation approaches. The code will be released on:
https://github.com/LooKing9218/ReliableRESeg.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Improving algorithm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deshadow-Anything: When Segment Anything Model Meets Zero-shot shadow
  removal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11715v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11715v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Feng Zhang, Tian Yi Song, Jia Wei Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segment Anything (SAM), an advanced universal image segmentation model
trained on an expansive visual dataset, has set a new benchmark in image
segmentation and computer vision. However, it faced challenges when it came to
distinguishing between shadows and their backgrounds. To address this, we
developed Deshadow-Anything, considering the generalization of large-scale
datasets, and we performed Fine-tuning on large-scale datasets to achieve image
shadow removal. The diffusion model can diffuse along the edges and textures of
an image, helping to remove shadows while preserving the details of the image.
Furthermore, we design Multi-Self-Attention Guidance (MSAG) and adaptive input
perturbation (DDPM-AIP) to accelerate the iterative training speed of
diffusion. Experiments on shadow removal tasks demonstrate that these methods
can effectively improve image restoration performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>it need revised</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SASSL: Enhancing <span class="highlight-title">Self-Supervised</span> Learning via Neural Style Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.01187v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.01187v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renan A. Rojas-Gomez, Karan Singhal, Ali Etemad, Alex Bijamov, Warren R. Morningstar, Philip Andrew Mansfield
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning relies heavily on data augmentation to extract
meaningful representations from unlabeled images. While existing
state-of-the-art augmentation pipelines incorporate a wide range of primitive
transformations, these often disregard natural image structure. Thus, augmented
samples can exhibit degraded semantic information and low stylistic diversity,
affecting downstream performance of self-supervised representations. To
overcome this, we propose SASSL: Style Augmentations for Self Supervised
Learning, a novel augmentation technique based on Neural Style Transfer. The
method decouples semantic and stylistic attributes in images and applies
transformations exclusively to the style while preserving content, generating
diverse augmented samples that better retain their semantic properties.
Experimental results show our technique achieves a top-1 classification
performance improvement of more than 2% on ImageNet compared to the
well-established MoCo v2. We also measure transfer learning performance across
five diverse datasets, observing significant improvements of up to 3.75%. Our
experiments indicate that decoupling style from content information and
transferring style across datasets to diversify augmentations can significantly
improve downstream performance of self-supervised representations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large-scale data extraction from the UNOS organ donor documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15752v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15752v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marek Rychlik, Bekir Tanriover, Yan Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scope of our study is all UNOS data of the USA organ donors since 2008.
The data is not analyzable in a large scale in the past because it was captured
in PDF documents known as "Attachments", whereby every donor is represented by
dozens of PDF documents in heterogenous formats. To make the data analyzable,
one needs to convert the content inside these PDFs to an analyzable data
format, such as a standard SQL database. In this paper we will focus on 2022
UNOS data comprised of $\approx 400,000$ PDF documents spanning millions of
pages. The totality of UNOS data covers 15 years (2008--20022) and our results
will be quickly extended to the entire data. Our method captures a portion of
the data in DCD flowsheets, kidney perfusion data, and data captured during
patient hospital stay (e.g. vital signs, ventilator settings, etc.). The
current paper assumes that the reader is familiar with the content of the UNOS
data. The overview of the types of data and challenges they present is a
subject of another paper. Here we focus on demonstrating that the goal of
building a comprehensive, analyzable database from UNOS documents is an
attainable task, and we provide an overview of our methodology. The project
resulted in datasets by far larger than previously available even in this
preliminary phase.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Test Statistic Estimation-based Approach for Establishing
  Self-interpretable CNN-based Binary Classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06876v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06876v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sourya Sengupta, Mark A. Anastasio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interpretability is highly desired for deep neural network-based classifiers,
especially when addressing high-stake decisions in medical imaging. Commonly
used post-hoc interpretability methods have the limitation that they can
produce plausible but different interpretations of a given model, leading to
ambiguity about which one to choose. To address this problem, a novel
decision-theory-inspired approach is investigated to establish a
self-interpretable model, given a pre-trained deep binary black-box medical
image classifier. This approach involves utilizing a self-interpretable
encoder-decoder model in conjunction with a single-layer fully connected
network with unity weights. The model is trained to estimate the test statistic
of the given trained black-box deep binary classifier to maintain a similar
accuracy. The decoder output image, referred to as an equivalency map, is an
image that represents a transformed version of the to-be-classified image that,
when processed by the fixed fully connected layer, produces the same test
statistic value as the original classifier. The equivalency map provides a
visualization of the transformed image features that directly contribute to the
test statistic value and, moreover, permits quantification of their relative
contributions. Unlike the traditional post-hoc interpretability methods, the
proposed method is self-interpretable, quantitative. Detailed quantitative and
qualitative analyses have been performed with three different medical image
binary classification tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05134v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05134v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Grimal, Hervé Le Borgne, Olivier Ferret, Julien Tourille
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The progress in the generation of synthetic images has made it crucial to
assess their quality. While several metrics have been proposed to assess the
rendering of images, it is crucial for Text-to-Image (T2I) models, which
generate images based on a prompt, to consider additional aspects such as to
which extent the generated image matches the important content of the prompt.
Moreover, although the generated images usually result from a random starting
point, the influence of this one is generally not considered. In this article,
we propose a new metric based on prompt templates to study the alignment
between the content specified in the prompt and the corresponding generated
images. It allows us to better characterize the alignment in terms of the type
of the specified objects, their number, and their color. We conducted a study
on several recent T2I models about various aspects. An additional interesting
result we obtained with our approach is that image quality can vary drastically
depending on the noise used as a seed for the images. We also quantify the
influence of the number of concepts in the prompt, their order as well as their
(color) attributes. Finally, our method allows us to identify some seeds that
produce better images than others, opening novel directions of research on this
understudied topic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Consensus and Subjectivity of Skin Tone Annotation for ML Fairness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.09073v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.09073v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Candice Schumann, Gbolahan O. Olanubi, Auriel Wright, Ellis Monk Jr., Courtney Heldreth, Susanna Ricco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding different human attributes and how they affect model behavior
may become a standard need for all model creation and usage, from traditional
computer vision tasks to the newest multimodal generative AI systems. In
computer vision specifically, we have relied on datasets augmented with
perceived attribute signals (e.g., gender presentation, skin tone, and age) and
benchmarks enabled by these datasets. Typically labels for these tasks come
from human annotators. However, annotating attribute signals, especially skin
tone, is a difficult and subjective task. Perceived skin tone is affected by
technical factors, like lighting conditions, and social factors that shape an
annotator's lived experience. This paper examines the subjectivity of skin tone
annotation through a series of annotation experiments using the Monk Skin Tone
(MST) scale, a small pool of professional photographers, and a much larger pool
of trained crowdsourced annotators. Along with this study we release the Monk
Skin Tone Examples (MST-E) dataset, containing 1515 images and 31 videos spread
across the full MST scale. MST-E is designed to help train human annotators to
annotate MST effectively. Our study shows that annotators can reliably annotate
skin tone in a way that aligns with an expert in the MST scale, even under
challenging environmental conditions. We also find evidence that annotators
from different geographic regions rely on different mental models of MST
categories resulting in annotations that systematically vary across regions.
Given this, we advise practitioners to use a diverse set of annotators and a
higher replication count for each image when annotating skin tone for fairness
research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-Age Contrastive Learning for Age-Invariant Face Recognition <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11195v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11195v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyi Wang, Victor Sanchez, Chang-Tsun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-age facial images are typically challenging and expensive to collect,
making noise-free age-oriented datasets relatively small compared to
widely-used large-scale facial datasets. Additionally, in real scenarios,
images of the same subject at different ages are usually hard or even
impossible to obtain. Both of these factors lead to a lack of supervised data,
which limits the versatility of supervised methods for age-invariant face
recognition, a critical task in applications such as security and biometrics.
To address this issue, we propose a novel semi-supervised learning approach
named Cross-Age Contrastive Learning (CACon). Thanks to the identity-preserving
power of recent face synthesis models, CACon introduces a new contrastive
learning method that leverages an additional synthesized sample from the input
image. We also propose a new loss function in association with CACon to perform
contrastive learning on a triplet of samples. We demonstrate that our method
not only achieves state-of-the-art performance in homogeneous-dataset
experiments on several age-invariant face recognition benchmarks but also
outperforms other methods by a large margin in cross-dataset experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TREC iKAT 2023: The Interactive Knowledge Assistance Track <span class="highlight-title">Overview</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Aliannejadi, Zahra Abbasiantaeb, Shubham Chatterjee, Jeffery Dalton, Leif Azzopardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational Information Seeking stands as a pivotal research area with
significant contributions from previous works. The TREC Interactive Knowledge
Assistance Track (iKAT) builds on the foundational work of the TREC
Conversational Assistance Track (CAsT). However, iKAT distinctively emphasizes
the creation and research of conversational search agents that adapt responses
based on user's prior interactions and present context. The challenge lies in
enabling Conversational Search Agents (CSA) to incorporate this personalized
context to efficiency and effectively guide users through the relevant
information to them. iKAT also emphasizes decisional search tasks, where users
sift through data and information to weigh up options in order to reach a
conclusion or perform an action. These tasks, prevalent in everyday
information-seeking decisions -- be it related to travel, health, or shopping
-- often revolve around a subset of high-level information operators where
queries or questions about the information space include: finding options,
comparing options, identifying the pros and cons of options, etc. Given the
different personas and their information need (expressed through the sequence
of questions), diverse conversation trajectories will arise -- because the
answers to these similar queries will be very different. In this paper, we
report on the first year of TREC iKAT, describing the task, topics, data
collection, and evaluation framework. We further review the submissions and
summarize the findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TREC 2023 Overview Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Caseformer: <span class="highlight-title">Pre-train</span>ing for Legal Case Retrieval Based on Inter-Case
  Distinctions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.00333v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.00333v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihang Su, Qingyao Ai, Yueyue Wu, Yixiao Ma, Haitao Li, Yiqun Liu, Zhijing Wu, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Legal case retrieval aims to help legal workers find relevant cases related
to their cases at hand, which is important for the guarantee of fairness and
justice in legal judgments. While recent advances in neural retrieval methods
have significantly improved the performance of open-domain retrieval tasks
(e.g., Web search), their advantages have not been observed in legal case
retrieval due to their thirst for annotated data. As annotating large-scale
training data in legal domains is prohibitive due to the need for domain
expertise, traditional search techniques based on lexical matching such as
TF-IDF, BM25, and Query Likelihood are still prevalent in legal case retrieval
systems. While previous studies have designed several pre-training methods for
IR models in open-domain tasks, these methods are usually suboptimal in legal
case retrieval because they cannot understand and capture the key knowledge and
data structures in the legal corpus. To this end, we propose a novel
pre-training framework named Caseformer that enables the pre-trained models to
learn legal knowledge and domain-specific relevance information in legal case
retrieval without any human-labeled data. Through three unsupervised learning
tasks, Caseformer is able to capture the special language, document structure,
and relevance patterns of legal case documents, making it a strong backbone for
downstream legal case retrieval tasks. Experimental results show that our model
has achieved state-of-the-art performance in both zero-shot and full-data
fine-tuning settings. Also, experiments on both Chinese and English legal
datasets demonstrate that the effectiveness of Caseformer is
language-independent in legal case retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Search Model: Redefining Search Stack in the Era of LLMs <span class="chip">SIGIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14587v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14587v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern search engines are built on a stack of different components, including
query understanding, retrieval, multi-stage ranking, and question answering,
among others. These components are often optimized and deployed independently.
In this paper, we introduce a novel conceptual framework called large search
model, which redefines the conventional search stack by unifying search tasks
with one large language model (LLM). All tasks are formulated as autoregressive
text generation problems, allowing for the customization of tasks through the
use of natural language prompts. This proposed framework capitalizes on the
strong language understanding and reasoning capabilities of LLMs, offering the
potential to enhance search result quality while simultaneously simplifying the
existing cumbersome search stack. To substantiate the feasibility of this
framework, we present a series of proof-of-concept experiments and discuss the
potential challenges associated with implementing this approach within
real-world search systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGIR Forum, Vol. 57 No. 2 - December 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarial Item Promotion on Visually-Aware Recommender Systems by
  Guided Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.15826v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.15826v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lijian Chen, Wei Yuan, Tong Chen, Guanhua Ye, Quoc Viet Hung Nguyen, Hongzhi Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visually-aware recommender systems have found widespread application in
domains where visual elements significantly contribute to the inference of
users' potential preferences. While the incorporation of visual information
holds the promise of enhancing recommendation accuracy and alleviating the
cold-start problem, it is essential to point out that the inclusion of item
images may introduce substantial security challenges. Some existing works have
shown that the item provider can manipulate item exposure rates to its
advantage by constructing adversarial images. However, these works cannot
reveal the real vulnerability of visually-aware recommender systems because (1)
The generated adversarial images are markedly distorted, rendering them easily
detectable by human observers; (2) The effectiveness of the attacks is
inconsistent and even ineffective in some scenarios. To shed light on the real
vulnerabilities of visually-aware recommender systems when confronted with
adversarial images, this paper introduces a novel attack method, IPDGI (Item
Promotion by Diffusion Generated Image). Specifically, IPDGI employs a guided
diffusion model to generate adversarial samples designed to deceive
visually-aware recommender systems. Taking advantage of accurately modeling
benign images' distribution by diffusion models, the generated adversarial
images have high fidelity with original images, ensuring the stealth of our
IPDGI. To demonstrate the effectiveness of our proposed methods, we conduct
extensive experiments on two commonly used e-commerce recommendation datasets
(Amazon Beauty and Amazon Baby) with several typical visually-aware recommender
systems. The experimental results show that our attack method has a significant
improvement in both the performance of promoting the long-tailed (i.e.,
unpopular) items and the quality of generated adversarial images.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">105</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Play Fine-Tuning Converts Weak Language Models to Strong Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, Quanquan Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Harnessing the power of human-annotated data through Supervised Fine-Tuning
(SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we
delve into the prospect of growing a strong LLM out of a weak one without the
need for acquiring additional human-annotated data. We propose a new
fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a
supervised fine-tuned model. At the heart of SPIN lies a self-play mechanism,
where the LLM refines its capability by playing against instances of itself.
More specifically, the LLM generates its own training data from its previous
iterations, refining its policy by discerning these self-generated responses
from those obtained from human-annotated data. Our method progressively
elevates the LLM from a nascent model to a formidable one, unlocking the full
potential of human-annotated demonstration data for SFT. Theoretically, we
prove that the global optimum to the training objective function of our method
is achieved only when the LLM policy aligns with the target data distribution.
Empirically, we evaluate our method on several benchmark datasets including the
HuggingFace Open LLM Leaderboard, MT-Bench, and datasets from Big-Bench. Our
results show that SPIN can significantly improve the LLM's performance across a
variety of benchmarks and even outperform models trained through direct
preference optimization (DPO) supplemented with extra GPT-4 preference data.
This sheds light on the promise of self-play, enabling the achievement of
human-level performance in LLMs without the need for expert opponents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 6 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Autoregressive Text-to-Graph Framework for Joint Entity and Relation
  Extraction <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zaratiana Urchade, Nadi Tomeh, Pierre Holat, Thierry Charnois
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel method for joint entity and relation
extraction from unstructured text by framing it as a conditional sequence
generation problem. In contrast to conventional generative information
extraction models that are left-to-right token-level generators, our approach
is \textit{span-based}. It generates a linearized graph where nodes represent
text spans and edges represent relation triplets. Our method employs a
transformer encoder-decoder architecture with pointing mechanism on a dynamic
vocabulary of spans and relation types. Our model can capture the structural
characteristics and boundaries of entities and relations through span
representations while simultaneously grounding the generated output in the
original text thanks to the pointing mechanism. Evaluation on benchmark
datasets validates the effectiveness of our approach, demonstrating competitive
results. Code is available at https://github.com/urchade/ATG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01325v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01325v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen, Xia Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work elicits LLMs' inherent ability to handle long contexts without
fine-tuning. The limited length of the training sequence during training may
limit the application of Large Language Models (LLMs) on long input sequences
for inference. In this work, we argue that existing LLMs themselves have
inherent capabilities for handling long contexts. Based on this argument, we
suggest extending LLMs' context window by themselves to fully utilize the
inherent ability.We propose Self-Extend to stimulate LLMs' long context
handling potential. The basic idea is to construct bi-level attention
information: the group level and the neighbor level. The two levels are
computed by the original model's self-attention, which means the proposed does
not require any training. With only four lines of code modification, the
proposed method can effortlessly extend existing LLMs' context window without
any fine-tuning. We conduct comprehensive experiments and the results show that
the proposed method can effectively extend existing LLMs' context window's
length.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning solutions to some toy constrained optimization problems in
  infinite dimensional Hil<span class="highlight-title">bert</span> spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01306v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01306v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pinak Mandal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we present deep learning implementations of two popular
theoretical constrained optimization algorithms in infinite dimensional Hilbert
spaces, namely, the penalty and the augmented Lagrangian methods. We test these
algorithms on some toy problems originating in either calculus of variations or
physics. We demonstrate that both methods are able to produce decent
approximations for the test problems and are comparable in terms of different
errors. Leveraging the common occurrence of the Lagrange multiplier update rule
being computationally less expensive than solving subproblems in the penalty
method, we achieve significant speedups in cases when the output of the
constraint function is itself a function.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrating Edges into U-Net Models with Explainable Activation Maps for
  Brain Tumor Segmentation using MR Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01303v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01303v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subin Sahayam, Umarani Jayaraman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manual delineation of tumor regions from magnetic resonance (MR) images is
time-consuming, requires an expert, and is prone to human error. In recent
years, deep learning models have been the go-to approach for the segmentation
of brain tumors. U-Net and its' variants for semantic segmentation of medical
images have achieved good results in the literature. However, U-Net and its'
variants tend to over-segment tumor regions and may not accurately segment the
tumor edges. The edges of the tumor are as important as the tumor regions for
accurate diagnosis, surgical precision, and treatment planning. In the proposed
work, the authors aim to extract edges from the ground truth using a
derivative-like filter followed by edge reconstruction to obtain an edge ground
truth in addition to the brain tumor ground truth. Utilizing both ground
truths, the author studies several U-Net and its' variant architectures with
and without tumor edges ground truth as a target along with the tumor ground
truth for brain tumor segmentation. The author used the BraTS2020 benchmark
dataset to perform the study and the results are tabulated for the dice and
Hausdorff95 metrics. The mean and median metrics are calculated for the whole
tumor (WT), tumor core (TC), and enhancing tumor (ET) regions. Compared to the
baseline U-Net and its variants, the models that learned edges along with the
tumor regions performed well in core tumor regions in both training and
validation datasets. The improved performance of edge-trained models trained on
baseline models like U-Net and V-Net achieved performance similar to baseline
state-of-the-art models like Swin U-Net and hybrid MR-U-Net. The edge-target
trained models are capable of generating edge maps that can be useful for
treatment planning. Additionally, for further explainability of the results,
the activation map generated by the hybrid MR-U-Net has been studied.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Sparse Least Absolute Deviation Regression with Differential
  Privacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weidong Liu, Xiaojun Mao, Xiaofei Zhang, Xin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, privacy-preserving machine learning algorithms have
attracted increasing attention because of their important applications in many
scientific fields. However, in the literature, most privacy-preserving
algorithms demand learning objectives to be strongly convex and Lipschitz
smooth, which thus cannot cover a wide class of robust loss functions (e.g.,
quantile/least absolute loss). In this work, we aim to develop a fast
privacy-preserving learning solution for a sparse robust regression problem.
Our learning loss consists of a robust least absolute loss and an $\ell_1$
sparse penalty term. To fast solve the non-smooth loss under a given privacy
budget, we develop a Fast Robust And Privacy-Preserving Estimation (FRAPPE)
algorithm for least absolute deviation regression. Our algorithm achieves a
fast estimation by reformulating the sparse LAD problem as a penalized least
square estimation problem and adopts a three-stage noise injection to guarantee
the $(\epsilon,\delta)$-differential privacy. We show that our algorithm can
achieve better privacy and statistical accuracy trade-off compared with the
state-of-the-art privacy-preserving regression algorithms. In the end, we
conduct experiments to verify the efficiency of our proposed FRAPPE algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Transactions on Information Forensics and Security, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive Study of Knowledge Editing for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, Siyuan Cheng, Ziwen Xu, Xin Xu, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Lei Liang, Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown extraordinary capabilities in
understanding and generating text that closely mirrors human communication.
However, a primary limitation lies in the significant computational demands
during training, arising from their extensive parameterization. This challenge
is further intensified by the dynamic nature of the world, necessitating
frequent updates to LLMs to correct outdated information or integrate new
knowledge, thereby ensuring their continued relevance. Note that many
applications demand continual model adjustments post-training to address
deficiencies or undesirable behaviors. There is an increasing interest in
efficient, lightweight methods for on-the-fly model modifications. To this end,
recent years have seen a burgeoning in the techniques of knowledge editing for
LLMs, which aim to efficiently modify LLMs' behaviors within specific domains
while preserving overall performance across various inputs. In this paper, we
first define the knowledge editing problem and then provide a comprehensive
review of cutting-edge approaches. Drawing inspiration from educational and
cognitive research theories, we propose a unified categorization criterion that
classifies knowledge editing methods into three groups: resorting to external
knowledge, merging knowledge into the model, and editing intrinsic knowledge.
Furthermore, we introduce a new benchmark, KnowEdit, for a comprehensive
empirical evaluation of representative knowledge editing approaches.
Additionally, we provide an in-depth analysis of knowledge location, which can
provide a deeper understanding of the knowledge structures inherent within
LLMs. Finally, we discuss several potential applications of knowledge editing,
outlining its broad and impactful implications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing work; 50 pages, 265 citations; benchmark is available at
  https://huggingface.co/datasets/zjunlp/KnowEdit; code is available at
  https://github.com/zjunlp/EasyEdit; paper list is available at
  https://github.com/zjunlp/KnowledgeEditingPapers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GEqO: ML-Accelerated Semantic Equivalence Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01280v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01280v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brandon Haynes, Rana Alotaibi, Anna Pavlenko, Jyoti Leeka, Alekh Jindal, Yuanyuan Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large scale analytics engines have become a core dependency for modern
data-driven enterprises to derive business insights and drive actions. These
engines support a large number of analytic jobs processing huge volumes of data
on a daily basis, and workloads are often inundated with overlapping
computations across multiple jobs. Reusing common computation is crucial for
efficient cluster resource utilization and reducing job execution time.
Detecting common computation is the first and key step for reducing this
computational redundancy. However, detecting equivalence on large-scale
analytics engines requires efficient and scalable solutions that are fully
automated. In addition, to maximize computation reuse, equivalence needs to be
detected at the semantic level instead of just the syntactic level (i.e., the
ability to detect semantic equivalence of seemingly different-looking queries).
Unfortunately, existing solutions fall short of satisfying these requirements.
  In this paper, we take a major step towards filling this gap by proposing
GEqO, a portable and lightweight machine-learning-based framework for
efficiently identifying semantically equivalent computations at scale. GEqO
introduces two machine-learning-based filters that quickly prune out
nonequivalent subexpressions and employs a semi-supervised learning feedback
loop to iteratively improve its model with an intelligent sampling mechanism.
Further, with its novel database-agnostic featurization method, GEqO can
transfer the learning from one workload and database to another. Our extensive
empirical evaluation shows that, on TPC-DS-like queries, GEqO yields
significant performance gains-up to 200x faster than automated verifiers-and
finds up to 2x more equivalences than optimizer and signature-based equivalence
detection approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning-based agricultural management in partially observable
  environments subject to climate variability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoan Wang, Shaoping Xiao, Junchao Li, Jun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agricultural management, with a particular focus on fertilization strategies,
holds a central role in shaping crop yield, economic profitability, and
environmental sustainability. While conventional guidelines offer valuable
insights, their efficacy diminishes when confronted with extreme weather
conditions, such as heatwaves and droughts. In this study, we introduce an
innovative framework that integrates Deep Reinforcement Learning (DRL) with
Recurrent Neural Networks (RNNs). Leveraging the Gym-DSSAT simulator, we train
an intelligent agent to master optimal nitrogen fertilization management.
Through a series of simulation experiments conducted on corn crops in Iowa, we
compare Partially Observable Markov Decision Process (POMDP) models with Markov
Decision Process (MDP) models. Our research underscores the advantages of
utilizing sequential observations in developing more efficient nitrogen input
policies. Additionally, we explore the impact of climate variability,
particularly during extreme weather events, on agricultural outcomes and
management. Our findings demonstrate the adaptability of fertilization policies
to varying climate conditions. Notably, a fixed policy exhibits resilience in
the face of minor climate fluctuations, leading to commendable corn yields,
cost-effectiveness, and environmental conservation. However, our study
illuminates the need for agent retraining to acquire new optimal policies under
extreme weather events. This research charts a promising course toward
adaptable fertilization strategies that can seamlessly align with dynamic
climate scenarios, ultimately contributing to the optimization of crop
management practices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Rates of Kernel Ridge Regression under Source Condition in Large
  Dimensions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01270v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01270v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haobo Zhang, Yicheng Li, Weihao Lu, Qian Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by the studies of neural networks (e.g.,the neural tangent kernel
theory), we perform a study on the large-dimensional behavior of kernel ridge
regression (KRR) where the sample size $n \asymp d^{\gamma}$ for some $\gamma >
0$. Given an RKHS $\mathcal{H}$ associated with an inner product kernel defined
on the sphere $\mathbb{S}^{d}$, we suppose that the true function $f_{\rho}^{*}
\in [\mathcal{H}]^{s}$, the interpolation space of $\mathcal{H}$ with source
condition $s>0$. We first determined the exact order (both upper and lower
bound) of the generalization error of kernel ridge regression for the optimally
chosen regularization parameter $\lambda$. We then further showed that when
$0<s\le1$, KRR is minimax optimal; and when $s>1$, KRR is not minimax optimal
(a.k.a. he saturation effect). Our results illustrate that the curves of rate
varying along $\gamma$ exhibit the periodic plateau behavior and the multiple
descent behavior and show how the curves evolve with $s>0$. Interestingly, our
work provides a unified viewpoint of several recent works on kernel regression
in the large-dimensional setting, which correspond to $s=0$ and $s=1$
respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>61 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $f$-Divergence Based Classification: Beyond the Use of Cross-Entropy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicola Novello, Andrea M. Tonello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In deep learning, classification tasks are formalized as optimization
problems solved via the minimization of the cross-entropy. However, recent
advancements in the design of objective functions allow the $f$-divergence
measure to generalize the formulation of the optimization problem for
classification. With this goal in mind, we adopt a Bayesian perspective and
formulate the classification task as a maximum a posteriori probability
problem. We propose a class of objective functions based on the variational
representation of the $f$-divergence, from which we extract a list of five
posterior probability estimators leveraging well-known $f$-divergences. In
addition, driven by the challenge of improving the state-of-the-art approach,
we propose a bottom-up method that leads us to the formulation of a new
objective function (and posterior probability estimator) corresponding to a
novel $f$-divergence referred to as shifted log (SL). First, we theoretically
prove the convergence property of the posterior probability estimators. Then,
we numerically test the set of proposed objective functions in three
application scenarios: toy examples, image data sets, and signal
detection/decoding problems. The analyzed tasks demonstrate the effectiveness
of the proposed estimators and that the SL divergence achieves the highest
classification accuracy in almost all the scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fairness Certification for Natural Language Processing and Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Freiberger, Erik Buchmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Processing (NLP) plays an important role in our daily lives,
particularly due to the enormous progress of Large Language Models (LLM).
However, NLP has many fairness-critical use cases, e.g., as an expert system in
recruitment or as an LLM-based tutor in education. Since NLP is based on human
language, potentially harmful biases can diffuse into NLP systems and produce
unfair results, discriminate against minorities or generate legal issues.
Hence, it is important to develop a fairness certification for NLP approaches.
We follow a qualitative research approach towards a fairness certification for
NLP. In particular, we have reviewed a large body of literature on algorithmic
fairness, and we have conducted semi-structured expert interviews with a wide
range of experts from that area. We have systematically devised six fairness
criteria for NLP, which can be further refined into 18 sub-categories. Our
criteria offer a foundation for operationalizing and testing processes to
certify fairness, both from the perspective of the auditor and the audited
organization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In depth discussion of our results can be found in the Appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Concept Bottleneck Models Obey Locality? <span class="chip">NeurIPS'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01259v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01259v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naveen Raman, Mateo Espinosa Zarlenga, Juyeon Heo, Mateja Jamnik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concept-based learning improves a deep learning model's interpretability by
explaining its predictions via human-understandable concepts. Deep learning
models trained under this paradigm heavily rely on the assumption that neural
networks can learn to predict the presence or absence of a given concept
independently of other concepts. Recent work, however, strongly suggests that
this assumption may fail to hold in Concept Bottleneck Models (CBMs), a
quintessential family of concept-based interpretable architectures. In this
paper, we investigate whether CBMs correctly capture the degree of conditional
independence across concepts when such concepts are localised both spatially,
by having their values entirely defined by a fixed subset of features, and
semantically, by having their values correlated with only a fixed subset of
predefined concepts. To understand locality, we analyse how changes to features
outside of a concept's spatial or semantic locality impact concept predictions.
Our results suggest that even in well-defined scenarios where the presence of a
concept is localised to a fixed feature subspace, or whose semantics are
correlated to a small subset of other concepts, CBMs fail to learn this
locality. These results cast doubt upon the quality of concept representations
learnt by CBMs and strongly suggest that concept-based explanations may be
fragile to changes outside their localities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, accepted at NeurIPS'23 XAI Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Model-Free LQR Control over Rate-Limited Channels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aritra Mitra, Lintao Ye, Vijay Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the success of model-free methods for control design in many problem
settings, it is natural to ask how things will change if realistic
communication channels are utilized for the transmission of gradients or
policies. While the resulting problem has analogies with the formulations
studied under the rubric of networked control systems, the rich literature in
that area has typically assumed that the model of the system is known. As a
step towards bridging the fields of model-free control design and networked
control systems, we ask: \textit{Is it possible to solve basic control problems
- such as the linear quadratic regulator (LQR) problem - in a model-free manner
over a rate-limited channel?} Toward answering this question, we study a
setting where a worker agent transmits quantized policy gradients (of the LQR
cost) to a server over a noiseless channel with a finite bit-rate. We propose a
new algorithm titled Adaptively Quantized Gradient Descent (\texttt{AQGD}), and
prove that above a certain finite threshold bit-rate, \texttt{AQGD} guarantees
exponentially fast convergence to the globally optimal policy, with \textit{no
deterioration of the exponent relative to the unquantized setting}. More
generally, our approach reveals the benefits of adaptive quantization in
preserving fast linear convergence rates, and, as such, may be of independent
interest to the literature on compressed optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contrastive Sequential Interaction Network Learning on Co-Evolving
  Riemannian Spaces <span class="chip">WWW</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Sun, Junda Ye, Jiawei Zhang, Yong Yang, Mingsheng Liu, Feiyang Wang, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The sequential interaction network usually find itself in a variety of
applications, e.g., recommender system. Herein, inferring future interaction is
of fundamental importance, and previous efforts are mainly focused on the
dynamics in the classic zero-curvature Euclidean space. Despite the promising
results achieved by previous methods, a range of significant issues still
largely remains open: On the bipartite nature, is it appropriate to place user
and item nodes in one identical space regardless of their inherent difference?
On the network dynamics, instead of a fixed curvature space, will the
representation spaces evolve when new interactions arrive continuously? On the
learning paradigm, can we get rid of the label information costly to acquire?
To address the aforementioned issues, we propose a novel Contrastive model for
Sequential Interaction Network learning on Co-Evolving RiEmannian spaces,
CSINCERE. To the best of our knowledge, we are the first to introduce a couple
of co-evolving representation spaces, rather than a single or static space, and
propose a co-contrastive learning for the sequential interaction network. In
CSINCERE, we formulate a Cross-Space Aggregation for message-passing across
representation spaces of different Riemannian geometries, and design a Neural
Curvature Estimator based on Ricci curvatures for modeling the space evolvement
over time. Thereafter, we present a Reweighed Co-Contrast between the temporal
views of the sequential network, so that the couple of Riemannian spaces
interact with each other for the interaction prediction without labels.
Empirical results on 5 public datasets show the superiority of CSINCERE over
the state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The extension of ACM WebConf23 (WWW)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Encoding Binary Events from Continuous Time Series in Rooted Trees using
  Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01242v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01242v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Engelhardt Rasmussen, Siv Sørensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Broadband infrastructure owners do not always know how their customers are
connected in the local networks, which are structured as rooted trees. A recent
study is able to infer the topology of a local network using discrete time
series data from the leaves of the tree (customers). In this study we propose a
contrastive approach for learning a binary event encoder from continuous time
series data. As a preliminary result, we show that our approach has some
potential in learning a valuable encoder.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended abstract presented as a poster at the Northern Lights Deep
  Learning Conference 2024 in Troms{\o}, Norway</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Elimination Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Wang, Ge Cheng, Yun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) are widely applied across various domains, yet
they perform poorly in deep layers. Existing research typically attributes this
problem to node over-smoothing, where node representations become
indistinguishable after multiple rounds of propagation. In this paper, we delve
into the neighborhood propagation mechanism of GNNs and discover that the real
root cause of GNNs' performance degradation in deep layers lies in ineffective
neighborhood feature propagation. This propagation leads to an exponential
growth of a node's current representation at every propagation step, making it
extremely challenging to capture valuable dependencies between long-distance
nodes. To address this issue, we introduce Graph Elimination Networks (GENs),
which employ a specific algorithm to eliminate redundancies during neighborhood
propagation. We demonstrate that GENs can enhance nodes' perception of distant
neighborhoods and extend the depth of network propagation. Extensive
experiments show that GENs outperform the state-of-the-art methods on various
graph-level and node-level datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Includes 8 pages of main text and 4 pages of appendices</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Motif-aware Riemannian Graph Neural Network with Generative-Contrastive
  Learning <span class="chip">AAAI24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01232v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01232v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Sun, Zhenhao Huang, Zixi Wang, Feiyang Wang, Hao Peng, Philip Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graphs are typical non-Euclidean data of complex structures. In recent years,
Riemannian graph representation learning has emerged as an exciting alternative
to Euclidean ones. However, Riemannian methods are still in an early stage:
most of them present a single curvature (radius) regardless of structural
complexity, suffer from numerical instability due to the
exponential/logarithmic map, and lack the ability to capture motif regularity.
In light of the issues above, we propose the problem of \emph{Motif-aware
Riemannian Graph Representation Learning}, seeking a numerically stable encoder
to capture motif regularity in a diverse-curvature manifold without labels. To
this end, we present a novel Motif-aware Riemannian model with
Generative-Contrastive learning (MotifRGC), which conducts a minmax game in
Riemannian manifold in a self-supervised manner. First, we propose a new type
of Riemannian GCN (D-GCN), in which we construct a diverse-curvature manifold
by a product layer with the diversified factor, and replace the
exponential/logarithmic map by a stable kernel layer. Second, we introduce a
motif-aware Riemannian generative-contrastive learning to capture motif
regularity in the constructed manifold and learn motif-aware node
representation without external labels. Empirical results show the superiority
of MofitRGC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-Shot Position Debiasing for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01218v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01218v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongkun Liu, Zheng Chen, Mengqi Zhang, Zhaochun Ren, Zhumin Chen, Pengjie Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning has been demonstrated to be an effective method to improve the
domain performance of large language models (LLMs). However, LLMs might fit the
dataset bias and shortcuts for prediction, leading to poor generation
performance. Experimental result shows that LLMs are prone to exhibit position
bias, i.e., leveraging information positioned at the beginning or end, or
specific positional cues within the input. Existing works on mitigating
position bias require external bias knowledge or annotated non-biased samples,
which is unpractical in reality. In this work, we propose a zero-shot position
debiasing (ZOE) framework to mitigate position bias for LLMs. ZOE leverages
unsupervised responses from pre-trained LLMs for debiasing, thus without any
external knowledge or datasets. To improve the quality of unsupervised
responses, we propose a master-slave alignment (MSA) module to prune these
responses. Experiments on eight datasets and five tasks show that ZOE
consistently outperforms existing methods in mitigating four types of position
biases. Besides, ZOE achieves this by sacrificing only a small performance on
biased samples, which is simple and effective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 22 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Whole-examination AI estimation of fetal biometrics from 20-week
  ultrasound scans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01201v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01201v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Venturini, Samuel Budd, Alfonso Farruggia, Robert Wright, Jacqueline Matthew, Thomas G. Day, Bernhard Kainz, Reza Razavi, Jo V. Hajnal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current approach to fetal anomaly screening is based on biometric
measurements derived from individually selected ultrasound images. In this
paper, we introduce a paradigm shift that attains human-level performance in
biometric measurement by aggregating automatically extracted biometrics from
every frame across an entire scan, with no need for operator intervention. We
use a convolutional neural network to classify each frame of an ultrasound
video recording. We then measure fetal biometrics in every frame where
appropriate anatomy is visible. We use a Bayesian method to estimate the true
value of each biometric from a large number of measurements and
probabilistically reject outliers. We performed a retrospective experiment on
1457 recordings (comprising 48 million frames) of 20-week ultrasound scans,
estimated fetal biometrics in those scans and compared our estimates to the
measurements sonographers took during the scan. Our method achieves human-level
performance in estimating fetal biometrics and estimates well-calibrated
credible intervals in which the true biometric value is expected to lie.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 16 figures. Submitted to NPJ digital medicine. For
  associated video file, see
  http://wp.doc.ic.ac.uk/ifind/wp-content/uploads/sites/79/2023/12/realtime.gif</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JMA: a General Algorithm to Craft Nearly Optimal Targeted Adversarial
  Example 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01199v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01199v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benedetta Tondi, Wei Guo, Mauro Barni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most of the approaches proposed so far to craft targeted adversarial examples
against Deep Learning classifiers are highly suboptimal and typically rely on
increasing the likelihood of the target class, thus implicitly focusing on
one-hot encoding settings. In this paper, we propose a more general,
theoretically sound, targeted attack that resorts to the minimization of a
Jacobian-induced MAhalanobis distance (JMA) term, taking into account the
effort (in the input space) required to move the latent space representation of
the input sample in a given direction. The minimization is solved by exploiting
the Wolfe duality theorem, reducing the problem to the solution of a
Non-Negative Least Square (NNLS) problem. The proposed algorithm provides an
optimal solution to a linearized version of the adversarial example problem
originally introduced by Szegedy et al. \cite{szegedy2013intriguing}. The
experiments we carried out confirm the generality of the proposed attack which
is proven to be effective under a wide variety of output encoding schemes.
Noticeably, the JMA attack is also effective in a multi-label classification
scenario, being capable to induce a targeted modification of up to half the
labels in a complex multilabel classification scenario with 20 labels, a
capability that is out of reach of all the attacks proposed so far. As a
further advantage, the JMA attack usually requires very few iterations, thus
resulting more efficient than existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep-ELA: Deep Exploratory Landscape Analysis with <span class="highlight-title">Self-Supervised</span>
  <span class="highlight-title">Pretrain</span>ed <span class="highlight-title">Transformer</span>s for Single- and Multi-Objective Continuous
  Optimization Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moritz Vinzent Seiler, Pascal Kerschke, Heike Trautmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many recent works, the potential of Exploratory Landscape Analysis (ELA)
features to numerically characterize, in particular, single-objective
continuous optimization problems has been demonstrated. These numerical
features provide the input for all kinds of machine learning tasks on
continuous optimization problems, ranging, i.a., from High-level Property
Prediction to Automated Algorithm Selection and Automated Algorithm
Configuration. Without ELA features, analyzing and understanding the
characteristics of single-objective continuous optimization problems would be
impossible.
  Yet, despite their undisputed usefulness, ELA features suffer from several
drawbacks. These include, in particular, (1.) a strong correlation between
multiple features, as well as (2.) its very limited applicability to
multi-objective continuous optimization problems. As a remedy, recent works
proposed deep learning-based approaches as alternatives to ELA. In these works,
e.g., point-cloud transformers were used to characterize an optimization
problem's fitness landscape. However, these approaches require a large amount
of labeled training data.
  Within this work, we propose a hybrid approach, Deep-ELA, which combines (the
benefits of) deep learning and ELA features. Specifically, we pre-trained four
transformers on millions of randomly generated optimization problems to learn
deep representations of the landscapes of continuous single- and
multi-objective optimization problems. Our proposed framework can either be
used out-of-the-box for analyzing single- and multi-objective continuous
optimization problems, or subsequently fine-tuned to various tasks focussing on
algorithm behavior and problem understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Freeze the backbones: A Parameter-Efficient Contrastive Approach to
  Robust Medical Vision-Language <span class="highlight-title">Pre-train</span>ing <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiuming Qin, Che Liu, Sibo Cheng, Yike Guo, Rossella Arcucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern healthcare often utilises radiographic images alongside textual
reports for diagnostics, encouraging the use of Vision-Language Self-Supervised
Learning (VL-SSL) with large pre-trained models to learn versatile medical
vision representations. However, most existing VL-SSL frameworks are trained
end-to-end, which is computation-heavy and can lose vital prior information
embedded in pre-trained encoders. To address both issues, we introduce the
backbone-agnostic Adaptor framework, which preserves medical knowledge in
pre-trained image and text encoders by keeping them frozen, and employs a
lightweight Adaptor module for cross-modal learning. Experiments on medical
image classification and segmentation tasks across three datasets reveal that
our framework delivers competitive performance while cutting trainable
parameters by over 90% compared to current pre-training approaches. Notably,
when fine-tuned with just 1% of data, Adaptor outperforms several
Transformer-based methods trained on full datasets in medical image
segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fundamental Limitation of Semantic Communications: Neural Estimation for
  Rate-Distortion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongxu Li, Jianhao Huang, Chuan Huang, Xiaoqi Qin, Han Zhang, Ping Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the fundamental limit of semantic communications over the
discrete memoryless channel. We consider the scenario to send a semantic source
consisting of an observation state and its corresponding semantic state, both
of which are recovered at the receiver. To derive the performance limitation,
we adopt the semantic rate-distortion function (SRDF) to study the relationship
among the minimum compression rate, observation distortion, semantic
distortion, and channel capacity. For the case with unknown semantic source
distribution, while only a set of the source samples is available, we propose a
neural-network-based method by leveraging the generative networks to learn the
semantic source distribution. Furthermore, for a special case where the
semantic state is a deterministic function of the observation, we design a
cascade neural network to estimate the SRDF. For the case with perfectly known
semantic source distribution, we propose a general Blahut-Arimoto algorithm to
effectively compute the SRDF. Finally, experimental results validate our
proposed algorithms for the scenarios with ideal Gaussian semantic source and
some practical datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quadratic Time-Frequency Analysis of Vibration Signals for Diagnosing
  Bearing Faults 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01172v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01172v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Al-Sa'd, Tuomas Jalonen, Serkan Kiranyaz, Moncef Gabbouj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diagnosis of bearing faults is paramount to reducing maintenance costs and
operational breakdowns. Bearing faults are primary contributors to machine
vibrations, and analyzing their signal morphology offers insights into their
health status. Unfortunately, existing approaches are optimized for controlled
environments, neglecting realistic conditions such as time-varying rotational
speeds and the vibration's non-stationary nature. This paper presents a fusion
of time-frequency analysis and deep learning techniques to diagnose bearing
faults under time-varying speeds and varying noise levels. First, we formulate
the bearing fault-induced vibrations and discuss the link between their
non-stationarity and the bearing's inherent and operational parameters. We also
elucidate quadratic time-frequency distributions and validate their
effectiveness in resolving distinctive dynamic patterns associated with
different bearing faults. Based on this, we design a time-frequency
convolutional neural network (TF-CNN) to diagnose various faults in
rolling-element bearings. Our experimental findings undeniably demonstrate the
superior performance of TF-CNN in comparison to recently developed techniques.
They also assert its versatility in capturing fault-relevant non-stationary
features that couple with speed changes and show its exceptional resilience to
noise, consistently surpassing competing methods across various signal-to-noise
ratios and performance metrics. Altogether, the TF-CNN achieves substantial
accuracy improvements up to 15%, in severe noise conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedQV: Leveraging Quadratic Voting in Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01168v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01168v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyue Chu, Nikolaos Laoutaris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) permits different parties to collaboratively train a
global model without disclosing their respective local labels. A crucial step
of FL, that of aggregating local models to produce the global one, shares many
similarities with public decision-making, and elections in particular. In that
context, a major weakness of FL, namely its vulnerability to poisoning attacks,
can be interpreted as a consequence of the one person one vote (henceforth
1p1v) principle underpinning most contemporary aggregation rules. In this
paper, we propose FedQV, a novel aggregation algorithm built upon the quadratic
voting scheme, recently proposed as a better alternative to 1p1v-based
elections. Our theoretical analysis establishes that FedQV is a truthful
mechanism in which bidding according to one's true valuation is a dominant
strategy that achieves a convergence rate that matches those of
state-of-the-art methods. Furthermore, our empirical analysis using multiple
real-world datasets validates the superior performance of FedQV against
poisoning attacks. It also shows that combining FedQV with unequal voting
``budgets'' according to a reputation score increases its performance benefits
even further. Finally, we show that FedQV can be easily combined with
Byzantine-robust privacy-preserving mechanisms to enhance its robustness
against both poisoning and privacy attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Learning for SAR View Angle Inversion with Differentiable
  SAR Renderer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01165v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01165v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanni Wang, Hecheng Jia, Shilei Fu, Huiping Lin, Feng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The electromagnetic inverse problem has long been a research hotspot. This
study aims to reverse radar view angles in synthetic aperture radar (SAR)
images given a target model. Nonetheless, the scarcity of SAR data, combined
with the intricate background interference and imaging mechanisms, limit the
applications of existing learning-based approaches. To address these
challenges, we propose an interactive deep reinforcement learning (DRL)
framework, where an electromagnetic simulator named differentiable SAR render
(DSR) is embedded to facilitate the interaction between the agent and the
environment, simulating a human-like process of angle prediction. Specifically,
DSR generates SAR images at arbitrary view angles in real-time. And the
differences in sequential and semantic aspects between the view
angle-corresponding images are leveraged to construct the state space in DRL,
which effectively suppress the complex background interference, enhance the
sensitivity to temporal variations, and improve the capability to capture
fine-grained information. Additionally, in order to maintain the stability and
convergence of our method, a series of reward mechanisms, such as memory
difference, smoothing and boundary penalty, are utilized to form the final
reward function. Extensive experiments performed on both simulated and real
datasets demonstrate the effectiveness and robustness of our proposed method.
When utilized in the cross-domain area, the proposed method greatly mitigates
inconsistency between simulated and real domains, outperforming reference
methods significantly.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Train-Free Segmentation in MRI with Cubical Persistent Homology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anton François, Raphaël Tinarrage
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe a new general method for segmentation in MRI scans using
Topological Data Analysis (TDA), offering several advantages over traditional
machine learning approaches. It works in three steps, first identifying the
whole object to segment via automatic thresholding, then detecting a
distinctive subset whose topology is known in advance, and finally deducing the
various components of the segmentation. Although convoking classical ideas of
TDA, such an algorithm has never been proposed separately from deep learning
methods. To achieve this, our approach takes into account, in addition to the
homology of the image, the localization of representative cycles, a piece of
information that seems never to have been exploited in this context. In
particular, it offers the ability to perform segmentation without the need for
large annotated data sets. TDA also provides a more interpretable and stable
framework for segmentation by explicitly mapping topological features to
segmentation components. By adapting the geometric object to be detected, the
algorithm can be adjusted to a wide range of data segmentation challenges. We
carefully study the examples of glioblastoma segmentation in brain MRI, where a
sphere is to be detected, as well as myocardium in cardiac MRI, involving a
cylinder, and cortical plate detection in fetal brain MRI, whose 2D slices are
circles. We compare our method to state-of-the-art algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint, 17 pages, 19 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning-Based Detection for Marker Codes over Insertion and
  Deletion Channels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01155v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01155v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guochen Ma, Xiaopeng Jiao, Jianjun Mu, Hui Han, Yaming Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Marker code is an effective coding scheme to protect data from insertions and
deletions. It has potential applications in future storage systems, such as DNA
storage and racetrack memory. When decoding marker codes, perfect channel state
information (CSI), i.e., insertion and deletion probabilities, are required to
detect insertion and deletion errors. Sometimes, the perfect CSI is not easy to
obtain or the accurate channel model is unknown. Therefore, it is deserved to
develop detecting algorithms for marker code without the knowledge of perfect
CSI. In this paper, we propose two CSI-agnostic detecting algorithms for marker
code based on deep learning. The first one is a model-driven deep learning
method, which deep unfolds the original iterative detecting algorithm of marker
code. In this method, CSI become weights in neural networks and these weights
can be learned from training data. The second one is a data-driven method which
is an end-to-end system based on the deep bidirectional gated recurrent unit
network. Simulation results show that error performances of the proposed
methods are significantly better than that of the original detection algorithm
with CSI uncertainty. Furthermore, the proposed data-driven method exhibits
better error performances than other methods for unknown channel models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PAC-Bayes-Chernoff bounds for unbounded losses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01148v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01148v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ioar Casado, Luis A. Ortega, Andrés R. Masegosa, Aritz Pérez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new high-probability PAC-Bayes oracle bound for unbounded
losses. This result can be understood as a PAC-Bayes version of the Chernoff
bound. The proof technique relies on uniformly bounding the tail of certain
random variable based on the Cram\'er transform of the loss. We highlight two
applications of our main result. First, we show that our bound solves the open
problem of optimizing the free parameter on many PAC-Bayes bounds. Finally, we
show that our approach allows working with flexible assumptions on the loss
function, resulting in novel bounds that generalize previous ones and can be
minimized to obtain Gibbs-like posteriors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HAAQI-Net: A non-intrusive neural music quality assessment model for
  hearing aids 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dyah A. M. G. Wisnu, Epri Pratiwi, Stefano Rini, Ryandhimas E. Zezario, Hsin-Min Wang, Yu Tsao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces HAAQI-Net, a non-intrusive deep learning model for
music quality assessment tailored to hearing aid users. In contrast to
traditional methods like the Hearing Aid Audio Quality Index (HAAQI), HAAQI-Net
utilizes a Bidirectional Long Short-Term Memory (BLSTM) with attention. It
takes an assessed music sample and a hearing loss pattern as input, generating
a predicted HAAQI score. The model employs the pre-trained Bidirectional
Encoder representation from Audio Transformers (BEATs) for acoustic feature
extraction. Comparing predicted scores with ground truth, HAAQI-Net achieves a
Longitudinal Concordance Correlation (LCC) of 0.9257, Spearman's Rank
Correlation Coefficient (SRCC) of 0.9394, and Mean Squared Error (MSE) of
0.0080. Notably, this high performance comes with a substantial reduction in
inference time: from 62.52 seconds (by HAAQI) to 2.71 seconds (by HAAQI-Net),
serving as an efficient music quality assessment model for hearing aid users.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable Adaptive Tree-based Model Selection for Time Series
  Forecasting <span class="chip">ICDM 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01124v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01124v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthias Jakobs, Amal Saadallah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tree-based models have been successfully applied to a wide variety of tasks,
including time series forecasting. They are increasingly in demand and widely
accepted because of their comparatively high level of interpretability.
However, many of them suffer from the overfitting problem, which limits their
application in real-world decision-making. This problem becomes even more
severe in online-forecasting settings where time series observations are
incrementally acquired, and the distributions from which they are drawn may
keep changing over time. In this context, we propose a novel method for the
online selection of tree-based models using the TreeSHAP explainability method
in the task of time series forecasting. We start with an arbitrary set of
different tree-based models. Then, we outline a performance-based ranking with
a coherent design to make TreeSHAP able to specialize the tree-based
forecasters across different regions in the input time series. In this
framework, adequate model selection is performed online, adaptively following
drift detection in the time series. In addition, explainability is supported on
three levels, namely online input importance, model selection, and model output
explanation. An extensive empirical study on various real-world datasets
demonstrates that our method achieves excellent or on-par results in comparison
to the state-of-the-art approaches as well as several baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted and presented at ICDM 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Utilizing Autoregressive Networks for Full Lifecycle Data Generation of
  Rolling Bearings for RUL Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01119v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01119v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junliang Wang, Qinghua Zhang, Guanhua Zhu, Guoxi Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prediction of rolling bearing lifespan is of significant importance in
industrial production. However, the scarcity of high-quality, full lifecycle
data has been a major constraint in achieving precise predictions. To address
this challenge, this paper introduces the CVGAN model, a novel framework
capable of generating one-dimensional vibration signals in both horizontal and
vertical directions, conditioned on historical vibration data and remaining
useful life. In addition, we propose an autoregressive generation method that
can iteratively utilize previously generated vibration information to guide the
generation of current signals. The effectiveness of the CVGAN model is
validated through experiments conducted on the PHM 2012 dataset. Our findings
demonstrate that the CVGAN model, in terms of both MMD and FID metrics,
outperforms many advanced methods in both autoregressive and non-autoregressive
generation modes. Notably, training using the full lifecycle data generated by
the CVGAN model significantly improves the performance of the predictive model.
This result highlights the effectiveness of the data generated by CVGans in
enhancing the predictive power of these models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable manifold learning by uniform landmark sampling and constrained
  locally linear embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01100v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01100v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dehua Peng, Zhipeng Gui, Wenzhang Wei, Huayi Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a pivotal approach in machine learning and data science, manifold learning
aims to uncover the intrinsic low-dimensional structure within complex
nonlinear manifolds in high-dimensional space. By exploiting the manifold
hypothesis, various techniques for nonlinear dimension reduction have been
developed to facilitate visualization, classification, clustering, and gaining
key insights. Although existing manifold learning methods have achieved
remarkable successes, they still suffer from extensive distortions incurred in
the global structure, which hinders the understanding of underlying patterns.
Scalability issues also limit their applicability for handling large-scale
data. Here, we propose a scalable manifold learning (scML) method that can
manipulate large-scale and high-dimensional data in an efficient manner. It
starts by seeking a set of landmarks to construct the low-dimensional skeleton
of the entire data and then incorporates the non-landmarks into the landmark
space based on the constrained locally linear embedding (CLLE). We empirically
validated the effectiveness of scML on synthetic datasets and real-world
benchmarks of different types, and applied it to analyze the single-cell
transcriptomics and detect anomalies in electrocardiogram (ECG) signals. scML
scales well with increasing data sizes and exhibits promising performance in
preserving the global structure. The experiments demonstrate notable robustness
in embedding quality as the sample rate decreases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Parallel Audio Generation using Group Masked Language Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Myeonghun Jeong, Minchan Kim, Joun Yeop Lee, Nam Soo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a fast and high-quality codec language model for parallel audio
generation. While SoundStorm, a state-of-the-art parallel audio generation
model, accelerates inference speed compared to autoregressive models, it still
suffers from slow inference due to iterative sampling. To resolve this problem,
we propose Group-Masked Language Modeling~(G-MLM) and Group Iterative Parallel
Decoding~(G-IPD) for efficient parallel audio generation. Both the training and
sampling schemes enable the model to synthesize high-quality audio with a small
number of iterations by effectively modeling the group-wise conditional
dependencies. In addition, our model employs a cross-attention-based
architecture to capture the speaker style of the prompt voice and improves
computational efficiency. Experimental results demonstrate that our proposed
model outperforms the baselines in prompt-based audio generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Imperio: Language-Guided Backdoor Attacks for Arbitrary Model Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ka-Ho Chow, Wenqi Wei, Lei Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Revolutionized by the transformer architecture, natural language processing
(NLP) has received unprecedented attention. While advancements in NLP models
have led to extensive research into their backdoor vulnerabilities, the
potential for these advancements to introduce new backdoor threats remains
unexplored. This paper proposes Imperio, which harnesses the language
understanding capabilities of NLP models to enrich backdoor attacks. Imperio
provides a new model control experience. It empowers the adversary to control
the victim model with arbitrary output through language-guided instructions.
This is achieved using a language model to fuel a conditional trigger
generator, with optimizations designed to extend its language understanding
capabilities to backdoor instruction interpretation and execution. Our
experiments across three datasets, five attacks, and nine defenses confirm
Imperio's effectiveness. It can produce contextually adaptive triggers from
text descriptions and control the victim model with desired outputs, even in
scenarios not encountered during training. The attack maintains a high success
rate across complex datasets without compromising the accuracy of clean inputs
and also exhibits resilience against representative defenses. The source code
is available at \url{https://khchow.com/Imperio}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Global Convergence of Natural Policy Gradient with Hessian-aided
  Momentum Variance Reduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01084v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01084v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Feng, Ke Wei, Jinchi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural policy gradient (NPG) and its variants are widely-used policy search
methods in reinforcement learning. Inspired by prior work, a new NPG variant
coined NPG-HM is developed in this paper, which utilizes the Hessian-aided
momentum technique for variance reduction, while the sub-problem is solved via
the stochastic gradient descent method. It is shown that NPG-HM can achieve the
global last iterate $\epsilon$-optimality with a sample complexity of
$\mathcal{O}(\epsilon^{-2})$, which is the best known result for natural policy
gradient type methods under the generic Fisher non-degenerate policy
parameterizations. The convergence analysis is built upon a relaxed weak
gradient dominance property tailored for NPG under the compatible function
approximation framework, as well as a neat way to decompose the error when
handling the sub-problem. Moreover, numerical experiments on Mujoco-based
environments demonstrate the superior performance of NPG-HM over other
state-of-the-art policy gradient methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aircraft Landing Time Prediction with Deep Learning on Trajectory Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01083v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01083v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liping Huang, Sheng Zhang, Yicheng Zhang, Yi Zhang, Yifang Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aircraft landing time (ALT) prediction is crucial for air traffic management,
especially for arrival aircraft sequencing on the runway. In this study, a
trajectory image-based deep learning method is proposed to predict ALTs for the
aircraft entering the research airspace that covers the Terminal Maneuvering
Area (TMA). Specifically, the trajectories of all airborne arrival aircraft
within the temporal capture window are used to generate an image with the
target aircraft trajectory labeled as red and all background aircraft
trajectory labeled as blue. The trajectory images contain various information,
including the aircraft position, speed, heading, relative distances, and
arrival traffic flows. It enables us to use state-of-the-art deep convolution
neural networks for ALT modeling. We also use real-time runway usage obtained
from the trajectory data and the external information such as aircraft types
and weather conditions as additional inputs. Moreover, a convolution neural
network (CNN) based module is designed for automatic holding-related
featurizing, which takes the trajectory images, the leading aircraft holding
status, and their time and speed gap at the research airspace boundary as its
inputs. Its output is further fed into the final end-to-end ALT prediction. The
proposed ALT prediction approach is applied to Singapore Changi Airport (ICAO
Code: WSSS) using one-month Automatic Dependent Surveillance-Broadcast (ADS-B)
data from November 1 to November 30, 2022. Experimental results show that by
integrating the holding featurization, we can reduce the mean absolute error
(MAE) from 82.23 seconds to 43.96 seconds, and achieve an average accuracy of
96.1\%, with 79.4\% of the predictions errors being less than 60 seconds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In 2023 13th SESAR Innovation Days (SIDS2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constrained Online Two-stage Stochastic Optimization: Algorithm with
  (and without) Predictions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piao Hu, Jiashuo Jiang, Guodong Lyu, Hao Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider an online two-stage stochastic optimization with long-term
constraints over a finite horizon of $T$ periods. At each period, we take the
first-stage action, observe a model parameter realization and then take the
second-stage action from a feasible set that depends both on the first-stage
decision and the model parameter. We aim to minimize the cumulative objective
value while guaranteeing that the long-term average second-stage decision
belongs to a set. We develop online algorithms for the online two-stage problem
from adversarial learning algorithms. Also, the regret bound of our algorithm
can be reduced to the regret bound of embedded adversarial learning algorithms.
Based on this framework, we obtain new results under various settings. When the
model parameters are drawn from unknown non-stationary distributions and we are
given machine-learned predictions of the distributions, we develop a new
algorithm from our framework with a regret $O(W_T+\sqrt{T})$, where $W_T$
measures the total inaccuracy of the machine-learned predictions. We then
develop another algorithm that works when no machine-learned predictions are
given and show the performances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2302.00997</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Automatic Modulation Recognition through Robust Global Feature
  Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01056v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01056v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunpeng Qu, Zhilin Lu, Rui Zeng, Jintao Wang, Jian Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Modulation Recognition (AMR) plays a crucial role in wireless
communication systems. Deep learning AMR strategies have achieved tremendous
success in recent years. Modulated signals exhibit long temporal dependencies,
and extracting global features is crucial in identifying modulation schemes.
Traditionally, human experts analyze patterns in constellation diagrams to
classify modulation schemes. Classical convolutional-based networks, due to
their limited receptive fields, excel at extracting local features but struggle
to capture global relationships. To address this limitation, we introduce a
novel hybrid deep framework named TLDNN, which incorporates the architectures
of the transformer and long short-term memory (LSTM). We utilize the
self-attention mechanism of the transformer to model the global correlations in
signal sequences while employing LSTM to enhance the capture of temporal
dependencies. To mitigate the impact like RF fingerprint features and channel
characteristics on model generalization, we propose data augmentation
strategies known as segment substitution (SS) to enhance the model's robustness
to modulation-related features. Experimental results on widely-used datasets
demonstrate that our method achieves state-of-the-art performance and exhibits
significant advantages in terms of complexity. Our proposed framework serves as
a foundational backbone that can be extended to different datasets. We have
verified the effectiveness of our augmentation approach in enhancing the
generalization of the models, particularly in few-shot scenarios. Code is
available at \url{https://github.com/AMR-Master/TLDNN}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE Transactions on Vehicular Technology, 14 pages, 11
  figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Elastic Multi-Gradient Descent for Parallel Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01054v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01054v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Lyu, Wei Feng, Yuepan Li, Qing Sun, Fanhua Shang, Liang Wan, Liang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of Continual Learning (CL) is to continuously learn from new data
streams and accomplish the corresponding tasks. Previously studied CL assumes
that data are given in sequence nose-to-tail for different tasks, thus indeed
belonging to Serial Continual Learning (SCL). This paper studies the novel
paradigm of Parallel Continual Learning (PCL) in dynamic multi-task scenarios,
where a diverse set of tasks is encountered at different time points. PCL
presents challenges due to the training of an unspecified number of tasks with
varying learning progress, leading to the difficulty of guaranteeing effective
model updates for all encountered tasks. In our previous conference work, we
focused on measuring and reducing the discrepancy among gradients in a
multi-objective optimization problem, which, however, may still contain
negative transfers in every model update. To address this issue, in the dynamic
multi-objective optimization problem, we introduce task-specific elastic
factors to adjust the descent direction towards the Pareto front. The proposed
method, called Elastic Multi-Gradient Descent (EMGD), ensures that each update
follows an appropriate Pareto descent direction, minimizing any negative impact
on previously learned tasks. To balance the training between old and new tasks,
we also propose a memory editing mechanism guided by the gradient computed
using EMGD. This editing process updates the stored data points, reducing
interference in the Pareto descent direction from previous tasks. Experiments
on public datasets validate the effectiveness of our EMGD in the PCL setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submited to IEEE TPAMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PAC-Bayesian Domain Adaptation Bounds for Multi-view learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01048v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01048v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehdi Hennequin, Khalid Benabdeslem, Haytham Elghazel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a series of new results for domain adaptation in the
multi-view learning setting. The incorporation of multiple views in the domain
adaptation was paid little attention in the previous studies. In this way, we
propose an analysis of generalization bounds with Pac-Bayesian theory to
consolidate the two paradigms, which are currently treated separately. Firstly,
building on previous work by Germain et al., we adapt the distance between
distribution proposed by Germain et al. for domain adaptation with the concept
of multi-view learning. Thus, we introduce a novel distance that is tailored
for the multi-view domain adaptation setting. Then, we give Pac-Bayesian bounds
for estimating the introduced divergence. Finally, we compare the different new
bounds with the previous studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2004.11829 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sharp Analysis of Power Iteration for Tensor PCA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01047v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01047v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Wu, Kangjie Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the power iteration algorithm for the tensor PCA model
introduced in Richard and Montanari (2014). Previous work studying the
properties of tensor power iteration is either limited to a constant number of
iterations, or requires a non-trivial data-independent initialization. In this
paper, we move beyond these limitations and analyze the dynamics of randomly
initialized tensor power iteration up to polynomially many steps. Our
contributions are threefold: First, we establish sharp bounds on the number of
iterations required for power method to converge to the planted signal, for a
broad range of the signal-to-noise ratios. Second, our analysis reveals that
the actual algorithmic threshold for power iteration is smaller than the one
conjectured in literature by a polylog(n) factor, where n is the ambient
dimension. Finally, we propose a simple and effective stopping criterion for
power iteration, which provably outputs a solution that is highly correlated
with the true signal. Extensive numerical experiments verify our theoretical
results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CautionSuicide: A Deep Learning Based Approach for Detecting Suicidal
  Ideation in Real Time Chatbot Conversation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01023v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01023v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nelly Elsayed, Zag ElSayed, Murat Ozer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Suicide is recognized as one of the most serious concerns in the modern
society. Suicide causes tragedy that affects countries, communities, and
families. There are many factors that lead to suicidal ideations. Early
detection of suicidal ideations can help to prevent suicide occurrence by
providing the victim with the required professional support, especially when
the victim does not recognize the danger of having suicidal ideations. As
technology usage has increased, people share and express their ideations
digitally via social media, chatbots, and other digital platforms. In this
paper, we proposed a novel, simple deep learning-based model to detect suicidal
ideations in digital content, mainly focusing on chatbots as the primary data
source. In addition, we provide a framework that employs the proposed suicide
detection integration with a chatbot-based support system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 6 figures, 4 tables, Under review in IEEE conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting <span class="highlight-title">Transformer</span>'s Robustness and Efficacy in PPG Signal Artifact
  Detection with <span class="highlight-title">Self-Supervised</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01013v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01013v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thanh-Dung Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research at CHU Sainte Justine's Pediatric Critical Care Unit (PICU)
has revealed that traditional machine learning methods, such as semi-supervised
label propagation and K-nearest neighbors, outperform Transformer-based models
in artifact detection from PPG signals, mainly when data is limited. This study
addresses the underutilization of abundant unlabeled data by employing
self-supervised learning (SSL) to extract latent features from these data,
followed by fine-tuning on labeled data. Our experiments demonstrate that SSL
significantly enhances the Transformer model's ability to learn
representations, improving its robustness in artifact classification tasks.
Among various SSL techniques, including masking, contrastive learning, and DINO
(self-distillation with no labels)-contrastive learning exhibited the most
stable and superior performance in small PPG datasets. Further, we delve into
optimizing contrastive loss functions, which are crucial for contrastive SSL.
Inspired by InfoNCE, we introduce a novel contrastive loss function that
facilitates smoother training and better convergence, thereby enhancing
performance in artifact classification. In summary, this study establishes the
efficacy of SSL in leveraging unlabeled data, particularly in enhancing the
capabilities of the Transformer model. This approach holds promise for broader
applications in PICU environments, where annotated data is often limited.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under preparation to submit to IEEE for possible publications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Continual Anomaly Detection with Contrastively-learned
  <span class="highlight-title">Prompt</span> <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Liu, Kai Wu, Qiang Nie, Ying Chen, Bin-Bin Gao, Yong Liu, Jinbao Wang, Chengjie Wang, Feng Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised Anomaly Detection (UAD) with incremental training is crucial in
industrial manufacturing, as unpredictable defects make obtaining sufficient
labeled data infeasible. However, continual learning methods primarily rely on
supervised annotations, while the application in UAD is limited due to the
absence of supervision. Current UAD methods train separate models for different
classes sequentially, leading to catastrophic forgetting and a heavy
computational burden. To address this issue, we introduce a novel Unsupervised
Continual Anomaly Detection framework called UCAD, which equips the UAD with
continual learning capability through contrastively-learned prompts. In the
proposed UCAD, we design a Continual Prompting Module (CPM) by utilizing a
concise key-prompt-knowledge memory bank to guide task-invariant `anomaly'
model predictions using task-specific `normal' knowledge. Moreover,
Structure-based Contrastive Learning (SCL) is designed with the Segment
Anything Model (SAM) to improve prompt learning and anomaly segmentation
results. Specifically, by treating SAM's masks as structure, we draw features
within the same mask closer and push others apart for general feature
representations. We conduct comprehensive experiments and set the benchmark on
unsupervised continual anomaly detection and segmentation, demonstrating that
our method is significantly better than anomaly detection methods, even with
rehearsal training. The code will be available at
https://github.com/shirowalker/UCAD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Learning Classification of Alzheimer's Disease Stages Using
  Cerebrospinal Fluid Biomarkers Alone 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vivek Kumar Tiwari, Premananda Indic, Shawana Tabassum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Early diagnosis of Alzheimer's disease is a challenge because the existing
methodologies do not identify the patients in their preclinical stage, which
can last up to a decade prior to the onset of clinical symptoms. Several
research studies demonstrate the potential of cerebrospinal fluid biomarkers,
amyloid beta 1-42, T-tau, and P-tau, in early diagnosis of Alzheimer's disease
stages. In this work, we used machine learning models to classify different
stages of Alzheimer's disease based on the cerebrospinal fluid biomarker levels
alone. An electronic health record of patients from the National Alzheimer's
Coordinating Centre database was analyzed and the patients were subdivided
based on mini-mental state scores and clinical dementia ratings. Statistical
and correlation analyses were performed to identify significant differences
between the Alzheimer's stages. Afterward, machine learning classifiers
including K-Nearest Neighbors, Ensemble Boosted Tree, Ensemble Bagged Tree,
Support Vector Machine, Logistic Regression, and Naive Bayes classifiers were
employed to classify the Alzheimer's disease stages. The results demonstrate
that Ensemble Boosted Tree (84.4%) and Logistic Regression (73.4%) provide the
highest accuracy for binary classification, while Ensemble Bagged Tree (75.4%)
demonstrates better accuracy for multiclassification. The findings from this
research are expected to help clinicians in making an informed decision
regarding the early diagnosis of Alzheimer's from the cerebrospinal fluid
biomarkers alone, monitoring of the disease progression, and implementation of
appropriate intervention measures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Point Cloud Classification via Deep Set Linearized Optimal Transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Scott Mahan, Caroline Moosmüller, Alexander Cloninger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Deep Set Linearized Optimal Transport, an algorithm designed for
the efficient simultaneous embedding of point clouds into an $L^2-$space. This
embedding preserves specific low-dimensional structures within the Wasserstein
space while constructing a classifier to distinguish between various classes of
point clouds. Our approach is motivated by the observation that $L^2-$distances
between optimal transport maps for distinct point clouds, originating from a
shared fixed reference distribution, provide an approximation of the
Wasserstein-2 distance between these point clouds, under certain assumptions.
To learn approximations of these transport maps, we employ input convex neural
networks (ICNNs) and establish that, under specific conditions, Euclidean
distances between samples from these ICNNs closely mirror Wasserstein-2
distances between the true distributions. Additionally, we train a
discriminator network that attaches weights these samples and creates a
permutation invariant classifier to differentiate between different classes of
point clouds. We showcase the advantages of our algorithm over the standard
deep set approach through experiments on a flow cytometry dataset with a
limited number of labeled point clouds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Concurrent Self-testing of Neural Networks Using Uncertainty Fingerprint 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01458v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01458v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soyed Tuhin Ahmed, Mehdi B. tahoori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks (NNs) are increasingly used in always-on safety-critical
applications deployed on hardware accelerators (NN-HAs) employing various
memory technologies. Reliable continuous operation of NN is essential for
safety-critical applications. During online operation, NNs are susceptible to
single and multiple permanent and soft errors due to factors such as radiation,
aging, and thermal effects. Explicit NN-HA testing methods cannot detect
transient faults during inference, are unsuitable for always-on applications,
and require extensive test vector generation and storage. Therefore, in this
paper, we propose the \emph{uncertainty fingerprint} approach representing the
online fault status of NN. Furthermore, we propose a dual head NN topology
specifically designed to produce uncertainty fingerprints and the primary
prediction of the NN in \emph{a single shot}. During the online operation, by
matching the uncertainty fingerprint, we can concurrently self-test NNs with up
to $100\%$ coverage with a low false positive rate while maintaining a similar
performance of the primary task. Compared to existing works, memory overhead is
reduced by up to $243.7$ MB, multiply and accumulate (MAC) operation is reduced
by up to $10000\times$, and false-positive rates are reduced by up to $89\%$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProbMCL: Simple Probabilistic Contrastive Learning for Multi-label
  Visual Classification <span class="chip">ICASSP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Sajedi, Samir Khaki, Yuri A. Lawryshyn, Konstantinos N. Plataniotis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-label image classification presents a challenging task in many domains,
including computer vision and medical imaging. Recent advancements have
introduced graph-based and transformer-based methods to improve performance and
capture label dependencies. However, these methods often include complex
modules that entail heavy computation and lack interpretability. In this paper,
we propose Probabilistic Multi-label Contrastive Learning (ProbMCL), a novel
framework to address these challenges in multi-label image classification
tasks. Our simple yet effective approach employs supervised contrastive
learning, in which samples that share enough labels with an anchor image based
on a decision threshold are introduced as a positive set. This structure
captures label dependencies by pulling positive pair embeddings together and
pushing away negative samples that fall below the threshold. We enhance
representation learning by incorporating a mixture density network into
contrastive learning and generating Gaussian mixture distributions to explore
the epistemic uncertainty of the feature encoder. We validate the effectiveness
of our framework through experimentation with datasets from the computer vision
and medical imaging domains. Our method outperforms the existing
state-of-the-art methods while achieving a low computational footprint on both
datasets. Visualization analyses also demonstrate that ProbMCL-learned
classifiers maintain a meaningful semantic topology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for the IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Over-the-Air Federated Learning with Awareness of
  Interference and Data Heterogeneity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyed Mohammad Azimi-Abarghouyi, Viktoria Fodor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When implementing hierarchical federated learning over wireless networks,
scalability assurance and the ability to handle both interference and device
data heterogeneity are crucial. This work introduces a learning method designed
to address these challenges, along with a scalable transmission scheme that
efficiently uses a single wireless resource through over-the-air computation.
To provide resistance against data heterogeneity, we employ gradient
aggregations. Meanwhile, the impact of interference is minimized through
optimized receiver normalizing factors. For this, we model a multi-cluster
wireless network using stochastic geometry, and characterize the mean squared
error of the aggregation estimations as a function of the network parameters.
We show that despite the interference and the data heterogeneity, the proposed
scheme achieves high learning accuracy and can significantly outperform the
conventional hierarchical algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at IEEE WCNC 2024. Overlap with arXiv:2211.16162</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modular Learning of Deep Causal Generative Models for High-dimensional
  Causal Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01426v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01426v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Musfiqur Rahman, Murat Kocaoglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pearl's causal hierarchy establishes a clear separation between
observational, interventional, and counterfactual questions. Researchers
proposed sound and complete algorithms to compute identifiable causal queries
at a given level of the hierarchy using the causal structure and data from the
lower levels of the hierarchy. However, most of these algorithms assume that we
can accurately estimate the probability distribution of the data, which is an
impractical assumption for high-dimensional variables such as images. On the
other hand, modern generative deep learning architectures can be trained to
learn how to accurately sample from such high-dimensional distributions.
Especially with the recent rise of foundation models for images, it is
desirable to leverage pre-trained models to answer causal queries with such
high-dimensional data. To address this, we propose a sequential training
algorithm that, given the causal structure and a pre-trained conditional
generative model, can train a deep causal generative model, which utilizes the
pre-trained model and can provably sample from identifiable interventional and
counterfactual distributions. Our algorithm, called Modular-DCM, uses
adversarial training to learn the network weights, and to the best of our
knowledge, is the first algorithm that can make use of pre-trained models and
provably sample from any identifiable causal query in the presence of latent
confounders with high-dimensional data. We demonstrate the utility of our
algorithm using semi-synthetic and real-world datasets containing images as
variables in the causal structure.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Swap<span class="highlight-title">Transformer</span>: highway overtaking tactical planner model via imitation
  learning on OSHA <span class="highlight-title">dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Shamsoshoara, Safin B Salih, Pedram Aghazadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the high-level decision-making problem in highway
scenarios regarding lane changing and over-taking other slower vehicles. In
particular, this paper aims to improve the Travel Assist feature for automatic
overtaking and lane changes on highways. About 9 million samples including lane
images and other dynamic objects are collected in simulation. This data;
Overtaking on Simulated HighwAys (OSHA) dataset is released to tackle this
challenge. To solve this problem, an architecture called SwapTransformer is
designed and implemented as an imitation learning approach on the OSHA dataset.
Moreover, auxiliary tasks such as future points and car distance network
predictions are proposed to aid the model in better understanding the
surrounding environment. The performance of the proposed solution is compared
with a multi-layer perceptron (MLP) and multi-head self-attention networks as
baselines in a simulation environment. We also demonstrate the performance of
the model with and without auxiliary tasks. All models are evaluated based on
different metrics such as time to finish each lap, number of overtakes, and
speed difference with speed limit. The evaluation shows that the
SwapTransformer model outperforms other models in different traffic densities
in the inference phase.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 12 Figures, 1 Algorithm, 2 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VALD-MD: Visual Attribution via Latent Diffusion for Medical Diagnostics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01414v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01414v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ammar A. Siddiqui, Santosh Tirunagari, Tehseen Zia, David Windridge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual attribution in medical imaging seeks to make evident the
diagnostically-relevant components of a medical image, in contrast to the more
common detection of diseased tissue deployed in standard machine vision
pipelines (which are less straightforwardly interpretable/explainable to
clinicians). We here present a novel generative visual attribution technique,
one that leverages latent diffusion models in combination with domain-specific
large language models, in order to generate normal counterparts of abnormal
images. The discrepancy between the two hence gives rise to a mapping
indicating the diagnostically-relevant image components. To achieve this, we
deploy image priors in conjunction with appropriate conditioning mechanisms in
order to control the image generative process, including natural language text
prompts acquired from medical science and applied radiology. We perform
experiments and quantitatively evaluate our results on the COVID-19 Radiography
Database containing labelled chest X-rays with differing pathologies via the
Frechet Inception Distance (FID), Structural Similarity (SSIM) and Multi Scale
Structural Similarity Metric (MS-SSIM) metrics obtained between real and
generated images. The resulting system also exhibits a range of latent
capabilities including zero-shot localized disease induction, which are
evaluated with real examples from the cheXpert dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable network reconstruction in subquadratic time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01404v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01404v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiago P. Peixoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Network reconstruction consists in determining the unobserved pairwise
couplings between $N$ nodes given only observational data on the resulting
behavior that is conditioned on those couplings -- typically a time-series or
independent samples from a graphical model. A major obstacle to the scalability
of algorithms proposed for this problem is a seemingly unavoidable quadratic
complexity of $O(N^2)$, corresponding to the requirement of each possible
pairwise coupling being contemplated at least once, despite the fact that most
networks of interest are sparse, with a number of non-zero couplings that is
only $O(N)$. Here we present a general algorithm applicable to a broad range of
reconstruction problems that achieves its result in subquadratic time, with a
data-dependent complexity loosely upper bounded by $O(N^{3/2}\log N)$, but with
a more typical log-linear complexity of $O(N\log^2N)$. Our algorithm relies on
a stochastic second neighbor search that produces the best edge candidates with
high probability, thus bypassing an exhaustive quadratic search. In practice,
our algorithm achieves a performance that is many orders of magnitude faster
than the quadratic baseline, allows for easy parallelization, and thus enables
the reconstruction of networks with hundreds of thousands and even millions of
nodes and edges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerating Black-Box Molecular Property Optimization by Adaptively
  Learning Sparse Subspaces <span class="chip">NeurIPS
  2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farshud Sorourifar, Thomas Banker, Joel A. Paulson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Molecular property optimization (MPO) problems are inherently challenging
since they are formulated over discrete, unstructured spaces and the labeling
process involves expensive simulations or experiments, which fundamentally
limits the amount of available data. Bayesian optimization (BO) is a powerful
and popular framework for efficient optimization of noisy, black-box objective
functions (e.g., measured property values), thus is a potentially attractive
framework for MPO. To apply BO to MPO problems, one must select a structured
molecular representation that enables construction of a probabilistic surrogate
model. Many molecular representations have been developed, however, they are
all high-dimensional, which introduces important challenges in the BO process
-- mainly because the curse of dimensionality makes it difficult to define and
perform inference over a suitable class of surrogate models. This challenge has
been recently addressed by learning a lower-dimensional encoding of a SMILE or
graph representation of a molecule in an unsupervised manner and then
performing BO in the encoded space. In this work, we show that such methods
have a tendency to "get stuck," which we hypothesize occurs since the mapping
from the encoded space to property values is not necessarily well-modeled by a
Gaussian process. We argue for an alternative approach that combines numerical
molecular descriptors with a sparse axis-aligned Gaussian process model, which
is capable of rapidly identifying sparse subspaces that are most relevant to
modeling the unknown property function. We demonstrate that our proposed method
substantially outperforms existing MPO methods on a variety of benchmark and
real-world problems. Specifically, we show that our method can routinely find
near-optimal molecules out of a set of more than $>100$k alternatives within
100 or fewer expensive queries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures consisting of 6 and 4 plots, accepted to NeurIPS
  2023 Workshop on Adaptive Experimental Design and Active Learning in the Real
  World</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep autoregressive modeling for land use land cover 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01395v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01395v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Krapu, Mark Borsuk, Ryan Calder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Land use / land cover (LULC) modeling is a challenging task due to long-range
dependencies between geographic features and distinct spatial patterns related
to topography, ecology, and human development. We identify a close connection
between modeling of spatial patterns of land use and the task of image
inpainting from computer vision and conduct a study of a modified PixelCNN
architecture with approximately 19 million parameters for modeling LULC. In
comparison with a benchmark spatial statistical model, we find that the former
is capable of capturing much richer spatial correlation patterns such as roads
and water bodies but does not produce a calibrated predictive distribution,
suggesting the need for additional tuning. We find evidence of predictive
underdispersion with regard to important ecologically-relevant land use
statistics such as patch count and adjacency which can be ameliorated to some
extent by manipulating sampling variability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Backtracking New Q-Newton's method, Newton's flow, Voronoi's diagram and
  Stochastic root finding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01393v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01393v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Erik Fornaess, Mi Hu, Tuyen Trung Truong, Takayuki Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A new variant of Newton's method - named Backtracking New Q-Newton's method
(BNQN) - which has strong theoretical guarantee, is easy to implement, and has
good experimental performance, was recently introduced by the third author.
  Experiments performed previously showed some remarkable properties of the
basins of attractions for finding roots of polynomials and meromorphic
functions, with BNQN. In general, they look more smooth than that of Newton's
method.
  In this paper, we continue to experimentally explore in depth this remarkable
phenomenon, and connect BNQN to Newton's flow and Voronoi's diagram. This link
poses a couple of challenging puzzles to be explained. Experiments also
indicate that BNQN is more robust against random perturbations than Newton's
method and Random Relaxed Newton's method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>48 pages. Comments are welcome! arXiv admin note: text overlap with
  arXiv:2312.12166</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Intrusion Detection with Domain-Invariant Representation
  Learning in Latent Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17300v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17300v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Padmaksha Roy, Tyler Cody, Himanshu Singhal, Kevin Choi, Ming Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain generalization focuses on leveraging knowledge from multiple related
domains with ample training data and labels to enhance inference on unseen
in-distribution (IN) and out-of-distribution (OOD) domains. In our study, we
introduce a two-phase representation learning technique using multi-task
learning. This approach aims to cultivate a latent space from features spanning
multiple domains, encompassing both native and cross-domains, to amplify
generalization to IN and OOD territories. Additionally, we attempt to
disentangle the latent space by minimizing the mutual information between the
prior and latent space, effectively de-correlating spurious feature
correlations. Collectively, the joint optimization will facilitate
domain-invariant feature learning. We assess the model's efficacy across
multiple cybersecurity datasets, using standard classification metrics on both
unseen IN and OOD sets, and juxtapose the results with contemporary domain
generalization methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sample-Efficient Safety Assurances using Conformal Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.14082v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.14082v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rachel Luo, Shengjia Zhao, Jonathan Kuck, Boris Ivanovic, Silvio Savarese, Edward Schmerling, Marco Pavone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When deploying machine learning models in high-stakes robotics applications,
the ability to detect unsafe situations is crucial. Early warning systems can
provide alerts when an unsafe situation is imminent (in the absence of
corrective action). To reliably improve safety, these warning systems should
have a provable false negative rate; i.e. of the situations that are unsafe,
fewer than $\epsilon$ will occur without an alert. In this work, we present a
framework that combines a statistical inference technique known as conformal
prediction with a simulator of robot/environment dynamics, in order to tune
warning systems to provably achieve an $\epsilon$ false negative rate using as
few as $1/\epsilon$ data points. We apply our framework to a driver warning
system and a robotic grasping application, and empirically demonstrate
guaranteed false negative rate while also observing low false detection
(positive) rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Journal of Robotics Research, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenVoice: Versatile Instant Voice Cloning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.01479v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.01479v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zengyi Qin, Wenliang Zhao, Xumin Yu, Xin Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce OpenVoice, a versatile voice cloning approach that requires only
a short audio clip from the reference speaker to replicate their voice and
generate speech in multiple languages. OpenVoice represents a significant
advancement in addressing the following open challenges in the field: 1)
Flexible Voice Style Control. OpenVoice enables granular control over voice
styles, including emotion, accent, rhythm, pauses, and intonation, in addition
to replicating the tone color of the reference speaker. The voice styles are
not directly copied from and constrained by the style of the reference speaker.
Previous approaches lacked the ability to flexibly manipulate voice styles
after cloning. 2) Zero-Shot Cross-Lingual Voice Cloning. OpenVoice achieves
zero-shot cross-lingual voice cloning for languages not included in the
massive-speaker training set. Unlike previous approaches, which typically
require extensive massive-speaker multi-lingual (MSML) dataset for all
languages, OpenVoice can clone voices into a new language without any
massive-speaker training data for that language. OpenVoice is also
computationally efficient, costing tens of times less than commercially
available APIs that offer even inferior performance. To foster further research
in the field, we have made the source code and trained model publicly
accessible. We also provide qualitative results in our demo website. Prior to
its public release, our internal version of OpenVoice was used tens of millions
of times by users worldwide between May and October 2023, serving as the
backend of MyShell.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridging the Gaps: Learning Verifiable Model-Free Quadratic Programming
  Controllers Inspired by Model Predictive Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05332v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05332v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwen Lu, Zishuo Li, Yihan Zhou, Na Li, Yilin Mo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a new class of parameterized controllers, drawing
inspiration from Model Predictive Control (MPC). The controller resembles a
Quadratic Programming (QP) solver of a linear MPC problem, with the parameters
of the controller being trained via Deep Reinforcement Learning (DRL) rather
than derived from system models. This approach addresses the limitations of
common controllers with Multi-Layer Perceptron (MLP) or other general neural
network architecture used in DRL, in terms of verifiability and performance
guarantees, and the learned controllers possess verifiable properties like
persistent feasibility and asymptotic stability akin to MPC. On the other hand,
numerical examples illustrate that the proposed controller empirically matches
MPC and MLP controllers in terms of control performance and has superior
robustness against modeling uncertainty and noises. Furthermore, the proposed
controller is significantly more computationally efficient compared to MPC and
requires fewer parameters to learn than MLP controllers. Real-world experiments
on vehicle drift maneuvering task demonstrate the potential of these
controllers for robotics and other demanding control tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ tf.data service: A Case for Disaggregating ML Input Data Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.14826v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.14826v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Audibert, Yang Chen, Dan Graur, Ana Klimovic, Jiri Simsa, Chandramohan A. Thekkath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) computations commonly execute on expensive specialized
hardware, such as GPUs and TPUs, which provide high FLOPs and
performance-per-watt. For cost efficiency, it is essential to keep these
accelerators highly utilized. This requires preprocessing input data at the
rate at which the accelerators can ingest and perform ML computations on the
data. To avoid data stalls, the host CPU and RAM required for input data
processing per accelerator core used for ML computations varies across jobs.
Hence, the traditional approach of processing input data on ML accelerator
hosts with a fixed hardware ratio leads to either under-utilizing the
accelerators or the host CPU and RAM. In this paper, we address these concerns
by building a disaggregated ML data processing system.
  We present tf.data service, an open-source disaggregated input data
processing service built on top of tf.data in TensorFlow. We show that
disaggregating data preprocessing has three key advantages for large-scale ML
training jobs. First, the service can horizontally scale-out to right-size
CPU/RAM host resources for data processing in each job, saving 32x training
time and 26x cost, on average. Second, the service can share ephemeral
preprocessed data results across jobs, to optimize CPU usage and reduce
redundant computations. Finally, the service supports coordinated reads, a
technique that avoids stragglers due to different input sizes in distributed
training, reducing training time by 2.2x, on average. Our design is inspired by
lessons learned from deploying tf.data service in production, including
relaxing data visitation guarantees without impacting model accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-Efficient Multimodal Fusion on a Single GPU 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10144v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10144v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noël Vouitsis, Zhaoyan Liu, Satya Krishna Gorti, Valentin Villecroze, Jesse C. Cresswell, Guangwei Yu, Gabriel Loaiza-Ganem, Maksims Volkovs
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of multimodal alignment is to learn a single latent space that is
shared between multimodal inputs. The most powerful models in this space have
been trained using massive datasets of paired inputs and large-scale
computational resources, making them prohibitively expensive to train in many
practical scenarios. We surmise that existing unimodal encoders pre-trained on
large amounts of unimodal data should provide an effective bootstrap to create
multimodal models from unimodal ones at much lower costs. We therefore propose
FuseMix, a multimodal augmentation scheme that operates on the latent spaces of
arbitrary pre-trained unimodal encoders. Using FuseMix for multimodal
alignment, we achieve competitive performance -- and in certain cases
outperform state-of-the art methods -- in both image-text and audio-text
retrieval, with orders of magnitude less compute and data: for example, we
outperform CLIP on the Flickr30K text-to-image retrieval task with $\sim \!
600\times$ fewer GPU days and $\sim \! 80\times$ fewer image-text pairs.
Additionally, we show how our method can be applied to convert pre-trained
text-to-image generative models into audio-to-image ones. Code is available at:
https://github.com/layer6ai-labs/fusemix.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-Time Online Stock Forecasting Utilizing Integrated Quantitative and
  Qualitative Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15218v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15218v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Akash Bathini, Dagli Cihan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The application of Machine learning to finance has become a familiar
approach, even more so in stock market forecasting. The stock market is highly
volatile, and huge amounts of data are generated every minute globally. The
extraction of effective intelligence from this data is of critical importance.
However, a collaboration of numerical stock data with qualitative text data can
be a challenging task. In this work, we accomplish this by providing an
unprecedented, publicly available dataset with technical and fundamental data
and sentiment that we gathered from news archives, TV news captions, radio
transcripts, tweets, daily financial newspapers, etc. The text data entries
used for sentiment extraction total more than 1.4 Million. The dataset consists
of daily entries from January 2018 to December 2022 for eight companies
representing diverse industrial sectors and the Dow Jones Industrial Average
(DJIA) as a whole. Holistic Fundamental and Technical data is provided training
ready for Model learning and deployment. Most importantly, the data generated
could be used for incremental online learning with real-time data points
retrieved daily since no stagnant data was utilized. All the data was retired
from APIs or self-designed robust information retrieval technologies with
extremely low latency and zero monetary cost. These adaptable technologies
facilitate data extraction for any stock. Moreover, the utilization of
Spearman's rank correlation over real-time data, linking stock returns with
sentiment analysis has produced noteworthy results for the DJIA and the eight
other stocks, achieving accuracy levels surpassing 60%. The dataset is made
available at https://github.com/batking24/Huge-Stock-Dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lossy Image Compression with Conditional Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.06950v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.06950v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruihan Yang, Stephan Mandt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper outlines an end-to-end optimized lossy image compression framework
using diffusion generative models. The approach relies on the transform coding
paradigm, where an image is mapped into a latent space for entropy coding and,
from there, mapped back to the data space for reconstruction. In contrast to
VAE-based neural compression, where the (mean) decoder is a deterministic
neural network, our decoder is a conditional diffusion model. Our approach thus
introduces an additional ``content'' latent variable on which the reverse
diffusion process is conditioned and uses this variable to store information
about the image. The remaining ``texture'' variables characterizing the
diffusion process are synthesized at decoding time. We show that the model's
performance can be tuned toward perceptual metrics of interest. Our extensive
experiments involving multiple datasets and image quality assessment metrics
show that our approach yields stronger reported FID scores than the GAN-based
model, while also yielding competitive performance with VAE-based models in
several distortion metrics. Furthermore, training the diffusion with
$\mathcal{X}$-parameterization enables high-quality reconstructions in only a
handful of decoding steps, greatly affecting the model's practicality. Our code
is available at: \url{https://github.com/buggyyang/CDC_compression}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SLEM: Machine Learning for Path Modeling and Causal Inference with Super
  Learner Equation Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.04365v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.04365v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew J. Vowels
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal inference is a crucial goal of science, enabling researchers to arrive
at meaningful conclusions regarding the predictions of hypothetical
interventions using observational data. Path models, Structural Equation Models
(SEMs), and, more generally, Directed Acyclic Graphs (DAGs), provide a means to
unambiguously specify assumptions regarding the causal structure underlying a
phenomenon. Unlike DAGs, which make very few assumptions about the functional
and parametric form, SEM assumes linearity. This can result in functional
misspecification which prevents researchers from undertaking reliable effect
size estimation. In contrast, we propose Super Learner Equation Modeling, a
path modeling technique integrating machine learning Super Learner ensembles.
We empirically demonstrate its ability to provide consistent and unbiased
estimates of causal effects, its competitive performance for linear models when
compared with SEM, and highlight its superiority over SEM when dealing with
non-linear relationships. We provide open-source code, and a tutorial notebook
with example usage, accentuating the easy-to-use nature of the method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Era Splitting -- Invariant Learning for Decision Trees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14496v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14496v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timothy DeLise
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-life machine learning problems exhibit distributional shifts in the data
from one time to another or from on place to another. This behavior is beyond
the scope of the traditional empirical risk minimization paradigm, which
assumes i.i.d. distribution of data over time and across locations. The
emerging field of out-of-distribution (OOD) generalization addresses this
reality with new theory and algorithms which incorporate environmental, or
era-wise information into the algorithms. So far, most research has been
focused on linear models and/or neural networks. In this research we develop
two new splitting criteria for decision trees, which allow us to apply ideas
from OOD generalization research to decision tree models, including random
forest and gradient-boosting decision trees. The new splitting criteria use
era-wise information associated with each data point to allow tree-based models
to find split points that are optimal across all disjoint eras in the data,
instead of optimal over the entire data set pooled together, which is the
default setting. In this paper we describe the problem setup in the context of
financial markets. We describe the new splitting criteria in detail and develop
unique experiments to showcase the benefits of these new criteria, which
improve metrics in our experiments out-of-sample. The new criteria are
incorporated into the a state-of-the-art gradient boosted decision tree model
in the Scikit-Learn code base, which is made freely available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Approximation analysis of CNNs from a feature extraction view 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.09041v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.09041v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianfei Li, Han Feng, Ding-Xuan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning based on deep neural networks has been very successful in many
practical applications, but it lacks enough theoretical understanding due to
the network architectures and structures. In this paper we establish some
analysis for linear feature extraction by a deep multi-channel convolutional
neural networks (CNNs), which demonstrates the power of deep learning over
traditional linear transformations, like Fourier, wavelets, redundant
dictionary coding methods. Moreover, we give an exact construction presenting
how linear features extraction can be conducted efficiently with multi-channel
CNNs. It can be applied to lower the essential dimension for approximating a
high dimensional function. Rates of function approximation by such deep
networks implemented with channels and followed by fully-connected layers are
investigated as well. Harmonic analysis for factorizing linear features into
multi-resolution convolutions plays an essential role in our work.
Nevertheless, a dedicate vectorization of matrices is constructed, which
bridges 1D CNN and 2D CNN and allows us to have corresponding 2D analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Joint Learning of Linear Time-Invariant Dynamical Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.10955v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.10955v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Modi, Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, George Michailidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Linear time-invariant systems are very popular models in system theory and
applications. A fundamental problem in system identification that remains
rather unaddressed in extant literature is to leverage commonalities amongst
related linear systems to estimate their transition matrices more accurately.
To address this problem, the current paper investigates methods for jointly
estimating the transition matrices of multiple systems. It is assumed that the
transition matrices are unknown linear functions of some unknown shared basis
matrices. We establish finite-time estimation error rates that fully reflect
the roles of trajectory lengths, dimension, and number of systems under
consideration. The presented results are fairly general and show the
significant gains that can be achieved by pooling data across systems in
comparison to learning each system individually. Further, they are shown to be
robust against model misspecifications. To obtain the results, we develop novel
techniques that are of interest for addressing similar joint-learning problems.
They include tightly bounding estimation errors in terms of the
eigen-structures of transition matrices, establishing sharp high probability
bounds for singular values of dependent random matrices, and capturing effects
of misspecified transition matrices as the systems evolve over time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-driven Modeling and Inference for Bayesian Gaussian Process ODEs
  via Double Normalizing Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09222v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09222v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Xu, Shian Du, Junmei Yang, Xinghao Ding, John Paisley, Delu Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Gaussian processes have been used to model the vector field of
continuous dynamical systems, referred to as GPODEs, which are characterized by
a probabilistic ODE equation. Bayesian inference for these models has been
extensively studied and applied in tasks such as time series prediction.
However, the use of standard GPs with basic kernels like squared exponential
kernels has been common in GPODE research, limiting the model's ability to
represent complex scenarios. To address this limitation, we introduce
normalizing flows to reparameterize the ODE vector field, resulting in a
data-driven prior distribution, thereby increasing flexibility and expressive
power. We develop a data-driven variational learning algorithm that utilizes
analytically tractable probability density functions of normalizing flows,
enabling simultaneous learning and inference of unknown continuous dynamics.
Additionally, we also apply normalizing flows to the posterior inference of GP
ODEs to resolve the issue of strong mean-field assumptions in posterior
inference. By applying normalizing flows in both these ways, our model improves
accuracy and uncertainty estimates for Bayesian Gaussian Process ODEs. We
validate the effectiveness of our approach on simulated dynamical systems and
real-world human motion data, including time series prediction and missing data
recovery tasks. Experimental results show that our proposed method effectively
captures model uncertainty while improving accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaffold-Based Multi-Objective Drug Candidate Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.07175v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.07175v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Agustin Kruel, Andrew D. McNaughton, Neeraj Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In therapeutic design, balancing various physiochemical properties is crucial
for molecule development, similar to how Multiparameter Optimization (MPO)
evaluates multiple variables to meet a primary goal. While many molecular
features can now be predicted using \textit{in silico} methods, aiding early
drug development, the vast data generated from high throughput virtual
screening challenges the practicality of traditional MPO approaches. Addressing
this, we introduce a scaffold focused graph-based Markov chain Monte Carlo
framework (ScaMARS) built to generate molecules with optimal properties. This
innovative framework is capable of self-training and handling a wider array of
properties, sampling different chemical spaces according to the starting
scaffold. The benchmark analysis on several properties shows that ScaMARS has a
diversity score of 84.6\% and has a much higher success rate of 99.5\% compared
to conditional models. The integration of new features into MPO significantly
enhances its adaptability and effectiveness in therapeutic design, facilitating
the discovery of candidates that efficiently optimize multiple properties.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepTreeGANv2: Iterative Pooling of Point Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.00042v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.00042v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moritz Alfons Wilhelm Scham, Dirk Krücker, Kerstin Borras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In High Energy Physics, detailed and time-consuming simulations are used for
particle interactions with detectors. To bypass these simulations with a
generative model, the generation of large point clouds in a short time is
required, while the complex dependencies between the particles must be
correctly modelled. Particle showers are inherently tree-based processes, as
each particle is produced by the decay or detector interaction of a particle of
the previous generation. In this work, we present a significant extension to
DeepTreeGAN, featuring a critic, that is able to aggregate such point clouds
iteratively in a tree-based manner. We show that this model can reproduce
complex distributions, and we evaluate its performance on the public JetNet 150
dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2311.12616</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tensor PCA from basis in tensor space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.02803v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.02803v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Claudio Turchetti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The aim of this paper is to present a mathematical framework for tensor PCA.
The proposed approach is able to overcome the limitations of previous methods
that extract a low dimensional subspace by iteratively solving an optimization
problem. The core of the proposed approach is the derivation of a basis in
tensor space from a real self-adjoint tensor operator, thus reducing the
problem of deriving a basis to an eigenvalue problem. Three different cases
have been studied to derive: i) a basis from a self-adjoint tensor operator;
ii) a rank-1 basis; iii) a basis in a subspace. In particular, the equivalence
between eigenvalue equation for a real self-adjoint tensor operator and
standard matrix eigenvalue equation has been proven. For all the three cases
considered, a subspace approach has been adopted to derive a tensor PCA.
Experiments on image datasets validate the proposed mathematical framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Deep Neural Network -- Mechanistic Hybrid Model to Predict
  Pharmacokinetics in Rat 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09167v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09167v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Führer, Andrea Gruber, Holger Diedam, Andreas H. Göller, Stephan Menz, Sebastian Schneckener
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An important aspect in the development of small molecules as drugs or
agro-chemicals is their systemic availability after intravenous and oral
administration. The prediction of the systemic availability from the chemical
structure of a potential candidate is highly desirable, as it allows to focus
the drug or agrochemical development on compounds with a favorable kinetic
profile. However, such pre-dictions are challenging as the availability is the
result of the complex interplay between molecular properties, biology and
physiology and training data is rare. In this work we improve the hybrid model
developed earlier [1]. We reduce the median fold change error for the total
oral exposure from 2.85 to 2.35 and for intravenous administration from 1.95 to
1.62. This is achieved by training on a larger data set, improving the neural
network architecture as well as the parametrization of mechanistic model.
Further, we extend our approach to predict additional endpoints and to handle
different covariates, like sex and dosage form. In contrast to a pure machine
learning model, our model is able to predict new end points on which it has not
been trained. We demonstrate this feature by predicting the exposure over the
first 24h, while the model has only been trained on the total exposure.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Version accepted by Journal of Computer-Aided Molecular Design</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Modal Financial Time-Series Retrieval Through Latent Space
  Projections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16741v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16741v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Bamford, Andrea Coletta, Elizabeth Fons, Sriram Gopalakrishnan, Svitlana Vyetrenko, Tucker Balch, Manuela Veloso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Financial firms commonly process and store billions of time-series data,
generated continuously and at a high frequency. To support efficient data
storage and retrieval, specialized time-series databases and systems have
emerged. These databases support indexing and querying of time-series by a
constrained Structured Query Language(SQL)-like format to enable queries like
"Stocks with monthly price returns greater than 5%", and expressed in rigid
formats. However, such queries do not capture the intrinsic complexity of high
dimensional time-series data, which can often be better described by images or
language (e.g., "A stock in low volatility regime"). Moreover, the required
storage, computational time, and retrieval complexity to search in the
time-series space are often non-trivial. In this paper, we propose and
demonstrate a framework to store multi-modal data for financial time-series in
a lower-dimensional latent space using deep encoders, such that the latent
space projections capture not only the time series trends but also other
desirable information or properties of the financial time-series data (such as
price volatility). Moreover, our approach allows user-friendly query
interfaces, enabling natural language text or sketches of time-series, for
which we have developed intuitive interfaces. We demonstrate the advantages of
our method in terms of computational efficiency and accuracy on real historical
data as well as synthetic data, and highlight the utility of latent-space
projections in the storage and retrieval of financial time-series data with
intuitive query modalities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICAIF 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Memory Gym: Towards Endless Tasks to Benchmark Memory Capabilities of
  Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.17207v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.17207v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Pleines, Matthias Pallasch, Frank Zimmer, Mike Preuss
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Memory Gym presents a suite of 2D partially observable environments, namely
Mortar Mayhem, Mystery Path, and Searing Spotlights, designed to benchmark
memory capabilities in decision-making agents. These environments, originally
with finite tasks, are expanded into innovative, endless formats, mirroring the
escalating challenges of cumulative memory games such as ``I packed my bag''.
This progression in task design shifts the focus from merely assessing sample
efficiency to also probing the levels of memory effectiveness in dynamic,
prolonged scenarios. To address the gap in available memory-based Deep
Reinforcement Learning baselines, we introduce an implementation that
integrates Transformer-XL (TrXL) with Proximal Policy Optimization. This
approach utilizes TrXL as a form of episodic memory, employing a sliding window
technique. Our comparative study between the Gated Recurrent Unit (GRU) and
TrXL reveals varied performances across different settings. TrXL, on the finite
environments, demonstrates superior sample efficiency in Mystery Path and
outperforms in Mortar Mayhem. However, GRU is more efficient on Searing
Spotlights. Most notably, in all endless tasks, GRU makes a remarkable
resurgence, consistently outperforming TrXL by significant margins. Website and
Source Code: \url{https://github.com/MarcoMeter/endless-memory-gym/}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, 17 figures, 5 tables, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long
  Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19923v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19923v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Günther, Jackmin Ong, Isabelle Mohr, Alaeddine Abdessalem, Tanguy Abel, Mohammad Kalim Akram, Susana Guzman, Georgios Mastrapas, Saba Sturua, Bo Wang, Maximilian Werk, Nan Wang, Han Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text embedding models have emerged as powerful tools for transforming
sentences into fixed-sized feature vectors that encapsulate semantic
information. While these models are essential for tasks like information
retrieval, semantic clustering, and text re-ranking, most existing open-source
models, especially those built on architectures like BERT, struggle to
represent lengthy documents and often resort to truncation. One common approach
to mitigate this challenge involves splitting documents into smaller paragraphs
for embedding. However, this strategy results in a much larger set of vectors,
consequently leading to increased memory consumption and computationally
intensive vector searches with elevated latency.
  To address these challenges, we introduce Jina Embeddings 2, an open-source
text embedding model capable of accommodating up to 8192 tokens. This model is
designed to transcend the conventional 512-token limit and adeptly process long
documents. Jina Embeddings 2 not only achieves state-of-the-art performance on
a range of embedding-related tasks in the MTEB benchmark but also matches the
performance of OpenAI's proprietary ada-002 model. Additionally, our
experiments indicate that an extended context can enhance performance in tasks
such as NarrativeQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerated First-Order Optimization under Nonlinear Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.00316v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.00316v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Muehlebach, Michael I. Jordan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We exploit analogies between first-order algorithms for constrained
optimization and non-smooth dynamical systems to design a new class of
accelerated first-order algorithms for constrained optimization. Unlike
Frank-Wolfe or projected gradients, these algorithms avoid optimization over
the entire feasible set at each iteration. We prove convergence to stationary
points even in a nonconvex setting and we derive accelerated rates for the
convex setting both in continuous time, as well as in discrete time. An
important property of these algorithms is that constraints are expressed in
terms of velocities instead of positions, which naturally leads to sparse,
local and convex approximations of the feasible set (even if the feasible set
is nonconvex). Thus, the complexity tends to grow mildly in the number of
decision variables and in the number of constraints, which makes the algorithms
suitable for machine learning applications. We apply our algorithms to a
compressed sensing and a sparse regression problem, showing that we can treat
nonconvex $\ell^p$ constraints ($p<1$) efficiently, while recovering
state-of-the-art performance for $p=1$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pseudo-Hamiltonian neural networks for learning partial differential
  equations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.14374v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.14374v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sølve Eidnes, Kjetil Olsen Lye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pseudo-Hamiltonian neural networks (PHNN) were recently introduced for
learning dynamical systems that can be modelled by ordinary differential
equations. In this paper, we extend the method to partial differential
equations. The resulting model is comprised of up to three neural networks,
modelling terms representing conservation, dissipation and external forces, and
discrete convolution operators that can either be learned or be given as input.
We demonstrate numerically the superior performance of PHNN compared to a
baseline model that models the full dynamics by a single neural network.
Moreover, since the PHNN model consists of three parts with different physical
interpretations, these can be studied separately to gain insight into the
system, and the learned model is applicable also if external forces are removed
or changed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages, 18 figures; v3: expanded text and added numerical
  experiments, new subsections: 5.1, 6.3, 6.4</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive learning of density ratios in RKHS 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.16164v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.16164v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Werner Zellinger, Stefan Kindermann, Sergei V. Pereverzyev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating the ratio of two probability densities from finitely many
observations of the densities is a central problem in machine learning and
statistics with applications in two-sample testing, divergence estimation,
generative modeling, covariate shift adaptation, conditional density
estimation, and novelty detection. In this work, we analyze a large class of
density ratio estimation methods that minimize a regularized Bregman divergence
between the true density ratio and a model in a reproducing kernel Hilbert
space (RKHS). We derive new finite-sample error bounds, and we propose a
Lepskii type parameter choice principle that minimizes the bounds without
knowledge of the regularity of the density ratio. In the special case of
quadratic loss, our method adaptively achieves a minimax optimal error rate. A
numerical illustration is provided.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Collaborative Watermarking for Adversarial Speech Synthesis <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15224v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15224v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lauri Juvela, Xin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in neural speech synthesis have brought us technology that is not
only close to human naturalness, but is also capable of instant voice cloning
with little data, and is highly accessible with pre-trained models available.
Naturally, the potential flood of generated content raises the need for
synthetic speech detection and watermarking. Recently, considerable research
effort in synthetic speech detection has been related to the Automatic Speaker
Verification and Spoofing Countermeasure Challenge (ASVspoof), which focuses on
passive countermeasures. This paper takes a complementary view to generated
speech detection: a synthesis system should make an active effort to watermark
the generated speech in a way that aids detection by another machine, but
remains transparent to a human listener. We propose a collaborative training
scheme for synthetic speech watermarking and show that a HiFi-GAN neural
vocoder collaborating with the ASVspoof 2021 baseline countermeasure models
consistently improves detection performance over conventional classifier
training. Furthermore, we demonstrate how collaborative training can be paired
with augmentation strategies for added robustness against noise and
time-stretching. Finally, listening tests demonstrate that collaborative
training has little adverse effect on perceptual quality of vocoded speech.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pseudo-Hamiltonian system identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.06920v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.06920v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sigurd Holmsen, Sølve Eidnes, Signe Riemer-Sørensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying the underlying dynamics of physical systems can be challenging
when only provided with observational data. In this work, we consider systems
that can be modelled as first-order ordinary differential equations. By
assuming a certain pseudo-Hamiltonian formulation, we are able to learn the
analytic terms of internal dynamics even if the model is trained on data where
the system is affected by unknown damping and external disturbances. In cases
where it is difficult to find analytic terms for the disturbances, a hybrid
model that uses a neural network to learn these can still accurately identify
the dynamics of the system as if under ideal conditions. This makes the models
applicable in some situations where other system identification models fail.
Furthermore, we propose to use a fourth-order symmetric integration scheme in
the loss function and avoid actual integration in the training, and demonstrate
on varied examples how this leads to increased performance on noisy data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 21 figures, including the appendix; v2: restructured and
  modified the text, added Section 6</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harmonizing Covariance and Expressiveness for Deep Hamiltonian
  Regression in Crystalline Material Research: a Hybrid Cascaded Regression
  Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00744v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00744v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shi Yin, Xudong Zhu, Tianyu Gao, Haochong Zhang, Feng Wu, Lixin He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning for Hamiltonian regression of quantum systems in material
research necessitates satisfying the covariance laws, among which achieving
SO(3)-equivariance without sacrificing the expressiveness of networks remains
an elusive challenge due to the restriction to non-linear mappings on
guaranteeing theoretical equivariance. To alleviate the
covariance-expressiveness dilemma, we propose a hybrid framework with two
cascaded regression stages. The first stage, with a theoretically-guaranteed
covariant neural network modeling symmetry properties of 3D atom systems,
yields theoretically covariant features and baseline Hamiltonian predictions,
assisting the second stage in learning covariance. Meanwhile, the second stage,
powered by a non-linear 3D graph Transformer network we propose for structural
modeling of 3D atomic systems, refines the first stage's output as a
fine-grained prediction of Hamiltonians with better expressiveness capability.
The combination of a theoretically covariant yet inevitably less expressive
model with a highly expressive non-linear network enables precise,
generalizable predictions while maintaining robust covariance under coordinate
transformations. Our method achieves state-of-the-art performance in
Hamiltonian prediction for electronic structure calculations, confirmed through
experiments on five crystalline material databases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Estimating and Mitigating the Congestion Effect of Curbside Pick-ups and
  Drop-offs: A Causal Inference Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.02164v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.02164v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohui Liu, Sean Qian, Hock-Hai Teo, Wei Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Curb space is one of the busiest areas in urban road networks. Especially in
recent years, the rapid increase of ride-hailing trips and commercial
deliveries has induced massive pick-ups/drop-offs (PUDOs), which occupy the
limited curb space that was designed and built decades ago. These PUDOs could
jam curbside utilization and disturb the mainline traffic flow, evidently
leading to significant negative societal externalities. However, there is a
lack of an analytical framework that rigorously quantifies and mitigates the
congestion effect of PUDOs in the system view, particularly with little data
support and involvement of confounding effects. To bridge this research gap,
this paper develops a rigorous causal inference approach to estimate the
congestion effect of PUDOs on general regional networks. A causal graph is set
to represent the spatio-temporal relationship between PUDOs and traffic speed,
and a double and separated machine learning (DSML) method is proposed to
quantify how PUDOs affect traffic congestion. Additionally, a re-routing
formulation is developed and solved to encourage passenger walking and traffic
flow re-routing to achieve system optimization. Numerical experiments are
conducted using real-world data in the Manhattan area. On average, 100
additional units of PUDOs in a region could reduce the traffic speed by 3.70
and 4.54 mph on weekdays and weekends, respectively. Re-routing trips with
PUDOs on curb space could respectively reduce the system-wide total travel time
by 2.44% and 2.12% in Midtown and Central Park on weekdays. Sensitivity
analysis is also conducted to demonstrate the effectiveness and robustness of
the proposed framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Transportation Science</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10477v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10477v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Chen, Chunwei Wang, Kuo Yang, Jianhua Han, Lanqing Hong, Fei Mi, Hang Xu, Zhengying Liu, Wenyong Huang, Zhenguo Li, Dit-Yan Yeung, Lifeng Shang, Xin Jiang, Qun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of large language models (LLMs) has not only provided
numerous opportunities but also presented significant challenges. This becomes
particularly evident when LLMs inadvertently generate harmful or toxic content,
either unintentionally or because of intentional inducement. Existing alignment
methods usually direct LLMs toward the favorable outcomes by utilizing
human-annotated, flawless instruction-response pairs. Conversely, this study
proposes a novel alignment technique based on mistake analysis, which
deliberately exposes LLMs to erroneous content to learn the reasons for
mistakes and how to avoid them. In this case, mistakes are repurposed into
valuable data for alignment, effectively helping to avoid the production of
erroneous responses. Without external models or human annotations, our method
leverages a model's intrinsic ability to discern undesirable mistakes and
improves the safety of its generated responses. Experimental results reveal
that our method outperforms existing alignment approaches in enhancing model
safety while maintaining the overall utility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Arbitrary Distributions Mapping via SyMOT-Flow: A Flow-based Approach
  Integrating Maximum Mean Discrepancy and Optimal Transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.13815v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.13815v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Xiong, Qiaoqiao Ding, Xiaoqun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Finding a transformation between two unknown probability distributions from
finite samples is crucial for modeling complex data distributions and
performing tasks such as sample generation, domain adaptation and statistical
inference. One powerful framework for such transformations is normalizing flow,
which transforms an unknown distribution into a standard normal distribution
using an invertible network. In this paper, we introduce a novel model called
SyMOT-Flow that trains an invertible transformation by minimizing the symmetric
maximum mean discrepancy between samples from two unknown distributions, and an
optimal transport cost is incorporated as regularization to obtain a
short-distance and interpretable transformation. The resulted transformation
leads to more stable and accurate sample generation. Several theoretical
results are established for the proposed model and its effectiveness is
validated with low-dimensional illustrative examples as well as
high-dimensional bi-modality medical image generation through the forward and
reverse flows.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages article, 1 pages supplementary</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Contextual Lasso: Sparse Linear Models via Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.00878v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.00878v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Thompson, Amir Dezfouli, Robert Kohn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse linear models are one of several core tools for interpretable machine
learning, a field of emerging importance as predictive models permeate
decision-making in many domains. Unfortunately, sparse linear models are far
less flexible as functions of their input features than black-box models like
deep neural networks. With this capability gap in mind, we study a not-uncommon
situation where the input features dichotomize into two groups: explanatory
features, which are candidates for inclusion as variables in an interpretable
model, and contextual features, which select from the candidate variables and
determine their effects. This dichotomy leads us to the contextual lasso, a new
statistical estimator that fits a sparse linear model to the explanatory
features such that the sparsity pattern and coefficients vary as a function of
the contextual features. The fitting process learns this function
nonparametrically via a deep neural network. To attain sparse coefficients, we
train the network with a novel lasso regularizer in the form of a projection
layer that maps the network's output onto the space of $\ell_1$-constrained
linear models. An extensive suite of experiments on real and synthetic data
suggests that the learned models, which remain highly transparent, can be
sparser than the regular lasso without sacrificing the predictive power of a
standard deep neural network.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Advances in Neural Information Processing Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Autonomous Assessment of Demonstration Sufficiency via Bayesian Inverse
  Reinforcement Learning <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.15542v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.15542v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tu Trinh, Haoyu Chen, Daniel S. Brown
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We examine the problem of determining demonstration sufficiency: how can a
robot self-assess whether it has received enough demonstrations from an expert
to ensure a desired level of performance? To address this problem, we propose a
novel self-assessment approach based on Bayesian inverse reinforcement learning
and value-at-risk, enabling learning-from-demonstration ("LfD") robots to
compute high-confidence bounds on their performance and use these bounds to
determine when they have a sufficient number of demonstrations. We propose and
evaluate two definitions of sufficiency: (1) normalized expected value
difference, which measures regret with respect to the human's unobserved reward
function, and (2) percent improvement over a baseline policy. We demonstrate
how to formulate high-confidence bounds on both of these metrics. We evaluate
our approach in simulation for both discrete and continuous state-space domains
and illustrate the feasibility of developing a robotic system that can
accurately evaluate demonstration sufficiency. We also show that the robot can
utilize active learning in asking for demonstrations from specific states which
results in fewer demos needed for the robot to still maintain high confidence
in its policy. Finally, via a user study, we show that our approach
successfully enables robots to perform at users' desired performance levels,
without needing too many or perfectly optimal demonstrations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Prior version appears in proceedings of AAAI FSS-22 Symposium
  "Lessons Learned for Autonomous Assessment of Machine Abilities (LLAAMA)".
  Current version appears in proceedings of HRI '24, March 11-14, 2024,
  Boulder, CO, USA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Study on the Calibration of In-context Learning <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04021v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04021v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanlin Zhang, Yi-Fan Zhang, Yaodong Yu, Dhruv Madeka, Dean Foster, Eric Xing, Himabindu Lakkaraju, Sham Kakade
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate uncertainty quantification is crucial for the safe deployment of
language models (LMs), and prior research has demonstrated improvements in the
calibration of modern LMs. Our study focuses on in-context learning (ICL), a
prevalent method for adapting static LMs through tailored prompts, and examines
the balance between performance and calibration across a broad spectrum of
natural language understanding and reasoning tasks. Through comprehensive
experiments, we observe that, with an increasing number of ICL examples, models
initially exhibit increased miscalibration before achieving better calibration
and miscalibration tends to arise in low-shot settings. Moreover, we find that
methods aimed at improving usability, such as fine-tuning and chain-of-thought
(CoT) prompting, can lead to miscalibration and unreliable natural language
explanations, suggesting that new methods may be required for scenarios where
models are expected to be reliable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Spotlight Talk at NeurIPS 2023 Workshop on Failure Modes in the Age
  of Foundation Models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Graph Neural Networks in Intelligent Transportation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00713v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00713v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hourun Li, Yusheng Zhao, Zhengyang Mao, Yifang Qin, Zhiping Xiao, Jiaqi Feng, Yiyang Gu, Wei Ju, Xiao Luo, Ming Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intelligent Transportation System (ITS) is vital in improving traffic
congestion, reducing traffic accidents, optimizing urban planning, etc.
However, due to the complexity of the traffic network, traditional machine
learning and statistical methods are relegated to the background. With the
advent of the artificial intelligence era, many deep learning frameworks have
made remarkable progress in various fields and are now considered effective
methods in many areas. As a deep learning method, Graph Neural Networks (GNNs)
have emerged as a highly competitive method in the ITS field since 2019 due to
their strong ability to model graph-related problems. As a result, more and
more scholars pay attention to the applications of GNNs in transportation
domains, which have shown excellent performance. However, most of the research
in this area is still concentrated on traffic forecasting, while other ITS
domains, such as autonomous vehicles and urban planning, still require more
attention. This paper aims to review the applications of GNNs in six
representative and emerging ITS domains: traffic forecasting, autonomous
vehicles, traffic signal control, transportation safety, demand prediction, and
parking management. We have reviewed extensive graph-related studies from 2018
to 2023, summarized their methods, features, and contributions, and presented
them in informative tables or lists. Finally, we have identified the challenges
of applying GNNs to ITS and suggested potential future directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Risk-optimized Outlier Removal for Robust 3D Point Cloud Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.10875v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.10875v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinke Li, Junchi Lu, Henghui Ding, Changsheng Sun, Joey Tianyi Zhou, Chee Yeow Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growth of 3D sensing technology, deep learning system for 3D point
clouds has become increasingly important, especially in applications like
autonomous vehicles where safety is a primary concern. However, there are also
growing concerns about the reliability of these systems when they encounter
noisy point clouds, whether occurring naturally or introduced with malicious
intent. This paper highlights the challenges of point cloud classification
posed by various forms of noise, from simple background noise to malicious
backdoor attacks that can intentionally skew model predictions. While there's
an urgent need for optimized point cloud denoising, current point outlier
removal approaches, an essential step for denoising, rely heavily on
handcrafted strategies and are not adapted for higher-level tasks, such as
classification. To address this issue, we introduce an innovative point outlier
cleansing method that harnesses the power of downstream classification models.
By employing gradient-based attribution analysis, we define a novel concept:
point risk. Drawing inspiration from tail risk minimization in finance, we
recast the outlier removal process as an optimization problem, named PointCVaR.
Extensive experiments show that our proposed technique not only robustly
filters diverse point cloud outliers but also consistently and significantly
enhances existing robust methods for point cloud classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hybrid Internal Model: Learning Agile Legged Locomotion with Simulated
  Robot Response 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11460v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11460v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junfeng Long, Zirui Wang, Quanyi Li, Jiawei Gao, Liu Cao, Jiangmiao Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust locomotion control depends on accurate state estimations. However, the
sensors of most legged robots can only provide partial and noisy observations,
making the estimation particularly challenging, especially for external states
like terrain frictions and elevation maps. Inspired by the classical Internal
Model Control principle, we consider these external states as disturbances and
introduce Hybrid Internal Model (HIM) to estimate them according to the
response of the robot. The response, which we refer to as the hybrid internal
embedding, contains the robot's explicit velocity and implicit stability
representation, corresponding to two primary goals for locomotion tasks:
explicitly tracking velocity and implicitly maintaining stability. We use
contrastive learning to optimize the embedding to be close to the robot's
successor state, in which the response is naturally embedded. HIM has several
appealing benefits: It only needs the robot's proprioceptions, i.e., those from
joint encoders and IMU as observations. It innovatively maintains consistent
observations between simulation reference and reality that avoids information
loss in mimicking learning. It exploits batch-level information that is more
robust to noises and keeps better sample efficiency. It only requires 1 hour of
training on an RTX 4090 to enable a quadruped robot to traverse any terrain
under any disturbances. A wealth of real-world experiments demonstrates its
agility, even in high-difficulty tasks and cases never occurred during the
training process, revealing remarkable open-world generalizability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Use 1 hour to train a quadruped robot capable of traversing any
  terrain under any disturbances in the open world, Project Page:
  https://github.com/OpenRobotLab/HIMLoco</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficiently Disentangle Causal Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.01942v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.01942v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanpeng Li, Joel Hestness, Mohamed Elhoseiny, Liang Zhao, Kenneth Church
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes an efficient approach to learning disentangled
representations with causal mechanisms based on the difference of conditional
probabilities in original and new distributions. We approximate the difference
with models' generalization abilities so that it fits in the standard machine
learning framework and can be efficiently computed. In contrast to the
state-of-the-art approach, which relies on the learner's adaptation speed to
new distribution, the proposed approach only requires evaluating the model's
generalization ability. We provide a theoretical explanation for the advantage
of the proposed method, and our experiments show that the proposed technique is
1.9--11.0$\times$ more sample efficient and 9.4--32.4 times quicker than the
previous method on various tasks. The source code is available at
\url{https://github.com/yuanpeng16/EDCR}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Reliable Knowledge Processing Framework for Combustion Science using
  Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00544v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00544v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vansh Sharma, Venkat Raman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research explores the integration of large language models (LLMs) into
scientific data assimilation, focusing on combustion science as a case study.
Leveraging foundational models integrated with Retrieval-Augmented Generation
(RAG) framework, the study introduces an approach to process diverse combustion
research data, spanning experimental studies, simulations, and literature. The
multifaceted nature of combustion research emphasizes the critical role of
knowledge processing in navigating and extracting valuable information from a
vast and diverse pool of sources. The developed approach minimizes
computational and economic expenses while optimizing data privacy and accuracy.
It incorporates prompt engineering and offline open-source LLMs, offering user
autonomy in selecting base models. The study provides a thorough examination of
text segmentation strategies, conducts comparative studies between LLMs, and
explores various optimized prompts to demonstrate the effectiveness of the
framework. By incorporating an external database, the framework outperforms a
conventional LLM in generating accurate responses and constructing robust
arguments. Additionally, the study delves into the investigation of optimized
prompt templates for the purpose of efficient extraction of scientific
literature. The research addresses concerns related to hallucinations and false
research articles by introducing a custom workflow developed with a detection
algorithm to filter out inaccuracies. Despite identified areas for improvement,
the framework consistently delivers accurate domain-specific responses with
minimal human oversight. The prompt-agnostic approach introduced holds promise
for future deliberations. The study underscores the significance of integrating
LLMs and knowledge processing techniques in scientific research, providing a
foundation for advancements in data assimilation and utilization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages and 10 figures; Fixed figure resolution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SASSL: Enhancing <span class="highlight-title">Self-Supervised</span> Learning via Neural Style Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.01187v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.01187v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renan A. Rojas-Gomez, Karan Singhal, Ali Etemad, Alex Bijamov, Warren R. Morningstar, Philip Andrew Mansfield
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning relies heavily on data augmentation to extract
meaningful representations from unlabeled images. While existing
state-of-the-art augmentation pipelines incorporate a wide range of primitive
transformations, these often disregard natural image structure. Thus, augmented
samples can exhibit degraded semantic information and low stylistic diversity,
affecting downstream performance of self-supervised representations. To
overcome this, we propose SASSL: Style Augmentations for Self Supervised
Learning, a novel augmentation technique based on Neural Style Transfer. The
method decouples semantic and stylistic attributes in images and applies
transformations exclusively to the style while preserving content, generating
diverse augmented samples that better retain their semantic properties.
Experimental results show our technique achieves a top-1 classification
performance improvement of more than 2% on ImageNet compared to the
well-established MoCo v2. We also measure transfer learning performance across
five diverse datasets, observing significant improvements of up to 3.75%. Our
experiments indicate that decoupling style from content information and
transferring style across datasets to diversify augmentations can significantly
improve downstream performance of self-supervised representations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Auto-Modeling of Formal Verification for NextG Protocols: A
  Multimodal cross- and self-attention Large Language Model Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17353v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17353v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingda Yang, Ying Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Auto-modeling of Formal Verification with Real-world
Prompting for 5G and NextG protocols (AVRE), a novel system designed for the
formal verification of Next Generation (NextG) communication protocols,
addressing the increasing complexity and scalability challenges in network
protocol design and verification. Utilizing Large Language Models (LLMs), AVRE
transforms protocol descriptions into dependency graphs and formal models,
efficiently resolving ambiguities and capturing design intent. The system
integrates a transformer model with LLMs to autonomously establish quantifiable
dependency relationships through cross- and self-attention mechanisms. Enhanced
by iterative feedback from the HyFuzz experimental platform, AVRE significantly
advances the accuracy and relevance of formal verification in complex
communication protocols, offering a groundbreaking approach to validating
sophisticated communication systems. We compare CAL's performance with
state-of-the-art LLM-based models and traditional time sequence models,
demonstrating its superiority in accuracy and robustness, achieving an accuracy
of 95.94\% and an AUC of 0.98. This NLP-based approach enables, for the first
time, the creation of exploits directly from design documents, making
remarkable progress in scalable system verification and validation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prediction of Effective Elastic Moduli of Rocks using Graph Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19274v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19274v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaehong Chung, Rasool Ahmad, WaiChing Sun, Wei Cai, Tapan Mukerji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents a Graph Neural Networks (GNNs)-based approach for
predicting the effective elastic moduli of rocks from their digital CT-scan
images. We use the Mapper algorithm to transform 3D digital rock images into
graph datasets, encapsulating essential geometrical information. These graphs,
after training, prove effective in predicting elastic moduli. Our GNN model
shows robust predictive capabilities across various graph sizes derived from
various subcube dimensions. Not only does it perform well on the test dataset,
but it also maintains high prediction accuracy for unseen rocks and unexplored
subcube sizes. Comparative analysis with Convolutional Neural Networks (CNNs)
reveals the superior performance of GNNs in predicting unseen rock properties.
Moreover, the graph representation of microstructures significantly reduces GPU
memory requirements (compared to the grid representation for CNNs), enabling
greater flexibility in the batch size selection. This work demonstrates the
potential of GNN models in enhancing the prediction accuracy of rock properties
and boosting the efficiency of digital rock analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing with Low Budgets: a Comparison on the Black-box Optimization
  Benchmarking Suite and OpenAI Gym 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00077v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00077v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elena Raponi, Nathanael Rakotonirina Carraz, Jérémy Rapin, Carola Doerr, Olivier Teytaud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing ubiquity of machine learning (ML) has led it to enter various
areas of computer science, including black-box optimization (BBO). Recent
research is particularly concerned with Bayesian optimization (BO). BO-based
algorithms are popular in the ML community, as they are used for hyperparameter
optimization and more generally for algorithm configuration. However, their
efficiency decreases as the dimensionality of the problem and the budget of
evaluations increase. Meanwhile, derivative-free optimization methods have
evolved independently in the optimization community. Therefore, we urge to
understand whether cross-fertilization is possible between the two communities,
ML and BBO, i.e., whether algorithms that are heavily used in ML also work well
in BBO and vice versa. Comparative experiments often involve rather small
benchmarks and show visible problems in the experimental setup, such as poor
initialization of baselines, overfitting due to problem-specific setting of
hyperparameters, and low statistical significance.
  With this paper, we update and extend a comparative study presented by Hutter
et al. in 2013. We compare BBO tools for ML with more classical heuristics,
first on the well-known BBOB benchmark suite from the COCO environment and then
on Direct Policy Search for OpenAI Gym, a reinforcement learning benchmark. Our
results confirm that BO-based optimizers perform well on both benchmarks when
budgets are limited, albeit with a higher computational cost, while they are
often outperformed by algorithms from other families when the evaluation budget
becomes larger. We also show that some algorithms from the BBO community
perform surprisingly well on ML tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in IEEE Transactions on Evolutionary Computation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeXtreme: Transfer of Agile In-hand Manipulation from Simulation to
  Reality <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.13702v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.13702v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ankur Handa, Arthur Allshire, Viktor Makoviychuk, Aleksei Petrenko, Ritvik Singh, Jingzhou Liu, Denys Makoviichuk, Karl Van Wyk, Alexander Zhurkevich, Balakumar Sundaralingam, Yashraj Narang, Jean-Francois Lafleche, Dieter Fox, Gavriel State
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has demonstrated the ability of deep reinforcement learning (RL)
algorithms to learn complex robotic behaviours in simulation, including in the
domain of multi-fingered manipulation. However, such models can be challenging
to transfer to the real world due to the gap between simulation and reality. In
this paper, we present our techniques to train a) a policy that can perform
robust dexterous manipulation on an anthropomorphic robot hand and b) a robust
pose estimator suitable for providing reliable real-time information on the
state of the object being manipulated. Our policies are trained to adapt to a
wide range of conditions in simulation. Consequently, our vision-based policies
significantly outperform the best vision policies in the literature on the same
reorientation task and are competitive with policies that are given privileged
state information via motion capture systems. Our work reaffirms the
possibilities of sim-to-real transfer for dexterous manipulation in diverse
kinds of hardware and simulator setups, and in our case, with the Allegro Hand
and Isaac Gym GPU-based simulation. Furthermore, it opens up possibilities for
researchers to achieve such results with commonly-available, affordable robot
hands and cameras. Videos of the resulting policy and supplementary
information, including experiments and demos, can be found at
https://dextreme.org/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages. A smaller version of this paper is accepted to ICRA 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models Are Not Strong Abstract Reasoners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.19555v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.19555v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaël Gendron, Qiming Bao, Michael Witbrock, Gillian Dobbie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models have shown tremendous performance on a large variety of
natural language processing tasks, ranging from text comprehension to common
sense reasoning. However, the mechanisms responsible for this success remain
opaque, and it is unclear whether LLMs can achieve human-like cognitive
capabilities or whether these models are still fundamentally circumscribed.
Abstract reasoning is a fundamental task for cognition, consisting of finding
and applying a general pattern from few data. Evaluating deep neural
architectures on this task could give insight into their potential limitations
regarding reasoning and their broad generalisation abilities, yet this is
currently an under-explored area. In this paper, we introduce a new benchmark
for evaluating language models beyond memorization on abstract reasoning tasks.
We perform extensive evaluations of state-of-the-art LLMs, showing that they
currently achieve very limited performance in contrast with other natural
language tasks, even when applying techniques that have been shown to improve
performance on other NLP tasks. We argue that guiding LLM generation to follow
causal paths could help improve the generalisation and reasoning abilities of
LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>50 pages, 14 pages for the main paper and 36 pages for the
  supplement, 35 figures, 17 tables. V3: performed additional experiments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05134v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05134v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Grimal, Hervé Le Borgne, Olivier Ferret, Julien Tourille
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The progress in the generation of synthetic images has made it crucial to
assess their quality. While several metrics have been proposed to assess the
rendering of images, it is crucial for Text-to-Image (T2I) models, which
generate images based on a prompt, to consider additional aspects such as to
which extent the generated image matches the important content of the prompt.
Moreover, although the generated images usually result from a random starting
point, the influence of this one is generally not considered. In this article,
we propose a new metric based on prompt templates to study the alignment
between the content specified in the prompt and the corresponding generated
images. It allows us to better characterize the alignment in terms of the type
of the specified objects, their number, and their color. We conducted a study
on several recent T2I models about various aspects. An additional interesting
result we obtained with our approach is that image quality can vary drastically
depending on the noise used as a seed for the images. We also quantify the
influence of the number of concepts in the prompt, their order as well as their
(color) attributes. Finally, our method allows us to identify some seeds that
produce better images than others, opening novel directions of research on this
understudied topic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Low Variance Off-policy Evaluation with State-based Importance Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.03932v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.03932v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David M. Bossens, Philip S. Thomas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In off-policy reinforcement learning, a behaviour policy performs exploratory
interactions with the environment to obtain state-action-reward samples which
are then used to learn a target policy that optimises the expected return. This
leads to a problem of off-policy evaluation, where one needs to evaluate the
target policy from samples collected by the often unrelated behaviour policy.
Importance sampling is a traditional statistical technique that is often
applied to off-policy evaluation. While importance sampling estimators are
unbiased, their variance increases exponentially with the horizon of the
decision process due to computing the importance weight as a product of action
probability ratios, yielding estimates with low accuracy for domains involving
long-term planning. This paper proposes state-based importance sampling, which
drops the action probability ratios of sub-trajectories with ``negligible
states'' -- roughly speaking, those for which the chosen actions have no impact
on the return estimate -- from the computation of the importance weight.
Theoretical results show this reduces the ordinary importance sampling variance
from $O(\exp(H))$ to $O(\exp(X))$ where $X < H$ is the largest subtrajectory
with non-negligible states. To identify negligible states, two search
algorithms are proposed, one based on covariance testing and one based on
state-action values. We formulate state-based variants of ordinary importance
sampling, weighted importance sampling, per-decision importance sampling,
incremental importance sampling, doubly robust off-policy evaluation, and
stationary density ratio estimation. Experiments in four distinct domains show
that state-based methods consistently yield reduced variance and improved
accuracy compared to their traditional counterparts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Selective classification using a robust meta-learning approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05987v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05987v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nishant Jain, Karthikeyan Shanmugam, Pradeep Shenoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predictive uncertainty-a model's self awareness regarding its accuracy on an
input-is key for both building robust models via training interventions and for
test-time applications such as selective classification. We propose a novel
instance-conditioned reweighting approach that captures predictive uncertainty
using an auxiliary network and unifies these train- and test-time applications.
The auxiliary network is trained using a meta-objective in a bilevel
optimization framework. A key contribution of our proposal is the
meta-objective of minimizing the dropout variance, an approximation of Bayesian
Predictive uncertainty. We show in controlled experiments that we effectively
capture the diverse specific notions of uncertainty through this
meta-objective, while previous approaches only capture certain aspects. These
results translate to significant gains in real-world settings-selective
classification, label noise, domain adaptation, calibration-and across
datasets-Imagenet, Cifar100, diabetic retinopathy, Camelyon, WILDs,
Imagenet-C,-A,-R, Clothing1M, etc. For Diabetic Retinopathy, we see upto
3.4%/3.3% accuracy and AUC gains over SOTA in selective classification. We also
improve upon large-scale pretrained models such as PLEX.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kernel Density Estimation for Multiclass Quantification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00490v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00490v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alejandro Moreo, Pablo González, Juan José del Coz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several disciplines, like the social sciences, epidemiology, sentiment
analysis, or market research, are interested in knowing the distribution of the
classes in a population rather than the individual labels of the members
thereof. Quantification is the supervised machine learning task concerned with
obtaining accurate predictors of class prevalence, and to do so particularly in
the presence of label shift. The distribution-matching (DM) approaches
represent one of the most important families among the quantification methods
that have been proposed in the literature so far. Current DM approaches model
the involved populations by means of histograms of posterior probabilities. In
this paper, we argue that their application to the multiclass setting is
suboptimal since the histograms become class-specific, thus missing the
opportunity to model inter-class information that may exist in the data. We
propose a new representation mechanism based on multivariate densities that we
model via kernel density estimation (KDE). The experiments we have carried out
show our method, dubbed KDEy, yields superior quantification performance with
respect to previous DM approaches. We also investigate the KDE-based
representation within the maximum likelihood framework and show KDEy often
shows superior performance with respect to the expectation-maximization method
for quantification, arguably the strongest contender in the quantification
arena to date.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>fixed broken references to appendices</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NU-Class Net: A Novel Deep Learning-based Approach for Video Quality
  Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parham Zilouchian Moghaddam, Mehdi Modarressi, MohammadAmin Sadeghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video content has experienced a surge in popularity, asserting its dominance
over internet traffic and Internet of Things (IoT) networks. Video compression
has long been regarded as the primary means of efficiently managing the
substantial multimedia traffic generated by video-capturing devices.
Nevertheless, video compression algorithms entail significant computational
demands in order to achieve substantial compression ratios. This complexity
presents a formidable challenge when implementing efficient video coding
standards in resource-constrained embedded systems, such as IoT edge node
cameras. To tackle this challenge, this paper introduces NU-Class Net, an
innovative deep-learning model designed to mitigate compression artifacts
stemming from lossy compression codecs. This enhancement significantly elevates
the perceptible quality of low-bit-rate videos. By employing the NU-Class Net,
the video encoder within the video-capturing node can reduce output quality,
thereby generating low-bit-rate videos and effectively curtailing both
computation and bandwidth requirements at the edge. On the decoder side, which
is typically less encumbered by resource limitations, NU-Class Net is applied
after the video decoder to compensate for artifacts and approximate the quality
of the original video. Experimental results affirm the efficacy of the proposed
model in enhancing the perceptible quality of videos, especially those streamed
at low bit rates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RS5M and GeoRSCLIP: A Large Scale Vision-Language <span class="highlight-title">Dataset</span> and A Large
  Vision-Language Model for Remote Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.11300v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.11300v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zilun Zhang, Tiancheng Zhao, Yulong Guo, Jianwei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained Vision-Language Models (VLMs) utilizing extensive image-text
paired data have demonstrated unprecedented image-text association
capabilities, achieving remarkable results across various downstream tasks. A
critical challenge is how to make use of existing large-scale pre-trained VLMs,
which are trained on common objects, to perform the domain-specific transfer
for accomplishing domain-related downstream tasks. A critical challenge is how
to make use of existing large-scale pre-trained VLMs, which are trained on
common objects, to perform the domain-specific transfer for accomplishing
domain-related downstream tasks. In this paper, we propose a new framework that
includes the Domain pre-trained Vision-Language Model (DVLM), bridging the gap
between the General Vision-Language Model (GVLM) and domain-specific downstream
tasks. Moreover, we present an image-text paired dataset in the field of remote
sensing (RS), RS5M, which has 5 million RS images with English descriptions.
The dataset is obtained from filtering publicly available image-text paired
datasets and captioning label-only RS datasets with pre-trained VLM. These
constitute the first large-scale RS image-text paired dataset. Additionally, we
fine-tuned the CLIP model and tried several Parameter-Efficient Fine-Tuning
methods on RS5M to implement the DVLM. Experimental results show that our
proposed dataset is highly effective for various tasks, and our model GeoRSCLIP
improves upon the baseline or previous state-of-the-art model by $3\%\sim20\%$
in Zero-shot Classification (ZSC), $3\%\sim6\%$ in Remote Sensing Cross-Modal
Text-Image Retrieval (RSCTIR) and $4\%\sim5\%$ in Semantic Localization (SeLo)
tasks. Dataset and models have been released in:
\url{https://github.com/om-ai-lab/RS5M}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>RS5M dataset v5</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MusER: Musical Element-Based Regularization for Generating Symbolic
  Music with Emotion <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10307v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10307v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shulei Ji, Xinyu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating music with emotion is an important task in automatic music
generation, in which emotion is evoked through a variety of musical elements
(such as pitch and duration) that change over time and collaborate with each
other. However, prior research on deep learning-based emotional music
generation has rarely explored the contribution of different musical elements
to emotions, let alone the deliberate manipulation of these elements to alter
the emotion of music, which is not conducive to fine-grained element-level
control over emotions. To address this gap, we present a novel approach
employing musical element-based regularization in the latent space to
disentangle distinct elements, investigate their roles in distinguishing
emotions, and further manipulate elements to alter musical emotions.
Specifically, we propose a novel VQ-VAE-based model named MusER. MusER
incorporates a regularization loss to enforce the correspondence between the
musical element sequences and the specific dimensions of latent variable
sequences, providing a new solution for disentangling discrete sequences.
Taking advantage of the disentangled latent vectors, a two-level decoding
strategy that includes multiple decoders attending to latent vectors with
different semantics is devised to better predict the elements. By visualizing
latent space, we conclude that MusER yields a disentangled and interpretable
latent space and gain insights into the contribution of distinct elements to
the emotional dimensions (i.e., arousal and valence). Experimental results
demonstrate that MusER outperforms the state-of-the-art models for generating
emotional music in both objective and subjective evaluation. Besides, we
rearrange music through element transfer and attempt to alter the emotion of
music by transferring emotion-distinguishable elements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-01-01T00:00:00Z">2024-01-01</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">26</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph-Convolutional Autoencoder Ensembles for the Humanities,
  Illustrated with a Study of the American Slave Trade 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00824v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00824v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Lippincott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a graph-aware autoencoder ensemble framework, with associated
formalisms and tooling, designed to facilitate deep learning for scholarship in
the humanities. By composing sub-architectures to produce a model isomorphic to
a humanistic domain we maintain interpretability while providing function
signatures for each sub-architectural choice, allowing both traditional and
computational researchers to collaborate without disrupting established
practices. We illustrate a practical application of our approach to a
historical study of the American post-Atlantic slave trade, and make several
specific technical contributions: a novel hybrid graph-convolutional
autoencoder mechanism, batching policies for common graph topologies, and
masking techniques for particular use-cases. The effectiveness of the framework
for broadening participation of diverse domains is demonstrated by a growing
suite of two dozen studies, both collaborations with humanists and established
tasks from machine learning literature, spanning a variety of fields and data
modalities. We make performance comparisons of several different architectural
choices and conclude with an ambitious list of imminent next steps for this
research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>More in-depth technical companion to "A general neural ensemble
  technique to support traditional scholarship", Digital Humanities 2020</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Computational Framework for Behavioral Assessment of LLM Therapists 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Ying Chiu, Ashish Sharma, Inna Wanyin Lin, Tim Althoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of ChatGPT and other large language models (LLMs) has greatly
increased interest in utilizing LLMs as therapists to support individuals
struggling with mental health challenges. However, due to the lack of
systematic studies, our understanding of how LLM therapists behave, i.e., ways
in which they respond to clients, is significantly limited. Understanding their
behavior across a wide range of clients and situations is crucial to accurately
assess their capabilities and limitations in the high-risk setting of mental
health, where undesirable behaviors can lead to severe consequences. In this
paper, we propose BOLT, a novel computational framework to study the
conversational behavior of LLMs when employed as therapists. We develop an
in-context learning method to quantitatively measure the behavior of LLMs based
on 13 different psychotherapy techniques including reflections, questions,
solutions, normalizing, and psychoeducation. Subsequently, we compare the
behavior of LLM therapists against that of high- and low-quality human therapy,
and study how their behavior can be modulated to better reflect behaviors
observed in high-quality therapy. Our analysis of GPT and Llama-variants
reveals that these LLMs often resemble behaviors more commonly exhibited in
low-quality therapy rather than high-quality therapy, such as offering a higher
degree of problem-solving advice when clients share emotions, which is against
typical recommendations. At the same time, unlike low-quality therapy, LLMs
reflect significantly more upon clients' needs and strengths. Our analysis
framework suggests that despite the ability of LLMs to generate anecdotal
examples that appear similar to human therapists, LLM therapists are currently
not fully consistent with high-quality care, and thus require additional
research to ensure quality care.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ If LLM Is the Wizard, Then Code Is the Wand: A <span class="highlight-title">Survey</span> on How Code
  Empowers Large Language Models to Serve as Intelligent Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00812v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00812v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Yang, Jiateng Liu, John Wu, Chaoqi Yang, Yi R. Fung, Sha Li, Zixuan Huang, Xu Cao, Xingyao Wang, Yiquan Wang, Heng Ji, Chengxiang Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prominent large language models (LLMs) of today differ from past language
models not only in size, but also in the fact that they are trained on a
combination of natural language and formal language (code). As a medium between
humans and computers, code translates high-level goals into executable steps,
featuring standard syntax, logical consistency, abstraction, and modularity. In
this survey, we present an overview of the various benefits of integrating code
into LLMs' training data. Specifically, beyond enhancing LLMs in code
generation, we observe that these unique properties of code help (i) unlock the
reasoning ability of LLMs, enabling their applications to a range of more
complex natural language tasks; (ii) steer LLMs to produce structured and
precise intermediate steps, which can then be connected to external execution
ends through function calls; and (iii) take advantage of code compilation and
execution environment, which also provides diverse feedback for model
improvement. In addition, we trace how these profound capabilities of LLMs,
brought by code, have led to their emergence as intelligent agents (IAs) in
situations where the ability to understand instructions, decompose goals, plan
and execute actions, and refine from feedback are crucial to their success on
downstream tasks. Finally, we present several key challenges and future
directions of empowering LLMs with code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PerSHOP -- A Persian <span class="highlight-title">dataset</span> for shopping dialogue systems modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keyvan Mahmoudi, Heshaam Faili
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, dialogue systems are used in many fields of industry and research.
There are successful instances of these systems, such as Apple Siri, Google
Assistant, and IBM Watson. Task-oriented dialogue system is a category of
these, that are used in specific tasks. They can perform tasks such as booking
plane tickets or making restaurant reservations. Shopping is one of the most
popular areas on these systems. The bot replaces the human salesperson and
interacts with the customers by speaking. To train the models behind the scenes
of these systems, annotated data is needed. In this paper, we developed a
dataset of dialogues in the Persian language through crowd-sourcing. We
annotated these dialogues to train a model. This dataset contains nearly 22k
utterances in 15 different domains and 1061 dialogues. This is the largest
Persian dataset in this field, which is provided freely so that future
researchers can use it. Also, we proposed some baseline models for natural
language understanding (NLU) tasks. These models perform two tasks for NLU:
intent classification and entity extraction. The F-1 score metric obtained for
intent classification is around 91% and for entity extraction is around 93%,
which can be a baseline for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00793v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00793v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinglong Luo, Yehong Zhang, Jiaqi Zhang, Xin Mu, Hui Wang, Yue Yu, Zenglin Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growing use of large language models hosted on cloud platforms to
offer inference services, privacy concerns are escalating, especially
concerning sensitive data like investment plans and bank account details.
Secure Multi-Party Computing (SMPC) emerges as a promising solution to protect
the privacy of inference data and model parameters. However, the application of
SMPC in Privacy-Preserving Inference (PPI) for large language models,
particularly those based on the Transformer architecture, often leads to
considerable slowdowns or declines in performance. This is largely due to the
multitude of nonlinear operations in the Transformer architecture, which are
not well-suited to SMPC and are difficult to circumvent or optimize
effectively. To address this concern, we introduce an advanced optimization
framework called SecFormer, designed to strike an optimal balance between
performance and efficiency in PPI for Transformer models. By implementing
knowledge distillation techniques, we successfully eliminate the high-cost
exponential and maximum operations in PPI without sacrificing model
performance. Additionally, we have developed a suite of efficient SMPC
protocols that utilize segmented polynomials and Goldschmidt's method to handle
other complex nonlinear functions within PPI, such as GeLU, LayerNorm, and
Softmax. Our extensive experiments reveal that SecFormer outperforms MPCFormer
in performance, showing improvements of $5.6\%$ and $24.2\%$ for
BERT$_{\text{BASE}}$ and BERT$_{\text{LARGE}}$, respectively. In terms of
efficiency, SecFormer is 3.4 and 3.2 times faster than Puma, demonstrating its
effectiveness and speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12pages, 15figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Astraios: Parameter-Efficient Instruction Tuning Code Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Terry Yue Zhuo, Armel Zebaze, Nitchakarn Suppattarachai, Leandro von Werra, Harm de Vries, Qian Liu, Niklas Muennighoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The high cost of full-parameter fine-tuning (FFT) of Large Language Models
(LLMs) has led to a series of parameter-efficient fine-tuning (PEFT) methods.
However, it remains unclear which methods provide the best cost-performance
trade-off at different model scales. We introduce Astraios, a suite of 28
instruction-tuned OctoCoder models using 7 tuning methods and 4 model sizes up
to 16 billion parameters. Through investigations across 5 tasks and 8 different
datasets encompassing both code comprehension and code generation tasks, we
find that FFT generally leads to the best downstream performance across all
scales, and PEFT methods differ significantly in their efficacy based on the
model scale. LoRA usually offers the most favorable trade-off between cost and
performance. Further investigation into the effects of these methods on both
model robustness and code security reveals that larger models tend to
demonstrate reduced robustness and less security. At last, we explore the
relationships among updated parameters, cross-entropy loss, and task
performance. We find that the tuning effectiveness observed in small models
generalizes well to larger models, and the validation loss in instruction
tuning can be a reliable indicator of overall downstream performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages (12 main), 19 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal Validity Change Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georg Wenzel, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal validity is an important property of text that is useful for many
downstream applications, such as recommender systems, conversational AI, or
story understanding. Existing benchmarking tasks often require models to
identify the temporal validity duration of a single statement. However, in many
cases, additional contextual information, such as sentences in a story or posts
on a social media profile, can be collected from the available text stream.
This contextual information may greatly alter the duration for which a
statement is expected to be valid. We propose Temporal Validity Change
Prediction, a natural language processing task benchmarking the capability of
machine learning models to detect contextual statements that induce such
change. We create a dataset consisting of temporal target statements sourced
from Twitter and crowdsource sample context statements. We then benchmark a set
of transformer-based language models on our dataset. Finally, we experiment
with temporal validity duration prediction as an auxiliary task to improve the
performance of the state-of-the-art model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 9 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ New Job, New Gender? Measuring the Social Bias in Image Generation
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00763v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00763v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Wang, Haonan Bai, Jen-tse Huang, Yuxuan Wan, Youliang Yuan, Haoyi Qiu, Nanyun Peng, Michael R. Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image generation models can generate or edit images from a given text. Recent
advancements in image generation technology, exemplified by DALL-E and
Midjourney, have been groundbreaking. These advanced models, despite their
impressive capabilities, are often trained on massive Internet datasets, making
them susceptible to generating content that perpetuates social stereotypes and
biases, which can lead to severe consequences. Prior research on assessing bias
within image generation models suffers from several shortcomings, including
limited accuracy, reliance on extensive human labor, and lack of comprehensive
analysis. In this paper, we propose BiasPainter, a novel metamorphic testing
framework that can accurately, automatically and comprehensively trigger social
bias in image generation models. BiasPainter uses a diverse range of seed
images of individuals and prompts the image generation models to edit these
images using gender, race, and age-neutral queries. These queries span 62
professions, 39 activities, 57 types of objects, and 70 personality traits. The
framework then compares the edited images to the original seed images, focusing
on any changes related to gender, race, and age. BiasPainter adopts a testing
oracle that these characteristics should not be modified when subjected to
neutral prompts. Built upon this design, BiasPainter can trigger the social
bias and evaluate the fairness of image generation models. To evaluate the
effectiveness of BiasPainter, we use BiasPainter to test five widely-used
commercial image generation software and models, such as stable diffusion and
Midjourney. Experimental results show that 100\% of the generated test cases
can successfully trigger social bias in image generation models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Earth is Flat? Unveiling Factual Errors in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00761v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00761v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Wang, Juluan Shi, Zhaopeng Tu, Youliang Yuan, Jen-tse Huang, Wenxiang Jiao, Michael R. Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) like ChatGPT are foundational in various
applications due to their extensive knowledge from pre-training and
fine-tuning. Despite this, they are prone to generating factual and commonsense
errors, raising concerns in critical areas like healthcare, journalism, and
education to mislead users. Current methods for evaluating LLMs' veracity are
limited by test data leakage or the need for extensive human labor, hindering
efficient and accurate error detection. To tackle this problem, we introduce a
novel, automatic testing framework, FactChecker, aimed at uncovering factual
inaccuracies in LLMs. This framework involves three main steps: First, it
constructs a factual knowledge graph by retrieving fact triplets from a
large-scale knowledge database. Then, leveraging the knowledge graph,
FactChecker employs a rule-based approach to generates three types of questions
(Yes-No, Multiple-Choice, and WH questions) that involve single-hop and
multi-hop relations, along with correct answers. Lastly, it assesses the LLMs'
responses for accuracy using tailored matching strategies for each question
type. Our extensive tests on six prominent LLMs, including text-davinci-002,
text-davinci-003, ChatGPT~(gpt-3.5-turbo, gpt-4), Vicuna, and LLaMA-2, reveal
that FactChecker can trigger factual errors in up to 45\% of questions in these
models. Moreover, we demonstrate that FactChecker's test cases can improve
LLMs' factual accuracy through in-context learning and fine-tuning (e.g.,
llama-2-13b-chat's accuracy increase from 35.3\% to 68.5\%). We are making all
code, data, and results available for future research endeavors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A & B == B & A: Triggering Logical Reasoning Failures in Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00757v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00757v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Wan, Wenxuan Wang, Yiliu Yang, Youliang Yuan, Jen-tse Huang, Pinjia He, Wenxiang Jiao, Michael R. Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have propelled Artificial
Intelligence (AI) to new heights, enabling breakthroughs in various tasks such
as writing assistance, code generation, and machine translation. A significant
distinction of advanced LLMs, such as ChatGPT, is their demonstrated ability to
"reason." However, evaluating the reasoning ability of LLMs remains a challenge
as most existing evaluations focus on their accuracy on the downstream tasks
rather than directly assessing their reasoning processes. Efforts have been
made to develop benchmarks and metrics to assess reasoning in LLMs, but they
suffer from data leakage or limited scope. In this paper, we introduce
LogicAsker, an automatic approach that comprehensively evaluates and improves
the logical reasoning abilities of LLMs under a set of atomic reasoning skills
based on propositional and predicate logic. The results provide insights into
LLMs' reasoning abilities and reveal the logical rules the LLMs did not learn
well. We evaluate LogicAsker on six widely deployed LLMs, including GPT-3,
ChatGPT, GPT-4, Bard, Vicuna, and Guanaco. The results show that test cases
from LogicAsker can find logical reasoning failures in different LLMs with a
rate of 25\% - 94\%. In addition, the test cases of LogicAsker can be further
used to design demonstration examples for in-context learning, which
effectively improves the logical reasoning ability of LLMs, e.g., 10\% for
GPT-4. As far as we know, our work is the first to create prompts based on
testing results to improve LLMs' formal reasoning ability effectively. All the
code, data, and results will be released for reproduction and future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Translation Testing via Syntactic Tree Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quanjun Zhang, Juan Zhai, Chunrong Fang, Jiawei Liu, Weisong Sun, Haichuan Hu, Qingyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine translation systems have been widely adopted in our daily life,
making life easier and more convenient. Unfortunately, erroneous translations
may result in severe consequences, such as financial losses. This requires to
improve the accuracy and the reliability of machine translation systems.
However, it is challenging to test machine translation systems because of the
complexity and intractability of the underlying neural models. To tackle these
challenges, we propose a novel metamorphic testing approach by syntactic tree
pruning (STP) to validate machine translation systems. Our key insight is that
a pruned sentence should have similar crucial semantics compared with the
original sentence. Specifically, STP (1) proposes a core semantics-preserving
pruning strategy by basic sentence structure and dependency relations on the
level of syntactic tree representation; (2) generates source sentence pairs
based on the metamorphic relation; (3) reports suspicious issues whose
translations break the consistency property by a bag-of-words model. We further
evaluate STP on two state-of-the-art machine translation systems (i.e., Google
Translate and Bing Microsoft Translator) with 1,200 source sentences as inputs.
The results show that STP can accurately find 5,073 unique erroneous
translations in Google Translate and 5,100 unique erroneous translations in
Bing Microsoft Translator (400% more than state-of-the-art techniques), with
64.5% and 65.4% precision, respectively. The reported erroneous translations
vary in types and more than 90% of them cannot be found by state-of-the-art
techniques. There are 9,393 erroneous translations unique to STP, which is
711.9% more than state-of-the-art techniques. Moreover, STP is quite effective
to detect translation errors for the original sentences with a recall reaching
74.0%, improving state-of-the-art techniques by 55.1% on average.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM Transactions on Software Engineering and Methodology
  2024 (TOSEM'24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ToolEyes: Fine-Grained Evaluation for Tool Learning Capabilities of
  Large Language Models in Real-world Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00741v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00741v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Ye, Guanyu Li, Songyang Gao, Caishuang Huang, Yilong Wu, Sixian Li, Xiaoran Fan, Shihan Dou, Qi Zhang, Tao Gui, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing evaluations of tool learning primarily focus on validating the
alignment of selected tools for large language models (LLMs) with expected
outcomes. However, these approaches rely on a limited set of scenarios where
answers can be pre-determined, diverging from genuine needs. Furthermore, a
sole emphasis on outcomes disregards the intricate capabilities essential for
LLMs to effectively utilize tools. To tackle this issue, we propose ToolEyes, a
fine-grained system tailored for the evaluation of the LLMs' tool learning
capabilities in authentic scenarios. The system meticulously examines seven
real-world scenarios, analyzing five dimensions crucial to LLMs in tool
learning: format alignment, intent comprehension, behavior planning, tool
selection, and answer organization. Additionally, ToolEyes incorporates a tool
library boasting approximately 600 tools, serving as an intermediary between
LLMs and the physical world. Evaluations involving ten LLMs across three
categories reveal a preference for specific scenarios and limited cognitive
abilities in tool learning. Intriguingly, expanding the model size even
exacerbates the hindrance to tool learning. These findings offer instructive
insights aimed at advancing the field of tool learning. The data is available
att https://github.com/Junjie-Ye/ToolEyes.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models aren't all that you need 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00698v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00698v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kiran Voderhobli Holla, Chaithanya Kumar, Aryan Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes the architecture and systems built towards solving the
SemEval 2023 Task 2: MultiCoNER II (Multilingual Complex Named Entity
Recognition) [1]. We evaluate two approaches (a) a traditional Conditional
Random Fields model and (b) a Large Language Model (LLM) fine-tuned with a
customized head and compare the two approaches. The novel ideas explored are:
1) Decaying auxiliary loss (with residual) - where we train the model on an
auxiliary task of Coarse-Grained NER and include this task as a part of the
loss function 2) Triplet token blending - where we explore ways of blending the
embeddings of neighboring tokens in the final NER layer prior to prediction 3)
Task-optimal heads - where we explore a variety of custom heads and learning
rates for the final layer of the LLM. We also explore multiple LLMs including
GPT-3 and experiment with a variety of dropout and other hyperparameter
settings before arriving at our final model which achieves micro & macro f1 of
0.85/0.84 (on dev) and 0.67/0.61 on the test data . We show that while
pre-trained LLMs, by themselves, bring about a large improvement in scores as
compared to traditional models, we also demonstrate that tangible improvements
to the Macro-F1 score can be made by augmenting the LLM with additional
feature/loss/model engineering techniques described above.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Large Language Models on Controllable Generation under
  Diversified Instructions <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihan Chen, Benfeng Xu, Quan Wang, Yi Liu, Zhendong Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) have exhibited impressive
instruction-following capabilities, it is still unclear whether and to what
extent they can respond to explicit constraints that might be entailed in
various instructions. As a significant aspect of LLM alignment, it is thus
important to formulate such a specialized set of instructions as well as
investigate the resulting behavior of LLMs. To address this vacancy, we propose
a new benchmark CoDI-Eval to systematically and comprehensively evaluate LLMs'
responses to instructions with various constraints. We construct a large
collection of constraints-attributed instructions as a test suite focused on
both generalization and coverage. Specifically, we advocate an instruction
diversification process to synthesize diverse forms of constraint expression
and also deliberate the candidate task taxonomy with even finer-grained
sub-categories. Finally, we automate the entire evaluation process to
facilitate further developments. Different from existing studies on
controllable text generation, CoDI-Eval extends the scope to the prevalent
instruction-following paradigm for the first time. We provide extensive
evaluations of representative LLMs (e.g., ChatGPT, Vicuna) on CoDI-Eval,
revealing their limitations in following instructions with specific constraints
and there is still a significant gap between open-source and commercial
closed-source LLMs. We believe this benchmark will facilitate research into
improving the controllability of LLMs' responses to instructions. Our data and
code are available at https://github.com/Xt-cyh/CoDI-Eval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large language model for Bible sentiment analysis: Sermon on the Mount 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahek Vora, Tom Blau, Vansh Kachhwal, Ashu M. G. Solo, Rohitash Chandra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The revolution of natural language processing via large language models has
motivated its use in multidisciplinary areas that include social sciences and
humanities and more specifically, comparative religion. Sentiment analysis
provides a mechanism to study the emotions expressed in text. Recently,
sentiment analysis has been used to study and compare translations of the
Bhagavad Gita, which is a fundamental and sacred Hindu text. In this study, we
use sentiment analysis for studying selected chapters of the Bible. These
chapters are known as the Sermon on the Mount. We utilize a pre-trained
language model for sentiment analysis by reviewing five translations of the
Sermon on the Mount, which include the King James version, the New
International Version, the New Revised Standard Version, the Lamsa Version, and
the Basic English Version. We provide a chapter-by-chapter and verse-by-verse
comparison using sentiment and semantic analysis and review the major
sentiments expressed. Our results highlight the varying sentiments across the
chapters and verses. We found that the vocabulary of the respective
translations is significantly different. We detected different levels of
humour, optimism, and empathy in the respective chapters that were used by
Jesus to deliver his message.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Digger: Detecting Copyright Content Mis-usage in Large Language Model
  Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00676v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00676v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haodong Li, Gelei Deng, Yi Liu, Kailong Wang, Yuekang Li, Tianwei Zhang, Yang Liu, Guoai Xu, Guosheng Xu, Haoyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training, which utilizes extensive and varied datasets, is a critical
factor in the success of Large Language Models (LLMs) across numerous
applications. However, the detailed makeup of these datasets is often not
disclosed, leading to concerns about data security and potential misuse. This
is particularly relevant when copyrighted material, still under legal
protection, is used inappropriately, either intentionally or unintentionally,
infringing on the rights of the authors.
  In this paper, we introduce a detailed framework designed to detect and
assess the presence of content from potentially copyrighted books within the
training datasets of LLMs. This framework also provides a confidence estimation
for the likelihood of each content sample's inclusion. To validate our
approach, we conduct a series of simulated experiments, the results of which
affirm the framework's effectiveness in identifying and addressing instances of
content misuse in LLM training processes. Furthermore, we investigate the
presence of recognizable quotes from famous literary works within these
datasets. The outcomes of our study have significant implications for ensuring
the ethical use of copyrighted materials in the development of LLMs,
highlighting the need for more transparent and responsible data management
practices in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting Anti-microbial Resistance using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunwoo Yoo, Bahrad Sokhansanj, James R. Brown, Gail Rosen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  During times of increasing antibiotic resistance and the spread of infectious
diseases like COVID-19, it is important to classify genes related to antibiotic
resistance. As natural language processing has advanced with transformer-based
language models, many language models that learn characteristics of nucleotide
sequences have also emerged. These models show good performance in classifying
various features of nucleotide sequences. When classifying nucleotide
sequences, not only the sequence itself, but also various background knowledge
is utilized. In this study, we use not only a nucleotide sequence-based
language model but also a text language model based on PubMed articles to
reflect more biological background knowledge in the model. We propose a method
to fine-tune the nucleotide sequence language model and the text language model
based on various databases of antibiotic resistance genes. We also propose an
LLM-based augmentation technique to supplement the data and an ensemble method
to effectively combine the two models. We also propose a benchmark for
evaluating the model. Our method achieved better performance than the
nucleotide sequence language model in the drug resistance class prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast and Optimal Weight Update for Pruned Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02938v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02938v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vladimír Boža
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pruning large language models (LLMs) is a challenging task due to their
enormous size. The primary difficulty is fine-tuning the model after pruning,
which is needed to recover the lost performance caused by dropping weights.
Recent approaches have either ignored fine-tuning entirely, focusing on
efficient pruning criteria, or attempted layer-wise weight updates, preserving
the behavior of each layer. However, even layer-wise weight updates can be
costly for LLMs, and previous works have resorted to various approximations.
  In our paper, we propose a fast and optimal weight update algorithm for
pruned layers based on the Alternating Direction Method of Multipliers (ADMM).
Coupled with a simple iterative pruning mask selection, our algorithm achieves
state-of-the-art pruning performance across a wide range of LLMs. Code is
available at https://github.com/fmfi-compbio/admm-pruning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Cambridge Law Corpus: A <span class="highlight-title">Dataset</span> for Legal AI Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.12269v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.12269v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Östling, Holli Sargeant, Huiyuan Xie, Ludwig Bull, Alexander Terenin, Leif Jonsson, Måns Magnusson, Felix Steffek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the Cambridge Law Corpus (CLC), a dataset for legal AI research.
It consists of over 250 000 court cases from the UK. Most cases are from the
21st century, but the corpus includes cases as old as the 16th century. This
paper presents the first release of the corpus, containing the raw text and
meta-data. Together with the corpus, we provide annotations on case outcomes
for 638 cases, done by legal experts. Using our annotated data, we have trained
and evaluated case outcome extraction with GPT-3, GPT-4 and RoBERTa models to
provide benchmarks. We include an extensive legal and ethical discussion to
address the potentially sensitive nature of this material. As a consequence,
the corpus will only be released for research purposes under certain
restrictions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Learnability of Watermarks for Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04469v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04469v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenchen Gu, Xiang Lisa Li, Percy Liang, Tatsunori Hashimoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Watermarking of language model outputs enables statistical detection of
model-generated text, which has many applications in the responsible deployment
of language models. Existing watermarking strategies operate by altering the
decoder of an existing language model, and the ability for a language model to
directly learn to generate the watermark would have significant implications
for the real-world deployment of watermarks. First, learned watermarks could be
used to build open models that naturally generate watermarked text, allowing
for open models to benefit from watermarking. Second, if watermarking is used
to determine the provenance of generated text, an adversary can hurt the
reputation of a victim model by spoofing its watermark and generating damaging
watermarked text. To investigate the learnability of watermarks, we propose
watermark distillation, which trains a student model to behave like a teacher
model that uses decoding-based watermarking. We test our approach on three
distinct decoding-based watermarking strategies and various hyperparameter
settings, finding that models can learn to generate watermarked text with high
detectability. We also find limitations to learnability, including the loss of
watermarking capabilities under fine-tuning on normal text and high sample
complexity when learning low-distortion watermarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Models are Bounded Pragmatic Speakers: Understanding RLHF from
  a Bayesian Cognitive Modeling Perspective <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17760v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17760v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khanh Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How do language models "think"? This paper formulates a probabilistic
cognitive model called the bounded pragmatic speaker, which can characterize
the operation of different variations of language models. Specifically, we
demonstrate that large language models fine-tuned with reinforcement learning
from human feedback (Ouyang et al., 2022) embody a model of thought that
conceptually resembles a fast-and-slow model (Kahneman, 2011), which
psychologists have attributed to humans. We discuss the limitations of
reinforcement learning from human feedback as a fast-and-slow model of thought
and propose avenues for expanding this framework. In essence, our research
highlights the value of adopting a cognitive probabilistic modeling approach to
gain insights into the comprehension, evaluation, and advancement of language
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the First Workshop on Theory of Mind in Communicating
  Agents at (TOM @ ICML 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aurora:Activating Chinese chat capability for Mixtral-8x7B sparse
  Mixture-of-Experts through Instruction-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14557v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14557v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rongsheng Wang, Haoming Chen, Ruizhe Zhou, Yaofei Duan, Kunyan Cai, Han Ma, Jiaxi Cui, Jian Li, Patrick Cheong-Iao Pang, Yapeng Wang, Tao Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing research has demonstrated that refining large language models (LLMs)
through the utilization of machine-generated instruction-following data
empowers these models to exhibit impressive zero-shot capabilities for novel
tasks, without requiring human-authored instructions. In this paper, we
systematically investigate, preprocess, and integrate three Chinese
instruction-following datasets with the aim of enhancing the Chinese
conversational capabilities of Mixtral-8x7B sparse Mixture-of-Experts model.
Through instruction fine-tuning on this carefully processed dataset, we
successfully construct the Mixtral-8x7B sparse Mixture-of-Experts model named
"Aurora." To assess the performance of Aurora, we utilize three widely
recognized benchmark tests: C-Eval, MMLU, and CMMLU. Empirical studies validate
the effectiveness of instruction fine-tuning applied to Mixtral-8x7B sparse
Mixture-of-Experts model. This work is pioneering in the execution of
instruction fine-tuning on a sparse expert-mixed model, marking a significant
breakthrough in enhancing the capabilities of this model architecture. Our
code, data and model are publicly available at
  https://github.com/WangRongsheng/Aurora
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Normalization of Lithuanian Text Using Regular Expressions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17660v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17660v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pijus Kasparaitis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text Normalization is an integral part of any text-to-speech synthesis
system. In a natural language text, there are elements such as numbers, dates,
abbreviations, etc. that belong to other semiotic classes. They are called
non-standard words (NSW) and need to be expanded into ordinary words. For this
purpose, it is necessary to identify the semiotic class of each NSW. The
taxonomy of semiotic classes adapted to the Lithuanian language is presented in
the work. Sets of rules are created for detecting and expanding NSWs based on
regular expressions. Experiments with three completely different data sets were
performed and the accuracy was assessed. Causes of errors are explained and
recommendations are given for the development of text normalization rules.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring AI-Generated Text in Student Writing: How Does AI Help? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.02478v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.02478v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David James Woo, Hengky Susanto, Chi Ho Yeung, Kai Guo, April Ka Yeng Fung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  English as foreign language_EFL_students' use of text generated from
artificial intelligence_AI_natural language generation_NLG_tools may improve
their writing quality. However, it remains unclear to what extent AI-generated
text in these students' writing might lead to higher-quality writing. We
explored 23 Hong Kong secondary school students' attempts to write stories
comprising their own words and AI-generated text. Human experts scored the
stories for dimensions of content, language and organization. We analyzed the
basic organization and structure and syntactic complexity of the stories'
AI-generated text and performed multiple linear regression and cluster
analyses. The results show the number of human words and the number of
AI-generated words contribute significantly to scores. Besides, students can be
grouped into competent and less competent writers who use more AI-generated
text or less AI-generated text compared to their peers. Comparisons of clusters
reveal some benefit of AI-generated text in improving the quality of both
high-scoring students' and low-scoring students' writing. The findings can
inform pedagogical strategies to use AI-generated text for EFL students'
writing and to address digital divides. This study contributes designs of NLG
tools and writing activities to implement AI-generated text in schools.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages, 11 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BSpell: A CNN-Blended <span class="highlight-title">BERT</span> Based Bangla Spell Checker 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.09709v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.09709v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chowdhury Rafeed Rahman, MD. Hasibur Rahman, Samiha Zakir, Mohammad Rafsan, Mohammed Eunus Ali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bangla typing is mostly performed using English keyboard and can be highly
erroneous due to the presence of compound and similarly pronounced letters.
Spelling correction of a misspelled word requires understanding of word typing
pattern as well as the context of the word usage. A specialized BERT model
named BSpell has been proposed in this paper targeted towards word for word
correction in sentence level. BSpell contains an end-to-end trainable CNN
sub-model named SemanticNet along with specialized auxiliary loss. This allows
BSpell to specialize in highly inflected Bangla vocabulary in the presence of
spelling errors. Furthermore, a hybrid pretraining scheme has been proposed for
BSpell that combines word level and character level masking. Comparison on two
Bangla and one Hindi spelling correction dataset shows the superiority of our
proposed approach. BSpell is available as a Bangla spell checking tool via
GitHub: https://github.com/Hasiburshanto/Bangla-Spell-Checker
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ViCrop: Perceiving Small Visual Details in Zero-shot Visual Question
  Answering with Multimodal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16033v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16033v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarui Zhang, Mahyar Khayatkhoei, Prateek Chhikara, Filip Ilievski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have recently achieved promising
zero-shot accuracy on visual question answering (VQA) -- a fundamental task
affecting various downstream applications and domains. Given the great
potential for the broad use of these models, it is important to investigate
their limitations in dealing with different image and question properties. In
this work, we investigate whether MLLMs can perceive details as well as larger
components in images. In particular, we show that their zero-shot accuracy in
answering visual questions is very sensitive to the size of the visual subject
related to the question, declining up to $45.91\%$ with size. Furthermore, we
show that this effect is causal by observing that human visual cropping can
significantly mitigate their sensitivity to size. To scale up the usefulness of
human cropping, we propose ViCrop, a general framework that utilizes automatic
visual cropping to enhance zero-shot VQA of MLLMs. We construct five variants
of ViCrop leveraging either external localization models or the decision
process of the given MLLM itself. Our results show that ViCrop improves MLLMs'
zero-shot accuracy across different VQA datasets, for example, enhances
BLIP2-T5's performance by $32.23\%$ on the TextVQA test set. To facilitate
further investigation of MLLMs' behaviors, our code is publicly released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 11 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">50</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Multi-domain Text Recognition Deep Neural Network
  Parameterization with Residual Adapters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00971v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00971v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayou Chao, Wei Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in deep neural networks have markedly enhanced the
performance of computer vision tasks, yet the specialized nature of these
networks often necessitates extensive data and high computational power.
Addressing these requirements, this study presents a novel neural network model
adept at optical character recognition (OCR) across diverse domains, leveraging
the strengths of multi-task learning to improve efficiency and generalization.
The model is designed to achieve rapid adaptation to new domains, maintain a
compact size conducive to reduced computational resource demand, ensure high
accuracy, retain knowledge from previous learning experiences, and allow for
domain-specific performance improvements without the need to retrain entirely.
Rigorous evaluation on open datasets has validated the model's ability to
significantly lower the number of trainable parameters without sacrificing
performance, indicating its potential as a scalable and adaptable solution in
the field of computer vision, particularly for applications in optical text
recognition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Augmentation Techniques for Cross-Domain WiFi CSI-based Human
  Activity Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00964v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00964v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Strohmayer, Martin Kampel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recognition of human activities based on WiFi Channel State Information
(CSI) enables contactless and visual privacy-preserving sensing in indoor
environments. However, poor model generalization, due to varying environmental
conditions and sensing hardware, is a well-known problem in this space. To
address this issue, in this work, data augmentation techniques commonly used in
image-based learning are applied to WiFi CSI to investigate their effects on
model generalization performance in cross-scenario and cross-system settings.
In particular, we focus on the generalization between line-of-sight (LOS) and
non-line-of-sight (NLOS) through-wall scenarios, as well as on the
generalization between different antenna systems, which remains under-explored.
We collect and make publicly available a dataset of CSI amplitude spectrograms
of human activities. Utilizing this data, an ablation study is conducted in
which activity recognition models based on the EfficientNetV2 architecture are
trained, allowing us to assess the effects of each augmentation on model
generalization performance. The gathered results show that specific
combinations of simple data augmentation techniques applied to CSI amplitude
data can significantly improve cross-scenario and cross-system generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boundary Attention: Learning to Find Faint Boundaries at Any Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00935v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00935v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mia Gaia Polansky, Charles Herrmann, Junhwa Hur, Deqing Sun, Dor Verbin, Todd Zickler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a differentiable model that explicitly models boundaries --
including contours, corners and junctions -- using a new mechanism that we call
boundary attention. We show that our model provides accurate results even when
the boundary signal is very weak or is swamped by noise. Compared to previous
classical methods for finding faint boundaries, our model has the advantages of
being differentiable; being scalable to larger images; and automatically
adapting to an appropriate level of geometric detail in each part of an image.
Compared to previous deep methods for finding boundaries via end-to-end
training, it has the advantages of providing sub-pixel precision, being more
resilient to noise, and being able to process any image at its native
resolution and aspect ratio.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website at boundaryattention.github.io:
  http://boundaryattention.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Refining <span class="highlight-title">Pre-Train</span>ed Motion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00850v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00850v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinglong Sun, Adam W. Harley, Leonidas J. Guibas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the difficulty of manually annotating motion in video, the current best
motion estimation methods are trained with synthetic data, and therefore
struggle somewhat due to a train/test gap. Self-supervised methods hold the
promise of training directly on real video, but typically perform worse. These
include methods trained with warp error (i.e., color constancy) combined with
smoothness terms, and methods that encourage cycle-consistency in the estimates
(i.e., tracking backwards should yield the opposite trajectory as tracking
forwards). In this work, we take on the challenge of improving state-of-the-art
supervised models with self-supervised training. We find that when the
initialization is supervised weights, most existing self-supervision techniques
actually make performance worse instead of better, which suggests that the
benefit of seeing the new data is overshadowed by the noise in the training
signal. Focusing on obtaining a ``clean'' training signal from real-world
unlabelled video, we propose to separate label-making and training into two
distinct stages. In the first stage, we use the pre-trained model to estimate
motion in a video, and then select the subset of motion estimates which we can
verify with cycle-consistency. This produces a sparse but accurate
pseudo-labelling of the video. In the second stage, we fine-tune the model to
reproduce these outputs, while also applying augmentations on the input. We
complement this boot-strapping method with simple techniques that densify and
re-balance the pseudo-labels, ensuring that we do not merely train on ``easy''
tracks. We show that our method yields reliable gains over fully-supervised
methods in real videos, for both short-term (flow-based) and long-range
(multi-frame) pixel tracking.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COSMO: COntrastive Streamlined MultimOdal Model with Interleaved
  <span class="highlight-title">Pre-Train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00849v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00849v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Jinpeng Wang, Linjie Li, Kevin Qinghong Lin, Jianfeng Wang, Kevin Lin, Zhengyuan Yang, Lijuan Wang, Mike Zheng Shou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the evolution of Vision-Language Pre-training, shifting from short-text
comprehension to encompassing extended textual contexts is pivotal. Recent
autoregressive vision-language models like \cite{flamingo, palme}, leveraging
the long-context capability of Large Language Models, have excelled in few-shot
text generation tasks but face challenges in alignment tasks. Addressing this
gap, we introduce the contrastive loss into text generation models, presenting
the COntrastive-Streamlined MultimOdal framework (\ModelName), strategically
partitioning the language model into dedicated unimodal text processing and
adept multimodal data handling components. \ModelName, our unified framework,
merges unimodal and multimodal elements, enhancing model performance for tasks
involving textual and visual data while notably reducing learnable parameters.
However, these models demand extensive long-text datasets, yet the availability
of high-quality long-text video datasets remains limited. To bridge this gap,
this work introduces \VideoDatasetName, an inaugural interleaved video-text
dataset featuring comprehensive captions, marking a significant step forward.
Demonstrating its impact, we illustrate how \VideoDatasetName{} enhances model
performance in image-text tasks. With 34% learnable parameters and utilizing
72\% of the available data, our model demonstrates significant superiority over
OpenFlamingo~\cite{openflamingo}. For instance, in the 4-shot flickr captioning
task, performance notably improves from 57.2% to 65.\%. The contributions of
\ModelName{} and \VideoDatasetName{} are underscored by notable performance
gains across 14 diverse downstream datasets encompassing both image-text and
video-text tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages; Website: http://fingerrec.github.io/cosmo</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mocap Everyone Everywhere: Lightweight Motion Capture With Smartwatches
  and a Head-Mounted Camera 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00847v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00847v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiye Lee, Hanbyul Joo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a lightweight and affordable motion capture method based on two
smartwatches and a head-mounted camera. In contrast to the existing approaches
that use six or more expert-level IMU devices, our approach is much more
cost-effective and convenient. Our method can make wearable motion capture
accessible to everyone everywhere, enabling 3D full-body motion capture in
diverse environments. As a key idea to overcome the extreme sparsity and
ambiguities of sensor inputs, we integrate 6D head poses obtained from the
head-mounted cameras for motion estimation. To enable capture in expansive
indoor and outdoor scenes, we propose an algorithm to track and update floor
level changes to define head poses, coupled with a multi-stage
Transformer-based regression module. We also introduce novel strategies
leveraging visual cues of egocentric images to further enhance the motion
capture quality while reducing ambiguities. We demonstrate the performance of
our method on various challenging scenarios, including complex outdoor
environments and everyday motions including object interactions and social
interactions among multiple individuals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://jiyewise.github.io/projects/MocapEvery/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deblurring 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Byeonghyeon Lee, Howoong Lee, Xiangyu Sun, Usman Ali, Eunbyung Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies in Radiance Fields have paved the robust way for novel view
synthesis with their photorealistic rendering quality. Nevertheless, they
usually employ neural networks and volumetric rendering, which are costly to
train and impede their broad use in various real-time applications due to the
lengthy rendering time. Lately 3D Gaussians splatting-based approach has been
proposed to model the 3D scene, and it achieves remarkable visual quality while
rendering the images in real-time. However, it suffers from severe degradation
in the rendering quality if the training images are blurry. Blurriness commonly
occurs due to the lens defocusing, object motion, and camera shake, and it
inevitably intervenes in clean image acquisition. Several previous studies have
attempted to render clean and sharp images from blurry input images using
neural fields. The majority of those works, however, are designed only for
volumetric rendering-based neural radiance fields and are not straightforwardly
applicable to rasterization-based 3D Gaussian splatting methods. Thus, we
propose a novel real-time deblurring framework, deblurring 3D Gaussian
Splatting, using a small Multi-Layer Perceptron (MLP) that manipulates the
covariance of each 3D Gaussian to model the scene blurriness. While deblurring
3D Gaussian Splatting can still enjoy real-time rendering, it can reconstruct
fine and sharp details from blurry images. A variety of experiments have been
conducted on the benchmark, and the results have revealed the effectiveness of
our approach for deblurring. Qualitative results are available at
https://benhenryl.github.io/Deblurring-3D-Gaussian-Splatting/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking RAFT for Efficient Optical Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Navid Eslami, Farnoosh Arefi, Amir M. Mansourian, Shohreh Kasaei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant progress in deep learning-based optical flow methods,
accurately estimating large displacements and repetitive patterns remains a
challenge. The limitations of local features and similarity search patterns
used in these algorithms contribute to this issue. Additionally, some existing
methods suffer from slow runtime and excessive graphic memory consumption. To
address these problems, this paper proposes a novel approach based on the RAFT
framework. The proposed Attention-based Feature Localization (AFL) approach
incorporates the attention mechanism to handle global feature extraction and
address repetitive patterns. It introduces an operator for matching pixels with
corresponding counterparts in the second frame and assigning accurate flow
values. Furthermore, an Amorphous Lookup Operator (ALO) is proposed to enhance
convergence speed and improve RAFTs ability to handle large displacements by
reducing data redundancy in its search operator and expanding the search space
for similarity extraction. The proposed method, Efficient RAFT
(Ef-RAFT),achieves significant improvements of 10% on the Sintel dataset and 5%
on the KITTI dataset over RAFT. Remarkably, these enhancements are attained
with a modest 33% reduction in speed and a mere 13% increase in memory usage.
The code is available at: https://github.com/n3slami/Ef-RAFT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GenH2R: Learning Generalizable Human-to-Robot Handover via Scalable
  Simulation, Demonstration, and Imitation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00929v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00929v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zifan Wang, Junyu Chen, Ziqing Chen, Pengwei Xie, Rui Chen, Li Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents GenH2R, a framework for learning generalizable
vision-based human-to-robot (H2R) handover skills. The goal is to equip robots
with the ability to reliably receive objects with unseen geometry handed over
by humans in various complex trajectories. We acquire such generalizability by
learning H2R handover at scale with a comprehensive solution including
procedural simulation assets creation, automated demonstration generation, and
effective imitation learning. We leverage large-scale 3D model repositories,
dexterous grasp generation methods, and curve-based 3D animation to create an
H2R handover simulation environment named \simabbns, surpassing the number of
scenes in existing simulators by three orders of magnitude. We further
introduce a distillation-friendly demonstration generation method that
automatically generates a million high-quality demonstrations suitable for
learning. Finally, we present a 4D imitation learning method augmented by a
future forecasting objective to distill demonstrations into a visuo-motor
handover policy. Experimental evaluations in both simulators and the real world
demonstrate significant improvements (at least +10\% success rate) over
baselines in all cases. The project page is https://GenH2R.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The project page is https://GenH2R.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sharp-NeRF: Grid-based Fast Deblurring Neural Radiance Fields Using
  Sharpness Prior <span class="chip">WACV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00825v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00825v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Byeonghyeon Lee, Howoong Lee, Usman Ali, Eunbyung Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (NeRF) have shown remarkable performance in neural
rendering-based novel view synthesis. However, NeRF suffers from severe visual
quality degradation when the input images have been captured under imperfect
conditions, such as poor illumination, defocus blurring, and lens aberrations.
Especially, defocus blur is quite common in the images when they are normally
captured using cameras. Although few recent studies have proposed to render
sharp images of considerably high-quality, yet they still face many key
challenges. In particular, those methods have employed a Multi-Layer Perceptron
(MLP) based NeRF, which requires tremendous computational time. To overcome
these shortcomings, this paper proposes a novel technique Sharp-NeRF -- a
grid-based NeRF that renders clean and sharp images from the input blurry
images within half an hour of training. To do so, we used several grid-based
kernels to accurately model the sharpness/blurriness of the scene. The
sharpness level of the pixels is computed to learn the spatially varying blur
kernels. We have conducted experiments on the benchmarks consisting of blurry
images and have evaluated full-reference and non-reference metrics. The
qualitative and quantitative results have revealed that our approach renders
the sharp novel views with vivid colors and fine details, and it has
considerably faster training time than the previous works. Our project page is
available at https://benhenryl.github.io/SharpNeRF/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WACV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GLIMPSE: Generalized Local Imaging with MLPs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00816v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00816v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        AmirEhsan Khorashadizadeh, Valentin Debarnot, Tianlin Liu, Ivan Dokmanić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning is the current de facto state of the art in tomographic
imaging. A common approach is to feed the result of a simple inversion, for
example the backprojection, to a convolutional neural network (CNN) which then
computes the reconstruction. Despite strong results on 'in-distribution' test
data similar to the training data, backprojection from sparse-view data
delocalizes singularities, so these approaches require a large receptive field
to perform well. As a consequence, they overfit to certain global structures
which leads to poor generalization on out-of-distribution (OOD) samples.
Moreover, their memory complexity and training time scale unfavorably with
image resolution, making them impractical for application at realistic clinical
resolutions, especially in 3D: a standard U-Net requires a substantial 140GB of
memory and 2600 seconds per epoch on a research-grade GPU when training on
1024x1024 images. In this paper, we introduce GLIMPSE, a local processing
neural network for computed tomography which reconstructs a pixel value by
feeding only the measurements associated with the neighborhood of the pixel to
a simple MLP. While achieving comparable or better performance with successful
CNNs like the U-Net on in-distribution test data, GLIMPSE significantly
outperforms them on OOD samples while maintaining a memory footprint almost
independent of image resolution; 5GB memory suffices to train on 1024x1024
images. Further, we built GLIMPSE to be fully differentiable, which enables
feats such as recovery of accurate projection angles if they are out of
calibration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accurate Leukocyte Detection Based on Deformable-DETR and Multi-Level
  Feature Fusion for Aiding Diagnosis of Blood Diseases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00926v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00926v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Chen, Chenyan Zhang, Ben Chen, Yiyu Huang, Yifei Sun, Changmiao Wang, Xianjun Fu, Yuxing Dai, Feiwei Qin, Yong Peng, Yu Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In standard hospital blood tests, the traditional process requires doctors to
manually isolate leukocytes from microscopic images of patients' blood using
microscopes. These isolated leukocytes are then categorized via automatic
leukocyte classifiers to determine the proportion and volume of different types
of leukocytes present in the blood samples, aiding disease diagnosis. This
methodology is not only time-consuming and labor-intensive, but it also has a
high propensity for errors due to factors such as image quality and
environmental conditions, which could potentially lead to incorrect subsequent
classifications and misdiagnosis. To address these issues, this paper proposes
an innovative method of leukocyte detection: the Multi-level Feature Fusion and
Deformable Self-attention DETR (MFDS-DETR). To tackle the issue of leukocyte
scale disparity, we designed the High-level Screening-feature Fusion Pyramid
(HS-FPN), enabling multi-level fusion. This model uses high-level features as
weights to filter low-level feature information via a channel attention module
and then merges the screened information with the high-level features, thus
enhancing the model's feature expression capability. Further, we address the
issue of leukocyte feature scarcity by incorporating a multi-scale deformable
self-attention module in the encoder and using the self-attention and
cross-deformable attention mechanisms in the decoder, which aids in the
extraction of the global features of the leukocyte feature maps. The
effectiveness, superiority, and generalizability of the proposed MFDS-DETR
method are confirmed through comparisons with other cutting-edge leukocyte
detection models using the private WBCDD, public LISC and BCCD datasets. Our
source code and private WBCCD dataset are available at
https://github.com/JustlfC03/MFDS-DETR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures, accept Computers in Biology and Medicine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retrieval-Augmented Egocentric Video Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00789v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00789v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jilan Xu, Yifei Huang, Junlin Hou, Guo Chen, Yuejie Zhang, Rui Feng, Weidi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding human actions from videos of first-person view poses
significant challenges. Most prior approaches explore representation learning
on egocentric videos only, while overlooking the potential benefit of
exploiting existing large-scale third-person videos. In this paper, (1) we
develop EgoInstructor, a retrieval-augmented multimodal captioning model that
automatically retrieves semantically relevant third-person instructional videos
to enhance the video captioning of egocentric videos. (2) For training the
cross-view retrieval module, we devise an automatic pipeline to discover
ego-exo video pairs from distinct large-scale egocentric and exocentric
datasets. (3) We train the cross-view retrieval module with a novel EgoExoNCE
loss that pulls egocentric and exocentric video features closer by aligning
them to shared text features that describe similar actions. (4) Through
extensive experiments, our cross-view retrieval module demonstrates superior
performance across seven benchmarks. Regarding egocentric video captioning,
EgoInstructor exhibits significant improvements by leveraging third-person
videos as references.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bracketing is All You Need: Unifying Image Restoration and Enhancement
  Tasks with Multi-Exposure Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhilu Zhang, Shuohao Zhang, Renlong Wu, Zifei Yan, Wangmeng Zuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is challenging but highly desired to acquire high-quality photos with
clear content in low-light environments. Although multi-image processing
methods (using burst, dual-exposure, or multi-exposure images) have made
significant progress in addressing this issue, they typically focus exclusively
on specific restoration or enhancement tasks, being insufficient in exploiting
multi-image. Motivated by that multi-exposure images are complementary in
denoising, deblurring, high dynamic range imaging, and super-resolution, we
propose to utilize bracketing photography to unify restoration and enhancement
tasks in this work. Due to the difficulty in collecting real-world pairs, we
suggest a solution that first pre-trains the model with synthetic paired data
and then adapts it to real-world unlabeled images. In particular, a temporally
modulated recurrent network (TMRNet) and self-supervised adaptation method are
proposed. Moreover, we construct a data simulation pipeline to synthesize pairs
and collect real-world images from 200 nighttime scenarios. Experiments on both
datasets show that our method performs favorably against the state-of-the-art
multi-image processing ones. The dataset, code, and pre-trained models are
available at https://github.com/cszhilu1998/BracketIRE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ New Job, New Gender? Measuring the Social Bias in Image Generation
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00763v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00763v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Wang, Haonan Bai, Jen-tse Huang, Yuxuan Wan, Youliang Yuan, Haoyi Qiu, Nanyun Peng, Michael R. Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image generation models can generate or edit images from a given text. Recent
advancements in image generation technology, exemplified by DALL-E and
Midjourney, have been groundbreaking. These advanced models, despite their
impressive capabilities, are often trained on massive Internet datasets, making
them susceptible to generating content that perpetuates social stereotypes and
biases, which can lead to severe consequences. Prior research on assessing bias
within image generation models suffers from several shortcomings, including
limited accuracy, reliance on extensive human labor, and lack of comprehensive
analysis. In this paper, we propose BiasPainter, a novel metamorphic testing
framework that can accurately, automatically and comprehensively trigger social
bias in image generation models. BiasPainter uses a diverse range of seed
images of individuals and prompts the image generation models to edit these
images using gender, race, and age-neutral queries. These queries span 62
professions, 39 activities, 57 types of objects, and 70 personality traits. The
framework then compares the edited images to the original seed images, focusing
on any changes related to gender, race, and age. BiasPainter adopts a testing
oracle that these characteristics should not be modified when subjected to
neutral prompts. Built upon this design, BiasPainter can trigger the social
bias and evaluate the fairness of image generation models. To evaluate the
effectiveness of BiasPainter, we use BiasPainter to test five widely-used
commercial image generation software and models, such as stable diffusion and
Midjourney. Experimental results show that 100\% of the generated test cases
can successfully trigger social bias in image generation models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Subspace Isolation: Many-to-Many <span class="highlight-title">Transformer</span> for Light Field
  Image Super-resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeke Zexi Hu, Xiaoming Chen, Vera Yuk Ying Chung, Yiran Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The effective extraction of spatial-angular features plays a crucial role in
light field image super-resolution (LFSR) tasks, and the introduction of
convolution and Transformers leads to significant improvement in this area.
Nevertheless, due to the large 4D data volume of light field images, many
existing methods opted to decompose the data into a number of lower-dimensional
subspaces and perform Transformers in each sub-space individually. As a side
effect, these methods inadvertently restrict the self-attention mechanisms to a
One-to-One scheme accessing only a limited subset of LF data, explicitly
preventing comprehensive optimization on all spatial and angular cues. In this
paper, we identify this limitation as subspace isolation and introduce a novel
Many-to-Many Transformer (M2MT) to address it. M2MT aggregates angular
information in the spatial subspace before performing the self-attention
mechanism. It enables complete access to all information across all
sub-aperture images (SAIs) in a light field image. Consequently, M2MT is
enabled to comprehensively capture long-range correlation dependencies. With
M2MT as the pivotal component, we develop a simple yet effective M2MT network
for LFSR. Our experimental results demonstrate that M2MT achieves
state-of-the-art performance across various public datasets. We further conduct
in-depth analysis using local attribution maps (LAM) to obtain visual
interpretability, and the results validate that M2MT is empowered with a truly
non-local context in both spatial and angular subspaces to mitigate subspace
isolation and acquire effective spatial-angular representation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffMorph: Text-less Image Morphing with Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shounak Chatterjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-conditioned image generation models are a prevalent use of AI image
synthesis, yet intuitively controlling output guided by an artist remains
challenging. Current methods require multiple images and textual prompts for
each object to specify them as concepts to generate a single customized image.
  On the other hand, our work, \verb|DiffMorph|, introduces a novel approach
that synthesizes images that mix concepts without the use of textual prompts.
Our work integrates a sketch-to-image module to incorporate user sketches as
input. \verb|DiffMorph| takes an initial image with conditioning artist-drawn
sketches to generate a morphed image.
  We employ a pre-trained text-to-image diffusion model and fine-tune it to
reconstruct each image faithfully. We seamlessly merge images and concepts from
sketches into a cohesive composition. The image generation capability of our
work is demonstrated through our results and a comparison of these with
prompt-based image generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion Models, Image Super-Resolution And Everything: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian B. Moser, Arundhati S. Shanbhag, Federico Raue, Stanislav Frolov, Sebastian Palacio, Andreas Dengel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion Models (DMs) represent a significant advancement in image
Super-Resolution (SR), aligning technical image quality more closely with human
preferences and expanding SR applications. DMs address critical limitations of
previous methods, enhancing overall realism and details in SR images. However,
DMs suffer from color-shifting issues, and their high computational costs call
for efficient sampling alternatives, underscoring the challenge of balancing
computational efficiency and image quality. This survey gives an overview of
DMs applied to image SR and offers a detailed analysis that underscores the
unique characteristics and methodologies within this domain, distinct from
broader existing reviews in the field. It presents a unified view of DM
fundamentals and explores research directions, including alternative input
domains, conditioning strategies, guidance, corruption spaces, and zero-shot
methods. This survey provides insights into the evolution of image SR with DMs,
addressing current trends, challenges, and future directions in this rapidly
evolving field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Skeleton2vec: A <span class="highlight-title">Self-supervised</span> Learning Framework with Contextualized
  Target Representations for Skeleton Sequence <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruizhuo Xu, Linzhi Huang, Mei Wang, Jiani Hu, Weihong Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised pre-training paradigms have been extensively explored in the
field of skeleton-based action recognition. In particular, methods based on
masked prediction have pushed the performance of pre-training to a new height.
However, these methods take low-level features, such as raw joint coordinates
or temporal motion, as prediction targets for the masked regions, which is
suboptimal. In this paper, we show that using high-level contextualized
features as prediction targets can achieve superior performance. Specifically,
we propose Skeleton2vec, a simple and efficient self-supervised 3D action
representation learning framework, which utilizes a transformer-based teacher
encoder taking unmasked training samples as input to create latent
contextualized representations as prediction targets. Benefiting from the
self-attention mechanism, the latent representations generated by the teacher
encoder can incorporate the global context of the entire training samples,
leading to a richer training task. Additionally, considering the high temporal
correlations in skeleton sequences, we propose a motion-aware tube masking
strategy which divides the skeleton sequence into several tubes and performs
persistent masking within each tube based on motion priors, thus forcing the
model to build long-range spatio-temporal connections and focus on
action-semantic richer regions. Extensive experiments on NTU-60, NTU-120, and
PKU-MMD datasets demonstrate that our proposed Skeleton2vec outperforms
previous methods and achieves state-of-the-art results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NightRain: Nighttime Video Deraining via Adaptive-Rain-Removal and
  Adaptive-Correction <span class="chip">AAAI24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beibei Lin, Yeying Jin, Wending Yan, Wei Ye, Yuan Yuan, Shunli Zhang, Robby Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing deep-learning-based methods for nighttime video deraining rely on
synthetic data due to the absence of real-world paired data. However, the
intricacies of the real world, particularly with the presence of light effects
and low-light regions affected by noise, create significant domain gaps,
hampering synthetic-trained models in removing rain streaks properly and
leading to over-saturation and color shifts. Motivated by this, we introduce
NightRain, a novel nighttime video deraining method with adaptive-rain-removal
and adaptive-correction. Our adaptive-rain-removal uses unlabeled rain videos
to enable our model to derain real-world rain videos, particularly in regions
affected by complex light effects. The idea is to allow our model to obtain
rain-free regions based on the confidence scores. Once rain-free regions and
the corresponding regions from our input are obtained, we can have region-based
paired real data. These paired data are used to train our model using a
teacher-student framework, allowing the model to iteratively learn from less
challenging regions to more challenging regions. Our adaptive-correction aims
to rectify errors in our model's predictions, such as over-saturation and color
shifts. The idea is to learn from clear night input training videos based on
the differences or distance between those input videos and their corresponding
predictions. Our model learns from these differences, compelling our model to
correct the errors. From extensive experiments, our method demonstrates
state-of-the-art performance. It achieves a PSNR of 26.73dB, surpassing
existing nighttime video deraining methods by a substantial margin of 13.7%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MultiFusionNet: Multilayer Multimodal Fusion of Deep Neural Networks for
  Chest X-Ray Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saurabh Agarwal, K. V. Arya, Yogesh Kumar Meena
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chest X-ray imaging is a critical diagnostic tool for identifying pulmonary
diseases. However, manual interpretation of these images is time-consuming and
error-prone. Automated systems utilizing convolutional neural networks (CNNs)
have shown promise in improving the accuracy and efficiency of chest X-ray
image classification. While previous work has mainly focused on using feature
maps from the final convolution layer, there is a need to explore the benefits
of leveraging additional layers for improved disease classification. Extracting
robust features from limited medical image datasets remains a critical
challenge. In this paper, we propose a novel deep learning-based multilayer
multimodal fusion model that emphasizes extracting features from different
layers and fusing them. Our disease detection model considers the
discriminatory information captured by each layer. Furthermore, we propose the
fusion of different-sized feature maps (FDSFM) module to effectively merge
feature maps from diverse layers. The proposed model achieves a significantly
higher accuracy of 97.21% and 99.60% for both three-class and two-class
classifications, respectively. The proposed multilayer multimodal fusion model,
along with the FDSFM module, holds promise for accurate disease classification
and can also be extended to other disease classifications in chest X-ray
images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BRAU-Net++: U-Shaped Hybrid CNN-<span class="highlight-title">Transformer</span> Network for Medical Image
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Libin Lan, Pengzhou Cai, Lu Jiang, Xiaojuan Liu, Yongmei Li, Yudong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate medical image segmentation is essential for clinical quantification,
disease diagnosis, treatment planning and many other applications. Both
convolution-based and transformer-based u-shaped architectures have made
significant success in various medical image segmentation tasks. The former can
efficiently learn local information of images while requiring much more
image-specific inductive biases inherent to convolution operation. The latter
can effectively capture long-range dependency at different feature scales using
self-attention, whereas it typically encounters the challenges of quadratic
compute and memory requirements with sequence length increasing. To address
this problem, through integrating the merits of these two paradigms in a
well-designed u-shaped architecture, we propose a hybrid yet effective
CNN-Transformer network, named BRAU-Net++, for an accurate medical image
segmentation task. Specifically, BRAU-Net++ uses bi-level routing attention as
the core building block to design our u-shaped encoder-decoder structure, in
which both encoder and decoder are hierarchically constructed, so as to learn
global semantic information while reducing computational complexity.
Furthermore, this network restructures skip connection by incorporating
channel-spatial attention which adopts convolution operations, aiming to
minimize local spatial information loss and amplify global
dimension-interaction of multi-scale features. Extensive experiments on three
public benchmark datasets demonstrate that our proposed approach surpasses
other state-of-the-art methods including its baseline: BRAU-Net under almost
all evaluation metrics. We achieve the average Dice-Similarity Coefficient
(DSC) of 82.47, 90.10, and 92.94 on Synapse multi-organ segmentation, ISIC-2018
Challenge, and CVC-ClinicDB, as well as the mIoU of 84.01 and 88.17 on
ISIC-2018 Challenge and CVC-ClinicDB, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures, 9 tables code:
  https://github.com/Caipengzhou/BRAU-Netplusplus</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Depth Map Denoising Network and Lightweight Fusion Network for Enhanced
  3D Face Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00719v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00719v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruizhuo Xu, Ke Wang, Chao Deng, Mei Wang, Xi Chen, Wenhui Huang, Junlan Feng, Weihong Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing availability of consumer depth sensors, 3D face
recognition (FR) has attracted more and more attention. However, the data
acquired by these sensors are often coarse and noisy, making them impractical
to use directly. In this paper, we introduce an innovative Depth map denoising
network (DMDNet) based on the Denoising Implicit Image Function (DIIF) to
reduce noise and enhance the quality of facial depth images for low-quality 3D
FR. After generating clean depth faces using DMDNet, we further design a
powerful recognition network called Lightweight Depth and Normal Fusion network
(LDNFNet), which incorporates a multi-branch fusion block to learn unique and
complementary features between different modalities such as depth and normal
images. Comprehensive experiments conducted on four distinct low-quality
databases demonstrate the effectiveness and robustness of our proposed methods.
Furthermore, when combining DMDNet and LDNFNet, we achieve state-of-the-art
results on the Lock3DFace database.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Pattern Recognition</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text2Avatar: Text to 3D Human Avatar Generation with Codebook-Driven
  Body Controllable Attribute 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoqun Gong, Yuqin Dai, Ronghui Li, Achun Bao, Jun Li, Jian Yang, Yachao Zhang, Xiu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating 3D human models directly from text helps reduce the cost and time
of character modeling. However, achieving multi-attribute controllable and
realistic 3D human avatar generation is still challenging due to feature
coupling and the scarcity of realistic 3D human avatar datasets. To address
these issues, we propose Text2Avatar, which can generate realistic-style 3D
avatars based on the coupled text prompts. Text2Avatar leverages a discrete
codebook as an intermediate feature to establish a connection between text and
avatars, enabling the disentanglement of features. Furthermore, to alleviate
the scarcity of realistic style 3D human avatar data, we utilize a pre-trained
unconditional 3D human avatar generation model to obtain a large amount of 3D
avatar pseudo data, which allows Text2Avatar to achieve realistic style
generation. Experimental results demonstrate that our method can generate
realistic 3D avatars from coupled textual data, which is challenging for other
existing methods in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Nonlocal Self-Similarity from Continuous Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yisi Luo, Xile Zhao, Deyu Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nonlocal self-similarity (NSS) is an important prior that has been
successfully applied in multi-dimensional data processing tasks, e.g., image
and video recovery. However, existing NSS-based methods are solely suitable for
meshgrid data such as images and videos, but are not suitable for emerging
off-meshgrid data, e.g., point cloud and climate data. In this work, we revisit
the NSS from the continuous representation perspective and propose a novel
Continuous Representation-based NonLocal method (termed as CRNL), which has two
innovative features as compared with classical nonlocal methods. First, based
on the continuous representation, our CRNL unifies the measure of
self-similarity for on-meshgrid and off-meshgrid data and thus is naturally
suitable for both of them. Second, the nonlocal continuous groups can be more
compactly and efficiently represented by the coupled low-rank function
factorization, which simultaneously exploits the similarity within each group
and across different groups, while classical nonlocal methods neglect the
similarity across groups. This elaborately designed coupled mechanism allows
our method to enjoy favorable performance over conventional NSS methods in
terms of both effectiveness and efficiency. Extensive multi-dimensional data
processing experiments on-meshgrid (e.g., image inpainting and image denoising)
and off-meshgrid (e.g., climate data prediction and point cloud recovery)
validate the versatility, effectiveness, and efficiency of our CRNL as compared
with state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Efficient and Effective Text-to-Video Retrieval with
  Coarse-to-Fine Visual Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaibin Tian, Yanhua Cheng, Yi Liu, Xinglin Hou, Quan Chen, Han Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, text-to-video retrieval methods based on CLIP have
experienced rapid development. The primary direction of evolution is to exploit
the much wider gamut of visual and textual cues to achieve alignment.
Concretely, those methods with impressive performance often design a heavy
fusion block for sentence (words)-video (frames) interaction, regardless of the
prohibitive computation complexity. Nevertheless, these approaches are not
optimal in terms of feature utilization and retrieval efficiency. To address
this issue, we adopt multi-granularity visual feature learning, ensuring the
model's comprehensiveness in capturing visual content features spanning from
abstract to detailed levels during the training phase. To better leverage the
multi-granularity features, we devise a two-stage retrieval architecture in the
retrieval phase. This solution ingeniously balances the coarse and fine
granularity of retrieval content. Moreover, it also strikes a harmonious
equilibrium between retrieval effectiveness and efficiency. Specifically, in
training phase, we design a parameter-free text-gated interaction block (TIB)
for fine-grained video representation learning and embed an extra Pearson
Constraint to optimize cross-modal representation learning. In retrieval phase,
we use coarse-grained video representations for fast recall of top-k
candidates, which are then reranked by fine-grained video representations.
Extensive experiments on four benchmarks demonstrate the efficiency and
effectiveness. Notably, our method achieves comparable performance with the
current state-of-the-art methods while being nearly 50 times faster.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An attempt to generate new bridge types from latent space of generative
  adversarial network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00700v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00700v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongjun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Try to generate new bridge types using generative artificial intelligence
technology. Symmetric structured image dataset of three-span beam bridge, arch
bridge, cable-stayed bridge and suspension bridge are used . Based on Python
programming language, TensorFlow and Keras deep learning platform framework ,
as well as Wasserstein loss function and Lipschitz constraints, generative
adversarial network is constructed and trained. From the obtained low
dimensional bridge-type latent space sampling, new bridge types with asymmetric
structures can be generated. Generative adversarial network can create new
bridge types by organically combining different structural components on the
basis of human original bridge types. It has a certain degree of human original
ability. Generative artificial intelligence technology can open up imagination
space and inspire humanity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Credible Teacher for Semi-Supervised Object Detection in Open Scene <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyu Zhuang, Kuo Wang, Liang Lin, Guanbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-Supervised Object Detection (SSOD) has achieved resounding success by
leveraging unlabeled data to improve detection performance. However, in Open
Scene Semi-Supervised Object Detection (O-SSOD), unlabeled data may contains
unknown objects not observed in the labeled data, which will increase
uncertainty in the model's predictions for known objects. It is detrimental to
the current methods that mainly rely on self-training, as more uncertainty
leads to the lower localization and classification precision of pseudo labels.
To this end, we propose Credible Teacher, an end-to-end framework. Credible
Teacher adopts an interactive teaching mechanism using flexible labels to
prevent uncertain pseudo labels from misleading the model and gradually reduces
its uncertainty through the guidance of other credible pseudo labels. Empirical
results have demonstrated our method effectively restrains the adverse effect
caused by O-SSOD and significantly outperforms existing counterparts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accpet by ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-supervised</span> learning for skin cancer diagnosis with limited training
  data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamish Haggerty, Rohitash Chandra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cancer diagnosis is a well-studied problem in machine learning since early
detection of cancer is often the determining factor in prognosis. Supervised
deep learning achieves excellent results in cancer image classification,
usually through transfer learning. However, these models require large amounts
of labelled data and for several types of cancer, large labelled datasets do
not exist. In this paper, we demonstrate that a model pre-trained using a
self-supervised learning algorithm known as Barlow Twins can outperform the
conventional supervised transfer learning pipeline. We juxtapose two base
models: i) pretrained in a supervised fashion on ImageNet; ii) pretrained in a
self-supervised fashion on ImageNet. Both are subsequently fine tuned on a
small labelled skin lesion dataset and evaluated on a large test set. We
achieve a mean test accuracy of 70\% for self-supervised transfer in comparison
to 66\% for supervised transfer. Interestingly, boosting performance further is
possible by self-supervised pretraining a second time (on unlabelled skin
lesion images) before subsequent fine tuning. This hints at an alternative path
to collecting more labelled data in settings where this is challenging - namely
just collecting more unlabelled images. Our framework is applicable to cancer
image classification models in the low-labelled data regime.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 1st Place Solution for 5th LSVOS Challenge: Referring Video Object
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoyan Luo, Yicheng Xiao, Yong Liu, Yitong Wang, Yansong Tang, Xiu Li, Yujiu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent transformer-based models have dominated the Referring Video Object
Segmentation (RVOS) task due to the superior performance. Most prior works
adopt unified DETR framework to generate segmentation masks in
query-to-instance manner. In this work, we integrate strengths of that leading
RVOS models to build up an effective paradigm. We first obtain binary mask
sequences from the RVOS models. To improve the consistency and quality of
masks, we propose Two-Stage Multi-Model Fusion strategy. Each stage rationally
ensembles RVOS models based on framework design as well as training strategy,
and leverages different video object segmentation (VOS) models to enhance mask
coherence by object propagation mechanism. Our method achieves 75.7% J&F on
Ref-Youtube-VOS validation set and 70% J&F on test set, which ranks 1st place
on 5th Large-scale Video Object Segmentation Challenge (ICCV 2023) track 3.
Code is available at https://github.com/RobertLuo1/iccv2023_RVOS_Challenge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing ADMM and Over-Relaxed ADMM Parameters for Linear Quadratic
  Problems <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00657v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00657v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jintao Song, Wenqi Lu, Yunwen Lei, Yuchao Tang, Zhenkuan Pan, Jinming Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Alternating Direction Method of Multipliers (ADMM) has gained significant
attention across a broad spectrum of machine learning applications.
Incorporating the over-relaxation technique shows potential for enhancing the
convergence rate of ADMM. However, determining optimal algorithmic parameters,
including both the associated penalty and relaxation parameters, often relies
on empirical approaches tailored to specific problem domains and contextual
scenarios. Incorrect parameter selection can significantly hinder ADMM's
convergence rate. To address this challenge, in this paper we first propose a
general approach to optimize the value of penalty parameter, followed by a
novel closed-form formula to compute the optimal relaxation parameter in the
context of linear quadratic problems (LQPs). We then experimentally validate
our parameter selection methods through random instantiations and diverse
imaging applications, encompassing diffeomorphic image registration, image
deblurring, and MRI reconstruction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">PROMPT</span>-IML: Image Manipulation Localization with <span class="highlight-title">Pre-train</span>ed Foundation
  Models Through <span class="highlight-title">Prompt</span> Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00653v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00653v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuntao Liu, Yuzhou Yang, Qichao Ying, Zhenxing Qian, Xinpeng Zhang, Sheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deceptive images can be shared in seconds with social networking services,
posing substantial risks. Tampering traces, such as boundary artifacts and
high-frequency information, have been significantly emphasized by massive
networks in the Image Manipulation Localization (IML) field. However, they are
prone to image post-processing operations, which limit the generalization and
robustness of existing methods. We present a novel Prompt-IML framework. We
observe that humans tend to discern the authenticity of an image based on both
semantic and high-frequency information, inspired by which, the proposed
framework leverages rich semantic knowledge from pre-trained visual foundation
models to assist IML. We are the first to design a framework that utilizes
visual foundation models specially for the IML task. Moreover, we design a
Feature Alignment and Fusion module to align and fuse features of semantic
features with high-frequency features, which aims at locating tampered regions
from multiple perspectives. Experimental results demonstrate that our model can
achieve better performance on eight typical fake image datasets and outstanding
robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Covert Hiding to Visual Editing: Robust Generative Video
  Steganography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueying Mao, Xiaoxiao Hu, Wanli Peng, Zhenliang Gan, Qichao Ying, Zhenxing Qian, Sheng Li, Xinpeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional video steganography methods are based on modifying the covert
space for embedding, whereas we propose an innovative approach that embeds
secret message within semantic feature for steganography during the video
editing process. Although existing traditional video steganography methods
display a certain level of security and embedding capacity, they lack adequate
robustness against common distortions in online social networks (OSNs). In this
paper, we introduce an end-to-end robust generative video steganography network
(RoGVS), which achieves visual editing by modifying semantic feature of videos
to embed secret message. We employ face-swapping scenario to showcase the
visual editing effects. We first design a secret message embedding module to
adaptively hide secret message into the semantic feature of videos. Extensive
experiments display that the proposed RoGVS method applied to facial video
datasets demonstrate its superiority over existing video and image
steganography techniques in terms of both robustness and capacity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Geometry Depth Consistency in RGBD Relative Pose Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00639v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00639v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sourav Kumar, Chiang-Heng Chien, Benjamin Kimia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relative pose estimation for RGBD cameras is crucial in a number of
applications. Previous approaches either rely on the RGB aspect of the images
to estimate pose thus not fully making use of depth in the estimation process
or estimate pose from the 3D cloud of points that each image produces, thus not
making full use of RGB information. This paper shows that if one pair of
correspondences is hypothesized from the RGB-based ranked-ordered
correspondence list, then the space of remaining correspondences is restricted
to corresponding pairs of curves nested around the hypothesized correspondence,
implicitly capturing depth consistency. This simple Geometric Depth Constraint
(GDC) significantly reduces potential matches. In effect this becomes a filter
on possible correspondences that helps reduce the number of outliers and thus
expedites RANSAC significantly. As such, the same budget of time allows for
more RANSAC iterations and therefore additional robustness and a significant
speedup. In addition, the paper proposed a Nested RANSAC approach that also
speeds up the process, as shown through experiments on TUM, ICL-NUIM, and RGBD
Scenes v2 datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ScatterFormer: Efficient Voxel <span class="highlight-title">Transformer</span> with Scattered Linear
  Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00912v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00912v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhang He, Ruihuang Li, Guowen Zhang, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Window-based transformers have demonstrated strong ability in large-scale
point cloud understanding by capturing context-aware representations with
affordable attention computation in a more localized manner. However, because
of the sparse nature of point clouds, the number of voxels per window varies
significantly. Current methods partition the voxels in each window into
multiple subsets of equal size, which cost expensive overhead in sorting and
padding the voxels, making them run slower than sparse convolution based
methods. In this paper, we present ScatterFormer, which, for the first time to
our best knowledge, could directly perform attention on voxel sets with
variable length. The key of ScatterFormer lies in the innovative Scatter Linear
Attention (SLA) module, which leverages the linear attention mechanism to
process in parallel all voxels scattered in different windows. Harnessing the
hierarchical computation units of the GPU and matrix blocking algorithm, we
reduce the latency of the proposed SLA module to less than 1 ms on moderate
GPUs. Besides, we develop a cross-window interaction module to simultaneously
enhance the local representation and allow the information flow across windows,
eliminating the need for window shifting. Our proposed ScatterFormer
demonstrates 73 mAP (L2) on the large-scale Waymo Open Dataset and 70.5 NDS on
the NuScenes dataset, running at an outstanding detection rate of 28 FPS. Code
is available at https://github.com/skyhehe123/ScatterFormer
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Improved Proxy-based Deep Metric Learning via Data-Augmented
  Domain Adaptation <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00617v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00617v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Ren, Chen Chen, Liqiang Wang, Kien Hua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Metric Learning (DML) plays an important role in modern computer vision
research, where we learn a distance metric for a set of image representations.
Recent DML techniques utilize the proxy to interact with the corresponding
image samples in the embedding space. However, existing proxy-based DML methods
focus on learning individual proxy-to-sample distance while the overall
distribution of samples and proxies lacks attention. In this paper, we present
a novel proxy-based DML framework that focuses on aligning the sample and proxy
distributions to improve the efficiency of proxy-based DML losses.
Specifically, we propose the Data-Augmented Domain Adaptation (DADA) method to
adapt the domain gap between the group of samples and proxies. To the best of
our knowledge, we are the first to leverage domain adaptation to boost the
performance of proxy-based DML. We show that our method can be easily plugged
into existing proxy-based DML losses. Our experiments on benchmarks, including
the popular CUB-200-2011, CARS196, Stanford Online Products, and In-Shop
Clothes Retrieval, show that our learning algorithm significantly improves the
existing proxy losses and achieves superior results compared to the existing
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ParisLuco3D: A high-quality target <span class="highlight-title">dataset</span> for domain generalization of
  LiDAR perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16542v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16542v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jules Sanchez, Louis Soum-Fontez, Jean-Emmanuel Deschaud, Francois Goulette
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR is an essential sensor for autonomous driving by collecting precise
geometric information regarding a scene. As the performance of various LiDAR
perception tasks has improved, generalizations to new environments and sensors
has emerged to test these optimized models in real-world conditions.
Unfortunately, the various annotation strategies of data providers complicate
the computation of cross-domain performances.
  This paper provides a novel dataset, ParisLuco3D, specifically designed for
cross-domain evaluation to make it easier to evaluate the performance utilizing
various source datasets. Alongside the dataset, online benchmarks for LiDAR
semantic segmentation, LiDAR object detection, and LiDAR tracking are provided
to ensure a fair comparison across methods.
  The ParisLuco3D dataset, evaluation scripts, and links to benchmarks can be
found at the following website: https://npm3d.fr/parisluco3d
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level
  Vision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14181v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14181v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, Weisi Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid evolution of Multi-modality Large Language Models (MLLMs) has
catalyzed a shift in computer vision from specialized models to general-purpose
foundation models. Nevertheless, there is still an inadequacy in assessing the
abilities of MLLMs on low-level visual perception and understanding. To address
this gap, we present Q-Bench, a holistic benchmark crafted to systematically
evaluate potential abilities of MLLMs on three realms: low-level visual
perception, low-level visual description, and overall visual quality
assessment. a) To evaluate the low-level perception ability, we construct the
LLVisionQA dataset, consisting of 2,990 diverse-sourced images, each equipped
with a human-asked question focusing on its low-level attributes. We then
measure the correctness of MLLMs on answering these questions. b) To examine
the description ability of MLLMs on low-level information, we propose the
LLDescribe dataset consisting of long expert-labelled golden low-level text
descriptions on 499 images, and a GPT-involved comparison pipeline between
outputs of MLLMs and the golden descriptions. c) Besides these two tasks, we
further measure their visual quality assessment ability to align with human
opinion scores. Specifically, we design a softmax-based strategy that enables
MLLMs to predict quantifiable quality scores, and evaluate them on various
existing image quality assessment (IQA) datasets. Our evaluation across the
three abilities confirms that MLLMs possess preliminary low-level visual
skills. However, these skills are still unstable and relatively imprecise,
indicating the need for specific enhancements on MLLMs towards these abilities.
We hope that our benchmark can encourage the research community to delve deeper
to discover and enhance these untapped potentials of MLLMs. Project Page:
https://q-future.github.io/Q-Bench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 11 tables, with updated results</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FENet: Focusing Enhanced Network for Lane Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17163v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17163v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liman Wang, Hanyang Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by human driving focus, this research pioneers networks augmented
with Focusing Sampling, Partial Field of View Evaluation, Enhanced FPN
architecture and Directional IoU Loss - targeted innovations addressing
obstacles to precise lane detection for autonomous driving. Experiments
demonstrate our Focusing Sampling strategy, emphasizing vital distant details
unlike uniform approaches, significantly boosts both benchmark and practical
curved/distant lane recognition accuracy essential for safety. While FENetV1
achieves state-of-the-art conventional metric performance via enhancements
isolating perspective-aware contexts mimicking driver vision, FENetV2 proves
most reliable on the proposed Partial Field analysis. Hence we specifically
recommend V2 for practical lane navigation despite fractional degradation on
standard entire-image measures. Future directions include collecting on-road
data and integrating complementary dual frameworks to further breakthroughs
guided by human perception principles. Code will be made available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages including appendix. The website will be released soon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NegVSR: Augmenting Negatives for Generalized Noise Modeling in
  Real-World Video Super-Resolution <span class="chip">AAAI2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14669v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14669v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yexing Song, Meilin Wang, Zhijing Yang, Xiaoyu Xian, Yukai Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The capability of video super-resolution (VSR) to synthesize high-resolution
(HR) video from ideal datasets has been demonstrated in many works. However,
applying the VSR model to real-world video with unknown and complex degradation
remains a challenging task. First, existing degradation metrics in most VSR
methods are not able to effectively simulate real-world noise and blur. On the
contrary, simple combinations of classical degradation are used for real-world
noise modeling, which led to the VSR model often being violated by
out-of-distribution noise. Second, many SR models focus on noise simulation and
transfer. Nevertheless, the sampled noise is monotonous and limited. To address
the aforementioned problems, we propose a Negatives augmentation strategy for
generalized noise modeling in Video Super-Resolution (NegVSR) task.
Specifically, we first propose sequential noise generation toward real-world
data to extract practical noise sequences. Then, the degeneration domain is
widely expanded by negative augmentation to build up various yet challenging
real-world noise sets. We further propose the augmented negative guidance loss
to learn robust features among augmented negatives effectively. Extensive
experiments on real-world datasets (e.g., VideoLQ and FLIR) show that our
method outperforms state-of-the-art methods with clear margins, especially in
visual quality. Project page is available at: https://negvsr.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2024, a effective data augmentation framework for
  real-world video super-resolution, see our demo at: https://negvsr.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ASSISTGUI: Task-Oriented Desktop Graphical User Interface Automation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.13108v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.13108v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Difei Gao, Lei Ji, Zechen Bai, Mingyu Ouyang, Peiran Li, Dongxing Mao, Qinchen Wu, Weichen Zhang, Peiyi Wang, Xiangwu Guo, Hengxu Wang, Luowei Zhou, Mike Zheng Shou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graphical User Interface (GUI) automation holds significant promise for
assisting users with complex tasks, thereby boosting human productivity.
Existing works leveraging Large Language Model (LLM) or LLM-based AI agents
have shown capabilities in automating tasks on Android and Web platforms.
However, these tasks are primarily aimed at simple device usage and
entertainment operations. This paper presents a novel benchmark, AssistGUI, to
evaluate whether models are capable of manipulating the mouse and keyboard on
the Windows platform in response to user-requested tasks. We carefully
collected a set of 100 tasks from nine widely-used software applications, such
as, After Effects and MS Word, each accompanied by the necessary project files
for better evaluation. Moreover, we propose an advanced Actor-Critic Embodied
Agent framework, which incorporates a sophisticated GUI parser driven by an
LLM-agent and an enhanced reasoning mechanism adept at handling lengthy
procedural tasks. Our experimental results reveal that our GUI Parser and
Reasoning mechanism outshine existing methods in performance. Nevertheless, the
potential remains substantial, with the best model attaining only a 46% success
rate on our benchmark. We conclude with a thorough analysis of the current
methods' limitations, setting the stage for future breakthroughs in this
domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://showlab.github.io/assistgui/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UDTIRI: An Online Open-Source Intelligent Road Inspection Benchmark
  Suite 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.08842v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.08842v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sicen Guo, Jiahang Li, Yi Feng, Dacheng Zhou, Denghuang Zhang, Chen Chen, Shuai Su, Xingyi Zhu, Qijun Chen, Rui Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the nascent domain of urban digital twins (UDT), the prospects for
leveraging cutting-edge deep learning techniques are vast and compelling.
Particularly within the specialized area of intelligent road inspection (IRI),
a noticeable gap exists, underscored by the current dearth of dedicated
research efforts and the lack of large-scale well-annotated datasets. To foster
advancements in this burgeoning field, we have launched an online open-source
benchmark suite, referred to as UDTIRI. Along with this article, we introduce
the road pothole detection task, the first online competition published within
this benchmark suite. This task provides a well-annotated dataset, comprising
1,000 RGB images and their pixel/instance-level ground-truth annotations,
captured in diverse real-world scenarios under different illumination and
weather conditions. Our benchmark provides a systematic and thorough evaluation
of state-of-the-art object detection, semantic segmentation, and instance
segmentation networks, developed based on either convolutional neural networks
or Transformers. We anticipate that our benchmark will serve as a catalyst for
the integration of advanced UDT techniques into IRI. By providing algorithms
with a more comprehensive understanding of diverse road conditions, we seek to
unlock their untapped potential and foster innovation in this critical domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Database webpage: https://www.udtiri.com/, Kaggle webpage:
  https://www.kaggle.com/datasets/jiahangli617/udtiri</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MABViT -- Modified Attention Block Enhances Vision <span class="highlight-title">Transformer</span>s <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.01324v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.01324v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahesh Ramesh, Aswinkumar Ramkumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have demonstrated the effectiveness of Gated Linear Units
(GLU) in enhancing transformer models, particularly in Large Language Models
(LLMs). Additionally, utilizing a parallel configuration within each
Transformer block rather than the conventional serialized method has been
revealed to accelerate the training of LLMs without significantly impacting
performance. However, when the MLP and attention block were run in parallel for
the image classification task, we observed a noticeable decline in performance.
We propose a novel transformer variant that integrates non-linearity within the
attention block to tackle this problem. We implemented the GLU-based activation
function on the Value tensor, and this new technique surpasses the current
state-of-the-art S/16 variant of Vision Transformers by 0.6% on the ImageNet-1K
dataset while utilizing fewer parameters. It also supersedes the B/16 variant
while using only half the parameters. Furthermore, we provide results with the
GELU activation function variant to confirm our assertions. Lastly, we showcase
that the MABViT variants exhibit greater potential when utilized in deep
transformers compared to the standard architecture.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Deployable AI Workshop, AAAI Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Full-scene Domain Generalization in Multi-agent Collaborative
  Bird's Eye View Segmentation for Connected and Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16754v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16754v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Senkang Hu, Zhengru Fang, Xianhao Chen, Yuguang Fang, Sam Kwong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative perception has recently gained significant attention in
autonomous driving, improving perception quality by enabling the exchange of
additional information among vehicles. However, deploying collaborative
perception systems can lead to domain shifts due to diverse environmental
conditions and data heterogeneity among connected and autonomous vehicles
(CAVs). To address these challenges, we propose a unified domain generalization
framework applicable in both training and inference stages of collaborative
perception. In the training phase, we introduce an Amplitude Augmentation
(AmpAug) method to augment low-frequency image variations, broadening the
model's ability to learn across various domains. We also employ a
meta-consistency training scheme to simulate domain shifts, optimizing the
model with a carefully designed consistency loss to encourage domain-invariant
representations. In the inference phase, we introduce an intra-system domain
alignment mechanism to reduce or potentially eliminate the domain discrepancy
among CAVs prior to inference. Comprehensive experiments substantiate the
effectiveness of our method in comparison with the existing state-of-the-art
works. Code will be released at https://github.com/DG-CAVs/DG-CoPerception.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffusionLight: Light Probes for Free by Painting a Chrome Ball 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.09168v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.09168v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pakkapon Phongthawee, Worameth Chinchuthakun, Nontaphat Sinsunthithet, Amit Raj, Varun Jampani, Pramook Khungurn, Supasorn Suwajanakorn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a simple yet effective technique to estimate lighting in a single
input image. Current techniques rely heavily on HDR panorama datasets to train
neural networks to regress an input with limited field-of-view to a full
environment map. However, these approaches often struggle with real-world,
uncontrolled settings due to the limited diversity and size of their datasets.
To address this problem, we leverage diffusion models trained on billions of
standard images to render a chrome ball into the input image. Despite its
simplicity, this task remains challenging: the diffusion models often insert
incorrect or inconsistent objects and cannot readily generate images in HDR
format. Our research uncovers a surprising relationship between the appearance
of chrome balls and the initial diffusion noise map, which we utilize to
consistently generate high-quality chrome balls. We further fine-tune an LDR
difusion model (Stable Diffusion XL) with LoRA, enabling it to perform exposure
bracketing for HDR light estimation. Our method produces convincing light
estimates across diverse settings and demonstrates superior generalization to
in-the-wild scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>For more info and code, please visit our website
  https://diffusionlight.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An attempt to generate new bridge types from latent space of variational
  autoencoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.03380v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.03380v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongjun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Try to generate new bridge types using generative artificial intelligence
technology. The grayscale images of the bridge facade with the change of
component width was rendered by 3dsMax animation software, and then the OpenCV
module performed an appropriate amount of geometric transformation (rotation,
horizontal scale, vertical scale) to obtain the image dataset of three-span
beam bridge, arch bridge, cable-stayed bridge and suspension bridge. Based on
Python programming language, TensorFlow and Keras deep learning platform
framework, variational autoencoder was constructed and trained, and
low-dimensional bridge-type latent space that is convenient for vector
operations was obtained. Variational autoencoder can combine two bridge types
on the basis of the original of human into one that is a new bridge type.
Generative artificial intelligence technology can assist bridge designers in
bridge-type innovation, and can be used as copilot.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VariabilityTrack:Multi-Object Tracking with Variable Speed Object
  Movement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.06424v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.06424v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Run Luo, JinLin Wei, Qiao Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-object tracking (MOT) aims at estimating bounding boxes and identities
of objects in videos. Most methods can be roughly classified as
tracking-by-detection and joint-detection-association paradigms. Although the
latter has elicited more attention and demonstrates comparable performance
relative than the former, we claim that the tracking-by-detection paradigm is
still the optimal solution in terms of tracking accuracy,such as
ByteTrack,which achieves 80.3 MOTA, 77.3 IDF1 and 63.1 HOTA on the test set of
MOT17 with 30 FPS running speed on a single V100 GPU.However, under complex
perspectives such as vehicle and UAV acceleration, the performance of such a
tracker using uniform Kalman filter will be greatly affected, resulting in
tracking loss.In this paper, we propose a variable speed Kalman filter
algorithm based on environmental feedback and improve the matching process,
which can greatly improve the tracking effect in complex variable speed scenes
while maintaining high tracking accuracy in relatively static scenes.
Eventually, higher MOTA and IDF1 results can be achieved on MOT17 test set than
ByteTrack
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>find some mistake in this paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lightweight texture transfer based on texture feature preset 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.16846v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.16846v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        ShiQi Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the task of texture transfer, reference texture images typically exhibit
highly repetitive texture features, and the texture transfer results from
different content images under the same style also share remarkably similar
texture patterns. Encoding such highly similar texture features often requires
deep layers and a large number of channels, making it is also the main source
of the entire model's parameter count and computational load, and inference
time. We propose a lightweight texture transfer based on texture feature preset
(TFP). TFP takes full advantage of the high repetitiveness of texture features
by providing preset universal texture feature maps for a given style. These
preset feature maps can be fused and decoded directly with shallow color
transfer feature maps of any content to generate texture transfer results,
thereby avoiding redundant texture information from being encoded repeatedly.
The texture feature map we preset is encoded through noise input images with
consistent distribution (standard normal distribution). This consistent input
distribution can completely avoid the problem of texture transfer
differentiation, and by randomly sampling different noise inputs, we can obtain
different texture features and texture transfer results under the same
reference style. Compared to state-of-the-art techniques, our TFP not only
produces visually superior results but also reduces the model size by 3.2-3538
times and speeds up the process by 1.8-5.6 times.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Low-Light Image and Video Enhancement: A Comprehensive <span class="highlight-title">Survey</span> and Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.10772v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.10772v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shen Zheng, Yiling Ma, Jinqian Pan, Changjie Lu, Gaurav Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a comprehensive survey of low-light image and video
enhancement, addressing two primary challenges in the field. The first
challenge is the prevalence of mixed over-/under-exposed images, which are not
adequately addressed by existing methods. In response, this work introduces two
enhanced variants of the SICE dataset: SICE_Grad and SICE_Mix, designed to
better represent these complexities. The second challenge is the scarcity of
suitable low-light video datasets for training and testing. To address this,
the paper introduces the Night Wenzhou dataset, a large-scale, high-resolution
video collection that features challenging fast-moving aerial scenes and
streetscapes with varied illuminations and degradation. This study also
conducts an extensive analysis of key techniques and performs comparative
experiments using the proposed and current benchmark datasets. The survey
concludes by highlighting emerging applications, discussing unresolved
challenges, and suggesting future research directions within the LLIE
community. The datasets are available at
https://github.com/ShenZheng2000/LLIE_Survey.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 10 tables, and 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Characteristic Guidance: Non-linear Correction for Diffusion Model at
  Large Guidance Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07586v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07586v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Candi Zheng, Yuan Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Popular guidance for denoising diffusion probabilistic model (DDPM) linearly
combines distinct conditional models together to provide enhanced control over
samples. However, this approach overlooks nonlinear effects that become
significant when guidance scale is large. To address this issue, we propose
characteristic guidance, a sampling method that provides first-principle
non-linear correction for classifier-free guided DDPMs. Such correction forces
the guided DDPMs to respect the Fokker-Planck equation of their underlying
diffusion process, in a way that is training-free, derivative-free, and
compatible with existing sampling methods. Experiments show that characteristic
guidance enhances control and reduces color and exposure issues in image
generation, proving effective in diverse applications ranging from latent space
sampling to solving physics problems like magnet phase transitions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distillation is All You Need for Practically Using Different <span class="highlight-title">Pre-train</span>ed
  Recommendation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00797v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00797v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqi Sun, Ruobing Xie, Junjie Zhang, Wayne Xin Zhao, Leyu Lin, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained recommendation models (PRMs) have attracted widespread attention
recently. However, their totally different model structure, huge model size and
computation cost hinder their application in practical recommender systems.
Hence, it is highly essential to explore how to practically utilize PRMs in
real-world recommendations. In this paper, we propose a novel joint knowledge
distillation from different pre-trained recommendation models named PRM-KD for
recommendation, which takes full advantages of diverse PRMs as teacher models
for enhancing student models efficiently. Specifically, PRM-KD jointly distills
diverse informative knowledge from multiple representative PRMs such as
UniSRec, Recformer, and UniM^2Rec. The knowledge from the above PRMs are then
smartly integrated into the student recommendation model considering their
confidence and consistency. We further verify the universality of PRM-KD with
various types of student models, including sequential recommendation, feature
interaction, and graph-based models. Extensive experiments on five real-world
datasets demonstrate the effectiveness and efficacy of PRM-KD, which could be
viewed as an economical shortcut in practically and conveniently making full
use of different PRMs in online systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recent Advances in Text Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Tracy Ke, Pengsheng Ji, Jiashun Jin, Wanshan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text analysis is an interesting research area in data science and has various
applications, such as in artificial intelligence, biomedical research, and
engineering. We review popular methods for text analysis, ranging from topic
modeling to the recent neural language models. In particular, we review
Topic-SCORE, a statistical approach to topic modeling, and discuss how to use
it to analyze MADStat - a dataset on statistical publications that we collected
and cleaned.
  The application of Topic-SCORE and other methods on MADStat leads to
interesting findings. For example, $11$ representative topics in statistics are
identified. For each journal, the evolution of topic weights over time can be
visualized, and these results are used to analyze the trends in statistical
research. In particular, we propose a new statistical model for ranking the
citation impacts of $11$ topics, and we also build a cross-topic citation graph
to illustrate how research results on different topics spread to one another.
  The results on MADStat provide a data-driven picture of the statistical
research in $1975$--$2015$, from a text analysis perspective.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Searching, fast and slow, through product catalogs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dayananda Ubrangala, Juhi Sharma, Sharath Kumar Rangappa, Kiran R, Ravi Prasad Kondapalli, Laurent Boué
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  String matching algorithms in the presence of abbreviations, such as in Stock
Keeping Unit (SKU) product catalogs, remains a relatively unexplored topic. In
this paper, we present a unified architecture for SKU search that provides both
a real-time suggestion system (based on a Trie data structure) as well as a
lower latency search system (making use of character level TF-IDF in
combination with language model vector embeddings) where users initiate the
search process explicitly. We carry out ablation studies that justify designing
a complex search system composed of multiple components to address the delicate
trade-off between speed and accuracy. Using SKU search in the Dynamics CRM as
an example, we show how our system vastly outperforms, in all aspects, the
results provided by the default search engine. Finally, we show how SKU
descriptions may be enhanced via generative text models (using gpt-3.5-turbo)
so that the consumers of the search results may get more context and a
generally better experience when presented with the results of their SKU
search.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ranking In Generalized Linear Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.00109v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.00109v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amitis Shidani, George Deligiannidis, Arnaud Doucet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the ranking problem in generalized linear bandits. At each time, the
learning agent selects an ordered list of items and observes stochastic
outcomes. In recommendation systems, displaying an ordered list of the most
attractive items is not always optimal as both position and item dependencies
result in a complex reward function. A very naive example is the lack of
diversity when all the most attractive items are from the same category. We
model the position and item dependencies in the ordered list and design UCB and
Thompson Sampling type algorithms for this problem. Our work generalizes
existing studies in several directions, including position dependencies where
position discount is a particular case, and connecting the ranking problem to
graph theory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A statistical significance testing approach for measuring term
  burstiness with applications to domain-specific terminology extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15790v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15790v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Sarria Hurtado, Todd Mullen, Taku Onodera, Paul Sheridan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A term in a corpus is said to be ``bursty'' (or overdispersed) when its
occurrences are concentrated in few out of many documents. In this paper, we
propose Residual Inverse Collection Frequency (RICF), a statistical
significance test inspired heuristic for quantifying term burstiness. The
chi-squared test is, to our knowledge, the sole test of statistical
significance among existing term burstiness measures. Chi-squared test term
burstiness scores are computed from the collection frequency statistic (i.e.,
the proportion that a specified term constitutes in relation to all terms
within a corpus). However, the document frequency of a term (i.e., the
proportion of documents within a corpus in which a specific term occurs) is
exploited by certain other widely used term burstiness measures. RICF addresses
this shortcoming of the chi-squared test by virtue of its term burstiness
scores systematically incorporating both the collection frequency and document
frequency statistics. We evaluate the RICF measure on a domain-specific
technical terminology extraction task using the GENIA Term corpus benchmark,
which comprises 2,000 annotated biomedical article abstracts. RICF generally
outperformed the chi-squared test in terms of precision at k score with percent
improvements of 0.00% (P@10), 6.38% (P@50), 6.38% (P@100), 2.27% (P@500), 2.61%
(P@1000), and 1.90% (P@5000). Furthermore, RICF performance was competitive
with the performances of other well-established measures of term burstiness.
Based on these findings, we consider our contributions in this paper as a
promising starting point for future exploration in leveraging statistical
significance testing in text analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 1 figure, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Wikiformer: <span class="highlight-title">Pre-train</span>ing with Structured Information of Wikipedia for
  Ad-hoc Retrieval <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10661v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10661v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihang Su, Qingyao Ai, Xiangsheng Li, Jia Chen, Yiqun Liu, Xiaolong Wu, Shengluan Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of deep learning and natural language processing
techniques, pre-trained language models have been widely used to solve
information retrieval (IR) problems. Benefiting from the pre-training and
fine-tuning paradigm, these models achieve state-of-the-art performance. In
previous works, plain texts in Wikipedia have been widely used in the
pre-training stage. However, the rich structured information in Wikipedia, such
as the titles, abstracts, hierarchical heading (multi-level title) structure,
relationship between articles, references, hyperlink structures, and the
writing organizations, has not been fully explored. In this paper, we devise
four pre-training objectives tailored for IR tasks based on the structured
knowledge of Wikipedia. Compared to existing pre-training methods, our approach
can better capture the semantic knowledge in the training corpus by leveraging
the human-edited structured data from Wikipedia. Experimental results on
multiple IR benchmark datasets show the superior performance of our model in
both zero-shot and fine-tuning settings compared to existing strong retrieval
baselines. Besides, experimental results in biomedical and legal domains
demonstrate that our approach achieves better performance in vertical domains
compared to previous models, especially in scenarios where long text similarity
matching is needed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Thirty-Eighth AAAI Conference on Artificial Intelligence (AAAI-24)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">62</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Downstream Task-Oriented Generative Model Selections on Synthetic Data
  Training for Fraud Detection Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinan Cheng, Chi-Hua Wang, Vamsi K. Potluru, Tucker Balch, Guang Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Devising procedures for downstream task-oriented generative model selections
is an unresolved problem of practical importance. Existing studies focused on
the utility of a single family of generative models. They provided limited
insights on how synthetic data practitioners select the best family generative
models for synthetic training tasks given a specific combination of machine
learning model class and performance metric. In this paper, we approach the
downstream task-oriented generative model selections problem in the case of
training fraud detection models and investigate the best practice given
different combinations of model interpretability and model performance
constraints. Our investigation supports that, while both Neural
Network(NN)-based and Bayesian Network(BN)-based generative models are both
good to complete synthetic training task under loose model interpretability
constrain, the BN-based generative models is better than NN-based when
synthetic training fraud detection model under strict model interpretability
constrain. Our results provides practical guidance for machine learning
practitioner who is interested in replacing their training dataset from real to
synthetic, and shed lights on more general downstream task-oriented generative
model selection problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The following article has been accepted by ICAIF22, Synthetic Data
  for AI in Finance; see
  https://sites.google.com/view/icaif-synthetic-2022/program</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Facebook Report on Privacy of fNIRS data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Imran Hossen, Sai Venkatesh Chilukoti, Liqun Shan, Vijay Srinivas Tida, Xiali Hei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The primary goal of this project is to develop privacy-preserving machine
learning model training techniques for fNIRS data. This project will build a
local model in a centralized setting with both differential privacy (DP) and
certified robustness. It will also explore collaborative federated learning to
train a shared model between multiple clients without sharing local fNIRS
datasets. To prevent unintentional private information leakage of such clients'
private datasets, we will also implement DP in the federated learning setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 5 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Meta-Model for Predicting the Need for Blood Transfusion in
  Non-traumatic ICU Patients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00972v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00972v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Rafiei, Ronald Moore, Tilendra Choudhary, Curtis Marshall, Geoffrey Smith, John D. Roback, Ravi M. Patel, Cassandra D. Josephson, Rishikesan Kamaleswaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective: Blood transfusions, crucial in managing anemia and coagulopathy in
ICU settings, require accurate prediction for effective resource allocation and
patient risk assessment. However, existing clinical decision support systems
have primarily targeted a particular patient demographic with unique medical
conditions and focused on a single type of blood transfusion. This study aims
to develop an advanced machine learning-based model to predict the probability
of transfusion necessity over the next 24 hours for a diverse range of
non-traumatic ICU patients.
  Methods: We conducted a retrospective cohort study on 72,072 adult
non-traumatic ICU patients admitted to a high-volume US metropolitan academic
hospital between 2016 and 2020. We developed a meta-learner and various machine
learning models to serve as predictors, training them annually with four-year
data and evaluating on the fifth, unseen year, iteratively over five years.
  Results: The experimental results revealed that the meta-model surpasses the
other models in different development scenarios. It achieved notable
performance metrics, including an Area Under the Receiver Operating
Characteristic (AUROC) curve of 0.97, an accuracy rate of 0.93, and an F1-score
of 0.89 in the best scenario.
  Conclusion: This study pioneers the use of machine learning models for
predicting blood transfusion needs in a diverse cohort of critically ill
patients. The findings of this evaluation confirm that our model not only
predicts transfusion requirements effectively but also identifies key
biomarkers for making transfusion decisions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improve Fidelity and Utility of Synthetic Credit Card Transaction Time
  Series from Data-centric Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00965v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00965v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Din-Yin Hsieh, Chi-Hua Wang, Guang Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploring generative model training for synthetic tabular data, specifically
in sequential contexts such as credit card transaction data, presents
significant challenges. This paper addresses these challenges, focusing on
attaining both high fidelity to actual data and optimal utility for machine
learning tasks. We introduce five pre-processing schemas to enhance the
training of the Conditional Probabilistic Auto-Regressive Model (CPAR),
demonstrating incremental improvements in the synthetic data's fidelity and
utility. Upon achieving satisfactory fidelity levels, our attention shifts to
training fraud detection models tailored for time-series data, evaluating the
utility of the synthetic data. Our findings offer valuable insights and
practical guidelines for synthetic data practitioners in the finance sector,
transitioning from real to synthetic datasets for training purposes, and
illuminating broader methodologies for synthesizing credit card transaction
time series.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The following article has been accepted by 2nd Workshop on Synthetic
  Data for AI in Finance; see
  https://sites.google.com/view/icaif-synthetic/home</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Augmentation Techniques for Cross-Domain WiFi CSI-based Human
  Activity Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00964v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00964v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Strohmayer, Martin Kampel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recognition of human activities based on WiFi Channel State Information
(CSI) enables contactless and visual privacy-preserving sensing in indoor
environments. However, poor model generalization, due to varying environmental
conditions and sensing hardware, is a well-known problem in this space. To
address this issue, in this work, data augmentation techniques commonly used in
image-based learning are applied to WiFi CSI to investigate their effects on
model generalization performance in cross-scenario and cross-system settings.
In particular, we focus on the generalization between line-of-sight (LOS) and
non-line-of-sight (NLOS) through-wall scenarios, as well as on the
generalization between different antenna systems, which remains under-explored.
We collect and make publicly available a dataset of CSI amplitude spectrograms
of human activities. Utilizing this data, an ablation study is conducted in
which activity recognition models based on the EfficientNetV2 architecture are
trained, allowing us to assess the effects of each augmentation on model
generalization performance. The gathered results show that specific
combinations of simple data augmentation techniques applied to CSI amplitude
data can significantly improve cross-scenario and cross-system generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Model Selection for Tabular Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00961v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00961v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avinash Amballa, Anmol Mekala, Gayathri Akkinapalli, Manas Madine, Naga Pavana Priya Yarrabolu, Przemyslaw A. Grabowicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structured data in the form of tabular datasets contain features that are
distinct and discrete, with varying individual and relative importances to the
target. Combinations of one or more features may be more predictive and
meaningful than simple individual feature contributions. R's mixed effect
linear models library allows users to provide such interactive feature
combinations in the model design. However, given many features and possible
interactions to select from, model selection becomes an exponentially difficult
task. We aim to automate the model selection process for predictions on tabular
datasets incorporating feature interactions while keeping computational costs
small. The framework includes two distinct approaches for feature selection: a
Priority-based Random Grid Search and a Greedy Search method. The
Priority-based approach efficiently explores feature combinations using prior
probabilities to guide the search. The Greedy method builds the solution
iteratively by adding or removing features based on their impact. Experiments
on synthetic demonstrate the ability to effectively capture predictive feature
combinations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Families of costs with zero and nonnegative MTW tensor in optimal
  transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00953v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00953v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Du Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We compute explicitly the MTW tensor (or cross curvature) for the optimal
transport problem on $\mathbb{R}^n$ with a cost function of form $\mathsf{c}(x,
y) = \mathsf{u}(x^{\mathfrak{t}}y)$, where $\mathsf{u}$ is a scalar function
with inverse $\mathsf{s}$, $x^{\ft}y$ is a nondegenerate bilinear pairing of
vectors $x, y$ belonging to an open subset of $\mathbb{R}^n$. The condition
that the MTW-tensor vanishes on null vectors under the Kim-McCann metric is a
fourth-order nonlinear ODE, which could be reduced to a linear ODE of the form
$\mathsf{s}^{(2)} - S\mathsf{s}^{(1)} + P\mathsf{s} = 0$ with constant
coefficients $P$ and $S$. The resulting inverse functions include {\it Lambert}
and {\it generalized inverse hyperbolic\slash trigonometric} functions. The
square Euclidean metric and $\log$-type costs are equivalent to instances of
these solutions. The optimal map for the family is also explicit. For cost
functions of a similar form on a hyperboloid model of the hyperbolic space and
unit sphere, we also express this tensor in terms of algebraic expressions in
derivatives of $\mathsf{s}$ using the Gauss-Codazzi equation, obtaining new
families of strictly regular costs for these manifolds, including new families
of {\it power function costs}. We analyze the $\sinh$-type hyperbolic cost,
providing examples of $\mathsf{c}$-convex functions and divergence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Lattice Sampling of Quantum Field Theories via Neural Operators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00828v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00828v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bálint Máté, François Fleuret
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of sampling discrete field configurations $\phi$ from
the Boltzmann distribution $[d\phi] Z^{-1} e^{-S[\phi]}$, where $S$ is the
lattice-discretization of the continuous Euclidean action $\mathcal S$ of some
quantum field theory. Since such densities arise as the approximation of the
underlying functional density $[\mathcal D\phi(x)] \mathcal Z^{-1} e^{-\mathcal
S[\phi(x)]}$, we frame the task as an instance of operator learning. In
particular, we propose to approximate a time-dependent operator $\mathcal V_t$
whose time integral provides a mapping between the functional distributions of
the free theory $[\mathcal D\phi(x)] \mathcal Z_0^{-1} e^{-\mathcal
S_{0}[\phi(x)]}$ and of the target theory $[\mathcal D\phi(x)]\mathcal
Z^{-1}e^{-\mathcal S[\phi(x)]}$. Whenever a particular lattice is chosen, the
operator $\mathcal V_t$ can be discretized to a finite dimensional,
time-dependent vector field $V_t$ which in turn induces a continuous
normalizing flow between finite dimensional distributions over the chosen
lattice. This flow can then be trained to be a diffeormorphism between the
discretized free and target theories $[d\phi] Z_0^{-1} e^{-S_{0}[\phi]}$,
$[d\phi] Z^{-1}e^{-S[\phi]}$. We run experiments on the $\phi^4$-theory to
explore to what extent such operator-based flow architectures generalize to
lattice sizes they were not trained on and show that pretraining on smaller
lattices can lead to speedup over training only a target lattice size.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph-Convolutional Autoencoder Ensembles for the Humanities,
  Illustrated with a Study of the American Slave Trade 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00824v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00824v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Lippincott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a graph-aware autoencoder ensemble framework, with associated
formalisms and tooling, designed to facilitate deep learning for scholarship in
the humanities. By composing sub-architectures to produce a model isomorphic to
a humanistic domain we maintain interpretability while providing function
signatures for each sub-architectural choice, allowing both traditional and
computational researchers to collaborate without disrupting established
practices. We illustrate a practical application of our approach to a
historical study of the American post-Atlantic slave trade, and make several
specific technical contributions: a novel hybrid graph-convolutional
autoencoder mechanism, batching policies for common graph topologies, and
masking techniques for particular use-cases. The effectiveness of the framework
for broadening participation of diverse domains is demonstrated by a growing
suite of two dozen studies, both collaborations with humanists and established
tasks from machine learning literature, spanning a variety of fields and data
modalities. We make performance comparisons of several different architectural
choices and conclude with an ambitious list of imminent next steps for this
research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>More in-depth technical companion to "A general neural ensemble
  technique to support traditional scholarship", Digital Humanities 2020</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GLIMPSE: Generalized Local Imaging with MLPs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00816v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00816v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        AmirEhsan Khorashadizadeh, Valentin Debarnot, Tianlin Liu, Ivan Dokmanić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning is the current de facto state of the art in tomographic
imaging. A common approach is to feed the result of a simple inversion, for
example the backprojection, to a convolutional neural network (CNN) which then
computes the reconstruction. Despite strong results on 'in-distribution' test
data similar to the training data, backprojection from sparse-view data
delocalizes singularities, so these approaches require a large receptive field
to perform well. As a consequence, they overfit to certain global structures
which leads to poor generalization on out-of-distribution (OOD) samples.
Moreover, their memory complexity and training time scale unfavorably with
image resolution, making them impractical for application at realistic clinical
resolutions, especially in 3D: a standard U-Net requires a substantial 140GB of
memory and 2600 seconds per epoch on a research-grade GPU when training on
1024x1024 images. In this paper, we introduce GLIMPSE, a local processing
neural network for computed tomography which reconstructs a pixel value by
feeding only the measurements associated with the neighborhood of the pixel to
a simple MLP. While achieving comparable or better performance with successful
CNNs like the U-Net on in-distribution test data, GLIMPSE significantly
outperforms them on OOD samples while maintaining a memory footprint almost
independent of image resolution; 5GB memory suffices to train on 1024x1024
images. Further, we built GLIMPSE to be fully differentiable, which enables
feats such as recovery of accurate projection angles if they are out of
calibration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">review</span> on different techniques used to combat the non-IID and
  heterogeneous nature of data in FL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00809v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00809v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Venkataraman Natarajan Iyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is a machine-learning approach enabling collaborative
model training across multiple decentralized edge devices that hold local data
samples, all without exchanging these samples. This collaborative process
occurs under the supervision of a central server orchestrating the training or
via a peer-to-peer network. The significance of FL is particularly pronounced
in industries such as healthcare and finance, where data privacy holds
paramount importance. However, training a model under the Federated learning
setting brings forth several challenges, with one of the most prominent being
the heterogeneity of data distribution among the edge devices. The data is
typically non-independently and non-identically distributed (non-IID), thereby
presenting challenges to model convergence. This report delves into the issues
arising from non-IID and heterogeneous data and explores current algorithms
designed to address these challenges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00793v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00793v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinglong Luo, Yehong Zhang, Jiaqi Zhang, Xin Mu, Hui Wang, Yue Yu, Zenglin Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growing use of large language models hosted on cloud platforms to
offer inference services, privacy concerns are escalating, especially
concerning sensitive data like investment plans and bank account details.
Secure Multi-Party Computing (SMPC) emerges as a promising solution to protect
the privacy of inference data and model parameters. However, the application of
SMPC in Privacy-Preserving Inference (PPI) for large language models,
particularly those based on the Transformer architecture, often leads to
considerable slowdowns or declines in performance. This is largely due to the
multitude of nonlinear operations in the Transformer architecture, which are
not well-suited to SMPC and are difficult to circumvent or optimize
effectively. To address this concern, we introduce an advanced optimization
framework called SecFormer, designed to strike an optimal balance between
performance and efficiency in PPI for Transformer models. By implementing
knowledge distillation techniques, we successfully eliminate the high-cost
exponential and maximum operations in PPI without sacrificing model
performance. Additionally, we have developed a suite of efficient SMPC
protocols that utilize segmented polynomials and Goldschmidt's method to handle
other complex nonlinear functions within PPI, such as GeLU, LayerNorm, and
Softmax. Our extensive experiments reveal that SecFormer outperforms MPCFormer
in performance, showing improvements of $5.6\%$ and $24.2\%$ for
BERT$_{\text{BASE}}$ and BERT$_{\text{LARGE}}$, respectively. In terms of
efficiency, SecFormer is 3.4 and 3.2 times faster than Puma, demonstrating its
effectiveness and speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12pages, 15figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inferring Heterogeneous Treatment Effects of Crashes on Highway Traffic:
  A Doubly Robust Causal Machine Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00781v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00781v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuang Li, Ziyuan Pu, Zhiyong Cui, Seunghyeon Lee, Xiucheng Guo, Dong Ngoduy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Highway traffic crashes exert a considerable impact on both transportation
systems and the economy. In this context, accurate and dependable emergency
responses are crucial for effective traffic management. However, the influence
of crashes on traffic status varies across diverse factors and may be biased
due to selection bias. Therefore, there arises a necessity to accurately
estimate the heterogeneous causal effects of crashes, thereby providing
essential insights to facilitate individual-level emergency decision-making.
This paper proposes a novel causal machine learning framework to estimate the
causal effect of different types of crashes on highway speed. The Neyman-Rubin
Causal Model (RCM) is employed to formulate this problem from a causal
perspective. The Conditional Shapley Value Index (CSVI) is proposed based on
causal graph theory to filter adverse variables, and the Structural Causal
Model (SCM) is then adopted to define the statistical estimand for causal
effects. The treatment effects are estimated by Doubly Robust Learning (DRL)
methods, which combine doubly robust causal inference with classification and
regression machine learning models. Experimental results from 4815 crashes on
Highway Interstate 5 in Washington State reveal the heterogeneous treatment
effects of crashes at varying distances and durations. The rear-end crashes
cause more severe congestion and longer durations than other types of crashes,
and the sideswipe crashes have the longest delayed impact. Additionally, the
findings show that rear-end crashes affect traffic greater at night, while
crash to objects has the most significant influence during peak hours.
Statistical hypothesis tests, error metrics based on matched "counterfactual
outcomes", and sensitive analyses are employed for assessment, and the results
validate the accuracy and effectiveness of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 13 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Edge Computing based Human-Robot Cognitive Fusion: A Medical Case Study
  in the Autism Spectrum Disorder Therapy <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00776v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00776v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, edge computing has served as a paradigm that enables many
future technologies like AI, Robotics, IoT, and high-speed wireless sensor
networks (like 5G) by connecting cloud computing facilities and services to the
end users. Especially in medical and healthcare applications, it provides
remote patient monitoring and increases voluminous multimedia. From the
robotics angle, robot-assisted therapy (RAT) is an active-assistive robotic
technology in rehabilitation robotics, attracting many researchers to study and
benefit people with disability like autism spectrum disorder (ASD) children.
However, the main challenge of RAT is that the model capable of detecting the
affective states of ASD people exists and can recall individual preferences.
Moreover, involving expert diagnosis and recommendations to guide robots in
updating the therapy approach to adapt to different statuses and scenarios is a
crucial part of the ASD therapy process. This paper proposes the architecture
of edge cognitive computing by combining human experts and assisted robots
collaborating in the same framework to help ASD patients with long-term
support. By integrating the real-time computing and analysis of a new cognitive
robotic model for ASD therapy, the proposed architecture can achieve a seamless
remote diagnosis, round-the-clock symptom monitoring, emergency warning,
therapy alteration, and advanced assistance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was accepted by the 38th AAAI 2024 workshop: "Cooperative
  Multi-Agent Systems Decision-Making and Learning: From Individual Needs to
  Swarm Intelligence"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Outlier Detection using Random Subspace and Subsampling
  Ensembles of Dirichlet Process Mixtures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00773v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00773v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongwook Kim, Juyeon Park, Hee Cheol Chung, Seonghyun Jeong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Probabilistic mixture models are acknowledged as a valuable tool for
unsupervised outlier detection owing to their interpretability and intuitive
grounding in statistical principles. Within this framework, Dirichlet process
mixture models emerge as a compelling alternative to conventional finite
mixture models for both clustering and outlier detection tasks. However,
despite their evident advantages, the widespread adoption of Dirichlet process
mixture models in unsupervised outlier detection has been hampered by
challenges related to computational inefficiency and sensitivity to outliers
during the construction of detectors. To tackle these challenges, we propose a
novel outlier detection method based on ensembles of Dirichlet process Gaussian
mixtures. The proposed method is a fully unsupervised algorithm that
capitalizes on random subspace and subsampling ensembles, not only ensuring
efficient computation but also enhancing the robustness of the resulting
outlier detector. Moreover, the proposed method leverages variational inference
for Dirichlet process mixtures to ensure efficient and fast computation.
Empirical studies with benchmark datasets demonstrate that our method
outperforms existing approaches for unsupervised outlier detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MPRE: Multi-perspective Patient Representation Extractor for Disease
  Prediction <span class="chip">ICDM 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyue Yu, Jiayi Wang, Wuman Luo, Rita Tse, Giovanni Pau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Patient representation learning based on electronic health records (EHR) is a
critical task for disease prediction. This task aims to effectively extract
useful information on dynamic features. Although various existing works have
achieved remarkable progress, the model performance can be further improved by
fully extracting the trends, variations, and the correlation between the trends
and variations in dynamic features. In addition, sparse visit records limit the
performance of deep learning models. To address these issues, we propose the
Multi-perspective Patient Representation Extractor (MPRE) for disease
prediction. Specifically, we propose Frequency Transformation Module (FTM) to
extract the trend and variation information of dynamic features in the
time-frequency domain, which can enhance the feature representation. In the 2D
Multi-Extraction Network (2D MEN), we form the 2D temporal tensor based on
trend and variation. Then, the correlations between trend and variation are
captured by the proposed dilated operation. Moreover, we propose the
First-Order Difference Attention Mechanism (FODAM) to calculate the
contributions of differences in adjacent variations to the disease diagnosis
adaptively. To evaluate the performance of MPRE and baseline methods, we
conduct extensive experiments on two real-world public datasets. The experiment
results show that MPRE outperforms state-of-the-art baseline methods in terms
of AUROC and AUPRC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICDM 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Saliency-Aware Regularized Graph Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjie Pei, Weina Xu, Zongze Wu, Weichao Li, Jinfan Wang, Guangming Lu, Xiangrong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The crux of graph classification lies in the effective representation
learning for the entire graph. Typical graph neural networks focus on modeling
the local dependencies when aggregating features of neighboring nodes, and
obtain the representation for the entire graph by aggregating node features.
Such methods have two potential limitations: 1) the global node saliency w.r.t.
graph classification is not explicitly modeled, which is crucial since
different nodes may have different semantic relevance to graph classification;
2) the graph representation directly aggregated from node features may have
limited effectiveness to reflect graph-level information. In this work, we
propose the Saliency-Aware Regularized Graph Neural Network (SAR-GNN) for graph
classification, which consists of two core modules: 1) a traditional graph
neural network serving as the backbone for learning node features and 2) the
Graph Neural Memory designed to distill a compact graph representation from
node features of the backbone. We first estimate the global node saliency by
measuring the semantic similarity between the compact graph representation and
node features. Then the learned saliency distribution is leveraged to
regularize the neighborhood aggregation of the backbone, which facilitates the
message passing of features for salient nodes and suppresses the less relevant
nodes. Thus, our model can learn more effective graph representation. We
demonstrate the merits of SAR-GNN by extensive experiments on seven datasets
across various types of graph data. Code will be released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Artificial Intelligence Journal with minor revision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Searching, fast and slow, through product catalogs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dayananda Ubrangala, Juhi Sharma, Sharath Kumar Rangappa, Kiran R, Ravi Prasad Kondapalli, Laurent Boué
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  String matching algorithms in the presence of abbreviations, such as in Stock
Keeping Unit (SKU) product catalogs, remains a relatively unexplored topic. In
this paper, we present a unified architecture for SKU search that provides both
a real-time suggestion system (based on a Trie data structure) as well as a
lower latency search system (making use of character level TF-IDF in
combination with language model vector embeddings) where users initiate the
search process explicitly. We carry out ablation studies that justify designing
a complex search system composed of multiple components to address the delicate
trade-off between speed and accuracy. Using SKU search in the Dynamics CRM as
an example, we show how our system vastly outperforms, in all aspects, the
results provided by the default search engine. Finally, we show how SKU
descriptions may be enhanced via generative text models (using gpt-3.5-turbo)
so that the consumers of the search results may get more context and a
generally better experience when presented with the results of their SKU
search.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion Models, Image Super-Resolution And Everything: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian B. Moser, Arundhati S. Shanbhag, Federico Raue, Stanislav Frolov, Sebastian Palacio, Andreas Dengel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion Models (DMs) represent a significant advancement in image
Super-Resolution (SR), aligning technical image quality more closely with human
preferences and expanding SR applications. DMs address critical limitations of
previous methods, enhancing overall realism and details in SR images. However,
DMs suffer from color-shifting issues, and their high computational costs call
for efficient sampling alternatives, underscoring the challenge of balancing
computational efficiency and image quality. This survey gives an overview of
DMs applied to image SR and offers a detailed analysis that underscores the
unique characteristics and methodologies within this domain, distinct from
broader existing reviews in the field. It presents a unified view of DM
fundamentals and explores research directions, including alternative input
domains, conditioning strategies, guidance, corruption spaces, and zero-shot
methods. This survey provides insights into the evolution of image SR with DMs,
addressing current trends, challenges, and future directions in this rapidly
evolving field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MultiFusionNet: Multilayer Multimodal Fusion of Deep Neural Networks for
  Chest X-Ray Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saurabh Agarwal, K. V. Arya, Yogesh Kumar Meena
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chest X-ray imaging is a critical diagnostic tool for identifying pulmonary
diseases. However, manual interpretation of these images is time-consuming and
error-prone. Automated systems utilizing convolutional neural networks (CNNs)
have shown promise in improving the accuracy and efficiency of chest X-ray
image classification. While previous work has mainly focused on using feature
maps from the final convolution layer, there is a need to explore the benefits
of leveraging additional layers for improved disease classification. Extracting
robust features from limited medical image datasets remains a critical
challenge. In this paper, we propose a novel deep learning-based multilayer
multimodal fusion model that emphasizes extracting features from different
layers and fusing them. Our disease detection model considers the
discriminatory information captured by each layer. Furthermore, we propose the
fusion of different-sized feature maps (FDSFM) module to effectively merge
feature maps from diverse layers. The proposed model achieves a significantly
higher accuracy of 97.21% and 99.60% for both three-class and two-class
classifications, respectively. The proposed multilayer multimodal fusion model,
along with the FDSFM module, holds promise for accurate disease classification
and can also be extended to other disease classifications in chest X-ray
images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An attempt to generate new bridge types from latent space of generative
  adversarial network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00700v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00700v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongjun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Try to generate new bridge types using generative artificial intelligence
technology. Symmetric structured image dataset of three-span beam bridge, arch
bridge, cable-stayed bridge and suspension bridge are used . Based on Python
programming language, TensorFlow and Keras deep learning platform framework ,
as well as Wasserstein loss function and Lipschitz constraints, generative
adversarial network is constructed and trained. From the obtained low
dimensional bridge-type latent space sampling, new bridge types with asymmetric
structures can be generated. Generative adversarial network can create new
bridge types by organically combining different structural components on the
basis of human original bridge types. It has a certain degree of human original
ability. Generative artificial intelligence technology can open up imagination
space and inspire humanity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models aren't all that you need 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00698v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00698v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kiran Voderhobli Holla, Chaithanya Kumar, Aryan Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes the architecture and systems built towards solving the
SemEval 2023 Task 2: MultiCoNER II (Multilingual Complex Named Entity
Recognition) [1]. We evaluate two approaches (a) a traditional Conditional
Random Fields model and (b) a Large Language Model (LLM) fine-tuned with a
customized head and compare the two approaches. The novel ideas explored are:
1) Decaying auxiliary loss (with residual) - where we train the model on an
auxiliary task of Coarse-Grained NER and include this task as a part of the
loss function 2) Triplet token blending - where we explore ways of blending the
embeddings of neighboring tokens in the final NER layer prior to prediction 3)
Task-optimal heads - where we explore a variety of custom heads and learning
rates for the final layer of the LLM. We also explore multiple LLMs including
GPT-3 and experiment with a variety of dropout and other hyperparameter
settings before arriving at our final model which achieves micro & macro f1 of
0.85/0.84 (on dev) and 0.67/0.61 on the test data . We show that while
pre-trained LLMs, by themselves, bring about a large improvement in scores as
compared to traditional models, we also demonstrate that tangible improvements
to the Macro-F1 score can be made by augmenting the LLM with additional
feature/loss/model engineering techniques described above.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-supervised</span> learning for skin cancer diagnosis with limited training
  data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamish Haggerty, Rohitash Chandra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cancer diagnosis is a well-studied problem in machine learning since early
detection of cancer is often the determining factor in prognosis. Supervised
deep learning achieves excellent results in cancer image classification,
usually through transfer learning. However, these models require large amounts
of labelled data and for several types of cancer, large labelled datasets do
not exist. In this paper, we demonstrate that a model pre-trained using a
self-supervised learning algorithm known as Barlow Twins can outperform the
conventional supervised transfer learning pipeline. We juxtapose two base
models: i) pretrained in a supervised fashion on ImageNet; ii) pretrained in a
self-supervised fashion on ImageNet. Both are subsequently fine tuned on a
small labelled skin lesion dataset and evaluated on a large test set. We
achieve a mean test accuracy of 70\% for self-supervised transfer in comparison
to 66\% for supervised transfer. Interestingly, boosting performance further is
possible by self-supervised pretraining a second time (on unlabelled skin
lesion images) before subsequent fine tuning. This hints at an alternative path
to collecting more labelled data in settings where this is challenging - namely
just collecting more unlabelled images. Our framework is applicable to cancer
image classification models in the low-labelled data regime.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stochastic Gradient Descent for Additive Nonparametric Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00691v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00691v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Chen, Jason M. Klusowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces an iterative algorithm designed to train additive
models with favorable memory storage and computational requirements. The
algorithm can be viewed as the functional counterpart of stochastic gradient
descent, applied to the coefficients of a truncated basis expansion of the
component functions. We show that the resulting estimator satisfies an oracle
inequality that allows for model mispecification. In the well-specified
setting, by choosing the learning rate carefully across three distinct stages
of training, we prove that its risk is minimax optimal in terms of the
dependence on the dimensionality of the data and the size of the training
sample.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inferring community structure in attributed hypergraphs using stochastic
  block models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00688v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00688v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazuki Nakajima, Takeaki Uno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hypergraphs are a representation of complex systems involving interactions
among more than two entities and allow to investigation of higher-order
structure and dynamics in real-world complex systems. Community structure is a
common property observed in empirical networks in various domains. Stochastic
block models have been employed to investigate community structure in networks.
Node attribute data, often accompanying network data, has been found to
potentially enhance the learning of community structure in dyadic networks. In
this study, we develop a statistical framework that incorporates node attribute
data into the learning of community structure in a hypergraph, employing a
stochastic block model. We demonstrate that our model, which we refer to as
HyperNEO, enhances the learning of community structure in synthetic and
empirical hypergraphs when node attributes are sufficiently associated with the
communities. Furthermore, we found that applying a dimensionality reduction
method, UMAP, to the learned representations obtained using stochastic block
models, including our model, maps nodes into a two-dimensional vector space
while largely preserving community structure in empirical hypergraphs. We
expect that our framework will broaden the investigation and understanding of
higher-order community structure in real-world complex systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 11 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Communication-Efficient Federated Learning for LEO Constellations
  Integrated with HAPs Using Hybrid NOMA-OFDM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00685v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00685v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Elmahallawy, Tie Luo, Khaled Ramadan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Space AI has become increasingly important and sometimes even necessary for
government, businesses, and society. An active research topic under this
mission is integrating federated learning (FL) with satellite communications
(SatCom) so that numerous low Earth orbit (LEO) satellites can collaboratively
train a machine learning model. However, the special communication environment
of SatCom leads to a very slow FL training process up to days and weeks. This
paper proposes NomaFedHAP, a novel FL-SatCom approach tailored to LEO
satellites, that (1) utilizes high-altitude platforms (HAPs) as distributed
parameter servers (PS) to enhance satellite visibility, and (2) introduces
non-orthogonal multiple access (NOMA) into LEO to enable fast and
bandwidth-efficient model transmissions. In addition, NomaFedHAP includes (3) a
new communication topology that exploits HAPs to bridge satellites among
different orbits to mitigate the Doppler shift, and (4) a new FL model
aggregation scheme that optimally balances models between different orbits and
shells. Moreover, we (5) derive a closed-form expression of the outage
probability for satellites in near and far shells, as well as for the entire
system. Our extensive simulations have validated the mathematical analysis and
demonstrated the superior performance of NomaFedHAP in achieving fast and
efficient FL model convergence with high accuracy as compared to the
state-of-the-art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Temporal Filter to Extract Doped Conducting Polymer Information
  Features from an Electronic Nose 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00684v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00684v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wiem Haj Ammar, Aicha Boujnah, Antoine Baron, Aimen Boubaker, Adel Kalboussi, Kamal Lmimouni, Sebastien Pecqueur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying relevant machine-learning features for multi-sensing platforms is
both an applicative limitation to recognize environments and a necessity to
interpret the physical relevance of transducers' complementarity in their
information processing. Particularly for long acquisitions, feature extraction
must be fully automatized without human intervention and resilient to
perturbations without increasing significantly the computational cost of a
classifier. In this study, we investigate on the relative resistance and
current modulation of a 24-dimensional conductimetric electronic nose, which
uses the exponential moving average as a floating reference in a low-cost
information descriptor for environment recognition. In particular, we
identified that depending on the structure of a linear classifier, the 'modema'
descriptor is optimized for different material sensing elements' contributions
to classify information patterns. The low-pass filtering optimization leads to
opposite behaviors between unsupervised and supervised learning: the latter one
favors longer integration of the reference, allowing to recognize five
different classes over 90%, while the first one prefers using the latest events
as its reference to clusterize patterns by environment nature. Its electronic
implementation shall greatly diminish the computational requirements of
conductimetric electronic noses for on-board environment recognition without
human supervision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Assimilation in Chaotic Systems Using Deep Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamad Abed El Rahman Hammoud, Naila Raboudi, Edriss S. Titi, Omar Knio, Ibrahim Hoteit
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data assimilation (DA) plays a pivotal role in diverse applications, ranging
from climate predictions and weather forecasts to trajectory planning for
autonomous vehicles. A prime example is the widely used ensemble Kalman filter
(EnKF), which relies on linear updates to minimize variance among the ensemble
of forecast states. Recent advancements have seen the emergence of deep
learning approaches in this domain, primarily within a supervised learning
framework. However, the adaptability of such models to untrained scenarios
remains a challenge. In this study, we introduce a novel DA strategy that
utilizes reinforcement learning (RL) to apply state corrections using full or
partial observations of the state variables. Our investigation focuses on
demonstrating this approach to the chaotic Lorenz '63 system, where the agent's
objective is to minimize the root-mean-squared error between the observations
and corresponding forecast states. Consequently, the agent develops a
correction strategy, enhancing model forecasts based on available system state
observations. Our strategy employs a stochastic action policy, enabling a Monte
Carlo-based DA framework that relies on randomly sampling the policy to
generate an ensemble of assimilated realizations. Results demonstrate that the
developed RL algorithm performs favorably when compared to the EnKF.
Additionally, we illustrate the agent's capability to assimilate non-Gaussian
data, addressing a significant limitation of the EnKF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ General-purpose foundation models for increased autonomy in
  robot-assisted surgery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Schmidgall, Ji Woong Kim, Alan Kuntz, Ahmed Ezzat Ghazi, Axel Krieger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The dominant paradigm for end-to-end robot learning focuses on optimizing
task-specific objectives that solve a single robotic problem such as picking up
an object or reaching a target position. However, recent work on high-capacity
models in robotics has shown promise toward being trained on large collections
of diverse and task-agnostic datasets of video demonstrations. These models
have shown impressive levels of generalization to unseen circumstances,
especially as the amount of data and the model complexity scale. Surgical robot
systems that learn from data have struggled to advance as quickly as other
fields of robot learning for a few reasons: (1) there is a lack of existing
large-scale open-source data to train models, (2) it is challenging to model
the soft-body deformations that these robots work with during surgery because
simulation cannot match the physical and visual complexity of biological
tissue, and (3) surgical robots risk harming patients when tested in clinical
trials and require more extensive safety measures. This perspective article
aims to provide a path toward increasing robot autonomy in robot-assisted
surgery through the development of a multi-modal, multi-task,
vision-language-action model for surgical robots. Ultimately, we argue that
surgical robots are uniquely positioned to benefit from general-purpose models
and provide three guiding actions toward increased autonomy in robot-assisted
surgery.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Digger: Detecting Copyright Content Mis-usage in Large Language Model
  Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00676v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00676v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haodong Li, Gelei Deng, Yi Liu, Kailong Wang, Yuekang Li, Tianwei Zhang, Yang Liu, Guoai Xu, Guosheng Xu, Haoyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training, which utilizes extensive and varied datasets, is a critical
factor in the success of Large Language Models (LLMs) across numerous
applications. However, the detailed makeup of these datasets is often not
disclosed, leading to concerns about data security and potential misuse. This
is particularly relevant when copyrighted material, still under legal
protection, is used inappropriately, either intentionally or unintentionally,
infringing on the rights of the authors.
  In this paper, we introduce a detailed framework designed to detect and
assess the presence of content from potentially copyrighted books within the
training datasets of LLMs. This framework also provides a confidence estimation
for the likelihood of each content sample's inclusion. To validate our
approach, we conduct a series of simulated experiments, the results of which
affirm the framework's effectiveness in identifying and addressing instances of
content misuse in LLM training processes. Furthermore, we investigate the
presence of recognizable quotes from famous literary works within these
datasets. The outcomes of our study have significant implications for ensuring
the ethical use of copyrighted materials in the development of LLMs,
highlighting the need for more transparent and responsible data management
practices in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ New Sample Complexity Bounds for (Regularized) Sample Average
  Approximation in Several Heavy-Tailed, Non-Lipschitzian, and High-Dimensional
  Cases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongcheng Liu, Jindong Tong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the sample complexity of sample average approximation (SAA) and its
simple variations, referred to as the regularized SAA (RSAA), in solving convex
and strongly convex stochastic programming (SP) problems under
heavy-tailed-ness, non-Lipschitz-ness, and/or high dimensionality. The presence
of such irregularities underscores critical vacua in the literature. In
response, this paper presents three sets of results: First, we show that the
(R)SAA is effective even if the objective function is not necessarily Lipschitz
and the underlying distribution admits some bounded central moments only at
(near-)optimal solutions. Second, when the SP's objective function is the sum
of a smooth term and a Lipschitz term, we prove that the (R)SAA's sample
complexity is completely independent from any complexity measures (e.g., the
covering number) of the feasible region. Third, we explicate the (R)SAA's
sample complexities with regard to the dependence on dimensionality $d$: When
some $p$th ($p\geq 2$) central moment of the underlying distribution is
bounded, we show that the required sample size grows at a rate no worse than
$\mathcal O\left(p d^{2/p}\right)$ under any one of the three structural
assumptions: (i) strong convexity w.r.t. the $q$-norm ($q\geq 1$); (ii) the
combination of restricted strong convexity and sparsity; and (iii) a
dimension-insensitive $q$-norm of an optimal solution. In both cases of (i) and
(iii), it is further required that $p\leq q/(q-1)$. As a direct implication,
the (R)SAA's complexity becomes (poly-)logarithmic in $d$, whenever $p\geq
c\cdot \ln d$ is admissible for some constant $c>0$. These new results deviate
from the SAA's typical sample complexities that grow polynomially with $d$.
Part of our proof is based on the average-replace-one (RO) stability, which
appears to be novel for the (R)SAA's analyses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Point Cloud in the Air 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00658v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00658v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulin Shao, Chenghong Bian, Li Yang, Qianqian Yang, Zhaoyang Zhang, Deniz Gunduz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Acquisition and processing of point clouds (PCs) is a crucial enabler for
many emerging applications reliant on 3D spatial data, such as robot
navigation, autonomous vehicles, and augmented reality. In most scenarios, PCs
acquired by remote sensors must be transmitted to an edge server for fusion,
segmentation, or inference. Wireless transmission of PCs not only puts on
increased burden on the already congested wireless spectrum, but also confronts
a unique set of challenges arising from the irregular and unstructured nature
of PCs. In this paper, we meticulously delineate these challenges and offer a
comprehensive examination of existing solutions while candidly acknowledging
their inherent limitations. In response to these intricacies, we proffer four
pragmatic solution frameworks, spanning advanced techniques, hybrid schemes,
and distributed data aggregation approaches. In doing so, our goal is to chart
a path toward efficient, reliable, and low-latency wireless PC transmission.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Discprecncies between Perturbation Evaluations of Graph Neural
  Network Attributions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Razieh Rezaei, Alireza Dizaji, Ashkan Khakzar, Anees Kazi, Nassir Navab, Daniel Rueckert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks are increasingly finding their way into the realm of graphs
and modeling relationships between features. Concurrently graph neural network
explanation approaches are being invented to uncover relationships between the
nodes of the graphs. However, there is a disparity between the existing
attribution methods, and it is unclear which attribution to trust. Therefore
research has introduced evaluation experiments that assess them from different
perspectives. In this work, we assess attribution methods from a perspective
not previously explored in the graph domain: retraining. The core idea is to
retrain the network on important (or not important) relationships as identified
by the attributions and evaluate how networks can generalize based on these
relationships. We reformulate the retraining framework to sidestep issues
lurking in the previous formulation and propose guidelines for correct
analysis. We run our analysis on four state-of-the-art GNN attribution methods
and five synthetic and real-world graph classification datasets. The analysis
reveals that attributions perform variably depending on the dataset and the
network. Most importantly, we observe that the famous GNNExplainer performs
similarly to an arbitrary designation of edge importance. The study concludes
that the retraining evaluation cannot be used as a generalized benchmark and
recommends it as a toolset to evaluate attributions on a specifically addressed
network, dataset, and sparsity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarially Trained Actor Critic for offline CMDPs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Honghao Wei, Xiyue Peng, Xin Liu, Arnob Ghosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a Safe Adversarial Trained Actor Critic (SATAC) algorithm for
offline reinforcement learning (RL) with general function approximation in the
presence of limited data coverage. SATAC operates as a two-player Stackelberg
game featuring a refined objective function. The actor (leader player)
optimizes the policy against two adversarially trained value critics (follower
players), who focus on scenarios where the actor's performance is inferior to
the behavior policy. Our framework provides both theoretical guarantees and a
robust deep-RL implementation. Theoretically, we demonstrate that when the
actor employs a no-regret optimization oracle, SATAC achieves two guarantees:
(i) For the first time in the offline RL setting, we establish that SATAC can
produce a policy that outperforms the behavior policy while maintaining the
same level of safety, which is critical to designing an algorithm for offline
RL. (ii) We demonstrate that the algorithm guarantees policy improvement across
a broad range of hyperparameters, indicating its practical robustness.
Additionally, we offer a practical version of SATAC and compare it with
existing state-of-the-art offline safe-RL algorithms in continuous control
environments. SATAC outperforms all baselines across a range of tasks, thus
validating the theoretical performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Efficiency: A Systematic <span class="highlight-title">Survey</span> of Resource-Efficient Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00625v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00625v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangji Bai, Zheng Chai, Chen Ling, Shiyu Wang, Jiaying Lu, Nan Zhang, Tingwei Shi, Ziyang Yu, Mengdan Zhu, Yifei Zhang, Carl Yang, Yue Cheng, Liang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The burgeoning field of Large Language Models (LLMs), exemplified by
sophisticated models like OpenAI's ChatGPT, represents a significant
advancement in artificial intelligence. These models, however, bring forth
substantial challenges in the high consumption of computational, memory,
energy, and financial resources, especially in environments with limited
resource capabilities. This survey aims to systematically address these
challenges by reviewing a broad spectrum of techniques designed to enhance the
resource efficiency of LLMs. We categorize methods based on their optimization
focus: computational, memory, energy, financial, and network resources and
their applicability across various stages of an LLM's lifecycle, including
architecture design, pretraining, finetuning, and system design. Additionally,
the survey introduces a nuanced categorization of resource efficiency
techniques by their specific resource types, which uncovers the intricate
relationships and mappings between various resources and corresponding
optimization techniques. A standardized set of evaluation metrics and datasets
is also presented to facilitate consistent and fair comparisons across
different models and techniques. By offering a comprehensive overview of the
current sota and identifying open research avenues, this survey serves as a
foundational reference for researchers and practitioners, aiding them in
developing more sustainable and efficient LLMs in a rapidly evolving landscape.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. GitHub repo:
  https://github.com/tiingweii-shii/Awesome-Resource-Efficient-LLM-Papers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Class-Incremental Learning with New-Class Augmented
  Self-Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00622v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00622v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Wu, Tianliu He, Sheng Sun, Yuwei Wang, Min Liu, Bo Gao, Xuefeng Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) enables collaborative model training among
participants while guaranteeing the privacy of raw data. Mainstream FL
methodologies overlook the dynamic nature of real-world data, particularly its
tendency to grow in volume and diversify in classes over time. This oversight
results in FL methods suffering from catastrophic forgetting, where models
inadvertently discard previously learned information upon assimilating new
data. In response to this challenge, we propose a novel Federated
Class-Incremental Learning (FCIL) method, named FCIL with New-Class Augmented
Self-Distillation (FedNASD). FedNASD combines new class scores, which are
inferred from current models, with historical models' predictions. Based on the
combined past and present knowledge, it incorporates self-distillation over
models on clients, aiming to achieve effective knowledge transfer from
historical models to current models. Theoretical analysis demonstrates that
FedNASD is equivalent to modeling old class scores as conditional probabilities
in the absence of new classes. Additionally, it reconciles the predictions of
new classes with current models to refine the conditional probabilities of
historical scores where new classes do not exist. Empirical experiments
demonstrate the superiority of FedNASD over four baseline algorithms in
reducing the average forgetting rate and boosting global accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Learnability of Watermarks for Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04469v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04469v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenchen Gu, Xiang Lisa Li, Percy Liang, Tatsunori Hashimoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Watermarking of language model outputs enables statistical detection of
model-generated text, which has many applications in the responsible deployment
of language models. Existing watermarking strategies operate by altering the
decoder of an existing language model, and the ability for a language model to
directly learn to generate the watermark would have significant implications
for the real-world deployment of watermarks. First, learned watermarks could be
used to build open models that naturally generate watermarked text, allowing
for open models to benefit from watermarking. Second, if watermarking is used
to determine the provenance of generated text, an adversary can hurt the
reputation of a victim model by spoofing its watermark and generating damaging
watermarked text. To investigate the learnability of watermarks, we propose
watermark distillation, which trains a student model to behave like a teacher
model that uses decoding-based watermarking. We test our approach on three
distinct decoding-based watermarking strategies and various hyperparameter
settings, finding that models can learn to generate watermarked text with high
detectability. We also find limitations to learnability, including the loss of
watermarking capabilities under fine-tuning on normal text and high sample
complexity when learning low-distortion watermarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Do Graph Neural Networks Help with Node Classification?
  Investigating the Impact of Homophily Principle on Node Distinguishability <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.14274v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.14274v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sitao Luan, Chenqing Hua, Minkai Xu, Qincheng Lu, Jiaqi Zhu, Xiao-Wen Chang, Jie Fu, Jure Leskovec, Doina Precup
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Homophily principle, i.e., nodes with the same labels are more likely to be
connected, has been believed to be the main reason for the performance
superiority of Graph Neural Networks (GNNs) over Neural Networks on node
classification tasks. Recent research suggests that, even in the absence of
homophily, the advantage of GNNs still exists as long as nodes from the same
class share similar neighborhood patterns. However, this argument only
considers intra-class Node Distinguishability (ND) but neglects inter-class ND,
which provides incomplete understanding of homophily on GNNs. In this paper, we
first demonstrate such deficiency with examples and argue that an ideal
situation for ND is to have smaller intra-class ND than inter-class ND. To
formulate this idea and study ND deeply, we propose Contextual Stochastic Block
Model for Homophily (CSBM-H) and define two metrics, Probabilistic Bayes Error
(PBE) and negative generalized Jeffreys divergence, to quantify ND. With the
metrics, we visualize and analyze how graph filters, node degree distributions
and class variances influence ND, and investigate the combined effect of intra-
and inter-class ND. Besides, we discovered the mid-homophily pitfall, which
occurs widely in graph datasets. Furthermore, we verified that, in real-work
tasks, the superiority of GNNs is indeed closely related to both intra- and
inter-class ND regardless of homophily levels. Grounded in this observation, we
propose a new hypothesis-testing based performance metric beyond homophily,
which is non-linear, feature-based and can provide statistical threshold value
for GNNs' the superiority. Experiments indicate that it is significantly more
effective than the existing homophily metrics on revealing the advantage and
disadvantage of graph-aware modes on both synthetic and benchmark real-world
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 37th Conference on Neural Information Processing Systems
  (NeurIPS 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identifying Constitutive Parameters for Complex Hyperelastic Materials
  using Physics-Informed Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15640v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15640v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Song, Hanxun Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying constitutive parameters in engineering and biological materials,
particularly those with intricate geometries and mechanical behaviors, remains
a longstanding challenge. The recent advent of Physics-Informed Neural Networks
(PINNs) offers promising solutions, but current frameworks are often limited to
basic constitutive laws and encounter practical constraints when combined with
experimental data. In this paper, we introduce a robust PINN-based framework
designed to identify material parameters for soft materials, specifically those
exhibiting complex constitutive behaviors, under large deformation in plane
stress conditions. Distinctively, our model emphasizes training PINNs with
multi-modal synthetic experimental datasets consisting of full-field
deformation and loading history, ensuring algorithm robustness even with noisy
data. Our results reveal that the PINNs framework can accurately identify
constitutive parameters of the incompressible Arruda-Boyce model for samples
with intricate geometries, maintaining an error below 5%, even with an
experimental noise level of 5%. We believe our framework provides a robust
modulus identification approach for complex solids, especially for those with
geometrical and constitutive complexity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 5 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ranking In Generalized Linear Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.00109v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.00109v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amitis Shidani, George Deligiannidis, Arnaud Doucet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the ranking problem in generalized linear bandits. At each time, the
learning agent selects an ordered list of items and observes stochastic
outcomes. In recommendation systems, displaying an ordered list of the most
attractive items is not always optimal as both position and item dependencies
result in a complex reward function. A very naive example is the lack of
diversity when all the most attractive items are from the same category. We
model the position and item dependencies in the ordered list and design UCB and
Thompson Sampling type algorithms for this problem. Our work generalizes
existing studies in several directions, including position dependencies where
position discount is a particular case, and connecting the ranking problem to
graph theory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Models are Bounded Pragmatic Speakers: Understanding RLHF from
  a Bayesian Cognitive Modeling Perspective <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17760v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17760v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khanh Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How do language models "think"? This paper formulates a probabilistic
cognitive model called the bounded pragmatic speaker, which can characterize
the operation of different variations of language models. Specifically, we
demonstrate that large language models fine-tuned with reinforcement learning
from human feedback (Ouyang et al., 2022) embody a model of thought that
conceptually resembles a fast-and-slow model (Kahneman, 2011), which
psychologists have attributed to humans. We discuss the limitations of
reinforcement learning from human feedback as a fast-and-slow model of thought
and propose avenues for expanding this framework. In essence, our research
highlights the value of adopting a cognitive probabilistic modeling approach to
gain insights into the comprehension, evaluation, and advancement of language
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the First Workshop on Theory of Mind in Communicating
  Agents at (TOM @ ICML 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Active Control of Flow over Rotating Cylinder by Multiple Jets using
  Deep Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.12083v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.12083v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kamyar Dobakhti, Jafar Ghazanfarian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The real power of artificial intelligence appears in reinforcement learning,
which is computationally and physically more sophisticated due to its dynamic
nature. Rotation and injection are some of the proven ways in active flow
control for drag reduction on blunt bodies. In this paper, rotation will be
added to the cylinder alongside the deep reinforcement learning (DRL)
algorithm, which uses multiple controlled jets to reach the maximum possible
drag suppression. Characteristics of the DRL code, including controlling
parameters, their limitations, and optimization of the DRL network for use with
rotation will be presented. This work will focus on optimizing the number and
positions of the jets, the sensors location, and the maximum allowed flow rate
to jets in the form of the maximum allowed flow rate of each actuation and the
total number of them per episode. It is found that combining the rotation and
DRL is promising since it suppresses the vortex shedding, stabilizes the Karman
vortex street, and reduces the drag coefficient by up to 49.75%. Also, it will
be shown that having more sensors at more locations is not always a good choice
and the sensor number and location should be determined based on the need of
the user and corresponding configuration. Also, allowing the agent to have
access to higher flow rates, mostly reduces the performance, except when the
cylinder rotates. In all cases, the agent can keep the lift coefficient at a
value near zero, or stabilize it at a smaller number.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>1st Edit: Parts of the introduction, simulation environment, and the
  network and reinforcement learning framework have been revised. --- 2nd Edit:
  Added real-world scenario, possible design of rotating cylinder, sensors
  limited to the body of the cylinder, Re = 200, compared the results and
  runtime of a shallow network with a higher number of neurons vs deeper
  network but lower number of neurons</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine Learning for Synthetic Data Generation: A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.04062v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.04062v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingzhou Lu, Minjie Shen, Huazheng Wang, Xiao Wang, Capucine van Rechem, Wenqi Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning heavily relies on data, but real-world applications often
encounter various data-related issues. These include data of poor quality,
insufficient data points leading to under-fitting of machine learning models,
and difficulties in data access due to concerns surrounding privacy, safety,
and regulations. In light of these challenges, the concept of synthetic data
generation emerges as a promising alternative that allows for data sharing and
utilization in ways that real-world data cannot facilitate. This paper presents
a comprehensive systematic review of existing studies that employ machine
learning models for the purpose of generating synthetic data. The review
encompasses various perspectives, starting with the applications of synthetic
data generation, spanning computer vision, speech, natural language processing,
healthcare, and business domains. Additionally, it explores different machine
learning methods, with particular emphasis on neural network architectures and
deep generative models. The paper also addresses the crucial aspects of privacy
and fairness concerns related to synthetic data generation. Furthermore, this
study identifies the challenges and opportunities prevalent in this emerging
field, shedding light on the potential avenues for future research. By delving
into the intricacies of synthetic data generation, this paper aims to
contribute to the advancement of knowledge and inspire further exploration in
synthetic data generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Byzantines can also Learn from History: Fall of Centered Clipping in
  Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.09894v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.09894v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kerem Ozfatura, Emre Ozfatura, Alptekin Kupcu, Deniz Gunduz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing popularity of the federated learning (FL) framework due to its
success in a wide range of collaborative learning tasks also induces certain
security concerns. Among many vulnerabilities, the risk of Byzantine attacks is
of particular concern, which refers to the possibility of malicious clients
participating in the learning process. Hence, a crucial objective in FL is to
neutralize the potential impact of Byzantine attacks and to ensure that the
final model is trustable. It has been observed that the higher the variance
among the clients' models/updates, the more space there is for Byzantine
attacks to be hidden. As a consequence, by utilizing momentum, and thus,
reducing the variance, it is possible to weaken the strength of known Byzantine
attacks. The centered clipping (CC) framework has further shown that the
momentum term from the previous iteration, besides reducing the variance, can
be used as a reference point to neutralize Byzantine attacks better. In this
work, we first expose vulnerabilities of the CC framework, and introduce a
novel attack strategy that can circumvent the defences of CC and other robust
aggregators and reduce their test accuracy up to %33 on best-case scenarios in
image classification tasks. Then, we propose a new robust and fast defence
mechanism that is effective against the proposed and other existing Byzantine
attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Transactions on Information Forensics and Security 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting inference after prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.13746v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.13746v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keshav Motwani, Daniela Witten
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has focused on the very common practice of prediction-based
inference: that is, (i) using a pre-trained machine learning model to predict
an unobserved response variable, and then (ii) conducting inference on the
association between that predicted response and some covariates. As pointed out
by Wang et al. (2020), applying a standard inferential approach in (ii) does
not accurately quantify the association between the unobserved (as opposed to
the predicted) response and the covariates. In recent work, Wang et al. (2020)
and Angelopoulos et al. (2023) propose corrections to step (ii) in order to
enable valid inference on the association between the unobserved response and
the covariates. Here, we show that the method proposed by Angelopoulos et al.
(2023) successfully controls the type 1 error rate and provides confidence
intervals with correct nominal coverage, regardless of the quality of the
pre-trained machine learning model used to predict the unobserved response.
However, the method proposed by Wang et al. (2020) provides valid inference
only under very strong conditions that rarely hold in practice: for instance,
if the machine learning model perfectly estimates the true regression function
in the study population of interest.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transfer Learning for Causal Effect Estimation <span class="chip">ICML 3</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.09126v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.09126v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Song Wei, Hanyu Zhang, Ronald Moore, Rishikesan Kamaleswaran, Yao Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a Transfer Causal Learning (TCL) framework when target and source
domains share the same covariate/feature spaces, aiming to improve causal
effect estimation accuracy in limited data. Limited data is very common in
medical applications, where some rare medical conditions, such as sepsis, are
of interest. Our proposed method, named \texttt{$\ell_1$-TCL}, incorporates
$\ell_1$ regularized TL for nuisance models (e.g., propensity score model); the
TL estimator of the nuisance parameters is plugged into downstream average
causal/treatment effect estimators (e.g., inverse probability weighted
estimator). We establish non-asymptotic recovery guarantees for the
\texttt{$\ell_1$-TCL} with generalized linear model (GLM) under the sparsity
assumption in the high-dimensional setting, and demonstrate the empirical
benefits of \texttt{$\ell_1$-TCL} through extensive numerical simulation for
GLM and recent neural network nuisance models. Our method is subsequently
extended to real data and generates meaningful insights consistent with medical
literature, a case where all baseline methods fail.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preliminary version, titled "Transfer causal learning: Causal effect
  estimation with knowledge transfer", has been presented in ICML 3rd Workshop
  on Interpretable Machine Learning in Healthcare (IMLH), 2023; see the arXiv
  version in v2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decision-Focused Model-based Reinforcement Learning for Reward Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.03365v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.03365v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhishek Sharma, Sonali Parbhoo, Omer Gottesman, Finale Doshi-Velez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decision-focused (DF) model-based reinforcement learning has recently been
introduced as a powerful algorithm that can focus on learning the MDP dynamics
that are most relevant for obtaining high returns. While this approach
increases the agent's performance by directly optimizing the reward, it does so
by learning less accurate dynamics from a maximum likelihood perspective. We
demonstrate that when the reward function is defined by preferences over
multiple objectives, the DF model may be sensitive to changes in the objective
preferences.In this work, we develop the robust decision-focused (RDF)
algorithm, which leverages the non-identifiability of DF solutions to learn
models that maximize expected returns while simultaneously learning models that
transfer to changes in the preference over multiple objectives. We demonstrate
the effectiveness of RDF on two synthetic domains and two healthcare
simulators, showing that it significantly improves the robustness of DF model
learning to changes in the reward function without compromising training-time
return.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ULDP-FL: Federated Learning with Across Silo User-Level Differential
  Privacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12210v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12210v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fumiyuki Kato, Li Xiong, Shun Takagi, Yang Cao, Masatoshi Yoshikawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Differentially Private Federated Learning (DP-FL) has garnered attention as a
collaborative machine learning approach that ensures formal privacy. Most DP-FL
approaches ensure DP at the record-level within each silo for cross-silo FL.
However, a single user's data may extend across multiple silos, and the desired
user-level DP guarantee for such a setting remains unknown. In this study, we
present Uldp-FL, a novel FL framework designed to guarantee user-level DP in
cross-silo FL where a single user's data may belong to multiple silos. Our
proposed algorithm directly ensures user-level DP through per-user weighted
clipping, departing from group-privacy approaches. We provide a theoretical
analysis of the algorithm's privacy and utility. Additionally, we enhance the
utility of the proposed algorithm with an enhanced weighting strategy based on
user record distribution and design a novel private protocol that ensures no
additional information is revealed to the silos and the server. Experiments on
real-world datasets show substantial improvements in our methods in
privacy-utility trade-offs under user-level DP compared to baseline methods. To
the best of our knowledge, our work is the first FL framework that effectively
provides user-level DP in the general cross-silo FL setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COMBHelper: A Neural Approach to Reduce Search Space for Graph
  Combinatorial Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.09086v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.09086v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Tian, Sourav Medya, Wei Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combinatorial Optimization (CO) problems over graphs appear routinely in many
applications such as in optimizing traffic, viral marketing in social networks,
and matching for job allocation. Due to their combinatorial nature, these
problems are often NP-hard. Existing approximation algorithms and heuristics
rely on the search space to find the solutions and become time-consuming when
this space is large. In this paper, we design a neural method called COMBHelper
to reduce this space and thus improve the efficiency of the traditional CO
algorithms based on node selection. Specifically, it employs a Graph Neural
Network (GNN) to identify promising nodes for the solution set. This pruned
search space is then fed to the traditional CO algorithms. COMBHelper also uses
a Knowledge Distillation (KD) module and a problem-specific boosting module to
bring further efficiency and efficacy. Our extensive experiments show that the
traditional CO algorithms with COMBHelper are at least 2 times faster than
their original versions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Asynchronous Evolution of Deep Neural Network Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.04102v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.04102v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jason Liang, Hormoz Shahrzad, Risto Miikkulainen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many evolutionary algorithms (EAs) take advantage of parallel evaluation of
candidates. However, if evaluation times vary significantly, many worker nodes
(i.e.,\ compute clients) are idle much of the time, waiting for the next
generation to be created. Evolutionary neural architecture search (ENAS), a
class of EAs that optimizes the architecture and hyperparameters of deep neural
networks, is particularly vulnerable to this issue. This paper proposes a
generic asynchronous evaluation strategy (AES) that is then adapted to work
with ENAS. AES increases throughput by maintaining a queue of up to $K$
individuals ready to be sent to the workers for evaluation and proceeding to
the next generation as soon as $M<<K$ individuals have been evaluated. A
suitable value for $M$ is determined experimentally, balancing diversity and
efficiency. To showcase the generality and power of AES, it was first evaluated
in eight-line sorting network design (a single-population optimization task
with limited evaluation-time variability), achieving an over two-fold speedup.
Next, it was evaluated in 11-bit multiplexer design (a single-population
discovery task with extended variability), where a 14-fold speedup was
observed. It was then scaled up to ENAS for image captioning (a
multi-population open-ended-optimization task), resulting in an over two-fold
speedup. In all problems, a multifold performance improvement was observed,
suggesting that AES is a promising method for parallelizing the evolution of
complex systems with long and variable evaluation times, such as those in ENAS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UDTIRI: An Online Open-Source Intelligent Road Inspection Benchmark
  Suite 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.08842v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.08842v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sicen Guo, Jiahang Li, Yi Feng, Dacheng Zhou, Denghuang Zhang, Chen Chen, Shuai Su, Xingyi Zhu, Qijun Chen, Rui Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the nascent domain of urban digital twins (UDT), the prospects for
leveraging cutting-edge deep learning techniques are vast and compelling.
Particularly within the specialized area of intelligent road inspection (IRI),
a noticeable gap exists, underscored by the current dearth of dedicated
research efforts and the lack of large-scale well-annotated datasets. To foster
advancements in this burgeoning field, we have launched an online open-source
benchmark suite, referred to as UDTIRI. Along with this article, we introduce
the road pothole detection task, the first online competition published within
this benchmark suite. This task provides a well-annotated dataset, comprising
1,000 RGB images and their pixel/instance-level ground-truth annotations,
captured in diverse real-world scenarios under different illumination and
weather conditions. Our benchmark provides a systematic and thorough evaluation
of state-of-the-art object detection, semantic segmentation, and instance
segmentation networks, developed based on either convolutional neural networks
or Transformers. We anticipate that our benchmark will serve as a catalyst for
the integration of advanced UDT techniques into IRI. By providing algorithms
with a more comprehensive understanding of diverse road conditions, we seek to
unlock their untapped potential and foster innovation in this critical domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Database webpage: https://www.udtiri.com/, Kaggle webpage:
  https://www.kaggle.com/datasets/jiahangli617/udtiri</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MABViT -- Modified Attention Block Enhances Vision <span class="highlight-title">Transformer</span>s <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.01324v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.01324v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahesh Ramesh, Aswinkumar Ramkumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have demonstrated the effectiveness of Gated Linear Units
(GLU) in enhancing transformer models, particularly in Large Language Models
(LLMs). Additionally, utilizing a parallel configuration within each
Transformer block rather than the conventional serialized method has been
revealed to accelerate the training of LLMs without significantly impacting
performance. However, when the MLP and attention block were run in parallel for
the image classification task, we observed a noticeable decline in performance.
We propose a novel transformer variant that integrates non-linearity within the
attention block to tackle this problem. We implemented the GLU-based activation
function on the Value tensor, and this new technique surpasses the current
state-of-the-art S/16 variant of Vision Transformers by 0.6% on the ImageNet-1K
dataset while utilizing fewer parameters. It also supersedes the B/16 variant
while using only half the parameters. Furthermore, we provide results with the
GELU activation function variant to confirm our assertions. Lastly, we showcase
that the MABViT variants exhibit greater potential when utilized in deep
transformers compared to the standard architecture.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Deployable AI Workshop, AAAI Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Foundation Model Meets Federated Learning: Motivations, Challenges,
  and Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.15546v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.15546v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiming Zhuang, Chen Chen, Lingjuan Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The intersection of the Foundation Model (FM) and Federated Learning (FL)
provides mutual benefits, presents a unique opportunity to unlock new
possibilities in AI research, and address critical challenges in AI and
real-world applications. FL expands the availability of data for FMs and
enables computation sharing, distributing the training process and reducing the
burden on FL participants. It promotes collaborative FM development,
democratizing the process and fostering inclusivity and innovation. On the
other hand, FM, with its enormous size, pre-trained knowledge, and exceptional
performance, serves as a robust starting point for FL, facilitating faster
convergence and better performance under non-iid data. Additionally, leveraging
FM to generate synthetic data enriches data diversity, reduces overfitting, and
preserves privacy. By examining the interplay between FL and FM, this paper
aims to deepen the understanding of their synergistic relationship,
highlighting the motivations, challenges, and future directions. Through an
exploration of the challenges faced by FL and FM individually and their
interconnections, we aim to inspire future research directions that can further
enhance both fields, driving advancements and propelling the development of
privacy-preserving and scalable AI systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffusionLight: Light Probes for Free by Painting a Chrome Ball 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.09168v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.09168v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pakkapon Phongthawee, Worameth Chinchuthakun, Nontaphat Sinsunthithet, Amit Raj, Varun Jampani, Pramook Khungurn, Supasorn Suwajanakorn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a simple yet effective technique to estimate lighting in a single
input image. Current techniques rely heavily on HDR panorama datasets to train
neural networks to regress an input with limited field-of-view to a full
environment map. However, these approaches often struggle with real-world,
uncontrolled settings due to the limited diversity and size of their datasets.
To address this problem, we leverage diffusion models trained on billions of
standard images to render a chrome ball into the input image. Despite its
simplicity, this task remains challenging: the diffusion models often insert
incorrect or inconsistent objects and cannot readily generate images in HDR
format. Our research uncovers a surprising relationship between the appearance
of chrome balls and the initial diffusion noise map, which we utilize to
consistently generate high-quality chrome balls. We further fine-tune an LDR
difusion model (Stable Diffusion XL) with LoRA, enabling it to perform exposure
bracketing for HDR light estimation. Our method produces convincing light
estimates across diverse settings and demonstrates superior generalization to
in-the-wild scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>For more info and code, please visit our website
  https://diffusionlight.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Boosting Adaptive Learning under Concept Drift for Multistream
  Classification <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10841v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10841v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        En Yu, Jie Lu, Bin Zhang, Guangquan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multistream classification poses significant challenges due to the necessity
for rapid adaptation in dynamic streaming processes with concept drift. Despite
the growing research outcomes in this area, there has been a notable oversight
regarding the temporal dynamic relationships between these streams, leading to
the issue of negative transfer arising from irrelevant data. In this paper, we
propose a novel Online Boosting Adaptive Learning (OBAL) method that
effectively addresses this limitation by adaptively learning the dynamic
correlation among different streams. Specifically, OBAL operates in a
dual-phase mechanism, in the first of which we design an Adaptive COvariate
Shift Adaptation (AdaCOSA) algorithm to construct an initialized ensemble model
using archived data from various source streams, thus mitigating the covariate
shift while learning the dynamic correlations via an adaptive re-weighting
strategy. During the online process, we employ a Gaussian Mixture Model-based
weighting mechanism, which is seamlessly integrated with the acquired
correlations via AdaCOSA to effectively handle asynchronous drift. This
approach significantly improves the predictive performance and stability of the
target stream. We conduct comprehensive experiments on several synthetic and
real-world data streams, encompassing various drifting scenarios and types. The
results clearly demonstrate that OBAL achieves remarkable advancements in
addressing multistream classification problems by effectively leveraging
positive knowledge derived from multiple sources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An attempt to generate new bridge types from latent space of variational
  autoencoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.03380v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.03380v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongjun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Try to generate new bridge types using generative artificial intelligence
technology. The grayscale images of the bridge facade with the change of
component width was rendered by 3dsMax animation software, and then the OpenCV
module performed an appropriate amount of geometric transformation (rotation,
horizontal scale, vertical scale) to obtain the image dataset of three-span
beam bridge, arch bridge, cable-stayed bridge and suspension bridge. Based on
Python programming language, TensorFlow and Keras deep learning platform
framework, variational autoencoder was constructed and trained, and
low-dimensional bridge-type latent space that is convenient for vector
operations was obtained. Variational autoencoder can combine two bridge types
on the basis of the original of human into one that is a new bridge type.
Generative artificial intelligence technology can assist bridge designers in
bridge-type innovation, and can be used as copilot.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Passive Inference Attacks on Split Learning via Adversarial
  Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10483v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10483v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaochen Zhu, Xinjian Luo, Yuncheng Wu, Yangfan Jiang, Xiaokui Xiao, Beng Chin Ooi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Split Learning (SL) has emerged as a practical and efficient alternative to
traditional federated learning. While previous attempts to attack SL have often
relied on overly strong assumptions or targeted easily exploitable models, we
seek to develop more practical attacks. We introduce SDAR, a novel attack
framework against SL with an honest-but-curious server. SDAR leverages
auxiliary data and adversarial regularization to learn a decodable simulator of
the client's private model, which can effectively infer the client's private
features under the vanilla SL, and both features and labels under the U-shaped
SL. We perform extensive experiments in both configurations to validate the
effectiveness of our proposed attacks. Notably, in challenging but practical
scenarios where existing passive attacks struggle to reconstruct the client's
private data effectively, SDAR consistently achieves attack performance
comparable to active attacks. On CIFAR-10, at the deep split level of 7, SDAR
achieves private feature reconstruction with less than 0.025 mean squared error
in both the vanilla and the U-shaped SL, and attains a label inference accuracy
of over 98% in the U-shaped setting, while existing attacks fail to produce
non-trivial results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 20 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Characteristic Guidance: Non-linear Correction for Diffusion Model at
  Large Guidance Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07586v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07586v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Candi Zheng, Yuan Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Popular guidance for denoising diffusion probabilistic model (DDPM) linearly
combines distinct conditional models together to provide enhanced control over
samples. However, this approach overlooks nonlinear effects that become
significant when guidance scale is large. To address this issue, we propose
characteristic guidance, a sampling method that provides first-principle
non-linear correction for classifier-free guided DDPMs. Such correction forces
the guided DDPMs to respect the Fokker-Planck equation of their underlying
diffusion process, in a way that is training-free, derivative-free, and
compatible with existing sampling methods. Experiments show that characteristic
guidance enhances control and reduces color and exposure issues in image
generation, proving effective in diverse applications ranging from latent space
sampling to solving physics problems like magnet phase transitions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Less or More From Teacher: Exploiting Trilateral Geometry For Knowledge
  Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.15112v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.15112v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengming Hu, Haolun Wu, Xuan Li, Chen Ma, Xi Chen, Jun Yan, Boyu Wang, Xue Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation aims to train a compact student network using soft
supervision from a larger teacher network and hard supervision from ground
truths. However, determining an optimal knowledge fusion ratio that balances
these supervisory signals remains challenging. Prior methods generally resort
to a constant or heuristic-based fusion ratio, which often falls short of a
proper balance. In this study, we introduce a novel adaptive method for
learning a sample-wise knowledge fusion ratio, exploiting both the correctness
of teacher and student, as well as how well the student mimics the teacher on
each sample. Our method naturally leads to the intra-sample trilateral
geometric relations among the student prediction ($S$), teacher prediction
($T$), and ground truth ($G$). To counterbalance the impact of outliers, we
further extend to the inter-sample relations, incorporating the teacher's
global average prediction $\bar{T}$ for samples within the same class. A simple
neural network then learns the implicit mapping from the intra- and
inter-sample relations to an adaptive, sample-wise knowledge fusion ratio in a
bilevel-optimization manner. Our approach provides a simple, practical, and
adaptable solution for knowledge distillation that can be employed across
various architectures and model sizes. Extensive experiments demonstrate
consistent improvements over other loss re-weighting methods on image
classification, attack detection, and click-through rate prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InRank: Incremental Low-Rank Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.11250v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.11250v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Zhao, Yifei Zhang, Beidi Chen, Florian Schäfer, Anima Anandkumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The theory of greedy low-rank learning (GLRL) aims to explain the impressive
generalization capabilities of deep learning. It proves that stochastic
gradient-based training implicitly regularizes neural networks towards low-rank
solutions through a gradual increase of the rank during training. However,
there is a gap between theory and practice since GLRL requires an infinitesimal
initialization of the weights, which is not practical due to the fact that it
is a saddle point. In this work, we remove the assumption of infinitesimal
initialization by focusing on cumulative weight updates. We prove the
cumulative weight updates follow an incremental low-rank trajectory for
arbitrary orthogonal initialization of weights in a three-layer linear network.
Empirically, we demonstrate that our theory holds on a broad range of neural
networks (e.g., transformers) and standard training algorithms (e.g., SGD,
Adam). However, existing training algorithms do not exploit the low-rank
property to improve computational efficiency as the networks are not
parameterized in low-rank. To remedy this, we design a new training algorithm
Incremental Low-Rank Learning (InRank), which explicitly expresses cumulative
weight updates as low-rank matrices while incrementally augmenting their ranks
during training. We evaluate InRank on GPT-2, and our results indicate that
InRank achieves comparable prediction performance as the full-rank counterpart
while requiring at most 33% of the total ranks throughout training. We also
propose an efficient version of InRank that achieves a reduction of 37% in
total training time and 36% in model size when training GPT-medium on
WikiText-103 from scratch.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data Valuation for Vertical Federated Learning: A Model-free and
  Privacy-preserving Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.08364v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.08364v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Han, Leye Wang, Junjie Wu, Xiao Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vertical Federated learning (VFL) is a promising paradigm for predictive
analytics, empowering an organization (i.e., task party) to enhance its
predictive models through collaborations with multiple data suppliers (i.e.,
data parties) in a decentralized and privacy-preserving way. Despite the
fast-growing interest in VFL, the lack of effective and secure tools for
assessing the value of data owned by data parties hinders the application of
VFL in business contexts. In response, we propose FedValue, a
privacy-preserving, task-specific but model-free data valuation method for VFL,
which consists of a data valuation metric and a federated computation method.
Specifically, we first introduce a novel data valuation metric, namely
MShapley-CMI. The metric evaluates a data party's contribution to a predictive
analytics task without the need of executing a machine learning model, making
it well-suited for real-world applications of VFL. Next, we develop an
innovative federated computation method that calculates the MShapley-CMI value
for each data party in a privacy-preserving manner. Extensive experiments
conducted on six public datasets validate the efficacy of FedValue for data
valuation in the context of VFL. In addition, we illustrate the practical
utility of FedValue with a case study involving federated movie
recommendations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> of Methods, Challenges and Perspectives in Causality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.00293v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.00293v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaël Gendron, Michael Witbrock, Gillian Dobbie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning models have shown success in a large variety of tasks by
extracting correlation patterns from high-dimensional data but still struggle
when generalizing out of their initial distribution. As causal engines aim to
learn mechanisms independent from a data distribution, combining Deep Learning
with Causality can have a great impact on the two fields. In this paper, we
further motivate this assumption. We perform an extensive overview of the
theories and methods for Causality from different perspectives, with an
emphasis on Deep Learning and the challenges met by the two domains. We show
early attempts to bring the fields together and the possible perspectives for
the future. We finish by providing a large variety of applications for
techniques from Causality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, 37 pages for the main paper and 3 pages for the supplement,
  8 figures, submitted to ACM Computing Surveys</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ New Job, New Gender? Measuring the Social Bias in Image Generation
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00763v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00763v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Wang, Haonan Bai, Jen-tse Huang, Yuxuan Wan, Youliang Yuan, Haoyi Qiu, Nanyun Peng, Michael R. Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image generation models can generate or edit images from a given text. Recent
advancements in image generation technology, exemplified by DALL-E and
Midjourney, have been groundbreaking. These advanced models, despite their
impressive capabilities, are often trained on massive Internet datasets, making
them susceptible to generating content that perpetuates social stereotypes and
biases, which can lead to severe consequences. Prior research on assessing bias
within image generation models suffers from several shortcomings, including
limited accuracy, reliance on extensive human labor, and lack of comprehensive
analysis. In this paper, we propose BiasPainter, a novel metamorphic testing
framework that can accurately, automatically and comprehensively trigger social
bias in image generation models. BiasPainter uses a diverse range of seed
images of individuals and prompts the image generation models to edit these
images using gender, race, and age-neutral queries. These queries span 62
professions, 39 activities, 57 types of objects, and 70 personality traits. The
framework then compares the edited images to the original seed images, focusing
on any changes related to gender, race, and age. BiasPainter adopts a testing
oracle that these characteristics should not be modified when subjected to
neutral prompts. Built upon this design, BiasPainter can trigger the social
bias and evaluate the fairness of image generation models. To evaluate the
effectiveness of BiasPainter, we use BiasPainter to test five widely-used
commercial image generation software and models, such as stable diffusion and
Midjourney. Experimental results show that 100\% of the generated test cases
can successfully trigger social bias in image generation models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion Models, Image Super-Resolution And Everything: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian B. Moser, Arundhati S. Shanbhag, Federico Raue, Stanislav Frolov, Sebastian Palacio, Andreas Dengel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion Models (DMs) represent a significant advancement in image
Super-Resolution (SR), aligning technical image quality more closely with human
preferences and expanding SR applications. DMs address critical limitations of
previous methods, enhancing overall realism and details in SR images. However,
DMs suffer from color-shifting issues, and their high computational costs call
for efficient sampling alternatives, underscoring the challenge of balancing
computational efficiency and image quality. This survey gives an overview of
DMs applied to image SR and offers a detailed analysis that underscores the
unique characteristics and methodologies within this domain, distinct from
broader existing reviews in the field. It presents a unified view of DM
fundamentals and explores research directions, including alternative input
domains, conditioning strategies, guidance, corruption spaces, and zero-shot
methods. This survey provides insights into the evolution of image SR with DMs,
addressing current trends, challenges, and future directions in this rapidly
evolving field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Point Cloud in the Air 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00658v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00658v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulin Shao, Chenghong Bian, Li Yang, Qianqian Yang, Zhaoyang Zhang, Deniz Gunduz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Acquisition and processing of point clouds (PCs) is a crucial enabler for
many emerging applications reliant on 3D spatial data, such as robot
navigation, autonomous vehicles, and augmented reality. In most scenarios, PCs
acquired by remote sensors must be transmitted to an edge server for fusion,
segmentation, or inference. Wireless transmission of PCs not only puts on
increased burden on the already congested wireless spectrum, but also confronts
a unique set of challenges arising from the irregular and unstructured nature
of PCs. In this paper, we meticulously delineate these challenges and offer a
comprehensive examination of existing solutions while candidly acknowledging
their inherent limitations. In response to these intricacies, we proffer four
pragmatic solution frameworks, spanning advanced techniques, hybrid schemes,
and distributed data aggregation approaches. In doing so, our goal is to chart
a path toward efficient, reliable, and low-latency wireless PC transmission.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level
  Vision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14181v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14181v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, Weisi Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid evolution of Multi-modality Large Language Models (MLLMs) has
catalyzed a shift in computer vision from specialized models to general-purpose
foundation models. Nevertheless, there is still an inadequacy in assessing the
abilities of MLLMs on low-level visual perception and understanding. To address
this gap, we present Q-Bench, a holistic benchmark crafted to systematically
evaluate potential abilities of MLLMs on three realms: low-level visual
perception, low-level visual description, and overall visual quality
assessment. a) To evaluate the low-level perception ability, we construct the
LLVisionQA dataset, consisting of 2,990 diverse-sourced images, each equipped
with a human-asked question focusing on its low-level attributes. We then
measure the correctness of MLLMs on answering these questions. b) To examine
the description ability of MLLMs on low-level information, we propose the
LLDescribe dataset consisting of long expert-labelled golden low-level text
descriptions on 499 images, and a GPT-involved comparison pipeline between
outputs of MLLMs and the golden descriptions. c) Besides these two tasks, we
further measure their visual quality assessment ability to align with human
opinion scores. Specifically, we design a softmax-based strategy that enables
MLLMs to predict quantifiable quality scores, and evaluate them on various
existing image quality assessment (IQA) datasets. Our evaluation across the
three abilities confirms that MLLMs possess preliminary low-level visual
skills. However, these skills are still unstable and relatively imprecise,
indicating the need for specific enhancements on MLLMs towards these abilities.
We hope that our benchmark can encourage the research community to delve deeper
to discover and enhance these untapped potentials of MLLMs. Project Page:
https://q-future.github.io/Q-Bench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 11 tables, with updated results</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-12-31T00:00:00Z">2023-12-31</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">25</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> of Personality, Persona, and Profile in Conversational Agents
  and Chatbots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard Sutcliffe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a review of personality in neural conversational agents (CAs),
also called chatbots. First, we define Personality, Persona, and Profile. We
explain all personality schemes which have been used in CAs, and list models
under the scheme(s) which they use. Second we describe 21 datasets which have
been developed in recent CA personality research. Third, we define the methods
used to embody personality in a CA, and review recent models using them.
Fourth, we survey some relevant reviews on CAs, personality, and related
topics. Finally, we draw conclusions and identify some research challenges for
this important emerging field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 6 tables, 207 references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DocLLM: A layout-aware generative language model for multimodal document
  understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00908v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00908v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongsheng Wang, Natraj Raman, Mathieu Sibue, Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, Xiaomo Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enterprise documents such as forms, invoices, receipts, reports, contracts,
and other similar records, often carry rich semantics at the intersection of
textual and spatial modalities. The visual cues offered by their complex
layouts play a crucial role in comprehending these documents effectively. In
this paper, we present DocLLM, a lightweight extension to traditional large
language models (LLMs) for reasoning over visual documents, taking into account
both textual semantics and spatial layout. Our model differs from existing
multimodal LLMs by avoiding expensive image encoders and focuses exclusively on
bounding box information to incorporate the spatial layout structure.
Specifically, the cross-alignment between text and spatial modalities is
captured by decomposing the attention mechanism in classical transformers to a
set of disentangled matrices. Furthermore, we devise a pre-training objective
that learns to infill text segments. This approach allows us to address
irregular layouts and heterogeneous content frequently encountered in visual
documents. The pre-trained model is fine-tuned using a large-scale instruction
dataset, covering four core document intelligence tasks. We demonstrate that
our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks,
and generalizes well to 4 out of 5 previously unseen datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ State of What Art? A Call for Multi-<span class="highlight-title">Prompt</span> LLM Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna Shahaf, Gabriel Stanovsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large language models (LLMs) have led to the development
of various evaluation benchmarks. These benchmarks typically rely on a single
instruction template for evaluating all LLMs on a specific task. In this paper,
we comprehensively analyze the brittleness of results obtained via
single-prompt evaluations across 6.5M instances, involving 20 different LLMs
and 39 tasks from 3 benchmarks. To improve robustness of the analysis, we
propose to evaluate LLMs with a set of diverse prompts instead. We discuss
tailored evaluation metrics for specific use cases (e.g., LLM developers vs.
developers interested in a specific downstream task), ensuring a more reliable
and meaningful assessment of LLM capabilities. We then implement these criteria
and conduct evaluations of multiple models, providing insights into the true
strengths and limitations of current LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LaFFi: Leveraging Hybrid Natural Language Feedback for Fine-tuning
  Language Models <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00907v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00907v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianxi Li, Yingyue Cao, Jikun Kang, Tianpei Yang, Xi Chen, Jun Jin, Matthew E. Taylor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning Large Language Models (LLMs) adapts a trained model to specific
downstream tasks, significantly improving task-specific performance. Supervised
Fine-Tuning (SFT) is a common approach, where an LLM is trained to produce
desired answers. However, LLMs trained with SFT sometimes make simple mistakes
and result in hallucinations on reasoning tasks such as question-answering.
Without external feedback, it is difficult for SFT to learn a good mapping
between the question and the desired answer, especially with a small dataset.
This paper introduces an alternative to SFT called Natural Language Feedback
for Finetuning LLMs (LaFFi). LaFFi has LLMs directly predict the feedback they
will receive from an annotator. We find that requiring such reflection can
significantly improve the accuracy in in-domain question-answering tasks,
providing a promising direction for the application of natural language
feedback in the realm of SFT LLMs. Additional ablation studies show that the
portion of human-annotated data in the annotated datasets affects the
fine-tuning performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted in Human-Centric Representation Learning workshop at
  AAAI 2024 (https://hcrl-workshop.github.io/2024/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Analysis of Embedding Layers and Similarity Scores using Siamese
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yash Bingi, Yiqiao Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Lanugage Models (LLMs) are gaining increasing popularity in a variety
of use cases, from language understanding and writing to assistance in
application development. One of the most important aspects for optimal
funcionality of LLMs is embedding layers. Word embeddings are distributed
representations of words in a continuous vector space. In the context of LLMs,
words or tokens from the input text are transformed into high-dimensional
vectors using unique algorithms specific to the model. Our research examines
the embedding algorithms from leading companies in the industry, such as
OpenAI, Google's PaLM, and BERT. Using medical data, we have analyzed
similarity scores of each embedding layer, observing differences in performance
among each algorithm. To enhance each model and provide an additional encoding
layer, we also implemented Siamese Neural Networks. After observing changes in
performance with the addition of the model, we measured the carbon footage per
epoch of training. The carbon footprint associated with large language models
(LLMs) is a significant concern, and should be taken into consideration when
selecting algorithms for a variety of use cases. Overall, our research compared
the accuracy different, leading embedding algorithms and their carbon footage,
allowing for a holistic review of each embedding algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Effectiveness of Instruction Tuning in Biomedical Language
  Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omid Rohanian, Mohammadmahdi Nouriborji, David A. Clifton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs), particularly those similar to ChatGPT, have
significantly influenced the field of Natural Language Processing (NLP). While
these models excel in general language tasks, their performance in
domain-specific downstream tasks such as biomedical and clinical Named Entity
Recognition (NER), Relation Extraction (RE), and Medical Natural Language
Inference (NLI) is still evolving. In this context, our study investigates the
potential of instruction tuning for biomedical language processing, applying
this technique to two general LLMs of substantial scale. We present a
comprehensive, instruction-based model trained on a dataset that consists of
approximately $200,000$ instruction-focused samples. This dataset represents a
carefully curated compilation of existing data, meticulously adapted and
reformatted to align with the specific requirements of our instruction-based
tasks. This initiative represents an important step in utilising such models to
achieve results on par with specialised encoder-only models like BioBERT and
BioClinicalBERT for various classical biomedical NLP tasks. Our work includes
an analysis of the dataset's composition and its impact on model performance,
providing insights into the intricacies of instruction tuning. By sharing our
codes, models, and the distinctively assembled instruction-based dataset, we
seek to encourage ongoing research and development in this area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Networks Against (and For) Self-Training: Classification with
  Small Labeled and Large Unlabeled Sets <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00575v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00575v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Payam Karisani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a semi-supervised text classifier based on self-training using one
positive and one negative property of neural networks. One of the weaknesses of
self-training is the semantic drift problem, where noisy pseudo-labels
accumulate over iterations and consequently the error rate soars. In order to
tackle this challenge, we reshape the role of pseudo-labels and create a
hierarchical order of information. In addition, a crucial step in self-training
is to use the classifier confidence prediction to select the best candidate
pseudo-labels. This step cannot be efficiently done by neural networks, because
it is known that their output is poorly calibrated. To overcome this challenge,
we propose a hybrid metric to replace the plain confidence measurement. Our
metric takes into account the prediction uncertainty via a subsampling
technique. We evaluate our model in a set of five standard benchmarks, and show
that it significantly outperforms a set of ten diverse baseline models.
Furthermore, we show that the improvement achieved by our model is additive to
language model pretraining, which is a widely used technique for using
unlabeled documents. Our code is available at
https://github.com/p-karisani/RST.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL Findings 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multi-Task, Multi-Modal Approach for Predicting Categorical and
  Dimensional Emotions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex-Răzvan Ispas, Théo Deschamps-Berger, Laurence Devillers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech emotion recognition (SER) has received a great deal of attention in
recent years in the context of spontaneous conversations. While there have been
notable results on datasets like the well known corpus of naturalistic dyadic
conversations, IEMOCAP, for both the case of categorical and dimensional
emotions, there are few papers which try to predict both paradigms at the same
time. Therefore, in this work, we aim to highlight the performance contribution
of multi-task learning by proposing a multi-task, multi-modal system that
predicts categorical and dimensional emotions. The results emphasise the
importance of cross-regularisation between the two types of emotions. Our
approach consists of a multi-task, multi-modal architecture that uses parallel
feature refinement through self-attention for the feature of each modality. In
order to fuse the features, our model introduces a set of learnable bridge
tokens that merge the acoustic and linguistic features with the help of
cross-attention. Our experiments for categorical emotions on 10-fold validation
yield results comparable to the current state-of-the-art. In our configuration,
our multi-task approach provides better results compared to learning each
paradigm separately. On top of that, our best performing model achieves a high
result for valence compared to the previous multi-task experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Companion Publication of the 25th International Conference on
  Multimodal Interaction (pp. 311-317)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HSC-<span class="highlight-title">GPT</span>: A Large Language Model for Human Settlements Construction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Ran, Yao Xueqi, Jiang Xuhui, Han Zhengqi, Guo Jingze, Zhang Xianyue, Lin Chunyu, Liu Chumin, Zhao Jing, Lian Zeke, Zhang Jingjing, Li Keke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of human settlement construction encompasses a range of spatial
designs and management tasks, including urban planning and landscape
architecture design. These tasks involve a plethora of instructions and
descriptions presented in natural language, which are essential for
understanding design requirements and producing effective design solutions.
Recent research has sought to integrate natural language processing (NLP) and
generative artificial intelligence (AI) into human settlement construction
tasks. Due to the efficient processing and analysis capabilities of AI with
data, significant successes have been achieved in design within this domain.
However, this task still faces several fundamental challenges. The semantic
information involved includes complex spatial details, diverse data source
formats, high sensitivity to regional culture, and demanding requirements for
innovation and rigor in work scenarios. These factors lead to limitations when
applying general generative AI in this field, further exacerbated by a lack of
high-quality data for model training. To address these challenges, this paper
first proposes HSC-GPT, a large-scale language model framework specifically
designed for tasks in human settlement construction, considering the unique
characteristics of this domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Chinchilla-Optimal: Accounting for Inference in Language Model
  Scaling Laws <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikhil Sardana, Jonathan Frankle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model (LLM) scaling laws are empirical formulas that estimate
changes in model quality as a result of increasing parameter count and training
data. However, these formulas, including the popular DeepMind Chinchilla
scaling laws, neglect to include the cost of inference. We modify the
Chinchilla scaling laws to calculate the optimal LLM parameter count and
pre-training data size to train and deploy a model of a given quality and
inference demand. We conduct our analysis both in terms of a compute budget and
real-world costs and find that LLM researchers expecting reasonably large
inference demand (~1B requests) should train models smaller and longer than
Chinchilla-optimal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures, To appear in the 3rd NeurIPS Workshop on
  Efficient Natural Language and Speech Processing (ENLSP), 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BatchEval: Towards Human-like Text Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00437v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00437v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiwen Yuan, Shaoxiong Feng, Yiwei Li, Xinglin Wang, Boyuan Pan, Heda Wang, Kan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Significant progress has been made in automatic text evaluation with the
introduction of large language models (LLMs) as evaluators. However, current
sample-wise evaluation paradigm suffers from the following issues: (1)
Sensitive to prompt design; (2) Poor resistance to noise; (3) Inferior ensemble
performance with static reference. Inspired by the fact that humans treat both
criterion definition and inter sample comparison as references for evaluation,
we propose BatchEval, a paradigm that conducts batch-wise evaluation
iteratively to alleviate the above problems. We explore variants under this
paradigm and confirm the optimal settings are two stage procedure with
heterogeneous batch composition strategy and decimal scoring format.
Comprehensive experiments across 3 LLMs on 4 text evaluation tasks demonstrate
that BatchEval outperforms state-of-the-art methods by 10.5% on Pearson
correlations with only 64% API cost on average. Further analyses have been
conducted to verify the robustness, generalization, and working mechanism of
BatchEval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GeoGalactica: A Scientific Large Language Model in Geoscience 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00434v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00434v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhouhan Lin, Cheng Deng, Le Zhou, Tianhang Zhang, Yi Xu, Yutong Xu, Zhongmou He, Yuanyuan Shi, Beiya Dai, Yunchong Song, Boyi Zeng, Qiyuan Chen, Tao Shi, Tianyu Huang, Yiwei Xu, Shu Wang, Luoyi Fu, Weinan Zhang, Junxian He, Chao Ma, Yunqiang Zhu, Xinbing Wang, Chenghu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved huge success for their general
knowledge and ability to solve a wide spectrum of tasks in natural language
processing (NLP). Due to their impressive abilities, LLMs have shed light on
potential inter-discipline applications to foster scientific discoveries of a
specific domain by using artificial intelligence (AI for science, AI4S). In the
meantime, utilizing NLP techniques in geoscience research and practice is wide
and convoluted, contributing from knowledge extraction and document
classification to question answering and knowledge discovery. In this work, we
take the initial step to leverage LLM for science, through a rather
straightforward approach. We try to specialize an LLM into geoscience, by
further pre-training the model with a vast amount of texts in geoscience, as
well as supervised fine-tuning (SFT) the resulting model with our custom
collected instruction tuning dataset. These efforts result in a model
GeoGalactica consisting of 30 billion parameters. To our best knowledge, it is
the largest language model for the geoscience domain. More specifically,
GeoGalactica is from further pre-training of Galactica. We train GeoGalactica
over a geoscience-related text corpus containing 65 billion tokens curated from
extensive data sources in the big science project Deep-time Digital Earth
(DDE), preserving as the largest geoscience-specific text corpus. Then we
fine-tune the model with 1 million pairs of instruction-tuning data consisting
of questions that demand professional geoscience knowledge to answer. In this
technical report, we will illustrate in detail all aspects of GeoGalactica,
including data collection, data cleaning, base model selection, pre-training,
SFT, and evaluation. We open-source our data curation tools and the checkpoints
of GeoGalactica during the first 3/4 of pre-training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ keqing: knowledge-based question answering is a nature chain-of-thought
  mentor of LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00426v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00426v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaojie Wang, Yishi Xu, Zhong Peng, Chenxi Zhang, Bo Chen, Xinrun Wang, Lei Feng, Bo An
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have exhibited remarkable performance on various
natural language processing (NLP) tasks, especially for question answering.
However, in the face of problems beyond the scope of knowledge, these LLMs tend
to talk nonsense with a straight face, where the potential solution could be
incorporating an Information Retrieval (IR) module and generating response
based on these retrieved knowledge. In this paper, we present a novel framework
to assist LLMs, such as ChatGPT, to retrieve question-related structured
information on the knowledge graph, and demonstrate that Knowledge-based
question answering (Keqing) could be a nature Chain-of-Thought (CoT) mentor to
guide the LLM to sequentially find the answer entities of a complex question
through interpretable logical chains. Specifically, the workflow of Keqing will
execute decomposing a complex question according to predefined templates,
retrieving candidate entities on knowledge graph, reasoning answers of
sub-questions, and finally generating response with reasoning paths, which
greatly improves the reliability of LLM's response. The experimental results on
KBQA datasets show that Keqing can achieve competitive performance and
illustrate the logic of answering each question.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SDIF-DA: A Shallow-to-Deep Interaction Framework with Data Augmentation
  for Multi-modal Intent Detection <span class="chip">ICASSP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00424v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00424v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijue Huang, Libo Qin, Bingbing Wang, Geng Tu, Ruifeng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal intent detection aims to utilize various modalities to understand
the user's intentions, which is essential for the deployment of dialogue
systems in real-world scenarios. The two core challenges for multi-modal intent
detection are (1) how to effectively align and fuse different features of
modalities and (2) the limited labeled multi-modal intent training data. In
this work, we introduce a shallow-to-deep interaction framework with data
augmentation (SDIF-DA) to address the above challenges. Firstly, SDIF-DA
leverages a shallow-to-deep interaction module to progressively and effectively
align and fuse features across text, video, and audio modalities. Secondly, we
propose a ChatGPT-based data augmentation approach to automatically augment
sufficient training data. Experimental results demonstrate that SDIF-DA can
effectively align and fuse multi-modal features by achieving state-of-the-art
performance. In addition, extensive analyses show that the introduced data
augmentation approach can successfully distill knowledge from the large
language model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAGTruth: A Hallucination Corpus for Developing Trustworthy
  Retrieval-Augmented Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanhao Wu, Juno Zhu, Siliang Xu, Kashun Shum, Cheng Niu, Randy Zhong, Juntong Song, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) has become a main technique for
alleviating hallucinations in large language models (LLMs). Despite the
integration of RAG, LLMs may still present unsupported or contradictory claims
to the retrieved contents. In order to develop effective hallucination
prevention strategies under RAG, it is important to create benchmark datasets
that can measure the extent of hallucination. This paper presents RAGTruth, a
corpus tailored for analyzing word-level hallucinations in various domains and
tasks within the standard RAG frameworks for LLM applications. RAGTruth
comprises nearly 18,000 naturally generated responses from diverse LLMs using
RAG. These responses have undergone meticulous manual annotations at both the
individual cases and word levels, incorporating evaluations of hallucination
intensity. We not only benchmark hallucination frequencies across different
LLMs, but also critically assess the effectiveness of several existing
hallucination detection methodologies. Furthermore, we show that using a
high-quality dataset such as RAGTruth, it is possible to finetune a relatively
small LLM and achieve a competitive level of performance in hallucination
detection when compared to the existing prompt-based approaches using
state-of-the-art large language models such as GPT-4.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FusionMind -- Improving question and answering with external context
  fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shreyas Verma, Manoj Parmar, Palash Choudhary, Sanchita Porwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Answering questions using pre-trained language models (LMs) and knowledge
graphs (KGs) presents challenges in identifying relevant knowledge and
performing joint reasoning.We compared LMs (fine-tuned for the task) with the
previously published QAGNN method for the Question-answering (QA) objective and
further measured the impact of additional factual context on the QAGNN
performance. The QAGNN method employs LMs to encode QA context and estimate KG
node importance, and effectively update the question choice entity
representations using Graph Neural Networks (GNNs). We further experimented
with enhancing the QA context encoding by incorporating relevant knowledge
facts for the question stem. The models are trained on the OpenbookQA dataset,
which contains ~6000 4-way multiple choice questions and is widely used as a
benchmark for QA tasks. Through our experimentation, we found that
incorporating knowledge facts context led to a significant improvement in
performance. In contrast, the addition of knowledge graphs to language models
resulted in only a modest increase. This suggests that the integration of
contextual knowledge facts may be more impactful for enhancing question
answering performance compared to solely adding knowledge graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting Evoked Emotions in Conversations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enas Altarawneh, Ameeta Agrawal, Michael Jenkin, Manos Papagelis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding and predicting the emotional trajectory in multi-party
multi-turn conversations is of great significance. Such information can be
used, for example, to generate empathetic response in human-machine interaction
or to inform models of pre-emptive toxicity detection. In this work, we
introduce the novel problem of Predicting Emotions in Conversations (PEC) for
the next turn (n+1), given combinations of textual and/or emotion input up to
turn n. We systematically approach the problem by modeling three dimensions
inherently connected to evoked emotions in dialogues, including (i) sequence
modeling, (ii) self-dependency modeling, and (iii) recency modeling. These
modeling dimensions are then incorporated into two deep neural network
architectures, a sequence model and a graph convolutional network model. The
former is designed to capture the sequence of utterances in a dialogue, while
the latter captures the sequence of utterances and the network formation of
multi-party dialogues. We perform a comprehensive empirical evaluation of the
various proposed models for addressing the PEC problem. The results indicate
(i) the importance of the self-dependency and recency model dimensions for the
prediction task, (ii) the quality of simpler sequence models in short
dialogues, (iii) the importance of the graph neural models in improving the
predictions in long dialogues.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Text Embeddings with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a novel and simple method for obtaining
high-quality text embeddings using only synthetic data and less than 1k
training steps. Unlike existing methods that often depend on multi-stage
intermediate pre-training with billions of weakly-supervised text pairs,
followed by fine-tuning with a few labeled datasets, our method does not
require building complex training pipelines or relying on manually collected
datasets that are often constrained by task diversity and language coverage. We
leverage proprietary LLMs to generate diverse synthetic data for hundreds of
thousands of text embedding tasks across nearly 100 languages. We then
fine-tune open-source decoder-only LLMs on the synthetic data using standard
contrastive loss. Experiments demonstrate that our method achieves strong
performance on highly competitive text embedding benchmarks without using any
labeled data. Furthermore, when fine-tuned with a mixture of synthetic and
labeled data, our model sets new state-of-the-art results on the BEIR and MTEB
benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Argumentation in Waltz's "Emerging Structure of International Politics'' 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00366v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00366v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Magdalena Wolska, Bernd Fröhlich, Katrin Girgensohn, Sassan Gholiagha, Dora Kiesel, Jürgen Neyer, Patrick Riehmann, Mitja Sienknecht, Benno Stein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an annotation scheme for argumentative and domain-specific aspects
of scholarly articles on the theory of International Relations. At
argumentation level we identify Claims and Support/Attack relations. At domain
level we model discourse content in terms of Theory and Data-related
statements. We annotate Waltz's 1993 text on structural realism and show that
our scheme can be reliably applied by domain experts enables insights on two
research questions on justifications of claims.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Context-aware Decoding Reduces Hallucination in Query-focused
  Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14335v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14335v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichao Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Query-focused summarization (QFS) aims to provide a summary of a single
document/multi documents that can satisfy the information needs of a given
query. It is useful for various real-world applications, such as abstractive
snippet generation or more recent retrieval augmented generation (RAG). A
prototypical QFS pipeline consists of a retriever (sparse or dense retrieval)
and a generator (usually a large language model). However, applying large
language models (LLM) potentially leads to hallucinations, especially when the
evidence contradicts the prior belief of LLMs. There has been growing interest
in developing new decoding methods to improve generation quality and reduce
hallucination. In this work, we conduct a large-scale reproducibility study on
one recently proposed decoding method -- Context-aware Decoding (CAD). In
addition to replicating CAD's experiments on news summarization datasets, we
include experiments on QFS datasets, and conduct more rigorous analysis on
computational complexity and hyperparameter sensitivity. Experiments with eight
different language models show that performance-wise, CAD improves QFS quality
by (1) reducing factuality errors/hallucinations while (2) mostly retaining the
match of lexical patterns, measured by ROUGE scores, while also at a cost of
increased inference-time FLOPs and reduced decoding speed. The code
implementation based on Huggingface Library is made available
https://github.com/zhichaoxu-shufe/context-aware-decoding-qfs
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physical Computing: A Category Theoretic Perspective on Physical
  Computation and System Compositionality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.00392v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.00392v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nima Dehghani, Gianluca Caterina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a category theory-based framework to redefine physical
computing in light of advancements in quantum computing and non-standard
computing systems. By integrating classical definitions within this broader
perspective, the paper rigorously recontextualizes what constitutes physical
computing devices and processes. It demonstrates how the compositional nature
and relational structures of physical computing systems can be coherently
formalized using category theory. This approach not only encapsulates recent
formalisms in physical computing but also offers a structured method to explore
the dynamic interactions within these systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Large Language Models Infer Causation from Correlation? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05836v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05836v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijing Jin, Jiarui Liu, Zhiheng Lyu, Spencer Poff, Mrinmaya Sachan, Rada Mihalcea, Mona Diab, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal inference is one of the hallmarks of human intelligence. While the
field of CausalNLP has attracted much interest in the recent years, existing
causal inference datasets in NLP primarily rely on discovering causality from
empirical knowledge (e.g., commonsense knowledge). In this work, we propose the
first benchmark dataset to test the pure causal inference skills of large
language models (LLMs). Specifically, we formulate a novel task Corr2Cause,
which takes a set of correlational statements and determines the causal
relationship between the variables. We curate a large-scale dataset of more
than 200K samples, on which we evaluate seventeen existing LLMs. Through our
experiments, we identify a key shortcoming of LLMs in terms of their causal
inference skills, and show that these models achieve almost close to random
performance on the task. This shortcoming is somewhat mitigated when we try to
re-purpose LLMs for this skill via finetuning, but we find that these models
still fail to generalize -- they can only perform causal inference in
in-distribution settings when variable names and textual expressions used in
the queries are similar to those in the training set, but fail in
out-of-distribution settings generated by perturbing these queries. Corr2Cause
is a challenging task for LLMs, and would be helpful in guiding future research
on improving LLMs' pure reasoning skills and generalizability. Our data is at
https://huggingface.co/datasets/causalnlp/corr2cause. Our code is at
https://github.com/causalNLP/corr2cause.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v2.0: added 5 fine-tuned model performance; de-duplicated data; and
  provided more fine-grained error analysis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Experimenting AI Technologies for Disinformation Combat: the IDMO
  Project 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11097v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11097v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Canale, Alberto Messina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Italian Digital Media Observatory (IDMO) project, part of a European
initiative, focuses on countering disinformation and fake news. This report
outlines contributions from Rai-CRITS to the project, including: (i) the
creation of novel datasets for testing technologies (ii) development of an
automatic model for categorizing Pagella Politica verdicts to facilitate
broader analysis (iii) creation of an automatic model for recognizing textual
entailment with exceptional accuracy on the FEVER dataset (iv) assessment using
GPT-4 to detecting content treatment style (v) a game to raise awareness about
fake news at national events.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Forbidden Facts: An Investigation of Competing Objectives in Llama-2 <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.08793v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.08793v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tony T. Wang, Miles Wang, Kaivalya Hariharan, Nir Shavit
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs often face competing pressures (for example helpfulness vs.
harmlessness). To understand how models resolve such conflicts, we study
Llama-2-chat models on the forbidden fact task. Specifically, we instruct
Llama-2 to truthfully complete a factual recall statement while forbidding it
from saying the correct answer. This often makes the model give incorrect
answers. We decompose Llama-2 into 1000+ components, and rank each one with
respect to how useful it is for forbidding the correct answer. We find that in
aggregate, around 35 components are enough to reliably implement the full
suppression behavior. However, these components are fairly heterogeneous and
many operate using faulty heuristics. We discover that one of these heuristics
can be exploited via a manually designed adversarial attack which we call The
California Attack. Our results highlight some roadblocks standing in the way of
being able to successfully interpret advanced ML systems. Project website
available at https://forbiddenfacts.github.io .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the ATTRIB and SoLaR workshops at NeurIPS 2023; (v3:
  clarified experimental details)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Empower Nested Boolean Logic via <span class="highlight-title">Self-Supervised</span> Curriculum Learning <span class="chip">EMNLP2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05450v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05450v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongqiu Wu, Linfeng Liu, Hai Zhao, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Beyond the great cognitive powers showcased by language models, it is crucial
to scrutinize whether their reasoning capabilities stem from strong
generalization or merely exposure to relevant data. As opposed to constructing
increasingly complex logic, this paper probes into the boolean logic, the root
capability of a logical reasoner. We find that any pre-trained language models
even including large language models only behave like a random selector in the
face of multi-nested boolean logic, a task that humans can handle with ease. To
empower language models with this fundamental capability, this paper proposes a
new self-supervised learning method \textit{Curriculum Logical Reasoning}
(\textsc{Clr}), where we augment the training data with nested boolean logic
chain step-by-step, and program the training from simpler logical patterns
gradually to harder ones. This new training paradigm allows language models to
effectively generalize to much harder and longer-hop logic, which can hardly be
learned through naive training. Furthermore, we show that boolean logic is a
great foundation for improving the subsequent general logical tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">24</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WoodScape Motion Segmentation for Autonomous Driving -- CVPR 2023 OmniCV
  Workshop Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00910v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00910v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saravanabalagi Ramachandran, Nathaniel Cibik, Ganesh Sistu, John McDonald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion segmentation is a complex yet indispensable task in autonomous
driving. The challenges introduced by the ego-motion of the cameras, radial
distortion in fisheye lenses, and the need for temporal consistency make the
task more complicated, rendering traditional and standard Convolutional Neural
Network (CNN) approaches less effective. The consequent laborious data
labeling, representation of diverse and uncommon scenarios, and extensive data
capture requirements underscore the imperative of synthetic data for improving
machine learning model performance. To this end, we employ the PD-WoodScape
synthetic dataset developed by Parallel Domain, alongside the WoodScape fisheye
dataset. Thus, we present the WoodScape fisheye motion segmentation challenge
for autonomous driving, held as part of the CVPR 2023 Workshop on
Omnidirectional Computer Vision (OmniCV). As one of the first competitions
focused on fisheye motion segmentation, we aim to explore and evaluate the
potential and impact of utilizing synthetic data in this domain. In this paper,
we provide a detailed analysis on the competition which attracted the
participation of 112 global teams and a total of 234 submissions. This study
delineates the complexities inherent in the task of motion segmentation,
emphasizes the significance of fisheye datasets, articulate the necessity for
synthetic datasets and the resultant domain gap they engender, outlining the
foundational blueprint for devising successful solutions. Subsequently, we
delve into the details of the baseline experiments and winning methods
evaluating their qualitative and quantitative results, providing with useful
insights.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bringing Back the Context: Camera Trap Species Identification as Link
  Prediction on Multimodal Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vardaan Pahuja, Weidi Luo, Yu Gu, Cheng-Hao Tu, Hong-You Chen, Tanya Berger-Wolf, Charles Stewart, Song Gao, Wei-Lun Chao, Yu Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Camera traps are valuable tools in animal ecology for biodiversity monitoring
and conservation. However, challenges like poor generalization to deployment at
new unseen locations limit their practical application. Images are naturally
associated with heterogeneous forms of context possibly in different
modalities. In this work, we leverage the structured context associated with
the camera trap images to improve out-of-distribution generalization for the
task of species identification in camera traps. For example, a photo of a wild
animal may be associated with information about where and when it was taken, as
well as structured biology knowledge about the animal species. While typically
overlooked by existing work, bringing back such context offers several
potential benefits for better image understanding, such as addressing data
scarcity and enhancing generalization. However, effectively integrating such
heterogeneous context into the visual domain is a challenging problem. To
address this, we propose a novel framework that reformulates species
classification as link prediction in a multimodal knowledge graph (KG). This
framework seamlessly integrates various forms of multimodal context for visual
recognition. We apply this framework for out-of-distribution species
classification on the iWildCam2020-WILDS and Snapshot Mountain Zebra datasets
and achieve competitive performance with state-of-the-art approaches.
Furthermore, our framework successfully incorporates biological taxonomy for
improved generalization and enhances sample efficiency for recognizing
under-represented species.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SteinDreamer: Variance Reduction for Text-to-3D Score Distillation via
  Stein Identity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peihao Wang, Zhiwen Fan, Dejia Xu, Dilin Wang, Sreyas Mohan, Forrest Iandola, Rakesh Ranjan, Yilei Li, Qiang Liu, Zhangyang Wang, Vikas Chandra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Score distillation has emerged as one of the most prevalent approaches for
text-to-3D asset synthesis. Essentially, score distillation updates 3D
parameters by lifting and back-propagating scores averaged over different
views. In this paper, we reveal that the gradient estimation in score
distillation is inherent to high variance. Through the lens of variance
reduction, the effectiveness of SDS and VSD can be interpreted as applications
of various control variates to the Monte Carlo estimator of the distilled
score. Motivated by this rethinking and based on Stein's identity, we propose a
more general solution to reduce variance for score distillation, termed Stein
Score Distillation (SSD). SSD incorporates control variates constructed by
Stein identity, allowing for arbitrary baseline functions. This enables us to
include flexible guidance priors and network architectures to explicitly
optimize for variance reduction. In our experiments, the overall pipeline,
dubbed SteinDreamer, is implemented by instantiating the control variate with a
monocular depth estimator. The results suggest that SSD can effectively reduce
the distillation variance and consistently improve visual quality for both
object- and scene-level generation. Moreover, we demonstrate that SteinDreamer
achieves faster convergence than existing methods due to more stable gradient
updates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://vita-group.github.io/SteinDreamer/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Taming Mode Collapse in Score Distillation for Text-to-3D Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00909v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00909v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peihao Wang, Dejia Xu, Zhiwen Fan, Dilin Wang, Sreyas Mohan, Forrest Iandola, Rakesh Ranjan, Yilei Li, Qiang Liu, Zhangyang Wang, Vikas Chandra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the remarkable performance of score distillation in text-to-3D
generation, such techniques notoriously suffer from view inconsistency issues,
also known as "Janus" artifact, where the generated objects fake each view with
multiple front faces. Although empirically effective methods have approached
this problem via score debiasing or prompt engineering, a more rigorous
perspective to explain and tackle this problem remains elusive. In this paper,
we reveal that the existing score distillation-based text-to-3D generation
frameworks degenerate to maximal likelihood seeking on each view independently
and thus suffer from the mode collapse problem, manifesting as the Janus
artifact in practice. To tame mode collapse, we improve score distillation by
re-establishing in entropy term in the corresponding variational objective,
which is applied to the distribution of rendered images. Maximizing the entropy
encourages diversity among different views in generated 3D assets, thereby
mitigating the Janus problem. Based on this new objective, we derive a new
update rule for 3D score distillation, dubbed Entropic Score Distillation
(ESD). We theoretically reveal that ESD can be simplified and implemented by
just adopting the classifier-free guidance trick upon variational score
distillation. Although embarrassingly straightforward, our extensive
experiments successfully demonstrate that ESD can be an effective treatment for
Janus artifacts in score distillation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://vita-group.github.io/3D-Mode-Collapse/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Generalist FaceX via Learning Unified Facial Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Han, Jiangning Zhang, Junwei Zhu, Xiangtai Li, Yanhao Ge, Wei Li, Chengjie Wang, Yong Liu, Xiaoming Liu, Ying Tai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents FaceX framework, a novel facial generalist model capable
of handling diverse facial tasks simultaneously. To achieve this goal, we
initially formulate a unified facial representation for a broad spectrum of
facial editing tasks, which macroscopically decomposes a face into fundamental
identity, intra-personal variation, and environmental factors. Based on this,
we introduce Facial Omni-Representation Decomposing (FORD) for seamless
manipulation of various facial components, microscopically decomposing the core
aspects of most facial editing tasks. Furthermore, by leveraging the prior of a
pretrained StableDiffusion (SD) to enhance generation quality and accelerate
training, we design Facial Omni-Representation Steering (FORS) to first
assemble unified facial representations and then effectively steer the SD-aware
generation process by the efficient Facial Representation Controller (FRC).
%Without any additional features, Our versatile FaceX achieves competitive
performance compared to elaborate task-specific models on popular facial
editing tasks. Full codes and models will be available at
https://github.com/diffusion-facex/FaceX.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://diffusion-facex.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compressing Deep Image Super-resolution Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00523v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00523v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Jiang, Jakub Nawala, Fan Zhang, David Bull
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning techniques have been applied in the context of image
super-resolution (SR), achieving remarkable advances in terms of reconstruction
performance. Existing techniques typically employ highly complex model
structures which result in large model sizes and slow inference speeds. This
often leads to high energy consumption and restricts their adoption for
practical applications. To address this issue, this work employs a three-stage
workflow for compressing deep SR models which significantly reduces their
memory requirement. Restoration performance has been maintained through
teacher-student knowledge distillation using a newly designed distillation
loss. We have applied this approach to two popular image super-resolution
networks, SwinIR and EDSR, to demonstrate its effectiveness. The resulting
compact models, SwinIRmini and EDSRmini, attain an 89% and 96% reduction in
both model size and floating-point operations (FLOPs) respectively, compared to
their original versions. They also retain competitive super-resolution
performance compared to their original models and other commonly used SR
approaches. The source code and pre-trained models for these two lightweight SR
approaches are released at https://pikapi22.github.io/CDISM/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Video-GroundingDINO: Towards Open-Vocabulary Spatio-Temporal Video
  Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00901v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00901v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Syed Talal Wasim, Muzammal Naseer, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video grounding aims to localize a spatio-temporal section in a video
corresponding to an input text query. This paper addresses a critical
limitation in current video grounding methodologies by introducing an
Open-Vocabulary Spatio-Temporal Video Grounding task. Unlike prevalent
closed-set approaches that struggle with open-vocabulary scenarios due to
limited training data and predefined vocabularies, our model leverages
pre-trained representations from foundational spatial grounding models. This
empowers it to effectively bridge the semantic gap between natural language and
diverse visual content, achieving strong performance in closed-set and
open-vocabulary settings. Our contributions include a novel spatio-temporal
video grounding model, surpassing state-of-the-art results in closed-set
evaluations on multiple datasets and demonstrating superior performance in
open-vocabulary scenarios. Notably, the proposed model outperforms
state-of-the-art methods in closed-set settings on VidSTG (Declarative and
Interrogative) and HC-STVG (V1 and V2) datasets. Furthermore, in
open-vocabulary evaluations on HC-STVG V1 and YouCook-Interactions, our model
surpasses the recent best-performing models by $4.26$ m_vIoU and $1.83\%$
accuracy, demonstrating its efficacy in handling diverse linguistic and visual
concepts for improved video understanding. Our codes will be released at
https://github.com/TalalWasim/Video-GroundingDINO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAR-RARP50: Segmentation of surgical instrumentation and Action
  Recognition on Robot-Assisted Radical Prostatectomy Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitrios Psychogyios, Emanuele Colleoni, Beatrice Van Amsterdam, Chih-Yang Li, Shu-Yu Huang, Yuchong Li, Fucang Jia, Baosheng Zou, Guotai Wang, Yang Liu, Maxence Boels, Jiayu Huo, Rachel Sparks, Prokar Dasgupta, Alejandro Granados, Sebastien Ourselin, Mengya Xu, An Wang, Yanan Wu, Long Bai, Hongliang Ren, Atsushi Yamada, Yuriko Harai, Yuto Ishikawa, Kazuyuki Hayashi, Jente Simoens, Pieter DeBacker, Francesco Cisternino, Gabriele Furnari, Alex Mottrie, Federica Ferraguti, Satoshi Kondo, Satoshi Kasai, Kousuke Hirasawa, Soohee Kim, Seung Hyun Lee, Kyu Eun Lee, Hyoun-Joong Kong, Kui Fu, Chao Li, Shan An, Stefanie Krell, Sebastian Bodenstedt, Nicolas Ayobi, Alejandra Perez, Santiago Rodriguez, Juanita Puentes, Pablo Arbelaez, Omid Mohareri, Danail Stoyanov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surgical tool segmentation and action recognition are fundamental building
blocks in many computer-assisted intervention applications, ranging from
surgical skills assessment to decision support systems. Nowadays,
learning-based action recognition and segmentation approaches outperform
classical methods, relying, however, on large, annotated datasets. Furthermore,
action recognition and tool segmentation algorithms are often trained and make
predictions in isolation from each other, without exploiting potential
cross-task relationships. With the EndoVis 2022 SAR-RARP50 challenge, we
release the first multimodal, publicly available, in-vivo, dataset for surgical
action recognition and semantic instrumentation segmentation, containing 50
suturing video segments of Robotic Assisted Radical Prostatectomy (RARP). The
aim of the challenge is twofold. First, to enable researchers to leverage the
scale of the provided dataset and develop robust and highly accurate
single-task action recognition and tool segmentation approaches in the surgical
domain. Second, to further explore the potential of multitask-based learning
approaches and determine their comparative advantage against their single-task
counterparts. A total of 12 teams participated in the challenge, contributing 7
action recognition methods, 9 instrument segmentation techniques, and 4
multitask approaches that integrated both action recognition and instrument
segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Masked Modeling for <span class="highlight-title">Self-supervised</span> Representation Learning on Vision
  and Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00897v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00897v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Li, Luyuan Zhang, Zedong Wang, Di Wu, Lirong Wu, Zicheng Liu, Jun Xia, Cheng Tan, Yang Liu, Baigui Sun, Stan Z. Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the deep learning revolution marches on, self-supervised learning has
garnered increasing attention in recent years thanks to its remarkable
representation learning ability and the low dependence on labeled data. Among
these varied self-supervised techniques, masked modeling has emerged as a
distinctive approach that involves predicting parts of the original data that
are proportionally masked during training. This paradigm enables deep models to
learn robust representations and has demonstrated exceptional performance in
the context of computer vision, natural language processing, and other
modalities. In this survey, we present a comprehensive review of the masked
modeling framework and its methodology. We elaborate on the details of
techniques within masked modeling, including diverse masking strategies,
recovering targets, network architectures, and more. Then, we systematically
investigate its wide-ranging applications across domains. Furthermore, we also
explore the commonalities and differences between masked modeling methods in
different fields. Toward the end of this paper, we conclude by discussing the
limitations of current techniques and point out several potential avenues for
advancing masked modeling research. A paper list project with this survey is
available at \url{https://github.com/Lupin1998/Awesome-MIM}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint v1, 19 pages of main texts. GitHub project at
  https://github.com/Lupin1998/Awesome-MIM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyzing Local Representations of <span class="highlight-title">Self-supervised</span> Vision <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00463v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00463v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ani Vanyan, Alvard Barseghyan, Hakob Tamazyan, Vahan Huroyan, Hrant Khachatrian, Martin Danelljan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a comparative analysis of various self-supervised
Vision Transformers (ViTs), focusing on their local representative power.
Inspired by large language models, we examine the abilities of ViTs to perform
various computer vision tasks with little to no fine-tuning. We design an
evaluation framework to analyze the quality of local, i.e. patch-level,
representations in the context of few-shot semantic segmentation, instance
identification, object retrieval, and tracking. We discover that contrastive
learning based methods like DINO produce more universal patch representations
that can be immediately applied for downstream tasks with no parameter tuning,
compared to masked image modeling. The embeddings learned using the latter
approach, e.g. in masked autoencoders, have high variance features that harm
distance-based algorithms, such as k-NN, and do not contain useful information
for most downstream tasks. Furthermore, we demonstrate that removing these
high-variance features enhances k-NN by providing an analysis of the benchmarks
for this work and for Scale-MAE, a recent extension of masked autoencoders.
Finally, we find an object instance retrieval setting where DINOv2, a model
pretrained on two orders of magnitude more data, performs worse than its less
compute-intensive counterpart DINO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RainSD: Rain Style Diversification Module for Image Synthesis
  Enhancement using Feature-Level Style Distribution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyeonjae Jeon, Junghyun Seo, Taesoo Kim, Sungho Son, Jungki Lee, Gyeungho Choi, Yongseob Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving technology nowadays targets to level 4 or beyond, but the
researchers are faced with some limitations for developing reliable driving
algorithms in diverse challenges. To promote the autonomous vehicles to spread
widely, it is important to address safety issues on this technology. Among
various safety concerns, the sensor blockage problem by severe weather
conditions can be one of the most frequent threats for multi-task learning
based perception algorithms during autonomous driving. To handle this problem,
the importance of the generation of proper datasets is becoming more
significant. In this paper, a synthetic road dataset with sensor blockage
generated from real road dataset BDD100K is suggested in the format of BDD100K
annotation. Rain streaks for each frame were made by an experimentally
established equation and translated utilizing the image-to-image translation
network based on style transfer. Using this dataset, the degradation of the
diverse multi-task networks for autonomous driving, such as lane detection,
driving area segmentation, and traffic object detection, has been thoroughly
evaluated and analyzed. The tendency of the performance degradation of deep
neural network-based perception systems for autonomous vehicle has been
analyzed in depth. Finally, we discuss the limitation and the future directions
of the deep neural network-based perception algorithms and autonomous driving
dataset generation based on image-to-image translation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Double-well Net for Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00456v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00456v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Liu, Jun Liu, Raymond Chan, Xue-Cheng Tai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, our goal is to integrate classical mathematical models with
deep neural networks by introducing two novel deep neural network models for
image segmentation known as Double-well Nets. Drawing inspiration from the
Potts model, our models leverage neural networks to represent a region force
functional. We extend the well-know MBO (Merriman-Bence-Osher) scheme to solve
the Potts model. The widely recognized Potts model is approximated using a
double-well potential and then solved by an operator-splitting method, which
turns out to be an extension of the well-known MBO scheme. Subsequently, we
replace the region force functional in the Potts model with a UNet-type
network, which is data-driven, and also introduce control variables to enhance
effectiveness. The resulting algorithm is a neural network activated by a
function that minimizes the double-well potential. What sets our proposed
Double-well Nets apart from many existing deep learning methods for image
segmentation is their strong mathematical foundation. They are derived from the
network approximation theory and employ the MBO scheme to approximately solve
the Potts model. By incorporating mathematical principles, Double-well Nets
bridge the MBO scheme and neural networks, and offer an alternative perspective
for designing networks with mathematical backgrounds. Through comprehensive
experiments, we demonstrate the performance of Double-well Nets, showcasing
their superior accuracy and robustness compared to state-of-the-art neural
networks. Overall, our work represents a valuable contribution to the field of
image segmentation by combining the strengths of classical variational models
and deep neural networks. The Double-well Nets introduce an innovative approach
that leverages mathematical foundations to enhance segmentation performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TrailBlazer: Trajectory Control for Diffusion-Based Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00896v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00896v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wan-Duo Kurt Ma, J. P. Lewis, W. Bastiaan Kleijn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Within recent approaches to text-to-video (T2V) generation, achieving
controllability in the synthesized video is often a challenge. Typically, this
issue is addressed by providing low-level per-frame guidance in the form of
edge maps, depth maps, or an existing video to be altered. However, the process
of obtaining such guidance can be labor-intensive. This paper focuses on
enhancing controllability in video synthesis by employing straightforward
bounding boxes to guide the subject in various ways, all without the need for
neural network training, finetuning, optimization at inference time, or the use
of pre-existing videos. Our algorithm, TrailBlazer, is constructed upon a
pre-trained (T2V) model, and easy to implement. The subject is directed by a
bounding box through the proposed spatial and temporal attention map editing.
Moreover, we introduce the concept of keyframing, allowing the subject
trajectory and overall appearance to be guided by both a moving bounding box
and corresponding prompts, without the need to provide a detailed mask. The
method is efficient, with negligible additional computation relative to the
underlying pre-trained model. Despite the simplicity of the bounding box
guidance, the resulting motion is surprisingly natural, with emergent effects
including perspective and movement toward the virtual camera as the box size
increases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 16 figures, Project Page:
  https://hohonu-vicml.github.io/Trailblazer.Page/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive <span class="highlight-title">Overview</span> of Fish-Eye Camera Distortion Correction
  Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Xu, De-Wei Han, Kang Li, Jun-Jie Li, Zhao-Yuan Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The fisheye camera, with its unique wide field of view and other
characteristics, has found extensive applications in various fields. However,
the fisheye camera suffers from significant distortion compared to pinhole
cameras, resulting in distorted images of captured objects. Fish-eye camera
distortion is a common issue in digital image processing, requiring effective
correction techniques to enhance image quality. This review provides a
comprehensive overview of various methods used for fish-eye camera distortion
correction. The article explores the polynomial distortion model, which
utilizes polynomial functions to model and correct radial distortions.
Additionally, alternative approaches such as panorama mapping, grid mapping,
direct methods, and deep learning-based methods are discussed. The review
highlights the advantages, limitations, and recent advancements of each method,
enabling readers to make informed decisions based on their specific needs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TSGAN: An Optical-to-SAR Dual Conditional GAN for Optical based SAR
  Temporal Shifting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00440v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00440v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moien Rangzan, Sara Attarchi, Richard Gloaguen, Seyed Kazem Alavipanah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In contrast to the well-investigated field of SAR-to-Optical translation,
this study explores the lesser-investigated domain of Optical-to-SAR
translation, a challenging field due to the ill-posed nature of this
translation. The complexity arises as a single optical data can have multiple
SAR representations based on the SAR viewing geometry. We propose a novel
approach, termed SAR Temporal Shifting, which inputs an optical data from the
desired timestamp along with a SAR data from a different temporal point but
with a consistent viewing geometry as the expected SAR data, both complemented
with a change map of optical data during the intervening period. This model
modifies the SAR data based on the changes observed in optical data to generate
the SAR data for the desired timestamp. Our model, a dual conditional
Generative Adversarial Network (GAN), named Temporal Shifting GAN (TSGAN),
incorporates a siamese encoder in both the Generator and the Discriminator. To
prevent the model from overfitting on the input SAR data, we employed a change
weighted loss function. Our approach surpasses traditional translation methods
by eliminating the GAN's fiction phenomenon, particularly in unchanged regions,
resulting in higher SSIM and PSNR in these areas. Additionally, modifications
to the Pix2Pix architecture and the inclusion of attention mechanisms have
enhanced the model's performance on all regions of the data. This research
paves the way for leveraging legacy optical datasets, the most abundant and
longstanding source of Earth datary data, extending their use to SAR domains
and temporal analyses. To foster further research, we provide the code,
datasets used in our study, and a framework for generating paired SAR-Optical
datasets for new regions of interest. These resources are available on
github.com/moienr/TemporalGAN
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stable Unlearnable Example: Enhancing the Robustness of Unlearnable
  Examples via Stable Error-Minimizing Noise <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13091v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13091v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixin Liu, Kaidi Xu, Xun Chen, Lichao Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The open source of large amounts of image data promotes the development of
deep learning techniques. Along with this comes the privacy risk of these
open-source image datasets being exploited by unauthorized third parties to
train deep learning models for commercial or illegal purposes. To avoid the
abuse of public data, a poisoning-based technique, the unlearnable example, is
proposed to significantly degrade the generalization performance of models by
adding a kind of imperceptible noise to the data. To further enhance its
robustness against adversarial training, existing works leverage iterative
adversarial training on both the defensive noise and the surrogate model.
However, it still remains unknown whether the robustness of unlearnable
examples primarily comes from the effect of enhancement in the surrogate model
or the defensive noise. Observing that simply removing the adversarial noise on
the training process of the defensive noise can improve the performance of
robust unlearnable examples, we identify that solely the surrogate model's
robustness contributes to the performance. Furthermore, we found a negative
correlation exists between the robustness of defensive noise and the protection
performance, indicating defensive noise's instability issue. Motivated by this,
to further boost the robust unlearnable example, we introduce stable
error-minimizing noise (SEM), which trains the defensive noise against random
perturbation instead of the time-consuming adversarial perturbation to improve
the stability of defensive noise. Through extensive experiments, we demonstrate
that SEM achieves a new state-of-the-art performance on CIFAR-10, CIFAR-100,
and ImageNet Subset in terms of both effectiveness and efficiency. The code is
available at https://github.com/liuyixin-louis/Stable-Unlearnable-Example.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PGS: Pose-Guided Supervision for Mitigating Clothes-Changing in Person
  Re-Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05634v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05634v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quoc-Huy Trinh, Nhat-Tan Bui, Dinh-Hieu Hoang, Phuoc-Thao Vo Thi, Hai-Dang Nguyen, Debesh Jha, Ulas Bagci, Ngan Le, Minh-Triet Tran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Person Re-Identification (Re-ID) task seeks to enhance the tracking of
multiple individuals by surveillance cameras. It provides additional support
for multimodal tasks, including text-based person retrieval and human matching.
Among the significant challenges faced in Re-ID, one of the most prominent is
dealing with clothes-changing, where the same person may appear in different
outfits. While previous methods have made notable progress in maintaining
clothing data consistency and handling clothing change data, they still tend to
rely excessively on clothing information, which can limit performance due to
the dynamic nature of human appearances. To mitigate this challenge, we propose
the Pose-Guided Supervision (PGS), an effective framework for learning pose
guidance within the Re-ID task. Our PGS consists of three modules: a human
encoder, a pose encoder, and a Pose-to-Human Projection module (PHP). The pose
encoder module utilizes a frozen pre-trained model while we fine-tune a
pre-trained human-centric model for the human encoder module. Our PHP transfers
pose knowledge from the pose encoder module to the human encoder module through
multiple projectors. Our framework, following extensive experimentation on five
benchmark datasets, consistently surpasses the performance of current
state-of-the-art methods. Our code is available at
https://github.com/huyquoctrinh/PGS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instruct-ReID: A Multi-purpose Person Re-identification Task with
  Instructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.07520v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.07520v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weizhen He, Yiheng Deng, Shixiang Tang, Qihao Chen, Qingsong Xie, Yizhou Wang, Lei Bai, Feng Zhu, Rui Zhao, Wanli Ouyang, Donglian Qi, Yunfeng Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human intelligence can retrieve any person according to both visual and
language descriptions. However, the current computer vision community studies
specific person re-identification (ReID) tasks in different scenarios
separately, which limits the applications in the real world. This paper strives
to resolve this problem by proposing a new instruct-ReID task that requires the
model to retrieve images according to the given image or language instructions.
Our instruct-ReID is a more general ReID setting, where existing 6 ReID tasks
can be viewed as special cases by designing different instructions. We propose
a large-scale OmniReID benchmark and an adaptive triplet loss as a baseline
method to facilitate research in this new setting. Experimental results show
that the proposed multi-purpose ReID model, trained on our OmniReID benchmark
without fine-tuning, can improve +0.5%, +0.6%, +7.7% mAP on Market1501, MSMT17,
CUHK03 for traditional ReID, +6.4%, +7.1%, +11.2% mAP on PRCC, VC-Clothes, LTCC
for clothes-changing ReID, +11.7% mAP on COCAS+ real2 for clothes template
based clothes-changing ReID when using only RGB images, +24.9% mAP on COCAS+
real2 for our newly defined language-instructed ReID, +4.3% on LLCM for
visible-infrared ReID, +2.6% on CUHK-PEDES for text-to-image ReID. The
datasets, the model, and code will be available at
https://github.com/hwz-zju/Instruct-ReID.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Precise localization of corneal reflections in eye images using deep
  learning trained on synthetic data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.05673v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.05673v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sean Anthony Byrne, Marcus Nyström, Virmarie Maquiling, Enkelejda Kasneci, Diederick C. Niehorster
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a deep learning method for accurately localizing the center of a
single corneal reflection (CR) in an eye image. Unlike previous approaches, we
use a convolutional neural network (CNN) that was trained solely using
simulated data. Using only simulated data has the benefit of completely
sidestepping the time-consuming process of manual annotation that is required
for supervised training on real eye images. To systematically evaluate the
accuracy of our method, we first tested it on images with simulated CRs placed
on different backgrounds and embedded in varying levels of noise. Second, we
tested the method on high-quality videos captured from real eyes. Our method
outperformed state-of-the-art algorithmic methods on real eye images with a 35%
reduction in terms of spatial precision, and performed on par with
state-of-the-art on simulated images in terms of spatial accuracy.We conclude
that our method provides a precise method for CR center localization and
provides a solution to the data availability problem which is one of the
important common roadblocks in the development of deep learning models for gaze
estimation. Due to the superior CR center localization and ease of application,
our method has the potential to improve the accuracy and precision of CR-based
eye trackers
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Behavioural Research Methods</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generation Of Colors using Bidirectional Long Short Term Memory Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.06542v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.06542v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        A. Sinha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human vision can distinguish between a vast spectrum of colours, estimated to
be between 2 to 7 million discernible shades. However, this impressive range
does not inherently imply that all these colours have been precisely named and
described within our lexicon. We often associate colours with familiar objects
and concepts in our daily lives. This research endeavors to bridge the gap
between our visual perception of countless shades and our ability to articulate
and name them accurately. A novel model has been developed to achieve this
goal, leveraging Bidirectional Long Short-Term Memory (BiLSTM) networks with
Active learning. This model operates on a proprietary dataset meticulously
curated for this study. The primary objective of this research is to create a
versatile tool for categorizing and naming previously unnamed colours or
identifying intermediate shades that elude traditional colour terminology. The
findings underscore the potential of this innovative approach in
revolutionizing our understanding of colour perception and language. Through
rigorous experimentation and analysis, this study illuminates a promising
avenue for Natural Language Processing (NLP) applications in diverse
industries. By facilitating the exploration of the vast colour spectrum the
potential applications of NLP are extended beyond conventional boundaries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Turning Waste into Wealth: Leveraging Low-Quality Samples for Enhancing
  Continuous Conditional Generative Adversarial Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.10273v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.10273v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Ding, Yongwei Wang, Zuheng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continuous Conditional Generative Adversarial Networks (CcGANs) enable
generative modeling conditional on continuous scalar variables (termed
regression labels). However, they can produce subpar fake images due to limited
training data. Although Negative Data Augmentation (NDA) effectively enhances
unconditional and class-conditional GANs by introducing anomalies into real
training images, guiding the GANs away from low-quality outputs, its impact on
CcGANs is limited, as it fails to replicate negative samples that may occur
during the CcGAN sampling. We present a novel NDA approach called Dual-NDA
specifically tailored for CcGANs to address this problem. Dual-NDA employs two
types of negative samples: visually unrealistic images generated from a
pre-trained CcGAN and label-inconsistent images created by manipulating real
images' labels. Leveraging these negative samples, we introduce a novel
discriminator objective alongside a modified CcGAN training algorithm.
Empirical analysis on UTKFace and Steering Angle reveals that Dual-NDA
consistently enhances the visual fidelity and label consistency of fake images
generated by CcGANs, exhibiting a substantial performance gain over the vanilla
NDA. Moreover, by applying Dual-NDA, CcGANs demonstrate a remarkable
advancement beyond the capabilities of state-of-the-art conditional GANs and
diffusion models, establishing a new pinnacle of performance. Our codes can be
found at https://github.com/UBCDingXin/Dual-NDA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nuclear Morphometry using a Deep Learning-based Algorithm has Prognostic
  Relevance for Canine Cutaneous Mast Cell Tumors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15031v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15031v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Haghofer, Eda Parlak, Alexander Bartel, Taryn A. Donovan, Charles-Antoine Assenmacher, Pompei Bolfa, Michael J. Dark, Andrea Fuchs-Baumgartinger, Andrea Klang, Kathrin Jäger, Robert Klopfleisch, Sophie Merz, Barbara Richter, F. Yvonne Schulman, Jonathan Ganz, Josef Scharinger, Marc Aubreville, Stephan M. Winkler, Matti Kiupel, Christof A. Bertram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Variation in nuclear size and shape is an important criterion of malignancy
for many tumor types; however, categorical estimates by pathologists have poor
reproducibility. Measurements of nuclear characteristics (morphometry) can
improve reproducibility, but manual methods are time consuming. In this study,
we evaluated fully automated morphometry using a deep learning-based algorithm
in 96 canine cutaneous mast cell tumors with information on patient survival.
Algorithmic morphometry was compared with karyomegaly estimates by 11
pathologists, manual nuclear morphometry of 12 cells by 9 pathologists, and the
mitotic count as a benchmark. The prognostic value of automated morphometry was
high with an area under the ROC curve regarding the tumor-specific survival of
0.943 (95% CI: 0.889 - 0.996) for the standard deviation (SD) of nuclear area,
which was higher than manual morphometry of all pathologists combined (0.868,
95% CI: 0.737 - 0.991) and the mitotic count (0.885, 95% CI: 0.765 - 1.00). At
the proposed thresholds, the hazard ratio for algorithmic morphometry (SD of
nuclear area $\geq 9.0 \mu m^2$) was 18.3 (95% CI: 5.0 - 67.1), for manual
morphometry (SD of nuclear area $\geq 10.9 \mu m^2$) 9.0 (95% CI: 6.0 - 13.4),
for karyomegaly estimates 7.6 (95% CI: 5.7 - 10.1), and for the mitotic count
30.5 (95% CI: 7.8 - 118.0). Inter-rater reproducibility for karyomegaly
estimates was fair ($\kappa$ = 0.226) with highly variable
sensitivity/specificity values for the individual pathologists. Reproducibility
for manual morphometry (SD of nuclear area) was good (ICC = 0.654). This study
supports the use of algorithmic morphometry as a prognostic test to overcome
the limitations of estimates and manual measurements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking the Paradigm of Content Constraints in GAN-based Unpaired
  Image-to-Image Translation <span class="chip">AAAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.10867v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.10867v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiuding Cai, Yaoyao Zhu, Dong Miao, Linjie Fu, Yu Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In an unpaired setting, lacking sufficient content constraints for
image-to-image translation (I2I) tasks, GAN-based approaches are usually prone
to model collapse. Current solutions can be divided into two categories,
reconstruction-based and Siamese network-based. The former requires that the
transformed or transforming image can be perfectly converted back to the
original image, which is sometimes too strict and limits the generative
performance. The latter involves feeding the original and generated images into
a feature extractor and then matching their outputs. This is not efficient
enough, and a universal feature extractor is not easily available. In this
paper, we propose EnCo, a simple but efficient way to maintain the content by
constraining the representational similarity in the latent space of patch-level
features from the same stage of the \textbf{En}coder and de\textbf{Co}der of
the generator. For the similarity function, we use a simple MSE loss instead of
contrastive loss, which is currently widely used in I2I tasks. Benefits from
the design, EnCo training is extremely efficient, while the features from the
encoder produce a more positive effect on the decoding, leading to more
satisfying generations. In addition, we rethink the role played by
discriminators in sampling patches and propose a discriminative
attention-guided (DAG) patch sampling strategy to replace random sampling. DAG
is parameter-free and only requires negligible computational overhead, while
significantly improving the performance of the model. Extensive experiments on
multiple datasets demonstrate the effectiveness and advantages of EnCo, and we
achieve multiple state-of-the-art compared to previous methods. Our code is
available at https://github.com/XiudingCai/EnCo-pytorch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Wildfire Smoke Detection with Cross Contrast Patch Embedding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10116v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10116v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chong Wang, Cheng Xu, Adeel Akram, Zhilin Shan, Qixing Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Transformer-based deep networks have increasingly shown significant
advantages over CNNs. Some existing work has applied it in the field of
wildfire recognition or detection. However, we observed that the vanilla
Transformer is not friendly for extracting smoke features. Because low-level
information such as color, transparency and texture is very important for smoke
recognition, and transformer pays more attention to the semantic relevance
between middle- or high-level features, and is not sensitive to the subtle
changes of low-level features along the space. To solve this problem, we
propose the Cross Contrast Patch Embedding(CCPE) module based on the Swin
Transformer, which uses the multi-scales spatial frequency contrast information
in both vertical and horizontal directions to improve the discrimination of the
network on the underlying details. The fuzzy boundary of smoke makes the
positive and negative label assignment for instances in a dilemma, which is
another challenge for wildfires detection. To solve this problem, a Separable
Negative Sampling Mechanism(SNSM) is proposed. By using two different negative
instance sampling strategies on positive images and negative images
respectively, the problem of supervision signal confusion caused by label
diversity in the process of network training is alleviated. This paper also
releases the RealFire Test, the largest real wildfire test set so far, to
evaluate the proposed method and promote future research. It contains 50,535
images from 3,649 video clips. The proposed method has been extensively tested
and evaluated on RealFire Test dataset, and has a significant performance
improvement compared with the baseline detection models.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ V2X communication coverage analysis for connected vehicles in
  intelligent transportation networks: A case study for the city of Xanthi,
  Greece 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evangelos Bazinas, Andreas Gregoriades, Marios Raspopoulos, Michael Georgiades
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intelligent transportation systems (ITS) have been developed to improve
traffic flow, efficiency, and safety in transportation. Technological
advancements in communication such as the Vehicle-to-Everything (V2X),
Vehicle-to-Vehicle (V2V) and Vehicle-to Infrastructure (V2I) enable the
real-time exchange of information between vehicles and other entities on the
road network, and thus play a significant role in their safety and efficiency.
This paper presents a simulation study that models V2V and V2I communication to
identify the most suitable range of data transmission between vehicles and
infrastructure. The provincial city of Xanthi, Greece is used as a cases study,
and the goal is to evaluate whether the proposed placement of Road Side Unit
(RSU) provided adequate communication coverage on the city's road network. An
analysis through different scenarios identified improvements in traffic
management, driving behavior and environmental conditions under different RSU
coverage. The results highlight that the communication range of 400 meters is
the most adequate option for optimum traffic management in the city of Xanthi.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Wireless World Research Forum, Meeting 49, March 28th-30th 2023,
  Pozna\'n, Poland, Towards sustainable and automated communications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Text Embeddings with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a novel and simple method for obtaining
high-quality text embeddings using only synthetic data and less than 1k
training steps. Unlike existing methods that often depend on multi-stage
intermediate pre-training with billions of weakly-supervised text pairs,
followed by fine-tuning with a few labeled datasets, our method does not
require building complex training pipelines or relying on manually collected
datasets that are often constrained by task diversity and language coverage. We
leverage proprietary LLMs to generate diverse synthetic data for hundreds of
thousands of text embedding tasks across nearly 100 languages. We then
fine-tune open-source decoder-only LLMs on the synthetic data using standard
contrastive loss. Experiments demonstrate that our method achieves strong
performance on highly competitive text embedding benchmarks without using any
labeled data. Furthermore, when fine-tuned with a mixture of synthetic and
labeled data, our model sets new state-of-the-art results on the BEIR and MTEB
benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Context-aware Decoding Reduces Hallucination in Query-focused
  Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14335v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14335v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichao Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Query-focused summarization (QFS) aims to provide a summary of a single
document/multi documents that can satisfy the information needs of a given
query. It is useful for various real-world applications, such as abstractive
snippet generation or more recent retrieval augmented generation (RAG). A
prototypical QFS pipeline consists of a retriever (sparse or dense retrieval)
and a generator (usually a large language model). However, applying large
language models (LLM) potentially leads to hallucinations, especially when the
evidence contradicts the prior belief of LLMs. There has been growing interest
in developing new decoding methods to improve generation quality and reduce
hallucination. In this work, we conduct a large-scale reproducibility study on
one recently proposed decoding method -- Context-aware Decoding (CAD). In
addition to replicating CAD's experiments on news summarization datasets, we
include experiments on QFS datasets, and conduct more rigorous analysis on
computational complexity and hyperparameter sensitivity. Experiments with eight
different language models show that performance-wise, CAD improves QFS quality
by (1) reducing factuality errors/hallucinations while (2) mostly retaining the
match of lexical patterns, measured by ROUGE scores, while also at a cost of
increased inference-time FLOPs and reduced decoding speed. The code
implementation based on Huggingface Library is made available
https://github.com/zhichaoxu-shufe/context-aware-decoding-qfs
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Bi-Step Grounding Paradigm for Large Language Models in Recommendation
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.08434v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.08434v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keqin Bao, Jizhi Zhang, Wenjie Wang, Yang Zhang, Zhengyi Yang, Yancheng Luo, Chong Chen, Fuli Feng, Qi Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the focus on Large Language Models (LLMs) in the field of recommendation
intensifies, the optimization of LLMs for recommendation purposes (referred to
as LLM4Rec) assumes a crucial role in augmenting their effectiveness in
providing recommendations. However, existing approaches for LLM4Rec often
assess performance using restricted sets of candidates, which may not
accurately reflect the models' overall ranking capabilities. In this paper, our
objective is to investigate the comprehensive ranking capacity of LLMs and
propose a two-step grounding framework known as BIGRec (Bi-step Grounding
Paradigm for Recommendation). It initially grounds LLMs to the recommendation
space by fine-tuning them to generate meaningful tokens for items and
subsequently identifies appropriate actual items that correspond to the
generated tokens. By conducting extensive experiments on two datasets, we
substantiate the superior performance, capacity for handling few-shot
scenarios, and versatility across multiple domains exhibited by BIGRec.
Furthermore, we observe that the marginal benefits derived from increasing the
quantity of training samples are modest for BIGRec, implying that LLMs possess
the limited capability to assimilate statistical information, such as
popularity and collaborative filtering, due to their robust semantic priors.
These findings also underline the efficacy of integrating diverse statistical
information into the LLM4Rec framework, thereby pointing towards a potential
avenue for future research. Our code and data are available at
https://github.com/SAI990323/Grounding4Rec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLaRA: Aligning Large Language Models with Sequential Recommenders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02445v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02445v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Liao, Sihang Li, Zhengyi Yang, Jiancan Wu, Yancheng Yuan, Xiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation aims to predict the subsequent items matching user
preference based on her/his historical interactions. With the development of
Large Language Models (LLMs), there is growing interest in exploring the
potential of LLMs for sequential recommendation by framing it as a language
modeling task. Prior works represent items in the textual prompts using either
ID indexing or text indexing and feed the prompts into LLMs, but falling short
of either encapsulating comprehensive world knowledge or exhibiting sufficient
sequential understanding. To harness the complementary strengths of traditional
recommenders (which encode user behavioral knowledge) and LLMs (which possess
world knowledge about items), we propose LLaRA -- a Large Language and
Recommendation Assistant framework. Specifically, LLaRA represents items in
LLM's input prompts using a novel hybrid approach that integrates ID-based item
embeddings from traditional recommenders with textual item features. Viewing
the ``sequential behavior of the user'' as a new modality in recommendation, we
employ an adapter to bridge the modality gap between ID embeddings of the
traditional recommenders and the input space of LLMs. Furthermore, instead of
directly exposing the hybrid prompt to LLMs, we apply a curriculum learning
approach to gradually ramp up training complexity. We first warm up the LLM
with text-only prompting, which aligns more naturally with the LLM's language
modeling capabilities. Thereafter, we progressively transition to hybrid
prompting, training the adapter to incorporate behavioral knowledge from the
traditional sequential recommender into the LLM. Extensive experiments
demonstrate the efficacy of LLaRA framework. Our code and data are available at
https://github.com/ljy0ustc/LLaRA .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Compact Representation for Bayesian Neural Networks By Removing
  Permutation Symmetry <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Z. Xiao, Weiyang Liu, Robert Bamler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian neural networks (BNNs) are a principled approach to modeling
predictive uncertainties in deep learning, which are important in
safety-critical applications. Since exact Bayesian inference over the weights
in a BNN is intractable, various approximate inference methods exist, among
which sampling methods such as Hamiltonian Monte Carlo (HMC) are often
considered the gold standard. While HMC provides high-quality samples, it lacks
interpretable summary statistics because its sample mean and variance is
meaningless in neural networks due to permutation symmetry. In this paper, we
first show that the role of permutations can be meaningfully quantified by a
number of transpositions metric. We then show that the recently proposed
rebasin method allows us to summarize HMC samples into a compact representation
that provides a meaningful explicit uncertainty estimate for each weight in a
neural network, thus unifying sampling methods with variational inference. We
show that this compact representation allows us to compare trained BNNs
directly in weight space across sampling methods and variational inference, and
to efficiently prune neural networks trained without explicit Bayesian
frameworks by exploiting uncertainty estimates from HMC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NeurIPS 2023 Workshop on Unifying Representations in
  Neural Models; 4 pages + appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WoodScape Motion Segmentation for Autonomous Driving -- CVPR 2023 OmniCV
  Workshop Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00910v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00910v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saravanabalagi Ramachandran, Nathaniel Cibik, Ganesh Sistu, John McDonald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion segmentation is a complex yet indispensable task in autonomous
driving. The challenges introduced by the ego-motion of the cameras, radial
distortion in fisheye lenses, and the need for temporal consistency make the
task more complicated, rendering traditional and standard Convolutional Neural
Network (CNN) approaches less effective. The consequent laborious data
labeling, representation of diverse and uncommon scenarios, and extensive data
capture requirements underscore the imperative of synthetic data for improving
machine learning model performance. To this end, we employ the PD-WoodScape
synthetic dataset developed by Parallel Domain, alongside the WoodScape fisheye
dataset. Thus, we present the WoodScape fisheye motion segmentation challenge
for autonomous driving, held as part of the CVPR 2023 Workshop on
Omnidirectional Computer Vision (OmniCV). As one of the first competitions
focused on fisheye motion segmentation, we aim to explore and evaluate the
potential and impact of utilizing synthetic data in this domain. In this paper,
we provide a detailed analysis on the competition which attracted the
participation of 112 global teams and a total of 234 submissions. This study
delineates the complexities inherent in the task of motion segmentation,
emphasizes the significance of fisheye datasets, articulate the necessity for
synthetic datasets and the resultant domain gap they engender, outlining the
foundational blueprint for devising successful solutions. Subsequently, we
delve into the details of the baseline experiments and winning methods
evaluating their qualitative and quantitative results, providing with useful
insights.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SVFAP: <span class="highlight-title">Self-supervised</span> Video Facial Affect Perceiver 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00416v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00416v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Licai Sun, Zheng Lian, Kexin Wang, Yu He, Mingyu Xu, Haiyang Sun, Bin Liu, Jianhua Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video-based facial affect analysis has recently attracted increasing
attention owing to its critical role in human-computer interaction. Previous
studies mainly focus on developing various deep learning architectures and
training them in a fully supervised manner. Although significant progress has
been achieved by these supervised methods, the longstanding lack of large-scale
high-quality labeled data severely hinders their further improvements.
Motivated by the recent success of self-supervised learning in computer vision,
this paper introduces a self-supervised approach, termed Self-supervised Video
Facial Affect Perceiver (SVFAP), to address the dilemma faced by supervised
methods. Specifically, SVFAP leverages masked facial video autoencoding to
perform self-supervised pre-training on massive unlabeled facial videos.
Considering that large spatiotemporal redundancy exists in facial videos, we
propose a novel temporal pyramid and spatial bottleneck Transformer as the
encoder of SVFAP, which not only enjoys low computational cost but also
achieves excellent performance. To verify the effectiveness of our method, we
conduct experiments on nine datasets spanning three downstream tasks, including
dynamic facial expression recognition, dimensional emotion recognition, and
personality recognition. Comprehensive results demonstrate that SVFAP can learn
powerful affect-related representations via large-scale self-supervised
pre-training and it significantly outperforms previous state-of-the-art methods
on all datasets. Codes will be available at https://github.com/sunlicai/SVFAP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Trans. on Affective Computing (February 8, 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Balanced Multi-modal Federated Learning via Cross-Modal Infiltration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00894v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00894v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunfeng Fan, Wenchao Xu, Haozhao Wang, Jiaqi Zhu, Song Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) underpins advancements in privacy-preserving
distributed computing by collaboratively training neural networks without
exposing clients' raw data. Current FL paradigms primarily focus on uni-modal
data, while exploiting the knowledge from distributed multimodal data remains
largely unexplored. Existing multimodal FL (MFL) solutions are mainly designed
for statistical or modality heterogeneity from the input side, however, have
yet to solve the fundamental issue,"modality imbalance", in distributed
conditions, which can lead to inadequate information exploitation and
heterogeneous knowledge aggregation on different modalities.In this paper, we
propose a novel Cross-Modal Infiltration Federated Learning (FedCMI) framework
that effectively alleviates modality imbalance and knowledge heterogeneity via
knowledge transfer from the global dominant modality. To avoid the loss of
information in the weak modality due to merely imitating the behavior of
dominant modality, we design the two-projector module to integrate the
knowledge from dominant modality while still promoting the local feature
exploitation of weak modality. In addition, we introduce a class-wise
temperature adaptation scheme to achieve fair performance across different
classes. Extensive experiments over popular datasets are conducted and give us
a gratifying confirmation of the proposed framework for fully exploring the
information of each modality in MFL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Client-wise Modality Selection for Balanced Multi-modal Federated
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00403v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00403v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunfeng Fan, Wenchao Xu, Haozhao Wang, Penghui Ruan, Song Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Selecting proper clients to participate in the iterative federated learning
(FL) rounds is critical to effectively harness a broad range of distributed
datasets. Existing client selection methods simply consider the variability
among FL clients with uni-modal data, however, have yet to consider clients
with multi-modalities. We reveal that traditional client selection scheme in
MFL may suffer from a severe modality-level bias, which impedes the
collaborative exploitation of multi-modal data, leading to insufficient local
data exploration and global aggregation. To tackle this challenge, we propose a
Client-wise Modality Selection scheme for MFL (CMSFed) that can comprehensively
utilize information from each modality via avoiding such client selection bias
caused by modality imbalance. Specifically, in each MFL round, the local data
from different modalities are selectively employed to participate in local
training and aggregation to mitigate potential modality imbalance of the global
model. To approximate the fully aggregated model update in a balanced way, we
introduce a novel local training loss function to enhance the weak modality and
align the divergent feature spaces caused by inconsistent modality adoption
strategies for different clients simultaneously. Then, a modality-level
gradient decoupling method is designed to derive respective submodular
functions to maintain the gradient diversity during the selection progress and
balance MFL according to local modality imbalance in each iteration. Our
extensive experiments showcase the superiority of CMSFed over baselines and its
effectiveness in multi-modal data exploitation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages,6 figures,2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Model-Driven Synthetic Training Image Generation: An Approach
  to Cognition in Rail Defect Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00393v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00393v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahatara Ferdousi, Chunsheng Yang, M. Anwar Hossain, Fedwa Laamarti, M. Shamim Hossain, Abdulmotaleb El Saddik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in cognitive computing, with the integration of deep
learning techniques, have facilitated the development of intelligent cognitive
systems (ICS). This is particularly beneficial in the context of rail defect
detection, where the ICS would emulate human-like analysis of image data for
defect patterns. Despite the success of Convolutional Neural Networks (CNN) in
visual defect classification, the scarcity of large datasets for rail defect
detection remains a challenge due to infrequent accident events that would
result in defective parts and images. Contemporary researchers have addressed
this data scarcity challenge by exploring rule-based and generative data
augmentation models. Among these, Variational Autoencoder (VAE) models can
generate realistic data without extensive baseline datasets for noise modeling.
This study proposes a VAE-based synthetic image generation technique for rail
defects, incorporating weight decay regularization and image reconstruction
loss to prevent overfitting. The proposed method is applied to create a
synthetic dataset for the Canadian Pacific Railway (CPR) with just 50 real
samples across five classes. Remarkably, 500 synthetic samples are generated
with a minimal reconstruction loss of 0.021. A Visual Transformer (ViT) model
underwent fine-tuning using this synthetic CPR dataset, achieving high accuracy
rates (98%-99%) in classifying the five defect classes. This research offers a
promising solution to the data scarcity challenge in rail defect detection,
showcasing the potential for robust ICS development in this domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 13 figures, Springer Journal</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-01-08T05:26:39.469513924Z">
            2024-01-08 05:26:39 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
